---
title: data2day
draft: true
author: Shirin Glander
date: '2017-09-28'
categories: ["conference"]
tags: ["data science", "conference", "data2day", "machine learning", "business intelligence"]
thumbnailImagePosition: left
thumbnailImage: https://www.data2day.de/common/images/konferenzen/data2day2017.svg
metaAlignment: center
coverMeta: out
slug: ode_to_r
---



<div class="figure">
<img src="https://www.data2day.de/common/images/konferenzen/data2day2017.svg" />

</div>
<p>Yesterday and today I attended the <a href="www.data2day.de">data2day</a> conference in Heidelberg, Germany. Topics covered technical aspects of implementing Big Data, Data Science, Machine Learning, Artificial Intelligence and Internet of Things in business.</p>
<p>While I spent most of the time at my company’s conference stand, I did hear three very interesting talks.</p>
<p><br></p>
<ol style="list-style-type: decimal">
<li><strong>Scalable Machine Learning with Apache Spark for Fraud Detection</strong></li>
</ol>
<p>In this first talk I heard, Dr. Patrick Baier and Dr. Stanimir Dragiev presented their work at <a href="www.zalando.de/">Zalando</a>. They built a scalable machine learning framework with Apache Spark, Scala and AWS to assess and predict fraud in online transactions. <a href="www.zalando.de/">Zalando</a> is a German online store that sells clothes, shoes and accessories. Normally, they allow registered customers to buy via invoice, i.e. they receive their ordered items before they pay them. This leaves them vulnerable to fraud where item are not paid for. The goal of their data science team is to use customer and basket data to obtain a probability score for how likely a transaction is going to be fraudulent. High-risk payment options, like invoice, can then be disabled in transactions with high fraud probability. To build and run such machine learning models, the Zalando data science team uses a combination of Spark, Scala, R, AWS, SQL, Python, Docker, etc. In their workflow, they use a combination of static and dynamic features, imputing missing values and building a decision model. In order to scale their modeling workflow to process more requests, use more data in training, update models more frequently and/or run more models, they described a workflow that uses Spark, Scala and Amazon Web Services (AWS). Spark’s machine learning library can be used for modeling and scaled horizontally by increasing the number of clusters on which to run the models. Scala provides multi-threading functionality and AWS is used for storing data in S3 and extending computation power depending on changing needs. Finally, they include a model inspector into their workflow to assure comaprability of training and test data and check that the model is behaving as expected. Problems that they are dealing with include highly unbalanced data (which is getting even worse the better their models work as they keep reducing the number of fraud cases), delayed labeling due to the long process of the transactions, seasonality in data.</p>
<p><br></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Sparse Data: Don’t Mind the Gap!</strong></li>
</ol>
<p>In this talk, my colleagues from codecentric Dr. Daniel Pape and Dr. Michael Plümacher showed an example of how to deal with sparse data.</p>
<p><br></p>
<ol start="3" style="list-style-type: decimal">
<li>Tensor Flow &amp; Kubernetes</li>
</ol>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="de" dir="ltr">
Guten Morgen auf der <a href="https://twitter.com/data2day"><span class="citation">@data2day</span></a> Kommt uns doch mal am Stand besuchen :-) <a href="https://t.co/YK46ACdNj9">pic.twitter.com/YK46ACdNj9</a>
</p>
— codecentric AG (<span class="citation">@codecentric</span>) <a href="https://twitter.com/codecentric/status/912928993279606784">September 27, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
