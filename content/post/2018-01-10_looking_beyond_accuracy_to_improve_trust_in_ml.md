---
title: "Looking beyond accuracy to improve trust in machine learning"
thumbnailImagePosition: left
thumbnailImage: https://blog.codecentric.de/files/2018/01/lime_output_figure.png
metaAlignment: center
coverMeta: out
date: 2018-01-10
categories:
- machine learning
tags:
- codecentric
- machine learning
- lime
showPagination: true
showSocial: true
showTags: true
slug: "looking_beyond_accuracy_to_improve_trust_in_ml"
---

I have written another blogpost about [Looking beyond accuracy to improve trust in machine learning](https://blog.codecentric.de/en/2018/01/look-beyond-accuracy-improve-trust-machine-learning/) at my company [codecentric](https://blog.codecentric.de/en/)'s blog:

> Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex machine learning models are essentially black boxes and too complicated to understand, we need to use approximations to get a better sense of how they work. One such approach is LIME, which stands for Local Interpretable Model-agnostic Explanations and is a tool that helps understand and explain the decisions made by complex machine learning models.

Continue reading at <https://blog.codecentric.de/en/2018/01/look-beyond-accuracy-improve-trust-machine-learning/>...

Links to the entire example code and more info are given at the end of the blog post.

The blog post is [also available in German](https://blog.codecentric.de/2018/01/vertrauen-und-vorurteile-maschinellem-lernen/).

![](https://blog.codecentric.de/files/2018/01/lime_output_figure.png)