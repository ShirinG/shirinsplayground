---
title: "rOpenSci unconference 2018 + introduction to TensorFlow Probability & the 'greta' package"
draft: true
author: Shirin Glander
date: '2018-05-23'
categories: ["R"]
tags: ["R", "tensorflow", "greta"]
thumbnailImagePosition: left
thumbnailImage: 
metaAlignment: center
coverMeta: out
slug: 
---

<center>
![](https://shiring.github.io/netlify_images/rOpenSci/IMG_3637.jpg)
<\center>

On May 21st and 2nd, I had the honor of having been chosen to attend the rOpenSci unconference 2018 in Seattle.
It was a great event and I got to meet many amazing people!

[rOpenSci](https://ropensci.org/) is a non-profit organisation that maintains a number of widely used R [packages](https://ropensci.org/packages/) and is very active in promoting a [community](https://ropensci.org/community/) spirit around the R-world. Their core values are to have open and reproducible research, shared data and easy-to-use tools and to make all this accessible to a large number of people.

Part of creating a welcoming community infrastructure is their yearly unconference. At the unconference, about 60 R users from around the world get together to work on small projects that are relevant to the R community at the time.

The whole organisation team - most and foremost Stefanie Butland - did a wonderful job hosting this event. Everybody made sure that the spirit of the unconference was invclusive and very welcoming to everybody, from long-established fixtures in the R-world to newbies and anyone in between.

We started the official part of the unconference on Monday morning with a few "ice breaker": Stefanie would pose a question or make a statement and we would position ourselves in the room according to our answer and discuss with the people close to us. Starting with "Are you a dog or a cat person?" and finishing with "I know my place in the R community", we all quickly had a lot to talk about! It was a great way to meet many of the people we would spend the next two days with.




The unconf focuses on two days of community driven development, which can include new features of existing packages, lesson development, new packages and many other things. I spent my first day with the education group, talking about the opportunities and challenges of teaching R in a formal education setting. It was great to share experiences across disciplines [history, ecology, bioinformatics] and see that many of the struggles we as educators face are common across these disciplines. Our groupâ€™s goal is to form a community of educators in R who can share their materials more cohesively. More details on how the group outlined that sharing can be found here.

Beyond being apart of the education and metadata groups, the conference was an amazing chance to meet many of my fellow R-Ladies, including some members of R-Ladies Remote. The ice breaker on the first day, where we grouped ourselves across a continum based on questions like â€˜I know what my role in the R community isâ€™ started some great conversations with folks, like myself, who are still not clear on what our role is. After unconf I feel more confident in my role and more confident in being involved and contributing to the parts of the community I belong to in the future.

ROpensci accepts applications to attend unconf each year in February and whether you are a R developer, or a post doc like me who teahes sometimes, and uses a lot of R packages, I encourage you to apply to attend, its a great way to learn, about R and the community as a whole.

Huge thanks to the ROpenSci team for their great effort in planning this amazing event and ensuring that everyone was welcome and able to participate to their fullest. You can see all the diverse and amazing things to come out of unconf18 here

I had an amazing time this week participating in the 2018 ROpenSci Unconference, the sixth annual ROpenSci hackathon bringing together people to advance the tools and community for scientific computing with R. It was so inspiring to be among such a talented and dedicated group of people â€” special kudos goes to the organizing committee for curating such a great crowd. (I heard there were over 200 hundred nominations from which the 65 or so attendees were selected.)

The idea behind the unconference is to spend two full days hacking on projects of interest to the community. Before the conference begins, the participants suggest projects as Github issues and begin discussions there. On the first day of the conference (after an icebreaker), the participants vote for projects they'd be interested in working on, and then form up into groups of 2-6 people or so to work on them. And then everyone gets to work! You can get a sense of the activity by looking at the #runconf18 hashtag on Twitter (and especially the photos).


[This year's rOpenSci unconference](http://unconf18.ropensci.org/) was held at the Microsoft Reactor in Seattle.

The rOpenSci unconference is our annual event loosely modeled on Foo Camp. This event is unlike many other unconferences in that it is mostly invite-only (past attendees often recommend new ones) with a few spots set aside for self-nominations from the community at large. The agenda is mostly decided during the conference itself. Nominations are now closed..

For a fifth year running, rOpenSci is excited to announce our unconference 2018, this time in Downtown Seattle in the verdant Pacific Northwest. We're organizing this event to bring together scientists, developers, and open data enthusiasts from academia, industry, government, and non-profits to get together for a few days and hack on various projects. rOpenSci unconferences have a rich history. Past projects have related to open data, data visualization, data publication and open science using R. To ensure a safe, enjoyable, and friendly experience for everyone who participates, we follow a strict code of conduct and terms and conditions. We are assembling an exciting team of developers and enthusiasts representing academia, industry, government, and various open source projects.

<center>
![](https://shiring.github.io/netlify_images/rOpenSci/IMG_3716.jpg)
<\center>


## Working on `greta`

At this year's rOpenSci unconference in Seattle, there were again [many interesting, amazing and cool projects being worked on](https://github.com/ropenscilabs/runconf18-projects/blob/master/index.Rmd)!

The group I joined originally wanted to bring **TensorFlow Probability** to R.

> TensorFlow Probability is a library for probabilistic reasoning and statistical analysis in TensorFlow. As part of the TensorFlow ecosystem, TensorFlow Probability provides integration of probabilistic methods with deep networks, gradient-based inference via automatic differentiation, and scalability to large datasets and models via hardware acceleration (e.g., GPUs) and distributed computation.
https://github.com/tensorflow/probability

In the end, we - that is [Michael Quinn](https://github.com/michaelquinn32), [David Smith](https://github.com/revodavid), [Tiphaine Martin](https://github.com/TiphaineCMartin), [Matt Mulvahill](https://github.com/mmulvahill) and I - ended up mainly working with the R package `greta`, which has similar functionalities as TensorFlow Probabilities. We recreated some of the examples from the TensorFlow Probability package tutorials in `greta` and we also added a few additional examples for using `greta`.

Check out [the README repo for an overview and links to everything we've contributed](https://github.com/ropenscilabs/greta/tree/unconf/README.md); it is a forked repo from the [original package repo of `greta`](https://github.com/greta-dev/greta).

<center>
![](https://shiring.github.io/netlify_images/rOpenSci/IMG_3715.jpg)
<\center>

<center>
![](https://shiring.github.io/netlify_images/rOpenSci/IMG_3717.jpg)
<\center>

<center>
![](https://shiring.github.io/netlify_images/rOpenSci/IMG_3724.jpg)
<\center>

I joined the "Tensorflow Probability for R" team, where we worked mainly with the greta package, which uses Tensorflow Probability to implement a Markov-Chain Monte-Carlo system to find solutions to complex statistical models. I hadn't used greta before, so I focused on trying out some simple examples to understand how greta works. In the process I learned that greta is a really powerful package: it solves many of the same problems as stan, but with a really elegant R interface that generalizes beyond Bayesian models. (Over the next couple of weeks I'll elaborate the examples for the greta package and write a more detailed blog post.)

At the end of the second day, everyone gets together to "report out" on their projects: a three minute presentation to review the progress from the hackathon. You can browse the list of projects here: follow the links to the Github repositories and check out the Readme.md files for details on each project.

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">A sincere thank you to all participants in <a href="https://twitter.com/hashtag/runconf18?src=hash&amp;ref_src=twsrc%5Etfw">#runconf18</a> <br><br>This threadðŸ‘‡includes links to all project repos: <a href="https://t.co/2PhAz4zSuK">https://t.co/2PhAz4zSuK</a><a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> <a href="https://t.co/8SICcWkQ0v">pic.twitter.com/8SICcWkQ0v</a></p>&mdash; rOpenSci (@rOpenSci) <a href="https://twitter.com/rOpenSci/status/1000024996876468224?ref_src=twsrc%5Etfw">May 25, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

### Where does `greta` come in?

[`greta`](https://greta-dev.github.io/greta) offers a number of functions that make it easy to define statistical models, particularly for Bayesian statistics and uses Google's TensorFlow backend to compute the defined models. Because TensorFlow has been optimized for large-scale computing, multi-core and GPU calculations are supported as well, making `greta` partiuclarly efficient and useful for working with complex models. `greta` has been created by [Nick Golding](https://github.com/goldingn). 

> greta lets us build statistical models interactively in R, and then sample from them by MCMC. We build greta models with greta array objects, which behave much like Râ€™s array, matrix and vector objects for numeric data. Like those numeric data objects, greta arrays can be manipulated with functions and mathematical operators to create new greta arrays.
The key difference between greta arrays and numeric data objects is that when you do something to a greta array, greta doesnâ€™t calculate the values of the new greta array. Instead, it just remembers what operation to do, and works out the size and shape of the result.
https://greta-dev.github.io/greta/get_started.html#how_greta_works

Because TensorFlow is not natively an R package, `greta` makes use of Rstudioâ€™s [reticulate](https://rstudio.github.io/reticulate/articles/python_packages.html) and [tensorflow](https://tensorflow.rstudio.com/) packages to connect with the TensorFlow backend.

With the `tensorflow` R package, we can work with all the TensorFlow Python functions directly from within R.

### How does `greta` work?

> There are three layers to how greta defines a model: users manipulate greta arrays, these define nodes, and nodes then define Tensors. https://greta-dev.github.io/greta/technical_details.html

This is the minimum working example of the linear mixed model based on an example from a TensorFlow Probability Jupyter notebook. Full example can be found [here](https://github.com/ropenscilabs/greta/blob/unconf/vignettes/8_schools_example_model.Rmd).

```{r 8_schools_data, highlight = FALSE}
dat_n <- letters[1:8]
dat_y <- c(28.39, 7.94, -2.75 , 6.82, -0.64, 0.63, 18.01, 12.16)
dat_sigma <- c(14.9, 10.2, 16.3, 11.0, 9.4, 11.4, 10.4, 17.6)
```

```{r 8_schools_greta}
# variables and priors
avg_effect <- normal(0, 10)
avg_stddev <- normal(5, 1)
theta_prime <- normal(0, 1, dim = length(N))
school_effects <- mu + exp(logtau) * theta_prime

# likelihood
distribution(y) <- normal(school_effects, sigma_y)

# defining the model
m <- model(mu, logtau, theta_prime)
```

```{r eval=FALSE}
plot(m)
```

```{r echo=FALSE}
# plotting
DiagrammeR::render_graph(plot(m))
```

```{r}
# sampling
draws <- greta::mcmc(m)
plot(draws)
```

> The only list element of a greta array is node, an R6 object inheriting from the R6 class â€˜nodeâ€™, as well as one of the three node types: â€˜dataâ€™, â€˜operationâ€™ or â€˜variableâ€™.
These R6 node objects are where the magic happens. When created, each node points to its â€˜childâ€™ nodes - the nodes for the greta arrays that were used to create this one.
Each node also has a list of its parents, the nodes that have been created from this one.
When model() is called, that inheritance information is used to construct the directed acyclic graph (DAG) that defines the model. The inheritance also preserves intermediate nodes, such as those creates in multi-part operations, but not assigned as greta arrays.
Nodes also have a value member, which is an array for data nodes or an â€˜unknownsâ€™ object for other node types. The unknowns class is a thin wrapper around arrays, which prints the question marks. Generic functions for working on arrays (e.g. dim, length, print) make use these node values to return something familiar to the user.
In addition to remembering their shape and size and where they are in the DAG, each node has methods to define a corresponding TensorFlow Tensor object in a specified environment. That doesnâ€™t happen until the user runs  model(), which creates a â€˜dag_classâ€™ object to store the relevant nodes, the environment for the tensors, and methods to talk to the TensorFlow graph.
The node tf() method takes the dag as an argument, and defines a tensor representing itself in the tensorflow environment, with a name determined by the dag object.
Because R6 objects are pass-by-reference (rather than pass-by-value), the dag accumulates all of the defined tensors, rather than being re-written each time. Similarly, because nodes are R6 objects and know which are their children, they can make sure those children are defined as tensors before they are. The define_tf() member function ensures that that happens, enabling operation nodes to use the tensors for their children when defining their own tensor.
Hamiltonian Monte Carlo (HMC) requires all of the parameters to be transformed to a continuous scale for sampling. Variable nodes are therefore converted to tensors by first defining a â€˜freeâ€™ (unconstrained) version of themselves as a tensor, and then applying a transformation function to convert them back to the scale of interest.
distribution nodes are node objects just like the others, but they are not directly associated with greta arrays. Instead, greta arrays may have a distribution node in their distribution slot to indicate that their values are assumed to follow that distribution. The distribution node will also be listed as a parent node, and likewise the â€˜target nodeâ€™ will be listed as a child of the distribution. Distribution nodes also have child nodes (data, variables or operations) representing their parameters.
When they define themselves as tensors, distribution nodes define the log density of their target node/tensor given the tensors representing their parameters.
Those log-densities for these distributions are summed on the TensorFlow graph to create a Tensor for the joint log-density of the model. TensorFlowâ€™s automatic gradient capabilities are then used to define a Tensor for the gradient of the log-density with respect to each parameter in the model.
The dag R6 object contained within the model then exposes methods to send parameters to the TensorFlow graph and return the joint density and gradient.

> Create greta arrays represent observed data or fixed values.

You can [create `greta` arrays](https://greta-dev.github.io/greta/structures.html) or [convert R objects, like data frames into `greta` arrays](https://greta-dev.github.io/greta/as_data.html).

> variable() creates greta arrays representing unknown parameters, to be learned during model fitting. These parameters are not associated with a probability distribution. To create a variable greta array following a specific probability distribution, see distributions.

> probability distributions can be used to define random variables in a greta model. They return a variable greta array that follows the specified distribution. This variable greta array can be used to represent a parameter with prior distribution, or used with distribution to define a distribution over a data greta array.

> distribution defines probability distributions over observed data, e.g. to set a model likelihood.

> gretaâ€™s operators are used just like Râ€™s the standard arithmetic, logical and relational operators, but they return other greta arrays. Since the operations are only carried during sampling, the greta array objects have unknown values.

> functions in base R that are currently implemented to transform greta arrays. 
transformations for greta arrays, which may also be used as inverse link functions.



### TensorFlow Probability

TensorFlow Probability isn't part of the core TensorFlow package, though, so we won't have it loaded with `library(tensorflow)`. But we can use the `reticulate` package instead to import any Python module (aka library) into R and use it there. This way, we could use the original functions from the `tensorflow_probability` Python package in R.

We could, for example, work with the Edward2 functionalities from TensorFlow probabilities. 

> Edward is a Python library for probabilistic modeling, inference, and criticism. It is a testbed for fast experimentation and research with probabilistic models, ranging from classical hierarchical models on small data sets to complex deep probabilistic models on large data sets. Edward fuses three fields: Bayesian statistics and machine learning, deep learning, and probabilistic programming. [...] Edward is built on TensorFlow. It enables features such as computational graphs, distributed training, CPU/GPU integration, automatic differentiation, and visualization with TensorBoard.
http://edwardlib.org/

```{r eval=FALSE}
library(reticulate)
tf <- import("tensorflow")
tfp <- import("tensorflow_probability")
ed <- tfp$edward2
```

### Note on installing a working version of TensorFlow Probability for R

As TensorFlow Probability isn't part of the core TensorFlow package, we need to install the nightly bleeding edge version. However, we had a few problems installing a working version of TensorFlow Probability that had all the necessary submodules we wanted to use (like `edward2`). So, this is the version that worked in the end (as of today): 

- TensorFlow Probability version 0.0.1.dev20180515
- TensorFlow version 1.9.0.dev20180515

For full disclosure: I worked from within the R virtualenv **r-tensorflow** that was created when I ran `install_tensorflow()` from within R.
In this environment I installed:

```
pip install tfp-nightly==0.0.1.dev20180515
pip install tf-nightly==1.9.0.dev20180515
```

I used Python 3.6 on a Mac OS High Sierra version 10.13.4.

## Thanks

Huge thanks go out to my amazing `greta` team and to rOpenSci - particularly Stefanie Butland - for organizing auch a wonderful event!

Thank you also to all [sponsors](http://unconf18.ropensci.org/#sponsors), who made it possible for me to fly all the way over to the Pacific Northwest and attend the unconf!

## Session Information

```{r}
sessionInfo()
```

