---
title: "Explaining Black-Box Machine Learning Models - Code Part 1: tabular data + caret + iml"
draft: false
author: Shirin Glander
date: '2018-07-20'
categories: ["R"]
tags: ["machine learning", "caret", "iml", "lime"]
thumbnailImagePosition: left
thumbnailImage: post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/eda_plots-1.png
metaAlignment: center
coverMeta: out
slug: explaining_ml_models_code_caret_iml
---

This is code that will accompany an article that will appear in a special edition of a German IT magazine. The article is about explaining black-box machine learning models. In that article I'm showcasing three practical examples:

1. Explaining supervised classification models built on tabular data using `caret` and the `iml` package
2. Explaining image classification models with `keras` and `lime`
3. Explaining text classification models with `lime`

The second part has already been published [here](https://shirinsplayground.netlify.com/2018/06/keras_fruits_lime/).

Below, you will find the code for the first part: 

```{r libraries, warning=FALSE, message=FALSE}
# data wrangling
library(tidyverse)
library(readr)

# ml
library(caret)

# plotting
library(gridExtra)
library(grid)
library(ggridges)
library(ggthemes)
theme_set(theme_minimal())

# explaining models
# https://github.com/christophM/iml
library(iml)

# https://pbiecek.github.io/breakDown/
library(breakDown)

# https://pbiecek.github.io/DALEX/
library(DALEX)
```

## Supervised classfication models with tabular data

### The data

The example dataset I am using in this part is the [wine quality data](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009). Let's read it in and do some minor housekeeping, like 

- converting the response variable `quality` into two categories with roughly equal sizes and 
- replacing the spaces in the column names with "_" to make it easier to handle in the tidyverse

```{r}
wine_data <- read_csv("/Users/shiringlander/Documents/Github/ix_lime_etc/winequality-red.csv") %>%
  mutate(quality = as.factor(ifelse(quality < 6, "qual_low", "qual_high")))
colnames(wine_data) <- gsub(" ", "_", colnames(wine_data))
glimpse(wine_data)
```

### EDA

The first step in my machine learning workflows in exploratory data analysis (EDA). This can get pretty extensive, but here I am only looking at the distributions of my features and the class counts of my response variable.

```{r}
p1 <- wine_data %>%
  ggplot(aes(x = quality, fill = quality)) +
    geom_bar(alpha = 0.8) +
    scale_fill_tableau() +
    guides(fill = FALSE)
```

```{r}
p2 <- wine_data %>%
  gather(x, y, fixed_acidity:alcohol) %>%
  ggplot(aes(x = y, y = quality, color = quality, fill = quality)) +
    facet_wrap( ~ x, scale = "free", ncol = 3) +
    scale_fill_tableau() +
    scale_color_tableau() +
    geom_density_ridges(alpha = 0.8) +
    guides(fill = FALSE, color = FALSE)
```

```{r eda_plots, fig.width=10, fig.height=6, warning=FALSE, message=FALSE}
grid.arrange(p1, p2, ncol = 2, widths = c(0.3, 0.7))
```

### Model

For modeling, I am splitting the data into 80% for training and 20% for testing.

```{r}
set.seed(42)
idx <- createDataPartition(wine_data$quality, 
                           p = 0.8, 
                           list = FALSE, 
                           times = 1)

wine_train <- wine_data[ idx,]
wine_test  <- wine_data[-idx,]
```

I am using 5-fold cross-validation, repeated 3x and scale and center the data. The example model I am using here is a Random Forest model.

```{r eval=FALSE}
fit_control <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 3)

set.seed(42)
rf_model <- train(quality ~ ., 
                  data = wine_train, 
                  method = "rf", 
                  preProcess = c("scale", "center"),
                  trControl = fit_control,
                  verbose = FALSE)
```

```{r echo=FALSE}
load("/Users/shiringlander/Documents/Github/ix_lime_etc/output/rf_model.RData")
```

```{r}
rf_model
```

Now let's see how good our model is:

```{r}
test_predict <- predict(rf_model, wine_test)
confusionMatrix(test_predict, as.factor(wine_test$quality))
```

Okay, this model isn't too accurate but since my focus here is supposed to be on explaining the model, that's good enough for me at this point.

### Explaining/interpreting the model

There are several methods and tools that can be used to explain or interpret machine learning models. You can read more about them in [this ebook](https://christophm.github.io/interpretable-ml-book/). Here, I am going to show a few of them.

#### Feature importance

The first metric to look at for Random Forest models (and many other algorithms) is feature importance:

> "Variable importance evaluation functions can be separated into two groups: those that use the model information and those that do not. The advantage of using a model-based approach is that is more closely tied to the model performance and that it may be able to incorporate the correlation structure between the predictors into the importance calculation." https://topepo.github.io/caret/variable-importance.html

The `varImp()` function from the `caret` package can be used to calculate feature importance measures for most methods. For Random Forest classification models such as ours, the prediction error rate is calculated for 

1. permuted out-of-bag data of each tree and
2. permutations of every feature

These two measures are averaged and normalized as described here:

> "Here are the definitions of the variable importance measures. The first measure is computed from permuting OOB data: For each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression). Then the same is done after permuting each predictor variable. The difference between the two are then averaged over all trees, and normalized by the standard deviation of the differences. If the standard deviation of the differences is equal to 0 for a variable, the division is not done (but the average is almost always equal to 0 in that case).
The second measure is the total decrease in node impurities from splitting on the variable, averaged over all trees. For classification, the node impurity is measured by the Gini index. For regression, it is measured by residual sum of squares." randomForest help function for `importance()`
 
```{r}
rf_model_imp <- varImp(rf_model, scale = TRUE)
p1 <- rf_model_imp$importance %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  ggplot(aes(x = reorder(rowname, Overall), y = Overall)) +
    geom_bar(stat = "identity", fill = "#1F77B4", alpha = 0.8) +
    coord_flip()
```

We can also use a ROC curve for evaluating feature importance. For this, we have the `caret::filterVarImp()` function:

> "The importance of each predictor is evaluated individually using a “filter” approach.
For classification, ROC curve analysis is conducted on each predictor. For two class problems, a series of cutoffs is applied to the predictor data to predict the class. The sensitivity and specificity are computed for each cutoff and the ROC curve is computed. The trapezoidal rule is used to compute the area under the ROC curve. This area is used as the measure of variable importance. For multi–class outcomes, the problem is decomposed into all pair-wise problems and the area under the curve is calculated for each class pair (i.e class 1 vs. class 2, class 2 vs. class 3 etc.). For a specific class, the maximum area under the curve across the relevant pair–wise AUC's is used as the variable importance measure.
For regression, the relationship between each predictor and the outcome is evaluated. An argument, nonpara, is used to pick the model fitting technique. When nonpara = FALSE, a linear model is fit and the absolute value of the $t$–value for the slope of the predictor is used. Otherwise, a loess smoother is fit between the outcome and the predictor. The $R^2$ statistic is calculated for this model against the intercept only null model." caret help for `filterVarImp()`

```{r}
roc_imp <- filterVarImp(x = wine_train[, -ncol(wine_train)], y = wine_train$quality)
p2 <- roc_imp %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  ggplot(aes(x = reorder(rowname, qual_high), y = qual_high)) +
    geom_bar(stat = "identity", fill = "#1F77B4", alpha = 0.8) +
    coord_flip()
```

```{r feature_imp_plots, fig.width=10, fig.height=5, warning=FALSE, message=FALSE}
grid.arrange(p1, p2, ncol = 2, widths = c(0.5, 0.5))
```

#### iml

The `iml` package combines a number of methods for explaining/interpreting machine learning model, like

- Feature importance
- Partial dependence plots
- Individual conditional expectation plots (ICE)
- Tree surrogate
- LocalModel: Local Interpretable Model-agnostic Explanations (similar to [lime](https://shirinsplayground.netlify.com/2018/06/keras_fruits_lime/))
- Shapley value for explaining single predictions

In order to work with `iml`, we need to adapt our data a bit by removing the response variable and the creating a new predictor object that holds the model, the data and the class labels.

> "The iml package uses R6 classes: New objects can be created by calling Predictor$new()." https://github.com/christophM/iml/blob/master/vignettes/intro.Rmd

```{r}
X <- wine_train %>%
  select(-quality) %>%
  as.data.frame()

predictor <- Predictor$new(rf_model, data = X, y = wine_train$quality)
str(predictor)
```

##### Partial Dependence & Individual Conditional Expectation plots (ICE)

Now we can explore some of the different methods. Let's start with [partial dependence plots](https://christophm.github.io/interpretable-ml-book/pdp.html) as we had already looked into feature importance.

> "Besides knowing which features were important, we are interested in how the features influence the predicted outcome. The Partial class implements partial dependence plots and individual conditional expectation curves. Each individual line represents the predictions (y-axis) for one data point when we change one of the features (e.g. 'lstat' on the x-axis). The highlighted line is the point-wise average of the individual lines and equals the partial dependence plot. The marks on the x-axis indicates the distribution of the 'lstat' feature, showing how relevant a region is for interpretation (little or no points mean that we should not over-interpret this region)." https://github.com/christophM/iml/blob/master/vignettes/intro.Rmd#partial-dependence

We can look at individual features, like the alcohol or pH and plot the curves:

```{r}
pdp_obj <- Partial$new(predictor, feature = "alcohol")
pdp_obj$center(min(wine_train$alcohol))
glimpse(pdp_obj$results)
```

> "The partial dependence plot calculates and plots the dependence of f(X) on a single or two features. It's the aggregate of all individual conditional expectation curves, that describe how, for a single observation, the prediction changes when the feature changes." iml help for `Partial`

```{r pd_plots, fig.width=10, fig.height=5}
pdp_obj$plot()
```

```{r pd_plots_2, fig.width=10, fig.height=5}
pdp_obj2 <- Partial$new(predictor, feature = c("sulphates", "pH"))
pdp_obj2$plot()
```

##### Feature interaction

>"Interactions between features are measured via the decomposition of the prediction function: If a feature j has no interaction with any other feature, the prediction function can be expressed as the sum of the partial function that depends only on j and the partial function that only depends on features other than j. If the variance of the full function is completely explained by the sum of the partial functions, there is no interaction between feature j and the other features. Any variance that is not explained can be attributed to the interaction and is used as a measure of interaction strength.
The interaction strength between two features is the proportion of the variance of the 2-dimensional partial dependence function that is not explained by the sum of the two 1-dimensional partial dependence functions.
The interaction measure takes on values between 0 (no interaction) to 1." iml help for `Interaction`

```{r eval=FALSE}
interact <- Interaction$new(predictor, feature = "alcohol")
```

```{r echo=FALSE}
load("/Users/shiringlander/Documents/Github/ix_lime_etc/output/interact.RData")
```

All of these methods have a plot argument. However, since I am writing this for an article, I want all my plots to have the same look. That's why I'm customizing the plots I want to use in my article as shown below.

```{r interaction_plot}
#plot(interact)
interact$results %>%
  ggplot(aes(x = reorder(.feature, .interaction), y = .interaction, fill = .class)) +
    facet_wrap(~ .class, ncol = 2) +
    geom_bar(stat = "identity", alpha = 0.8) +
    scale_fill_tableau() +
    coord_flip() +
    guides(fill = FALSE)
```

##### Tree Surrogate

The tree surrogate method uses decision trees on the predictions.

> "A conditional inference tree is fitted on the predicted \hat{y} from the machine learning model and the data. The partykit package and function are used to fit the tree. By default a tree of maximum depth of 2 is fitted to improve interpretability." iml help for `TreeSurrogate`

```{r}
tree <- TreeSurrogate$new(predictor, maxdepth = 5)
```

The R^2 value gives an estimate of the goodness of fit or how well the decision tree approximates the model.

```{r}
tree$r.squared
```

```{r tree_surr_plot, fig.width=10, fig.height=6}
#plot(tree)
tree$results %>%
  mutate(prediction = colnames(select(., .y.hat.qual_high, .y.hat.qual_low))[max.col(select(., .y.hat.qual_high, .y.hat.qual_low),
                                                                                     ties.method = "first")],
         prediction = ifelse(prediction == ".y.hat.qual_low", "qual_low", "qual_high")) %>%
  ggplot(aes(x = prediction, fill = prediction)) +
    facet_wrap(~ .path, ncol = 5) +
    geom_bar(alpha = 0.8) +
    scale_fill_tableau() +
    guides(fill = FALSE)
```

##### LocalModel: Local Interpretable Model-agnostic Explanations

LocalModel is a implementation of the LIME algorithm from [Ribeiro et al. 2016](https://arxiv.org/abs/1602.04938), similar to [lime](https://shirinsplayground.netlify.com/2018/06/keras_fruits_lime/).

According to the LIME principle, we can look at individual predictions. Here, for example on the first row of the test set:

```{r warning=FALSE, message=FALSE}
X2 <- wine_test[, -12]
i = 1
lime_explain <- LocalModel$new(predictor, x.interest = X2[i, ])
lime_explain$results
```

```{r}
#plot(lime_explain)
p1 <- lime_explain$results %>%
  ggplot(aes(x = reorder(feature.value, -effect), y = effect, fill = .class)) +
    facet_wrap(~ .class, ncol = 2) +
    geom_bar(stat = "identity", alpha = 0.8) +
    scale_fill_tableau() +
    coord_flip() +
    labs(title = paste0("Test case #", i)) +
    guides(fill = FALSE)
```

... or for the sixth row:

```{r}
i = 6
lime_explain$explain(X2[i, ])
p2 <- lime_explain$results %>%
  ggplot(aes(x = reorder(feature.value, -effect), y = effect, fill = .class)) +
    facet_wrap(~ .class, ncol = 2) +
    geom_bar(stat = "identity", alpha = 0.8) +
    scale_fill_tableau() +
    coord_flip() +
    labs(title = paste0("Test case #", i)) +
    guides(fill = FALSE)
```

```{r lime_plot_1, fig.width=10, fig.height=3, warning=FALSE, message=FALSE}
grid.arrange(p1, p2, ncol = 2)
```

##### Shapley value for explaining single predictions

Another way to interpret individual predictions is whit Shapley values.

> "Shapley computes feature contributions for single predictions with the Shapley value, an approach from cooperative game theory. The features values of an instance cooperate to achieve the prediction. The Shapley value fairly distributes the difference of the instance's prediction and the datasets average prediction among the features." iml help for `Shapley`

More information about Shapley values can be found [here](https://christophm.github.io/interpretable-ml-book/shapley.html).

```{r }
shapley <- Shapley$new(predictor, x.interest = X2[1, ])
```

```{r}
head(shapley$results)
```

```{r shapley_plot}
#shapley$plot()
shapley$results %>%
  ggplot(aes(x = reorder(feature.value, -phi), y = phi, fill = class)) +
    facet_wrap(~ class, ncol = 2) +
    geom_bar(stat = "identity", alpha = 0.8) +
    scale_fill_tableau() +
    coord_flip() +
    guides(fill = FALSE)
```

#### breakDown

Another package worth mentioning is [breakDown](https://cran.r-project.org/web/packages/breakDown/index.html). It provides

> "Model agnostic tool for decomposition of predictions from black boxes. Break Down Table shows contributions of every variable to a final prediction. Break Down Plot presents variable contributions in a concise graphical way. This package work for binary classifiers and general regression models." https://cran.r-project.org/web/packages/breakDown/index.html

The `broken()` function decomposes model predictions and outputs the contributions of each feature to the final prediction.

```{r}
predict.function <- function(model, new_observation) {
  predict(model, new_observation, type="prob")[,2]
}
predict.function(rf_model, X2[1, ])

br <- broken(model = rf_model, 
             new_observation = X2[1, ], 
             data = X, 
             baseline = "Intercept", 
             predict.function = predict.function, 
             keep_distributions = TRUE)
br
```

The plot function shows the average predictions and the final prognosis:

```{r breakdown_plot_1}
#plot(br)
data.frame(y = br$contribution,
           x = br$variable) %>%
  ggplot(aes(x = reorder(x, y), y = y)) +
    geom_bar(stat = "identity", fill = "#1F77B4", alpha = 0.8) +
    coord_flip()
```

If we set `keep_distributions = TRUE`, we can plot these distributions of partial predictions, as well as the average.

```{r breakdown_plot_2}
plot(br, plot_distributions = TRUE)
```

#### DALEX: Descriptive mAchine Learning EXplanations

The third package I want to showcase is [DALEX](https://cran.r-project.org/web/packages/DALEX/index.html), which stands for Descriptive mAchine Learning EXplanations and contains a collection of functions that help with interpreting/explaining black-box models.

> "Machine Learning (ML) models are widely used and have various applications in classification or regression. Models created with boosting, bagging, stacking or similar techniques are often used due to their high performance, but such black-box models usually lack of interpretability. DALEX package contains various explainers that help to understand the link between input variables and model output. The single_variable() explainer extracts conditional response of a model as a function of a single selected variable. It is a wrapper over packages 'pdp' and 'ALEPlot'. The single_prediction() explainer attributes parts of a model prediction to particular variables used in the model. It is a wrapper over 'breakDown' package. The variable_dropout() explainer calculates variable importance scores based on variable shuffling. All these explainers can be plotted with generic plot() function and compared across different models." https://cran.r-project.org/web/packages/DALEX/index.html

We first create an explain object, that has the correct structure for use with the `DALEX` package.

```{r}
p_fun <- function(object, newdata){predict(object, newdata = newdata, type = "prob")[, 2]}
yTest <- as.numeric(wine_test$quality)

explainer_classif_rf <- DALEX::explain(rf_model, label = "rf",
                                       data = wine_test, y = yTest,
                                       predict_function = p_fun)
```

##### Model performance

With DALEX we can do several things, for example analyze model performance as the distribution of residuals.

```{r}
mp_classif_rf <- model_performance(explainer_classif_rf)
```

```{r dalex_perf_plot_1}
plot(mp_classif_rf)
```

```{r dalex_perf_plot_2}
plot(mp_classif_rf, geom = "boxplot")
```

##### Feature importance

Feature importance can be measured with `variable_importance()` function, which gives the loss from variable dropout.

```{r}
vi_classif_rf <- variable_importance(explainer_classif_rf, loss_function = loss_root_mean_square)
```

```{r dalex_var_imp_plot}
plot(vi_classif_rf)
```

##### Variable response

And we can calculate the marginal response for a single variable with the `variable_response()` function.

> "Calculates the average model response as a function of a single selected variable. Use the 'type' parameter to select the type of marginal response to be calculated. Currently for numeric variables we have Partial Dependency and Accumulated Local Effects implemented. Current implementation uses the 'pdp' package (Brandon M. Greenwell (2017). pdp: An R Package for Constructing Partial Dependence Plots. The R Journal, 9(1), 421–436.) and 'ALEPlot' (Dan Apley (2017). ALEPlot: Accumulated Local Effects Plots and Partial Dependence Plots.)" DALEX help for `variable_response`

As `type` we can choose between 'pdp' for Partial Dependence Plots and 'ale' for Accumulated Local Effects.

```{r}
pdp_classif_rf  <- variable_response(explainer_classif_rf, variable = "alcohol", type = "pdp")
```

```{r dalex_var_resp_plot_1}
plot(pdp_classif_rf)
```

```{r dalex_var_resp_plot_2}
ale_classif_rf  <- variable_response(explainer_classif_rf, variable = "alcohol", type = "ale")
plot(ale_classif_rf)
```

---

```{r}
sessionInfo()
```

