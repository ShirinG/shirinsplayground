---
title: "Code for case study - Customer Churn with Keras/TensorFlow and H2O"
draft: false
author: Shirin Glander
date: '2018-12-12'
categories: ["R"]
tags: ["R", "machine learning", "predictive analytics", "customer churn"]
thumbnailImagePosition: left
thumbnailImage: post/2018-12-12_customer_churn_code_files/figure-html/prop_table-1.png
metaAlignment: center
coverMeta: out
slug: customer_churn_code
---



<p>This is code that accompanies a <a href="https://www.dpunkt.de/buecher/13208/9783864906107-data-science.html">book</a> chapter on customer churn that I have written for the German dpunkt Verlag. The book is in German and will probably appear in February: <a href="https://www.dpunkt.de/buecher/13208/9783864906107-data-science.html" class="uri">https://www.dpunkt.de/buecher/13208/9783864906107-data-science.html</a>.</p>
<p>The code you find below can be used to recreate all figures and analyses from this book chapter. Because the content is exclusively for the book, my descriptions around the code had to be minimal. But I’m sure, you can get the gist, even without the book. ;-)</p>
<div id="inspiration-sources" class="section level1">
<h1>Inspiration &amp; Sources</h1>
<p>Thank you to the following people for providing excellent code examples about customer churn:</p>
<ul>
<li>Matt Dancho: <a href="http://www.business-science.io/business/2017/11/28/customer_churn_analysis_keras.html" class="uri">http://www.business-science.io/business/2017/11/28/customer_churn_analysis_keras.html</a></li>
<li>JJ Allaire: <a href="https://github.com/rstudio/keras-customer-churn" class="uri">https://github.com/rstudio/keras-customer-churn</a></li>
<li>Susan Li: <a href="https://towardsdatascience.com/predict-customer-churn-with-r-9e62357d47b4" class="uri">https://towardsdatascience.com/predict-customer-churn-with-r-9e62357d47b4</a></li>
<li>John Sullivan: <a href="https://jtsulliv.github.io/churn-eda/" class="uri">https://jtsulliv.github.io/churn-eda/</a></li>
</ul>
</div>
<div id="setup" class="section level1">
<h1>Setup</h1>
<p>All analyses are done in R using RStudio. For detailed session information including R version, operating system and package versions, see the <code>sessionInfo()</code> output at the end of this document.</p>
<p>All figures are produced with ggplot2.</p>
<ul>
<li>Libraries</li>
</ul>
<pre class="r"><code># Load libraries
library(tidyverse) # for tidy data analysis
library(readr)     # for fast reading of input files
library(caret)     # for convenient splitting
library(mice)      # mice package for Multivariate Imputation by Chained Equations (MICE)
library(keras)     # for neural nets
library(lime)      # for explaining neural nets
library(rsample)   # for splitting training and test data
library(recipes)   # for preprocessing
library(yardstick) # for evaluation
library(ggthemes)  # for additional plotting themes
library(corrplot)  # for correlation

theme_set(theme_minimal())</code></pre>
<pre class="r"><code># Install Keras if you have not installed it before
# follow instructions if you haven&#39;t installed TensorFlow
install_keras()</code></pre>
<p><br></p>
</div>
<div id="data-preparation" class="section level1">
<h1>Data preparation</h1>
<div id="the-dataset" class="section level2">
<h2>The dataset</h2>
<p>The Telco Customer Churn data set is the same one that Matt Dancho used in his post (see above). It was downloaded from <a href="https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/">IBM Watson</a></p>
<pre class="r"><code>churn_data_raw &lt;- read_csv(&quot;WA_Fn-UseC_-Telco-Customer-Churn.csv&quot;)</code></pre>
<pre class="r"><code>glimpse(churn_data_raw)</code></pre>
<pre><code>## Observations: 7,043
## Variables: 21
## $ customerID       &lt;chr&gt; &quot;7590-VHVEG&quot;, &quot;5575-GNVDE&quot;, &quot;3668-QPYBK&quot;, &quot;77...
## $ gender           &lt;chr&gt; &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;...
## $ SeniorCitizen    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
## $ Partner          &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;N...
## $ Dependents       &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;N...
## $ tenure           &lt;dbl&gt; 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 5...
## $ PhoneService     &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;...
## $ MultipleLines    &lt;chr&gt; &quot;No phone service&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No phone ser...
## $ InternetService  &lt;chr&gt; &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;Fiber optic&quot;, &quot;F...
## $ OnlineSecurity   &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, ...
## $ OnlineBackup     &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, ...
## $ DeviceProtection &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, ...
## $ TechSupport      &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;N...
## $ StreamingTV      &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;...
## $ StreamingMovies  &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;N...
## $ Contract         &lt;chr&gt; &quot;Month-to-month&quot;, &quot;One year&quot;, &quot;Month-to-month...
## $ PaperlessBilling &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;...
## $ PaymentMethod    &lt;chr&gt; &quot;Electronic check&quot;, &quot;Mailed check&quot;, &quot;Mailed c...
## $ MonthlyCharges   &lt;dbl&gt; 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89....
## $ TotalCharges     &lt;dbl&gt; 29.85, 1889.50, 108.15, 1840.75, 151.65, 820....
## $ Churn            &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, ...</code></pre>
<div id="eda" class="section level3">
<h3>EDA</h3>
<ul>
<li>Proportion of churn</li>
</ul>
<pre class="r"><code>churn_data_raw %&gt;%
  count(Churn)</code></pre>
<pre><code>## # A tibble: 2 x 2
##   Churn     n
##   &lt;chr&gt; &lt;int&gt;
## 1 No     5174
## 2 Yes    1869</code></pre>
<ul>
<li>Plot categorical features</li>
</ul>
<pre class="r"><code>churn_data_raw %&gt;%
  mutate(SeniorCitizen = as.character(SeniorCitizen)) %&gt;%
  select(-customerID) %&gt;%
  select_if(is.character) %&gt;%
  select(Churn, everything()) %&gt;%
  gather(x, y, gender:PaymentMethod) %&gt;%
  count(Churn, x, y) %&gt;%
  ggplot(aes(x = y, y = n, fill = Churn, color = Churn)) +
    facet_wrap(~ x, ncol = 4, scales = &quot;free&quot;) +
    geom_bar(stat = &quot;identity&quot;, alpha = 0.5) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
          legend.position = &quot;top&quot;) +
    scale_color_tableau() +
    scale_fill_tableau()</code></pre>
<p><img src="/post/2018-12-12_customer_churn_code_files/figure-html/eda_chr-1.png" width="1152" /></p>
<ul>
<li>Plot numerical features</li>
</ul>
<pre class="r"><code>churn_data_raw %&gt;%
  select(-customerID) %&gt;%
  #select_if(is.numeric) %&gt;%
  select(Churn, MonthlyCharges, tenure, TotalCharges) %&gt;%
  gather(x, y, MonthlyCharges:TotalCharges) %&gt;%
  ggplot(aes(x = y, fill = Churn, color = Churn)) +
    facet_wrap(~ x, ncol = 3, scales = &quot;free&quot;) +
    geom_density(alpha = 0.5) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
          legend.position = &quot;top&quot;) +
    scale_color_tableau() +
    scale_fill_tableau()</code></pre>
<p><img src="/post/2018-12-12_customer_churn_code_files/figure-html/eda_num-1.png" width="1152" /></p>
<ul>
<li>Remove customer ID as it doesn’t provide information</li>
</ul>
<pre class="r"><code>churn_data &lt;- churn_data_raw %&gt;%
  select(-customerID)</code></pre>
</div>
<div id="dealing-with-missing-values" class="section level3">
<h3>Dealing with missing values</h3>
<ul>
<li>Pattern of missing data</li>
</ul>
<pre class="r"><code>md.pattern(churn_data, plot = FALSE)</code></pre>
<pre><code>##      gender SeniorCitizen Partner Dependents tenure PhoneService
## 7032      1             1       1          1      1            1
## 11        1             1       1          1      1            1
##           0             0       0          0      0            0
##      MultipleLines InternetService OnlineSecurity OnlineBackup
## 7032             1               1              1            1
## 11               1               1              1            1
##                  0               0              0            0
##      DeviceProtection TechSupport StreamingTV StreamingMovies Contract
## 7032                1           1           1               1        1
## 11                  1           1           1               1        1
##                     0           0           0               0        0
##      PaperlessBilling PaymentMethod MonthlyCharges Churn TotalCharges   
## 7032                1             1              1     1            1  0
## 11                  1             1              1     1            0  1
##                     0             0              0     0           11 11</code></pre>
<ul>
<li>Option 1: impute missing data =&gt; NOT done here!</li>
</ul>
<pre class="r"><code>imp &lt;- mice(data = churn_data,  print = FALSE)
train_data_impute &lt;- complete(imp, &quot;long&quot;)</code></pre>
<ul>
<li>Option 2: drop missing data =&gt; done here because not too much information is lost by removing it</li>
</ul>
<pre class="r"><code>churn_data &lt;- churn_data %&gt;%
  drop_na()</code></pre>
</div>
</div>
<div id="training-and-test-split" class="section level2">
<h2>Training and test split</h2>
<ul>
<li>Partition data into training and test set</li>
</ul>
<pre class="r"><code>set.seed(42)
index &lt;- createDataPartition(churn_data$Churn, p = 0.7, list = FALSE)</code></pre>
<ul>
<li>Partition test set again into validation and test set</li>
</ul>
<pre class="r"><code>train_data &lt;- churn_data[index, ]
test_data  &lt;- churn_data[-index, ]

index2 &lt;- createDataPartition(test_data$Churn, p = 0.5, list = FALSE)

valid_data &lt;- test_data[-index2, ]
test_data &lt;- test_data[index2, ]</code></pre>
<pre class="r"><code>nrow(train_data)</code></pre>
<pre><code>## [1] 4924</code></pre>
<pre class="r"><code>nrow(valid_data)</code></pre>
<pre><code>## [1] 1054</code></pre>
<pre class="r"><code>nrow(test_data)</code></pre>
<pre><code>## [1] 1054</code></pre>
</div>
<div id="pre-processing" class="section level2">
<h2>Pre-Processing</h2>
<ul>
<li>Create recipe for preprocessing</li>
</ul>
<blockquote>
<p>A recipe is a description of what steps should be applied to a data set in order to get it ready for data analysis.</p>
</blockquote>
<pre class="r"><code>recipe_churn &lt;- recipe(Churn ~ ., train_data) %&gt;%
  step_dummy(all_nominal(), -all_outcomes()) %&gt;%
  step_center(all_predictors(), -all_outcomes()) %&gt;%
  step_scale(all_predictors(), -all_outcomes()) %&gt;%
  prep(data = train_data)</code></pre>
<ul>
<li>Apply recipe to three datasets</li>
</ul>
<pre class="r"><code>train_data &lt;- bake(recipe_churn, new_data = train_data) %&gt;%
  select(Churn, everything())

valid_data &lt;- bake(recipe_churn, new_data = valid_data) %&gt;%
  select(Churn, everything())

test_data &lt;- bake(recipe_churn, new_data = test_data) %&gt;%
  select(Churn, everything())</code></pre>
<ul>
<li>For Keras create response variable as one-hot encoded matrix</li>
</ul>
<pre class="r"><code>train_y_drop &lt;- to_categorical(as.integer(as.factor(train_data$Churn)) - 1, 2)
colnames(train_y_drop) &lt;- c(&quot;No&quot;, &quot;Yes&quot;)

valid_y_drop &lt;- to_categorical(as.integer(as.factor(valid_data$Churn)) - 1, 2)
colnames(valid_y_drop) &lt;- c(&quot;No&quot;, &quot;Yes&quot;)

test_y_drop &lt;- to_categorical(as.integer(as.factor(test_data$Churn)) - 1, 2)
colnames(test_y_drop) &lt;- c(&quot;No&quot;, &quot;Yes&quot;)</code></pre>
<ul>
<li>Because we want to train on a binary outcome, we can delete the “No” column</li>
</ul>
<pre class="r"><code># if training with binary crossentropy
train_y_drop &lt;- train_y_drop[, 2, drop = FALSE]
head(train_y_drop)</code></pre>
<pre><code>##      Yes
## [1,]   0
## [2,]   1
## [3,]   1
## [4,]   0
## [5,]   1
## [6,]   0</code></pre>
<pre class="r"><code>valid_y_drop &lt;- valid_y_drop[, 2, drop = FALSE]
test_y_drop &lt;- test_y_drop[, 2, drop = FALSE]</code></pre>
<ul>
<li>Remove response variable from preprocessed data (for Keras)</li>
</ul>
<pre class="r"><code>train_data_bk &lt;- select(train_data, -Churn)
head(train_data_bk)</code></pre>
<pre><code>## # A tibble: 6 x 30
##   SeniorCitizen  tenure MonthlyCharges TotalCharges gender_Male Partner_Yes
##           &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;
## 1        -0.439  0.0765         -0.256       -0.163       0.979      -0.966
## 2        -0.439 -1.23           -0.359       -0.949       0.979      -0.966
## 3        -0.439 -0.983           1.17        -0.635      -1.02       -0.966
## 4        -0.439 -0.901          -1.16        -0.863      -1.02       -0.966
## 5        -0.439 -0.168           1.34         0.347      -1.02        1.03 
## 6        -0.439  1.22           -0.282        0.542       0.979      -0.966
## # ... with 24 more variables: Dependents_Yes &lt;dbl&gt;,
## #   PhoneService_Yes &lt;dbl&gt;, MultipleLines_No.phone.service &lt;dbl&gt;,
## #   MultipleLines_Yes &lt;dbl&gt;, InternetService_Fiber.optic &lt;dbl&gt;,
## #   InternetService_No &lt;dbl&gt;, OnlineSecurity_No.internet.service &lt;dbl&gt;,
## #   OnlineSecurity_Yes &lt;dbl&gt;, OnlineBackup_No.internet.service &lt;dbl&gt;,
## #   OnlineBackup_Yes &lt;dbl&gt;, DeviceProtection_No.internet.service &lt;dbl&gt;,
## #   DeviceProtection_Yes &lt;dbl&gt;, TechSupport_No.internet.service &lt;dbl&gt;,
## #   TechSupport_Yes &lt;dbl&gt;, StreamingTV_No.internet.service &lt;dbl&gt;,
## #   StreamingTV_Yes &lt;dbl&gt;, StreamingMovies_No.internet.service &lt;dbl&gt;,
## #   StreamingMovies_Yes &lt;dbl&gt;, Contract_One.year &lt;dbl&gt;,
## #   Contract_Two.year &lt;dbl&gt;, PaperlessBilling_Yes &lt;dbl&gt;,
## #   PaymentMethod_Credit.card..automatic. &lt;dbl&gt;,
## #   PaymentMethod_Electronic.check &lt;dbl&gt;, PaymentMethod_Mailed.check &lt;dbl&gt;</code></pre>
<pre class="r"><code>valid_data_bk &lt;- select(valid_data, -Churn)
test_data_bk &lt;- select(test_data, -Churn)</code></pre>
<ul>
<li>Alternative to above, to convert response variable into numeric format where 1 = Yes and 0 = No</li>
</ul>
<pre class="r"><code>train_data$Churn &lt;- ifelse(train_data$Churn == &quot;Yes&quot;, 1, 0)
valid_data$Churn &lt;- ifelse(valid_data$Churn == &quot;Yes&quot;, 1, 0)
test_data$Churn &lt;- ifelse(test_data$Churn == &quot;Yes&quot;, 1, 0)</code></pre>
</div>
<div id="modeling-with-keras" class="section level2">
<h2>Modeling with Keras</h2>
<ul>
<li>Define a simple MLP</li>
</ul>
<pre class="r"><code>model_keras &lt;- keras_model_sequential()

model_keras %&gt;% 
  layer_dense(units = 32, kernel_initializer = &quot;uniform&quot;, activation = &quot;relu&quot;, 
              input_shape = ncol(train_data_bk)) %&gt;% 
  layer_dropout(rate = 0.2) %&gt;%
  
  layer_dense(units = 16, kernel_initializer = &quot;uniform&quot;, activation = &quot;relu&quot;) %&gt;% 
  layer_dropout(rate = 0.2) %&gt;%
  
  layer_dense(units = 8, kernel_initializer = &quot;uniform&quot;, activation = &quot;relu&quot;) %&gt;% 
  layer_dropout(rate = 0.2) %&gt;%

  layer_dense(units = 1,
              kernel_initializer = &quot;uniform&quot;, activation = &quot;sigmoid&quot;) %&gt;%
  
  compile(
        optimizer = &#39;adamax&#39;,
        loss      = &#39;binary_crossentropy&#39;,
        metrics   = c(&quot;binary_accuracy&quot;, &quot;mse&quot;)
    )

summary(model_keras)</code></pre>
<pre><code>## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## dense_1 (Dense)                  (None, 32)                    992         
## ___________________________________________________________________________
## dropout_1 (Dropout)              (None, 32)                    0           
## ___________________________________________________________________________
## dense_2 (Dense)                  (None, 16)                    528         
## ___________________________________________________________________________
## dropout_2 (Dropout)              (None, 16)                    0           
## ___________________________________________________________________________
## dense_3 (Dense)                  (None, 8)                     136         
## ___________________________________________________________________________
## dropout_3 (Dropout)              (None, 8)                     0           
## ___________________________________________________________________________
## dense_4 (Dense)                  (None, 1)                     9           
## ===========================================================================
## Total params: 1,665
## Trainable params: 1,665
## Non-trainable params: 0
## ___________________________________________________________________________</code></pre>
<ul>
<li>Fit model (we could have used validation split on the trainings data instead of creating a validation set =&gt; see #)</li>
</ul>
<pre class="r"><code>fit_keras &lt;- fit(model_keras, 
    x = as.matrix(train_data_bk), 
    y = train_y_drop,
    batch_size = 32, 
    epochs = 20,
    #validation_split = 0.30,
    validation_data = list(as.matrix(valid_data_bk), valid_y_drop),
    verbose = 2
    )</code></pre>
<ul>
<li>Plot Keras training results</li>
</ul>
<pre class="r"><code>plot(fit_keras) +
  scale_color_tableau() +
  scale_fill_tableau()</code></pre>
<p><img src="/post/2018-12-12_customer_churn_code_files/figure-html/plot_fit_keras-1.png" width="960" /></p>
</div>
<div id="evaluation" class="section level2">
<h2>Evaluation</h2>
<ul>
<li>Predict classes and probabilities</li>
</ul>
<pre class="r"><code>pred_classes_test &lt;- predict_classes(object = model_keras, x = as.matrix(test_data_bk))
pred_proba_test  &lt;- predict_proba(object = model_keras, x = as.matrix(test_data_bk))</code></pre>
<ul>
<li>Create results table</li>
</ul>
<pre class="r"><code>test_results &lt;- tibble(
  actual_yes = as.factor(as.vector(test_y_drop)),
  pred_classes_test = as.factor(as.vector(pred_classes_test)),
  Yes = as.vector(pred_proba_test), 
  No = 1 - as.vector(pred_proba_test))
head(test_results)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   actual_yes pred_classes_test     Yes    No
##   &lt;fct&gt;      &lt;fct&gt;               &lt;dbl&gt; &lt;dbl&gt;
## 1 0          1                 0.567   0.433
## 2 0          0                 0.0200  0.980
## 3 1          1                 0.683   0.317
## 4 1          0                 0.330   0.670
## 5 0          0                 0.00589 0.994
## 6 0          0                 0.0459  0.954</code></pre>
<ul>
<li>Calculate confusion matrix</li>
</ul>
<pre class="r"><code>test_results %&gt;% 
  conf_mat(actual_yes, pred_classes_test)</code></pre>
<pre><code>##           Truth
## Prediction   0   1
##          0 694 125
##          1  80 155</code></pre>
<ul>
<li>Calculate metrics</li>
</ul>
<pre class="r"><code>test_results %&gt;% 
  metrics(actual_yes, pred_classes_test)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.806
## 2 kap      binary         0.475</code></pre>
<ul>
<li>Are under the ROC curve</li>
</ul>
<pre class="r"><code>test_results %&gt;% 
  roc_auc(actual_yes, Yes)</code></pre>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 roc_auc binary         0.839</code></pre>
<ul>
<li>Precision and recall</li>
</ul>
<pre class="r"><code>tibble(
    precision = test_results %&gt;% yardstick::precision(actual_yes, pred_classes_test) %&gt;% select(.estimate) %&gt;% as.numeric(),
    recall    = test_results %&gt;% yardstick::recall(actual_yes, pred_classes_test) %&gt;% select(.estimate) %&gt;% as.numeric()
)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   precision recall
##       &lt;dbl&gt;  &lt;dbl&gt;
## 1     0.847  0.897</code></pre>
<ul>
<li>F1-Statistic</li>
</ul>
<pre class="r"><code>test_results %&gt;% yardstick::f_meas(actual_yes, pred_classes_test, beta = 1)</code></pre>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 f_meas  binary         0.871</code></pre>
</div>
<div id="h2o" class="section level2">
<h2>H2O</h2>
<p>Shows an alternative to Keras!</p>
<ul>
<li>Initialise H2O instance and convert data to h2o frame</li>
</ul>
<pre class="r"><code>library(h2o)
h2o.init(nthreads = -1)</code></pre>
<pre><code>##  Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         4 minutes 37 seconds 
##     H2O cluster timezone:       Europe/Berlin 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.20.0.8 
##     H2O cluster version age:    2 months and 24 days  
##     H2O cluster name:           H2O_started_from_R_shiringlander_bzx929 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   3.39 GB 
##     H2O cluster total cores:    8 
##     H2O cluster allowed cores:  8 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.1 (2018-07-02)</code></pre>
<pre class="r"><code>h2o.no_progress()

train_hf &lt;- as.h2o(train_data)
valid_hf &lt;- as.h2o(valid_data)
test_hf &lt;- as.h2o(test_data)

response &lt;- &quot;Churn&quot;
features &lt;- setdiff(colnames(train_hf), response)</code></pre>
<pre class="r"><code># For binary classification, response should be a factor
train_hf[, response] &lt;- as.factor(train_hf[, response])
valid_hf[, response] &lt;- as.factor(valid_hf[, response])
test_hf[, response] &lt;- as.factor(test_hf[, response])</code></pre>
<pre class="r"><code>summary(train_hf$Churn, exact_quantiles = TRUE)</code></pre>
<pre><code>##  Churn  
##  0:3615 
##  1:1309</code></pre>
<pre class="r"><code>summary(valid_hf$Churn, exact_quantiles = TRUE)</code></pre>
<pre><code>##  Churn 
##  0:774 
##  1:280</code></pre>
<pre class="r"><code>summary(test_hf$Churn, exact_quantiles = TRUE)</code></pre>
<pre><code>##  Churn 
##  0:774 
##  1:280</code></pre>
<ul>
<li>Train model with <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html">AutoML</a>.</li>
</ul>
<blockquote>
<p>“During model training, you might find that the majority of your data belongs in a single class. For example, consider a binary classification model that has 100 rows, with 80 rows labeled as class 1 and the remaining 20 rows labeled as class 2. This is a common scenario, given that machine learning attempts to predict class 1 with the highest accuracy. It can also be an example of an imbalanced dataset, in this case, with a ratio of 4:1. The balance_classes option can be used to balance the class distribution. When enabled, H2O will either undersample the majority classes or oversample the minority classes. Note that the resulting model will also correct the final probabilities (“undo the sampling”) using a monotonic transform, so the predicted probabilities of the first model will differ from a second model. However, because AUC only cares about ordering, it won’t be affected. If this option is enabled, then you can also specify a value for the class_sampling_factors and max_after_balance_size options.” <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/balance_classes.html" class="uri">http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/balance_classes.html</a></p>
</blockquote>
<pre class="r"><code>aml &lt;- h2o.automl(x = features, 
                  y = response,
                  training_frame = train_hf,
                  validation_frame = valid_hf,
                  balance_classes = TRUE,
                  max_runtime_secs = 3600)

# View the AutoML Leaderboard
lb &lt;- aml@leaderboard

best_model &lt;- aml@leader

h2o.saveModel(best_model, &quot;/Users/shiringlander/Documents/Github/Data&quot;)</code></pre>
<ul>
<li>Prediction</li>
</ul>
<pre class="r"><code>pred &lt;- h2o.predict(best_model, test_hf[, -1])</code></pre>
<ul>
<li>Mean per class error</li>
</ul>
<pre class="r"><code>h2o.mean_per_class_error(best_model, train = TRUE, valid = TRUE, xval = TRUE)</code></pre>
<pre><code>##     train     valid      xval 
## 0.1717911 0.2172683 0.2350682</code></pre>
<ul>
<li>Confusion matrix on validation data</li>
</ul>
<pre class="r"><code>h2o.confusionMatrix(best_model, valid = TRUE)</code></pre>
<pre><code>## Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.258586059604449:
##          0   1    Error      Rate
## 0      478 130 0.213816  =130/608
## 1       49 173 0.220721   =49/222
## Totals 527 303 0.215663  =179/830</code></pre>
<pre class="r"><code>h2o.auc(best_model, train = TRUE)</code></pre>
<pre><code>## [1] 0.9039696</code></pre>
<pre class="r"><code>h2o.auc(best_model, valid = TRUE)</code></pre>
<pre><code>## [1] 0.8509068</code></pre>
<pre class="r"><code>h2o.auc(best_model, xval = TRUE)</code></pre>
<pre><code>## [1] 0.8397085</code></pre>
<ul>
<li>Performance and confusion matrix on test data</li>
</ul>
<pre class="r"><code>perf &lt;- h2o.performance(best_model, test_hf)
h2o.confusionMatrix(perf)</code></pre>
<pre><code>## Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.40790847476229:
##          0   1    Error       Rate
## 0      667 107 0.138243   =107/774
## 1       86 194 0.307143    =86/280
## Totals 753 301 0.183112  =193/1054</code></pre>
<ul>
<li>Plot performance</li>
</ul>
<pre class="r"><code>plot(perf)</code></pre>
<ul>
<li>More performance metrics extracted</li>
</ul>
<pre class="r"><code>h2o.logloss(perf)</code></pre>
<pre><code>## [1] 0.4008041</code></pre>
<pre class="r"><code>h2o.mse(perf)</code></pre>
<pre><code>## [1] 0.1278505</code></pre>
<pre class="r"><code>h2o.auc(perf)</code></pre>
<pre><code>## [1] 0.8622301</code></pre>
<pre class="r"><code>metrics &lt;- as.data.frame(h2o.metric(perf))
head(metrics)</code></pre>
<pre><code>##   threshold         f1          f2   f0point5  accuracy precision
## 1 0.8278177 0.01418440 0.008912656 0.03472222 0.7362429         1
## 2 0.8203439 0.02816901 0.017793594 0.06756757 0.7381404         1
## 3 0.8173635 0.04195804 0.026642984 0.09868421 0.7400380         1
## 4 0.8160146 0.04878049 0.031055901 0.11363636 0.7409867         1
## 5 0.8139018 0.05555556 0.035460993 0.12820513 0.7419355         1
## 6 0.8112067 0.07560137 0.048629531 0.16975309 0.7447818         1
##        recall specificity absolute_mcc min_per_class_accuracy
## 1 0.007142857           1   0.07249342            0.007142857
## 2 0.014285714           1   0.10261877            0.014285714
## 3 0.021428571           1   0.12580168            0.021428571
## 4 0.025000000           1   0.13594622            0.025000000
## 5 0.028571429           1   0.14540208            0.028571429
## 6 0.039285714           1   0.17074408            0.039285714
##   mean_per_class_accuracy tns fns fps tps tnr       fnr fpr         tpr
## 1               0.5035714 774 278   0   2   1 0.9928571   0 0.007142857
## 2               0.5071429 774 276   0   4   1 0.9857143   0 0.014285714
## 3               0.5107143 774 274   0   6   1 0.9785714   0 0.021428571
## 4               0.5125000 774 273   0   7   1 0.9750000   0 0.025000000
## 5               0.5142857 774 272   0   8   1 0.9714286   0 0.028571429
## 6               0.5196429 774 269   0  11   1 0.9607143   0 0.039285714
##   idx
## 1   0
## 2   1
## 3   2
## 4   3
## 5   4
## 6   5</code></pre>
<ul>
<li>Plot performance metrics</li>
</ul>
<pre class="r"><code>metrics %&gt;%
  gather(x, y, f1:tpr) %&gt;%
  ggplot(aes(x = threshold, y = y, group = x)) +
    facet_wrap(~ x, ncol = 2, scales = &quot;free&quot;) +
    geom_line()</code></pre>
<p><img src="/post/2018-12-12_customer_churn_code_files/figure-html/metrics-1.png" width="768" /></p>
<ul>
<li>Examine prediction thresholds</li>
</ul>
<pre class="r"><code>threshold &lt;- metrics[order(-metrics$accuracy), &quot;threshold&quot;][1]

finalRf_predictions &lt;- data.frame(actual = as.vector(test_hf$Churn), 
                                  as.data.frame(h2o.predict(object = best_model, 
                                                            newdata = test_hf)))

finalRf_predictions$accurate &lt;- ifelse(finalRf_predictions$actual == 
                                         finalRf_predictions$predict, &quot;ja&quot;, &quot;nein&quot;)

finalRf_predictions$predict_stringent &lt;- ifelse(finalRf_predictions$p1 &gt; threshold, 1, 
                                                ifelse(finalRf_predictions$p0 &gt; threshold, 0, &quot;unsicher&quot;))
finalRf_predictions$accurate_stringent &lt;- ifelse(finalRf_predictions$actual == 
                                                   finalRf_predictions$predict_stringent, &quot;ja&quot;, 
                                       ifelse(finalRf_predictions$predict_stringent == 
                                                &quot;unsicher&quot;, &quot;unsicher&quot;, &quot;nein&quot;))

finalRf_predictions %&gt;%
  group_by(actual, predict) %&gt;%
  dplyr::summarise(n = n())</code></pre>
<pre><code>## # A tibble: 4 x 3
## # Groups:   actual [?]
##   actual predict     n
##   &lt;fct&gt;  &lt;fct&gt;   &lt;int&gt;
## 1 0      0         602
## 2 0      1         172
## 3 1      0          63
## 4 1      1         217</code></pre>
<pre class="r"><code>finalRf_predictions %&gt;%
  group_by(actual, predict_stringent) %&gt;%
  dplyr::summarise(n = n())</code></pre>
<pre><code>## # A tibble: 6 x 3
## # Groups:   actual [?]
##   actual predict_stringent     n
##   &lt;fct&gt;  &lt;chr&gt;             &lt;int&gt;
## 1 0      0                   683
## 2 0      1                    63
## 3 0      unsicher             28
## 4 1      0                   101
## 5 1      1                   152
## 6 1      unsicher             27</code></pre>
<pre class="r"><code>finalRf_predictions %&gt;%
  gather(x, y, accurate, accurate_stringent) %&gt;%
  mutate(x = ifelse(x == &quot;accurate&quot;, &quot;Default Schwelle: 0.5&quot;, 
                    paste(&quot;Angepasste Schwelle:&quot;, round(threshold, digits = 2)))) %&gt;%
  ggplot(aes(x = actual, fill = y)) +
    facet_grid(~ x) +
    geom_bar(position = &quot;dodge&quot;) +
    scale_fill_tableau()</code></pre>
<p><img src="/post/2018-12-12_customer_churn_code_files/figure-html/default_vs_stringent-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>df &lt;- finalRf_predictions[, c(1, 3, 4)]

thresholds &lt;- seq(from = 0, to = 1, by = 0.1)

prop_table &lt;- data.frame(threshold = thresholds, 
                         prop_p0_true = NA, prop_p0_false = NA,
                         prop_p1_true = NA, prop_p1_false = NA)

for (threshold in thresholds) {

  pred_1 &lt;- ifelse(df$p1 &gt; threshold, 1, 0)
  pred_1_t &lt;- ifelse(pred_1 == df$actual, TRUE, FALSE)
  
  group &lt;- data.frame(df, 
                      &quot;pred_true&quot; = pred_1_t) %&gt;%
    group_by(actual, pred_true) %&gt;%
    dplyr::summarise(n = n())
  
  group_p0 &lt;- filter(group, actual == &quot;0&quot;)
  
  prop_p0_t &lt;- sum(filter(group_p0, pred_true == TRUE)$n) / sum(group_p0$n)
  prop_p0_f &lt;- sum(filter(group_p0, pred_true == FALSE)$n) / sum(group_p0$n)
  prop_table[prop_table$threshold == threshold, &quot;prop_p0_true&quot;] &lt;- prop_p0_t
  prop_table[prop_table$threshold == threshold, &quot;prop_p0_false&quot;] &lt;- prop_p0_f
  
  group_p1 &lt;- filter(group, actual == &quot;1&quot;)
  
  prop_p1_t &lt;- sum(filter(group_p1, pred_true == TRUE)$n) / sum(group_p1$n)
  prop_p1_f &lt;- sum(filter(group_p1, pred_true == FALSE)$n) / sum(group_p1$n)
  prop_table[prop_table$threshold == threshold, &quot;prop_p1_true&quot;] &lt;- prop_p1_t
  prop_table[prop_table$threshold == threshold, &quot;prop_p1_false&quot;] &lt;- prop_p1_f
}</code></pre>
<pre class="r"><code>prop_table %&gt;%
  gather(x, y, prop_p0_true, prop_p1_true) %&gt;%
  rename(Schwellenwert = threshold) %&gt;%
  mutate(x = ifelse(x == &quot;prop_p0_true&quot;, &quot;prop true p0&quot;,
         &quot;prop true p1&quot;)) %&gt;%
  ggplot(aes(x = Schwellenwert, y = y, color = x)) +
    geom_point() +
    geom_line() +
    scale_color_tableau()</code></pre>
<p><img src="/post/2018-12-12_customer_churn_code_files/figure-html/prop_table-1.png" width="576" style="display: block; margin: auto;" /></p>
<div id="costrevenue-calculation" class="section level3">
<h3>Cost/revenue calculation</h3>
<p>Let’s assume that</p>
<ol style="list-style-type: decimal">
<li>a marketing campaign + employee time will cost the company 1000€ per year for every customer that is included in the campaign.</li>
<li>the annual average revenue per customer is 2000€ (in more complex scenarios customers could be further divided into revenue groups to calculate how “valuable” they are and how harmful loosing them would be)</li>
<li>investing into unnecessary marketing doesn’t cause churn by itself (i.e. a customer who isn’t going to churn isn’t reacting negatively to the add campaign - which could happen in more complex scenarios).</li>
<li>without a customer churn model the company would target half of their customer (by chance) for ad-campaigns</li>
<li>without a customer churn model the company would lose about 25% of their customers to churn</li>
</ol>
<p>This would mean that compared to no intervention we would have</p>
<ul>
<li>prop_p0_true == customers who were correctly predicted to not churn did not cost anything (no marketing money was spent): +/-0€</li>
<li>prop_p0_false == customers that did not churn who are predicted to churn will be an empty investment: +/-0€ - 1500€</li>
<li>prop_p1_false == customer that were predicted to stay but churned: -2000€</li>
<li>prop_p1_true == customers that were correctly predicted to churn:</li>
<li>let’s say 100% of those could be kept by investing into marketing: +2000€ -1500€</li>
<li>let’s say 50% could be kept by investing into marketing: +2000€ * 0.5 -1500€</li>
</ul>
<p><br></p>
<ul>
<li>Let’s play around with some values:</li>
</ul>
<pre class="r"><code># Baseline
revenue &lt;- 2000
cost &lt;- 1000

customers_churn &lt;- filter(test_data, Churn == 1)
customers_churn_n &lt;- nrow(customers_churn)

customers_no_churn &lt;- filter(filter(test_data, Churn == 0))
customers_no_churn_n &lt;- nrow(customers_no_churn)

customers &lt;- customers_churn_n + customers_no_churn_n

ad_target_rate &lt;- 0.5
ad_cost_default &lt;- customers * ad_target_rate * cost

churn_rate_default &lt;- customers_churn_n / customers_no_churn_n
ann_revenue_default &lt;- customers_no_churn_n * revenue

net_win_default &lt;- ann_revenue_default - ad_cost_default
net_win_default</code></pre>
<pre><code>## [1] 1021000</code></pre>
<ul>
<li>How much revenue can we gain from predicting customer churn (assuming conversionr rate of 0.7):</li>
</ul>
<pre class="r"><code>conversion &lt;- 0.7

net_win_table &lt;- prop_table %&gt;%
  mutate(prop_p0_true_X = prop_p0_true * customers_no_churn_n * revenue,
         prop_p0_false_X = prop_p0_false * customers_no_churn_n * (revenue -cost),
         prop_p1_false_X = prop_p1_false * customers_churn_n * 0,
         prop_p1_true_X = prop_p1_true * customers_churn_n * ((revenue * conversion) - cost)) %&gt;%
  group_by(threshold) %&gt;%
  summarise(net_win = sum(prop_p0_true_X + prop_p0_false_X + prop_p1_false_X + prop_p1_true_X),
            net_win_compared = net_win - net_win_default) %&gt;%
  arrange(-net_win_compared)

net_win_table</code></pre>
<pre><code>## # A tibble: 11 x 3
##    threshold net_win net_win_compared
##        &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;
##  1       0.7 1558000           537000
##  2       0.8 1554000           533000
##  3       0.6 1551000           530000
##  4       0.9 1548000           527000
##  5       1   1548000           527000
##  6       0.5 1534000           513000
##  7       0.4 1515600           494600
##  8       0.3 1483400           462400
##  9       0.2 1417200           396200
## 10       0.1 1288200           267200
## 11       0    886000          -135000</code></pre>
</div>
</div>
<div id="lime" class="section level2">
<h2>LIME</h2>
<ul>
<li>Explaining predictions</li>
</ul>
<pre class="r"><code>Xtrain &lt;- as.data.frame(train_hf)
Xtest &lt;- as.data.frame(test_hf)

# run lime() on training set
explainer &lt;- lime::lime(x = Xtrain, 
                        model = best_model)

# run explain() on the explainer
explanation &lt;- lime::explain(x = Xtest[1:9, ], 
                             explainer = explainer, 
                             n_labels = 1,
                             n_features = 4,
                             kernel_width = 0.5)</code></pre>
<pre class="r"><code>plot_explanations(explanation)</code></pre>
<p><img src="/post/2018-12-12_customer_churn_code_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
<pre class="r"><code>explanation %&gt;%
  plot_features(ncol = 3)</code></pre>
<p><img src="/post/2018-12-12_customer_churn_code_files/figure-html/unnamed-chunk-50-1.png" width="1248" /></p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS  10.14.2
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] h2o_3.20.0.8    bindrcpp_0.2.2  corrplot_0.84   ggthemes_4.0.1 
##  [5] yardstick_0.0.2 recipes_0.1.4   rsample_0.0.3   lime_0.4.1     
##  [9] keras_2.2.4     mice_3.3.0      caret_6.0-81    lattice_0.20-38
## [13] forcats_0.3.0   stringr_1.3.1   dplyr_0.7.8     purrr_0.2.5    
## [17] readr_1.2.1     tidyr_0.8.2     tibble_1.4.2    ggplot2_3.1.0  
## [21] tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##  [1] minqa_1.2.4        colorspace_1.3-2   class_7.3-14      
##  [4] base64enc_0.1-3    rstudioapi_0.8     prodlim_2018.04.18
##  [7] fansi_0.4.0        lubridate_1.7.4    xml2_1.2.0        
## [10] codetools_0.2-15   splines_3.5.1      knitr_1.21        
## [13] shinythemes_1.1.2  zeallot_0.1.0      jsonlite_1.6      
## [16] nloptr_1.2.1       pROC_1.13.0        broom_0.5.1       
## [19] tfruns_1.4         shiny_1.2.0        compiler_3.5.1    
## [22] httr_1.3.1         backports_1.1.2    assertthat_0.2.0  
## [25] Matrix_1.2-15      lazyeval_0.2.1     cli_1.0.1         
## [28] later_0.7.5        htmltools_0.3.6    tools_3.5.1       
## [31] gtable_0.2.0       glue_1.3.0         reshape2_1.4.3    
## [34] Rcpp_1.0.0         cellranger_1.1.0   nlme_3.1-137      
## [37] blogdown_0.9       iterators_1.0.10   timeDate_3043.102 
## [40] gower_0.1.2        xfun_0.4           lme4_1.1-19       
## [43] rvest_0.3.2        mime_0.6           pan_1.6           
## [46] MASS_7.3-51.1      scales_1.0.0       ipred_0.9-8       
## [49] hms_0.4.2          promises_1.0.1     parallel_3.5.1    
## [52] yaml_2.2.0         reticulate_1.10    rpart_4.1-13      
## [55] stringi_1.2.4      tensorflow_1.10    foreach_1.4.4     
## [58] lava_1.6.4         bitops_1.0-6       rlang_0.3.0.1     
## [61] pkgconfig_2.0.2    evaluate_0.12      bindr_0.1.1       
## [64] labeling_0.3       htmlwidgets_1.3    tidyselect_0.2.5  
## [67] plyr_1.8.4         magrittr_1.5       bookdown_0.8      
## [70] R6_2.3.0           generics_0.0.2     mitml_0.3-6       
## [73] pillar_1.3.0       haven_2.0.0        whisker_0.3-2     
## [76] withr_2.1.2        RCurl_1.95-4.11    survival_2.43-3   
## [79] nnet_7.3-12        modelr_0.1.2       crayon_1.3.4      
## [82] jomo_2.6-5         utf8_1.1.4         rmarkdown_1.11    
## [85] grid_3.5.1         readxl_1.1.0       data.table_1.11.8 
## [88] ModelMetrics_1.2.2 digest_0.6.18      xtable_1.8-3      
## [91] httpuv_1.4.5       stats4_3.5.1       munsell_0.5.0     
## [94] glmnet_2.0-16</code></pre>
</div>
</div>
