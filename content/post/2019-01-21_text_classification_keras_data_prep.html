---
title: "How to prepare data for NLP (text classification) with Keras and TensorFlow"
draft: true
author: Shirin Glander
date: '2019-01-21'
categories: ["R", "Keras"]
tags: ["machine learning", "keras", "tensorflow", "nlp"]
thumbnailImagePosition: left
thumbnailImage: https://shiring.github.io/netlify_images/book-eyeglasses-eyewear-261857.jpg
metaAlignment: center
coverMeta: out
slug: text_classification_keras_data_prep
---



<p>In the past, I have written and taught quite a bit about image classification with Keras (<a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits/">e.g. here</a>). Text classification isn’t too different in terms of using the Keras principles to train a sequential or function model. You can even use Convolutional Neural Nets (CNNs) for text classification.</p>
<p>What is very different, however, is how to prepare raw text data for modeling. When you look at the <a href="https://jjallaire.github.io/deep-learning-with-r-notebooks/notebooks/3.4-classifying-movie-reviews.nb.html">IMDB example from the Deep Learning with R Book</a>, you get a great explanation of how to train the model. But because the <strong>preprocessed</strong> IMDB dataset comes with the <code>keras</code> package, it isn’t so straight-forward to use what you learned on your own data.</p>
<div id="how-can-a-computer-work-with-text" class="section level2">
<h2>How can a computer work with text?</h2>
<p>As with any neural network, we need to convert our data into a numeric format; in Keras and TensorFlow we work with tensors. The IMDB example data from the <code>keras</code> package has been preprocessed to a list of integers, where every integer corresponds to a word arranged by descending word frequency.</p>
<p>So, how do we make it from raw text to such a list of integers? Luckily, Keras offers a few convenience functions that make our lives much easier.</p>
<pre class="r"><code>library(keras)
library(tidyverse)</code></pre>
</div>
<div id="data" class="section level2">
<h2>Data</h2>
<p>In the example below, I am using a Kaggle dataset: <a href="https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews">Women’s e-commerce cloting reviews</a>. The data contains a text review of different items of clothing, as well as some additional information, like rating, division, etc. I will use the review title and text in order to classify whether or not the item was liked. I am creating the response variable from the rating: every item rates with 5 stars is considered “liked” (1), the rest as “not liked” (0). I am also combining review title and text.</p>
<pre class="r"><code>clothing_reviews &lt;- read_csv(&quot;/Users/shiringlander/Documents/Github/ix_lime_etc/Womens Clothing E-Commerce Reviews.csv&quot;) %&gt;%
  mutate(Liked = ifelse(Rating == 5, 1, 0),
         text = paste(Title, `Review Text`),
         text = gsub(&quot;NA&quot;, &quot;&quot;, text))</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   X1 = col_double(),
##   `Clothing ID` = col_double(),
##   Age = col_double(),
##   Title = col_character(),
##   `Review Text` = col_character(),
##   Rating = col_double(),
##   `Recommended IND` = col_double(),
##   `Positive Feedback Count` = col_double(),
##   `Division Name` = col_character(),
##   `Department Name` = col_character(),
##   `Class Name` = col_character()
## )</code></pre>
<pre class="r"><code>glimpse(clothing_reviews)</code></pre>
<pre><code>## Observations: 23,486
## Variables: 13
## $ X1                        &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, …
## $ `Clothing ID`             &lt;dbl&gt; 767, 1080, 1077, 1049, 847, 1080, 858,…
## $ Age                       &lt;dbl&gt; 33, 34, 60, 50, 47, 49, 39, 39, 24, 34…
## $ Title                     &lt;chr&gt; NA, NA, &quot;Some major design flaws&quot;, &quot;My…
## $ `Review Text`             &lt;chr&gt; &quot;Absolutely wonderful - silky and sexy…
## $ Rating                    &lt;dbl&gt; 4, 5, 3, 5, 5, 2, 5, 4, 5, 5, 3, 5, 5,…
## $ `Recommended IND`         &lt;dbl&gt; 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,…
## $ `Positive Feedback Count` &lt;dbl&gt; 0, 4, 0, 0, 6, 4, 1, 4, 0, 0, 14, 2, 2…
## $ `Division Name`           &lt;chr&gt; &quot;Initmates&quot;, &quot;General&quot;, &quot;General&quot;, &quot;Ge…
## $ `Department Name`         &lt;chr&gt; &quot;Intimate&quot;, &quot;Dresses&quot;, &quot;Dresses&quot;, &quot;Bot…
## $ `Class Name`              &lt;chr&gt; &quot;Intimates&quot;, &quot;Dresses&quot;, &quot;Dresses&quot;, &quot;Pa…
## $ Liked                     &lt;dbl&gt; 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,…
## $ text                      &lt;chr&gt; &quot; Absolutely wonderful - silky and sex…</code></pre>
<p>Whether an item was liked or not will be the response variable or label for classification of the reviews.</p>
<pre class="r"><code>clothing_reviews %&gt;%
  ggplot(aes(x = factor(Liked), fill = Liked)) +
    geom_bar(alpha = 0.8) +
    guides(fill = FALSE)</code></pre>
<p><img src="/post/2019-01-21_text_classification_keras_data_prep_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="tokenizers" class="section level2">
<h2>Tokenizers</h2>
<p>The first step is to tokenize the text. This means, converting our text into a sequence of integers where each integer corresponds to a word in the dictionary.</p>
<pre class="r"><code>text &lt;- clothing_reviews$text</code></pre>
<p>The <code>num_words</code> argument defines the number of words we want to consider (this will be our feature space). Because the output integers will be sorted according to decreasing word frequency, if we set 1000, we will only get the 1000 most frequent words in our corpus.</p>
<pre class="r"><code>max_features &lt;- 1000
tokenizer &lt;- text_tokenizer(num_words = max_features)</code></pre>
<p>Next, we need to fit the tokenizer to our text data. Note, that the <code>tokenizer</code> object is modified in place (as are models in Keras)!</p>
<pre class="r"><code>tokenizer %&gt;% 
  fit_text_tokenizer(text)</code></pre>
<p>After fitting the tokenizer, we can extract the following information: the number of documents …</p>
<pre class="r"><code>tokenizer$document_count</code></pre>
<pre><code>## [1] 23486</code></pre>
<p>… and the word-index list. Notice, that even though we set the maximum number of words to 1000, our index contains many more words. In fact, the index will keep all words in the index but when converting our reviews to vectors, the stored value <code>tokenizer$num_words</code> will be used to restrict to the most common words.</p>
<pre class="r"><code>tokenizer$word_index %&gt;%
  head()</code></pre>
<pre><code>## $raining
## [1] 13788
## 
## $yellow
## [1] 553
## 
## $four
## [1] 1501
## 
## $bottons
## [1] 7837
## 
## $woods
## [1] 7896
## 
## $`friend&#39;s`
## [1] 3525</code></pre>
<p>We now have the dictionary of integers and which words they should replace in our text. But we still don’t have a list of integers for our reviews. So, now we use the <code>texts_to_sequences</code> functions, which will do just that! Words, which weren’t among the top 1000 were excluded.</p>
<pre class="r"><code>text_seqs &lt;- texts_to_sequences(tokenizer, text)
text_seqs %&gt;%
  head()</code></pre>
<pre><code>## [[1]]
## [1] 249 494 924   3 595   3  63
## 
## [[2]]
##  [1]  19   7  17  35  84   2   8 221   5   9   4 114   3  37 328   2 135
## [18]   2 421  43  25  57   5 139  35  95   2  75   4  95   3  39 518   2
## [35]  19   1  88  11  31 423  38   4  56 474   1 401  43 160  30   4 132
## [52]  11 447 444   6 761  95
## 
## [[3]]
##  [1] 156 134   2  68 314 180  12   7  17   3  53 183   5   8  98  12  31
## [18]   2  57   1  95  42  18 240  22  10   2 230   7   8  30  42  15  42
## [35]   9 683  21   2 122  20 803   5  45   2   5   9  95  99  86  16  38
## [52] 581 256   1  24 673  16  63   3  26 267  10   1 182 673  68   4  23
## [69] 148 285 489   3 543 738 481 157 997   4 134  16   1 157 489 846 326
## [86]   1 455   5 706
## 
## [[4]]
##  [1]  18 292 220   2  19  19  19   7 592  35 209   3 652 310 189   2  33
## [18]   5   2 120 530  10  27 212
## 
## [[5]]
##  [1]  55  71   7  71   6  23  55   8  76 504   8   1 163 484   5   6   1
## [18]  49  88   8  33  14 262   3   5   6  15   5 855  64  14 257 376  19
## [35]   7  71
## 
## [[6]]
##  [1]  20  12   1  23  95   2  19 244  10   7  60   6  20  12   1  23  95
## [18]   2  39  38 285 278 324   3 115  33   4   9   7 492   7  17  16  23
## [35]  84  66  13   1  10 250   4 245  13  17   1 100   6  90   3  23 321
## [52]  15   5  18  42 428  20   4   8   3   1 100  43 378 506 111   1  13
## [69]   1   2  19   1  46   3   1 686  13   1 124  10   5  38 135  20  98
## [86]  11  31   2 370   7  17</code></pre>
<p>So, there we have it! From here on out, we can simply follow the <a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_cnn.R">IMDB example from the Keras documentation</a>:</p>
<pre class="r"><code># Set parameters:
maxlen &lt;- 100
batch_size &lt;- 32
embedding_dims &lt;- 50
filters &lt;- 64
kernel_size &lt;- 3
hidden_dims &lt;- 50
epochs &lt;- 5</code></pre>
<p>Because we can’t directly use this list of integers in our neural network, there is still some preprocessing to do. In the IMDB example, the lists are padded so that they all have the same length. The <code>pad_sequences</code> function will return a matrix, with columns for a given maximum number of words (or the number of words in the longest sequence). Here, we have 400 columns in our matrix. Reviews with fewer words were padded with zeros at the beginning before the indices. Longer reviews are cut after 400 words.</p>
<pre class="r"><code>x_train &lt;- text_seqs %&gt;%
  pad_sequences(maxlen = maxlen)
dim(x_train)</code></pre>
<pre><code>## [1] 23486   100</code></pre>
<p>Our response variable will be encoded with 1s (5-star review) and 0s (not 5-star reviews). Because we have a binary outcome, we only need this one vector.</p>
<pre class="r"><code>y_train &lt;- clothing_reviews$Liked
length(y_train)</code></pre>
<pre><code>## [1] 23486</code></pre>
</div>
<div id="embeddings" class="section level2">
<h2>Embeddings</h2>
<p>These padded word index matrices now need to be converted into something that gives information about the features (i.e. words) in a way that can be used for learning. Currently, the state-of-the-art for text models are word embeddings or word vectors, which are learned from the text data. Word embeddings encode the context of words in relatively few dimensions while maximizing the information that is contained in these vectors. Basically, word embeddings are values that are learned by a neural net just as <a href="https://shirinsplayground.netlify.com/2018/11/neural_nets_explained/">weights are learned by a multi-layer perceptron</a>.</p>
<p>Word embedding vectors represent the words and their contexts; thus, words with similar meanings (synonyms) or with close semantic relationships will have more similar embeddings. Moreover, word embeddings should reflect how words are related to each other. For example, the embeddings for “man” should be to “king” as “woman” is to “queen”.</p>
<p>In our model below, we want to learn the word embeddings from our (padded) word vectors and directly use these learned embeddings for classification. This part can now be the same as in the <a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_lstm.R">Keras examples for LSTMs</a> and <a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_cnn_lstm.R">CNNs</a></p>
<pre class="r"><code>model &lt;- keras_model_sequential() %&gt;% 
  layer_embedding(max_features, embedding_dims, input_length = maxlen) %&gt;%
  layer_dropout(0.2) %&gt;%
  layer_conv_1d(
    filters, kernel_size, 
    padding = &quot;valid&quot;, activation = &quot;relu&quot;, strides = 1
  ) %&gt;%
  layer_global_max_pooling_1d() %&gt;%
  layer_dense(hidden_dims) %&gt;%
  layer_dropout(0.2) %&gt;%
  layer_activation(&quot;relu&quot;) %&gt;%
  layer_dense(1) %&gt;%
  layer_activation(&quot;sigmoid&quot;) %&gt;% compile(
  loss = &quot;binary_crossentropy&quot;,
  optimizer = &quot;adam&quot;,
  metrics = &quot;accuracy&quot;
)</code></pre>
<pre class="r"><code>model %&gt;%
  fit(
    x_train,
    y_train,
    batch_size = batch_size,
    epochs = epochs,
    validation_split = 0.3
  )</code></pre>
</div>
<div id="alternative-preprocessing-functions" class="section level2">
<h2>Alternative preprocessing functions</h2>
<p>The above example follows the IMDB example from the Keras documentation, but there are alternative ways to preprocess your text for modeling with Keras:</p>
<ul>
<li><a href="https://jjallaire.github.io/deep-learning-with-r-notebooks/notebooks/6.1-one-hot-encoding-of-words-or-characters.nb.html">one-hot-encoding</a></li>
</ul>
<pre class="r"><code>one_hot_results &lt;- texts_to_matrix(tokenizer, text, mode = &quot;binary&quot;)
dim(one_hot_results)</code></pre>
<pre><code>## [1] 23486  1000</code></pre>
<ul>
<li><a href="https://jjallaire.github.io/deep-learning-with-r-notebooks/notebooks/6.1-one-hot-encoding-of-words-or-characters.nb.html">one-hot hashing</a></li>
</ul>
<pre class="r"><code>hashing_results &lt;- text_hashing_trick(text[1], n = 100)
hashing_results</code></pre>
<pre><code>## [1] 88 75 18 90  7 90 23</code></pre>
</div>
<div id="pretrained-embeddings" class="section level2">
<h2>Pretrained embeddings</h2>
<p>Here, we have learned word embeddings from our word vectors and directly used the output of the embedding layers as input for additional layers in our neural net. Because learning embeddings takes time and computational power, we could also start with <a href="https://keras.rstudio.com/articles/examples/pretrained_word_embeddings.html">pre-trained embeddings</a>, particulary if we don’t have a whole lot of training data. You can find an example for how to use <a href="https://jjallaire.github.io/deep-learning-with-r-notebooks/notebooks/6.1-using-word-embeddings.nb.html">GloVe embeddings here</a>.</p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS  10.14.2
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] bindrcpp_0.2.2  forcats_0.3.0   stringr_1.3.1   dplyr_0.7.8    
##  [5] purrr_0.2.5     readr_1.3.1     tidyr_0.8.2     tibble_2.0.1   
##  [9] ggplot2_3.1.0   tidyverse_1.2.1 keras_2.2.4    
## 
## loaded via a namespace (and not attached):
##  [1] reticulate_1.10  tidyselect_0.2.5 xfun_0.4         haven_2.0.0     
##  [5] lattice_0.20-38  colorspace_1.4-0 generics_0.0.2   htmltools_0.3.6 
##  [9] yaml_2.2.0       base64enc_0.1-3  utf8_1.1.4       rlang_0.3.1     
## [13] pillar_1.3.1     withr_2.1.2      glue_1.3.0       readxl_1.2.0    
## [17] modelr_0.1.2     bindr_0.1.1      plyr_1.8.4       tensorflow_1.10 
## [21] cellranger_1.1.0 munsell_0.5.0    blogdown_0.10    gtable_0.2.0    
## [25] rvest_0.3.2      evaluate_0.12    labeling_0.3     knitr_1.21      
## [29] tfruns_1.4       fansi_0.4.0      broom_0.5.1      Rcpp_1.0.0      
## [33] backports_1.1.3  scales_1.0.0     jsonlite_1.6     hms_0.4.2       
## [37] digest_0.6.18    stringi_1.2.4    bookdown_0.9     grid_3.5.1      
## [41] cli_1.0.1        tools_3.5.1      magrittr_1.5     lazyeval_0.2.1  
## [45] crayon_1.3.4     whisker_0.3-2    pkgconfig_2.0.2  zeallot_0.1.0   
## [49] Matrix_1.2-15    xml2_1.2.0       lubridate_1.7.4  rstudioapi_0.9.0
## [53] assertthat_0.2.0 rmarkdown_1.11   httr_1.4.0       R6_2.3.0        
## [57] nlme_3.1-137     compiler_3.5.1</code></pre>
</div>
