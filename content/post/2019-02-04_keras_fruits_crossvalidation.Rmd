---
title: "How to use cross-validation with the image data generator in Keras and TensorFlow"
draft: true
author: Shirin Glander
date: '2019-02-04'
categories: ["R", "keras"]
tags: ["R", "keras", "image classification", "tensorflow"]
thumbnailImagePosition: left
thumbnailImage: 
metaAlignment: center
coverMeta: out
slug: keras_fruits_crossvalidation
---

> I've been using keras and TensorFlow for a while now - and love its simplicity and straight-forward way to modeling. As part of the latest update to my [Workshop about deep learning with R and keras](https://shirinsplayground.netlify.com/2018/05/deep_learning_keras_tensorflow_18_07/) I've added a new example analysis:

https://shirinsplayground.netlify.com/2018/06/keras_fruits/

```{r}
library(keras)
library(tidyverse)
```

```{r}
# list of fruits to modle
fruit_list <- c("Kiwi", "Banana", "Apricot", "Avocado", "Cocos", "Clementine", "Mandarine", "Orange",
                "Limes", "Lemon", "Peach", "Plum", "Raspberry", "Strawberry", "Pineapple", "Pomegranate")

# number of output classes (i.e. fruits)
output_n <- length(fruit_list)

# image size to scale down to (original images are 100 x 100 px)
img_width <- 20
img_height <- 20
target_size <- c(img_width, img_height)

# RGB = 3 channels
channels <- 3

# path to image folders
train_image_files_path <- "/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Training/"
```

```{r}
train_data_gen = image_data_generator(
  rescale = 1/255,
  validation_split = 0.3
)
```

```{r}
# training images
train_image_array_gen <- flow_images_from_directory(train_image_files_path, 
                                          train_data_gen,
                                          subset = 'training',
                                          target_size = target_size,
                                          class_mode = "categorical",
                                          classes = fruit_list,
                                          seed = 42)

# validation images
valid_image_array_gen <- flow_images_from_directory(train_image_files_path, 
                                          train_data_gen,
                                          subset = 'validation',
                                          target_size = target_size,
                                          class_mode = "categorical",
                                          classes = fruit_list,
                                          seed = 42)
```

```{r}
cat("Number of images per class:")
table(factor(train_image_array_gen$classes))

cat("\nClass label vs index mapping:\n")
train_image_array_gen$class_indices
```

### Define model

```{r}
# number of training samples
train_samples <- train_image_array_gen$n
# number of validation samples
valid_samples <- valid_image_array_gen$n

# define batch size and number of epochs
batch_size <- 32
epochs <- 10
```

```{r warning=FALSE, message=FALSE}
# initialise model
model <- keras_model_sequential()

# add layers
model %>%
  layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = "same", input_shape = c(img_width, img_height, channels)) %>%
  layer_activation("relu") %>%
  
  # Second hidden layer
  layer_conv_2d(filter = 16, kernel_size = c(3,3), padding = "same") %>%
  layer_activation_leaky_relu(0.5) %>%
  layer_batch_normalization() %>%

  # Use max pooling
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_dropout(0.25) %>%
  
  # Flatten max filtered output into feature vector 
  # and feed into dense layer
  layer_flatten() %>%
  layer_dense(100) %>%
  layer_activation("relu") %>%
  layer_dropout(0.5) %>%

  # Outputs from dense layer are projected onto output layer
  layer_dense(output_n) %>% 
  layer_activation("softmax")

# compile
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(lr = 0.0001, decay = 1e-6),
  metrics = "accuracy"
)
```

```{r}
# fit
hist <- model %>% fit_generator(
  # training data
  train_image_array_gen,
  
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = epochs, 
  
  # validation data
  validation_data = valid_image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size)
)
```

```{r}
plot(hist)
```

```{r}
test_image_files_path = "/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Validation/"

test_datagen <- image_data_generator(rescale = 1/255)

test_generator <- flow_images_from_directory(
        test_image_files_path,
        test_datagen,
        target_size = target_size,
        class_mode = "categorical",
        classes = fruit_list,
        seed = 42)
```

```{r}
indices <- train_image_array_gen$class_indices %>%
  as.data.frame() %>%
  gather() %>%
  arrange(value)
```

```{r}
predictions <- as.data.frame(predict_generator(model, test_generator, steps = as.integer(test_generator$n / batch_size)))
```

```{r}
colnames(predictions) <- indices$key
predictions <- predictions %>%
  mutate(truth_idx = test_generator$classes) %>%
  left_join(indices, by = c("truth_idx" = "value")) %>%
  rename(truth_lbl = key)
```

```{r fig.height=10, eval=FALSE}
predictions %>%
  mutate(id = seq(1:test_generator$n)) %>%
  gather(pred_lbl, y, Kiwi:Pomegranate) %>%
  ggplot(aes(x = id, y = y), color = truth_lbl) +
    facet_wrap( ~ pred_lbl, scales = "free", ncol = 2) +
    geom_jitter()
```

```{r}
evaluate_generator(model, test_generator, steps = as.integer(test_generator$n / batch_size))
```

---

```{r}
sessionInfo()
```


