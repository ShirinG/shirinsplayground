---
title: "codecentric.AI Thug Cat Project in R: keypoint regression with Keras"
draft: true
author: Shirin Glander
date: '2019-07-01'
categories: ["R", "codecentric.AI"]
tags: ["R", "codecentric.AI", "Keras", "TensorFlow"]
thumbnailImagePosition: left
thumbnailImage: 
metaAlignment: center
coverMeta: out
slug: thug_cat_r
---

https://bootcamp.codecentric.ai/

Module 5: Neural Nets and Deep Learning
Lesson 5.10: Thug Cat Project
https://bootcamp.codecentric.ai/dl-course/module/5/43/

## The data

https://www.kaggle.com/crawford/cat-dataset

> The CAT dataset includes over 9,000 cat images. For each image, there are annotations of the head of cat with nine points, two for eyes, one for mouth, and six for ears.

> Weiwei Zhang, Jian Sun, and Xiaoou Tang, Cat Head Detection - How to Effectively Exploit Shape and Texture Features, Proc. of European Conf. Computer Vision, vol. 4, pp.802-816, 2008.

> The annotation data are stored in a file with the name of the corresponding image plus ."cat" at the end. There is one annotation file for each cat image. For each annotation file, the annotation data are stored in the following sequence:

- number of points
- Left Eye
- Right Eye
- Mouth
- Left Ear-1
- Left Ear-2
- Left Ear-3
- Right Ear-1
- Right Ear-2
- Right Ear-3

```{r}
path_to_images <- "/Users/shiringlander/Documents/Github/Data/cats/"
list_of_cat_images <- list.files(path_to_images, full.names = TRUE, recursive = TRUE, include.dirs = TRUE, pattern = "*.jpg$")
list_of_cat_annos <- list.files(path_to_images, full.names = TRUE, recursive = TRUE, include.dirs = TRUE, pattern = "*.cat$")

length(list_of_cat_images) == length(list_of_cat_annos)
length(list_of_cat_images)
head(list_of_cat_images)
```

```{r message = FALSE, warning=FALSE}
library(imager)
img <- list_of_cat_images[1]
image <- load.image(img)
dim(image)

library(readr)
keypoints = c("no_kps", 
              "lft_eye_x", "lft_eye_y", 
              "rgt_eye_x", "rgt_eye_y",
              "mouth_x", "mouth_y",
              "lft_ear_1_x", "lft_ear_1_y",
              "lft_ear_2_x", "lft_ear_2_y",
              "lft_ear_3_x", "lft_ear_3_y", 
              "rgt_ear_1_x", "rgt_ear_1_y", 
              "rgt_ear_2_x", "rgt_ear_2_y",
              "rgt_ear_3_x", "rgt_ear_3_y")

pts <- gsub(".jpg", ".jpg.cat", img)
points <- read_delim(pts, delim = " ", col_names = keypoints)

plot(image)
points(points[, grepl("_x", colnames(points))], points[, grepl("_y", colnames(points))])
```

### Crop images to same height and width and combine into one array

- Let's start with predicting only the mouth
- We want to crop, so some of the keypoints will be cut off; these images we want to exclude, so let's find them and write them into a new vector

```{r}
cutoff <- 250
```

```{r}
return_keypoints <- function(filename){
  pts <- gsub(".jpg", ".jpg.cat", filename)
  points <- read_delim(pts, delim = " ", col_names = keypoints) %>%
    slice(1) %>% 
    unlist(., use.names = TRUE)
  return(points)
}
```

```{r warning=FALSE, message=FALSE, eval=FALSE}
images_to_use <- c()

for(i in 1:length(list_of_cat_images)) {
  
  img <- list_of_cat_images[i]
  img_id <- gsub("/Users/shiringlander/Documents/Github/Data/cats//", "", img)
  kps_mouth <- return_keypoints(img)[6:7]
  
  if (any(kps_mouth > cutoff)) {
    cat("Mouth keypoints of image", img_id, "not in cutoff\n")
  } else {
    images_to_use <- c(images_to_use, img_id)
  }
}

length(images_to_use) #5541
saveRDS(images_to_use, file = "/Users/shiringlander/Documents/Github/shirinsplayground/data/images_to_use.rds")
```

- Let's load these images, convert them to an array and combine them into one array

```{r}
images_to_use <- readRDS(file = "/Users/shiringlander/Documents/Github/shirinsplayground/data/images_to_use.rds")

library(abind)

for(i in 1:length(images_to_use)) {
  image <- readImage(paste0("/Users/shiringlander/Documents/Github/Data/cats//", images_to_use[i]))
  #countcolors::plotArrayAsImage(image)
  img_id <- images_to_use[i]
  
  if (all(dim(image)[1:2] >= cutoff))
  image_crop <- cropImage(image, new_width = 1:cutoff, new_height = 1:cutoff, type = 'user_defined')
  
  if (i == 1) {
    image_crop_array <- abind(image_crop, along = 0)
    attr(image_crop_array, "dimnames")[[1]] <- c(img_id)
  } else {
    image_crop <- abind(image_crop, along = 0)
    image_crop_array <- abind(image_crop_array, image_crop, along = 1)
    attr(image_crop_array, "dimnames")[[1]][i] <- c(img_id)
  }
}
dim(image_crop_array)
str(image_crop_array)
```





## Predict only mouth

- "data.frame containing the filepaths relative to directory (or absolute paths if directory is NULL) of the images in a character column."
- "if class_mode is "other" it should contain the columns specified in y_col ("other": array of y_col data)."

```{r}
return_img_w <- function(filename){
  image <- load.image(filename)
  img_w <- dim(image)[1]
  return(img_w)
}

return_img_h <- function(filename){
  image <- load.image(filename)
  img_h <- dim(image)[2]
  return(img_h)
}
```

```{r message = FALSE, warning=FALSE, eval=FALSE}
library(tidyverse)
img_data_frame <- tibble(filename = list_of_cat_images[1:100]) %>%
  mutate(img_w = map_dbl(filename, return_img_w),
         img_h = map_dbl(filename, return_img_h),
         keypoints = map(filename, return_keypoints),
         kpt_mouth_x = map_dbl(keypoints, "mouth_x"),
         kpt_mouth_y = map_dbl(keypoints, "mouth_y")
)
head(img_data_frame)
```

```{r}
library(keras)
img_gen <- image_data_generator(rescale = 1/255, 
                                validation_split = 0.3)
```

```{r}
img_gen_flow <- flow_images_from_dataframe(img_data_frame, 
                           directory = NULL,
                           class_mode = "other",
                           x_col = "filename", 
                           y_col = c("kpt_mouth_x", "kpt_mouth_y"),
                           generator = img_gen, 
                           target_size = c(224, 224))
```

```{r}
feature_extractor <- application_resnet50(
  include_top = FALSE,
  input_shape = c(224, 224, 3)
)
```

```{r}
model_input <- layer_input(shape = c(224, 224, 3), dtype = 'float32', name = 'image_input')
main_network <- 
  model_input %>%
  layer_dense(units = 256, activation = 'relu', input_shape = c(224*224*3)) %>%
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 2, activation = 'relu')

output_y1 <- 
  main_network %>%
  layer_dense(units = 1, activation = "linear", name = "output_y1")

output_y2 <-
  main_network %>%
  layer_dense(units = 1, activation = "linear", name = "output_y2")

model <- keras_model(
  inputs = model_input,
  outputs = c(output_y1, output_y2)
)

model 
```

```{r}
model %>%
  compile(loss = 'mean_squared_error', # MSE for continuous output
          loss_weights = c(0.5, 0.5), # Weight both coords equally 
          optimizer = optimizer_adam())
```

```{r}
model %>% fit_generator(
  img_gen_flow,
  epochs = 20,
  steps_per_epoch = 100)
```







