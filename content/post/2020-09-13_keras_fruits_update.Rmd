---
title: "Update for Keras and TF 2.0: Image classification with keras."
draft: true
author: Dr. Shirin Elsinghorst
date: '2020-09-13'
categories: ["R", "keras"]
tags: ["R", "keras", "image classification", "tensorflow"]
thumbnailImagePosition: left
thumbnailImage: 
metaAlignment: center
coverMeta: out
slug: keras_fruits_update
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE, echo = TRUE, message = FALSE, warning = FALSE)
```

Recently, I have been getting a few comments on my [old article on image classification with Keras](https://shirinsplayground.netlify.com/2018/06/keras_fruits/), saying that they are getting errors with the code. And I have also gotten a few questions about **how to use a Keras model to predict on new images (of different size)**. Instead of replying to them all individually, I decided to write this updated version using the most recent Keras and Tensorflow versions (all package versions and system information can be found at the bottom of this article, as usual). 

Specific updates made compared to the [old article](https://shirinsplayground.netlify.com/2018/06/keras_fruits/):

- using TF 2.2.0
- using Keras 2.3.1

- used **validation split** instead of validation set
- used folder Validation/Test for showing how to **predict on new images**

---

If you have questions or would like to talk about this article (or something else data-related), you can now [book 15-minute timeslots](http://127.0.0.1:4321/page/bookme/) with me (it's free - one slot available per weekday): 

<img src="https://www.appointletcdn.com/loader/buttons/F62459.png" data-appointlet-organization="shirin-elsinghorst" data-appointlet-service="357892"><script src="https://www.appointletcdn.com/loader/loader.min.js" async="" defer=""></script>

*If you have been enjoying my content and would like to help me be able to create more, please consider sending me a donation at <button>[paypal.me](https://paypal.me/ShirinGlander)</button>. Thank you!* :-)

---

**Note**:

I have been having an [issue](https://github.com/rstudio/blogdown/issues/473) with `blogdown`, where the code below runs perfectly fine in my R console from within the RMarkdown file but whenever I include any function from the `keras` package, `blogdown::serve_site()` throws an error: `Error in render_page(f) : Failed to render 'content/post/keras.Rmd'`.

Since I have no idea what's going on there (`blogdown::serve_site()` builds my site without trouble as long as I don't include `keras` functions) and haven't gotten a reply to the [issue](https://github.com/rstudio/blogdown/issues/473) I posted to Github, I had to write this blogpost without actually running the code while building the site.

I worked around that problem by not evaluating most of the code below and instead ran the code locally on my computer. Objects and images, I then saved as *.RData* or *.png* files and manually put them back into the document. By just reading this blogpost as it rendered on my site, you won't notice much of a difference (except where I copy/pasted the output as comments below the code) but in order to be fully transparent, you can see on [Github](https://github.com/ShirinG/shirinsplayground/blob/master/content/post/2020-09-13_keras_fruits_update.Rmd) what I did with hidden code chunks to create this post.

---

```{r eval=TRUE}
# load libraries
library(tidyverse)
library(keras)
```

```{r}
# check if keras is available
is_keras_available()
#[1] TRUE
```

## Loading images (data)

The dataset I am using here is the [fruit images dataset from Kaggle](https://www.kaggle.com/moltean/fruits/data). I downloaded it to my computer and unpacked it. Because I don't want to build a model for all the different fruits, I define a list of fruits (corresponding to the folder names) that I want to include in the model.

```{r echo=TRUE, eval=FALSE}
# path to image folders
train_image_files_path <- "/fruits/Training/"
```

```{r echo=FALSE}
# path to image folders
train_image_files_path <- "/Users/shiringlander/Documents/Github/Data/fruits-360/Training/"
```

```{r}
# list of fruits to modle
fruit_list <- c("Kiwi", "Banana", "Apricot", "Avocado", "Cocos", "Clementine", "Mandarine", "Orange",
                "Limes", "Lemon", "Peach", "Plum", "Raspberry", "Strawberry", "Pineapple", "Pomegranate")

# number of output classes (i.e. fruits)
output_n <- length(fruit_list)

# image size to scale down to (original images are 100 x 100 px)
img_width <- 20
img_height <- 20
target_size <- c(img_width, img_height)

# RGB = 3 channels
channels <- 3

# define batch size
batch_size <- 32
```

The handy `image_data_generator()` and `flow_images_from_directory()` functions can be used to load images from a directory without having to store all data in memory at the same time. Instead `image_data_generator` will loop over the data and process the images in batches.

When we train our model, we want to have a way to judge how well it learned and if learning improves over the epochs. Therefore, we want to use validation data, to make these performance measures less biased compared to using the training data only. In Keras, we can either give a specific **validation set** (as I did in the [old article on image classification with Keras](https://shirinsplayground.netlify.com/2018/06/keras_fruits/)) or we define a validation split.

If you want to use data augmentation, you can directly define how and in what way you want to augment your images with `image_data_generator`. Here I am not augmenting the data, I only scale the pixel values to fall between 0 and 1.

```{r}
train_data_gen <- image_data_generator(
  rescale = 1/255,
  validation_split = 0.3)
```

Now we load the images into memory and resize them. 

```{r}
# training images
train_image_array_gen <- flow_images_from_directory(train_image_files_path, 
                                          train_data_gen,
                                          subset = 'training',
                                          target_size = target_size,
                                          class_mode = "categorical",
                                          classes = fruit_list,
                                          batch_size = batch_size,
                                          seed = 42)
#Found 5401 images belonging to 16 classes.

# validation images
valid_image_array_gen <- flow_images_from_directory(train_image_files_path, 
                                          train_data_gen,
                                          subset = 'validation',
                                          target_size = target_size,
                                          class_mode = "categorical",
                                          classes = fruit_list,
                                          batch_size = batch_size,
                                          seed = 42)
#Found 2308 images belonging to 16 classes.
```

```{r}
cat("Number of images per class:")
table(factor(train_image_array_gen$classes))
#Number of images per class:
#  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15 
#327 343 345 299 343 343 343 336 343 345 345 313 343 345 343 345
```

Note, that even though Keras' `image_data_generator` recognizes folder names as class labels for classification tasks, these labels will be converted into indices (alphabetical starting from 0) for training and prediction later. Thus, it is useful to create a library object that matches these indices back to human-interpretable labels.

```{r}
train_image_array_gen_t <- train_image_array_gen$class_indices %>%
  as.tibble()
```

```{r echo=FALSE, eval=FALSE}
train_image_array_gen_t <- train_image_array_gen$class_indices %>%
  as.tibble()
save(train_image_array_gen_t, file = "/Users/shiringlander/Documents/keras_objects/keras_fruits_update_post/train_image_array_gen_t.RData")
```

```{r echo=FALSE, eval=TRUE}
load("/Users/shiringlander/Documents/keras_objects/keras_fruits_update_post/train_image_array_gen_t.RData")
```

```{r eval=TRUE}
cat("\nClass label vs index mapping:\n")
train_image_array_gen_t
```

## Training the model

Now, I define and train the model just as before:

```{r}
# number of training samples
train_samples <- train_image_array_gen$n
# number of validation samples
valid_samples <- valid_image_array_gen$n

# define number of epochs
epochs <- 10
```

```{r warning=FALSE, message=FALSE}
# initialise model
model <- keras_model_sequential()

# add layers
model %>%
  layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = "same", input_shape = c(img_width, img_height, channels)) %>%
  layer_activation("relu") %>%
  
  # Second hidden layer
  layer_conv_2d(filter = 16, kernel_size = c(3,3), padding = "same") %>%
  layer_activation_leaky_relu(0.5) %>%
  layer_batch_normalization() %>%

  # Use max pooling
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_dropout(0.25) %>%
  
  # Flatten max filtered output into feature vector 
  # and feed into dense layer
  layer_flatten() %>%
  layer_dense(100) %>%
  layer_activation("relu") %>%
  layer_dropout(0.5) %>%

  # Outputs from dense layer are projected onto output layer
  layer_dense(output_n) %>% 
  layer_activation("softmax")

# compile
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(lr = 0.0001, decay = 1e-6),
  metrics = "accuracy"
)
```

Fitting the model: because I used `image_data_generator()` and `flow_images_from_directory()` I am now also using the `fit_generator()` to run the training.

```{r}
# fit
hist <- model %>% fit_generator(
  # training data
  train_image_array_gen,
  
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = epochs, 
  
  # validation data
  validation_data = valid_image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size)
)
```

```{}
Epoch 1/10
168/168 [==============================] - 9s 56ms/step - loss: 2.2358 - accuracy: 0.3038 - val_loss: 2.3285 - val_accuracy: 0.5477
Epoch 2/10
168/168 [==============================] - 6s 38ms/step - loss: 1.1505 - accuracy: 0.6161 - val_loss: 1.2697 - val_accuracy: 0.9054
Epoch 3/10
168/168 [==============================] - 6s 37ms/step - loss: 0.6588 - accuracy: 0.7812 - val_loss: 0.3950 - val_accuracy: 0.9227
Epoch 4/10
168/168 [==============================] - 6s 35ms/step - loss: 0.4223 - accuracy: 0.8624 - val_loss: 0.1600 - val_accuracy: 0.9609
Epoch 5/10
168/168 [==============================] - 6s 35ms/step - loss: 0.2775 - accuracy: 0.9091 - val_loss: 0.1180 - val_accuracy: 0.9648
Epoch 6/10
168/168 [==============================] - 6s 39ms/step - loss: 0.1981 - accuracy: 0.9383 - val_loss: 0.0844 - val_accuracy: 0.9727
Epoch 7/10
168/168 [==============================] - 6s 35ms/step - loss: 0.1514 - accuracy: 0.9527 - val_loss: 0.0674 - val_accuracy: 0.9731
Epoch 8/10
168/168 [==============================] - 6s 34ms/step - loss: 0.1116 - accuracy: 0.9685 - val_loss: 0.0669 - val_accuracy: 0.9805
Epoch 9/10
168/168 [==============================] - 6s 34ms/step - loss: 0.0897 - accuracy: 0.9722 - val_loss: 0.0466 - val_accuracy: 0.9818
Epoch 10/10
168/168 [==============================] - 6s 34ms/step - loss: 0.0699 - accuracy: 0.9795 - val_loss: 0.0698 - val_accuracy: 0.9787
```

In RStudio we are seeing the output as an interactive plot in the "Viewer" pane but we can also plot it:

```{r }
plot(hist)
```

![](/img/hist.png)

## Predicting on "new" set of images

**Here is what's new**: how to use the model we just trained for predicting on new images. Alternatively, load a previously trained model and use that for predictions. Just keep in mind, that these "new" images here aren't actually new. They come from the same data source, i.e. the same distribution and thus aren't independent enough to give an accurate assessment of how well the model generalizes.

```{r echo=TRUE, eval=FALSE}
# path to image folders
test_image_files_path <- "/fruits/Test/"
```

```{r echo=FALSE}
# path to image folders
test_image_files_path <- "/Users/shiringlander/Documents/Github/Data/fruits-360/Test/"
```

```{r}
test_datagen <- image_data_generator(rescale = 1/255)

test_generator <- flow_images_from_directory(
        test_image_files_path,
        test_datagen,
        target_size = target_size,
        class_mode = "categorical",
        classes = fruit_list,
        batch_size = 1,
        shuffle = FALSE,
        seed = 42)
#Found 2592 images belonging to 16 classes.
```

```{r}
test_generator$reset()
model %>%
  evaluate_generator(test_generator, 
                     steps = as.integer(test_generator$n))
```

```{}
      loss   accuracy 
0.04565864 0.99112654 
```

```{r}
classes <- test_generator$classes %>%
  factor() %>%
  table() %>%
  as.tibble()
colnames(classes)[1] <- "value"
```

```{r}
# create library of indices & class labels
indices <- train_image_array_gen$class_indices %>%
  as.data.frame() %>%
  gather() %>%
  mutate(value = as.character(value)) %>%
  left_join(classes, by = "value")
```

```{r}
# predict on test data
test_generator$reset()
predictions <- model %>% 
  predict_generator(
    generator = test_generator,
    steps = as.integer(test_generator$n)
    ) %>%
  round() %>%
  as.tibble()

colnames(predictions) <- indices$key

predictions <- predictions %>%
  mutate(truth_idx = as.character(test_generator$classes)) %>%
  left_join(indices, by = c("truth_idx" = "value"))
```

```{r}
pred_analysis <- predictions %>%
  mutate(img_id = seq(1:test_generator$n)) %>%
  gather(pred_lbl, y, Kiwi:Pomegranate) %>%
  group_by(img_id) %>%
  filter(y == max(y)) %>%
  arrange(img_id) %>%
  group_by(key, n, pred_lbl) %>%
  count()
```

```{r fig.width=6}
p <- pred_analysis %>%
  mutate(percentage_pred = nn / n * 100) %>%
  ggplot(aes(x = key, y = pred_lbl, 
             fill = percentage_pred,
             label = round(percentage_pred, 2))) +
    geom_tile() +
    scale_fill_continuous() +
    geom_text(color = "white") +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
p
```

```{r eval=FALSE, echo=FALSE}
pngfile <- fs::path("/Users/shiringlander/Documents/Github/shirinsplayground/static", "img",  "percentage_pred.png")
agg_png(pngfile, width = 50, height = 40, units = "cm", res = 100, scaling = 2)
plot(p)
invisible(dev.off())
knitr::include_graphics(pngfile)
```

![](/img/percentage_pred.png)

```{r}
p2 <- pred_analysis %>%
  mutate(prediction = case_when(
    key == pred_lbl ~ "correct",
    TRUE ~ "false"
  )) %>%
  group_by(key, prediction, n) %>%
  summarise(sum = sum(nn)) %>%
  mutate(percentage_pred = sum / n * 100) %>%
  ggplot(aes(x = key, y = prediction, 
             fill = percentage_pred,
             label = round(percentage_pred, 2))) +
    geom_tile() +
    scale_fill_continuous() +
    geom_text(color = "white") +
    coord_flip()
p2
```

```{r eval=FALSE, echo=FALSE}
pngfile <- fs::path("/Users/shiringlander/Documents/Github/shirinsplayground/static", "img",  "percentage_pred_cor.png")
agg_png(pngfile, width = 30, height = 30, units = "cm", res = 100, scaling = 2)
plot(p2)
invisible(dev.off())
knitr::include_graphics(pngfile)
```

![](/img/percentage_pred_cor.png)

As we can see, the model is quite accurate on the test data. However, we need to keep in mind that our images are very uniform, they all have the same white background and show the fruits centered and without anything else in the images. Thus, our model will not work with images that don't look similar as the ones we trained on (that's also why we can achieve such good results with such a small neural net).

---

```{r echo=FALSE}
xfun::session_info('blogdown')
```

```{r eval=TRUE}
devtools::session_info()
```


