---
title: "Using Keras Callbacks"
draft: true
author: Dr. Shirin Elsinghorst
date: '2020-09-27'
categories: ["R", "keras"]
tags: ["R", "keras", "image classification", "tensorflow"]
thumbnailImagePosition: left
thumbnailImage: 
metaAlignment: center
coverMeta: out
slug: keras_callbacks
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, echo = TRUE, message = FALSE, warning = FALSE)
```

This is article #3 in my recent series on how to get the most out of Keras. The two articles were:

1. [Update with TF 2.0: Image classification with Keras and TensorFlow](https://shirinsplayground.netlify.app/2020/09/keras_fruits_update/)
2. [Whose dream is this? When and how to use the Keras Functional API](https://shirinsplayground.netlify.app/2020/09/keras_funct_api/)

Keras comes with a number of built in [callbacks](https://keras.rstudio.com/articles/training_callbacks.html). Callbacks are useful for optimizing/customizing your training runs and are used with the `fit()` function.

---

**Workshop announcement:** Because this year's UseR 2020 in Munich couldn't happen as an in-person event, I will be giving my [workshop on Deep Learning with Keras and TensorFlow](https://user2020.r-project.org/program/tutorials/) as an online event on Thursday, 8th of October (13:00 UTC).

---

If you have questions or would like to talk about this article (or something else data-related), you can now [book 15-minute timeslots](https://shirinsplayground.netlify.app/page/bookme/) with me (it's free - one slot available per weekday): 

<img src="https://www.appointletcdn.com/loader/buttons/F62459.png" data-appointlet-organization="shirin-elsinghorst" data-appointlet-service="357892"><script src="https://www.appointletcdn.com/loader/loader.min.js" async="" defer=""></script>

*If you have been enjoying my content and would like to help me be able to create more, please consider sending me a donation at <button>[paypal.me](https://paypal.me/ShirinGlander)</button>. Thank you!* :-)

---

**Note**:

I have been having an [issue](https://github.com/rstudio/blogdown/issues/473) with `blogdown`, where the code below runs perfectly fine in my R console from within the RMarkdown file but whenever I include any function from the `keras` package, `blogdown::serve_site()` throws an error: `Error in render_page(f) : Failed to render 'content/post/keras.Rmd'`.

Since I have no idea what's going on there (`blogdown::serve_site()` builds my site without trouble as long as I don't include `keras` functions) and haven't gotten a reply to the [issue](https://github.com/rstudio/blogdown/issues/473) I posted to Github, I had to write this blogpost without actually running the code while building the site.

I worked around that problem by not evaluating most of the code below and instead ran the code locally on my computer. Objects and images, I then saved as *.RData* or *.png* files and manually put them back into the document. By just reading this blogpost as it rendered on my site, you won't notice much of a difference (except where I copy/pasted the output as comments below the code) but in order to be fully transparent, you can see on [Github](https://github.com/ShirinG/shirinsplayground/blob/master/content/post/2020-09-20_keras_func_api.Rmd) what I did with hidden code chunks to create this post.

---

```{r eval=TRUE}
# load libraries
library(tidyverse)
library(tensorflow)
library(keras)
```

```{r}
# check TF version
tf_version()
#[1] ‘2.2’

# check if keras is available
is_keras_available()
#[1] TRUE
```

## Loading images (data)

```{r echo=TRUE, eval=FALSE}
# path to image folders
train_image_files_path <- "/fruits/Training/"
```

```{r echo=FALSE}
# path to image folders
train_image_files_path <- "/Users/shiringlander/Documents/Github/Data/fruits-360/Training/"
```

```{r}
# list of fruits to modle
fruit_list <- c("Kiwi", "Banana", "Apricot", "Avocado", "Cocos", "Clementine", "Mandarine", "Orange",
                "Limes", "Lemon", "Peach", "Plum", "Raspberry", "Strawberry", "Pineapple", "Pomegranate")

# number of output classes (i.e. fruits)
output_n <- length(fruit_list)

# image size to scale down to (original images are 100 x 100 px)
img_width <- 20
img_height <- 20
target_size <- c(img_width, img_height)

# RGB = 3 channels
channels <- 3

# define batch size
batch_size <- 32
```

```{r}
train_data_gen <- image_data_generator(
  rescale = 1/255,
  validation_split = 0.3)
```

Now we load the images into memory and resize them. 

```{r}
# training images
train_image_array_gen <- flow_images_from_directory(train_image_files_path, 
                                          train_data_gen,
                                          subset = 'training',
                                          target_size = target_size,
                                          class_mode = "categorical",
                                          classes = fruit_list,
                                          batch_size = batch_size,
                                          seed = 42)
#Found 5401 images belonging to 16 classes.

# validation images
valid_image_array_gen <- flow_images_from_directory(train_image_files_path, 
                                          train_data_gen,
                                          subset = 'validation',
                                          target_size = target_size,
                                          class_mode = "categorical",
                                          classes = fruit_list,
                                          batch_size = batch_size,
                                          seed = 42)
#Found 2308 images belonging to 16 classes.
```

## Training the model

Now, I define and train the model just as before:

```{r}
# number of training samples
train_samples <- train_image_array_gen$n
# number of validation samples
valid_samples <- valid_image_array_gen$n

# define number of epochs
epochs <- 100
```

```{r}
# input layer
inputs <- layer_input(shape = c(img_width, img_height, channels))

# outputs compose input + dense layers
predictions <- inputs %>%
  layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = "same") %>%
  layer_activation("relu") %>%
  
  # Second hidden layer
  layer_conv_2d(filter = 16, kernel_size = c(3,3), padding = "same") %>%
  layer_activation_leaky_relu(0.5) %>%
  layer_batch_normalization() %>%

  # Use max pooling
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_dropout(0.25) %>%
  
  # Flatten max filtered output into feature vector 
  # and feed into dense layer
  layer_flatten() %>%
  layer_dense(100) %>%
  layer_activation("relu") %>%
  layer_dropout(0.5) %>%
  
  # Outputs from dense layer are projected onto output layer
  layer_dense(output_n) %>% 
  layer_activation("softmax")

# create and compile model
model <- keras_model(inputs = inputs, outputs = predictions)
```

## Callbacks

https://cran.r-project.org/web/packages/keras/vignettes/training_callbacks.html

### Early stopping

- how many epochs will be needed to get to an optimal validation loss. The examples so far have adopted the strategy of training for enough epochs that you begin overfitting, using the first run to figure out the proper number of epochs to train for, and then finally launching a new training run from scratch using this optimal number. Of course, this approach is wasteful. A much better way to handle this is to stop training when you measure that the validation loss in no longer improving.
Interrupting training when the validation loss is no longer improving (and saving the best model obtained during training). You can use callback_early_stopping to interrupt training once a target metric being monitored has stopped improving for a fixed number of epochs. For instance, this callback allows you to interrupt training as soon as you start overfitting, thus avoiding having to retrain your model for a smaller number of epochs.

```{r}
early_stop <- callback_early_stopping(monitor = "val_loss", 
                                      min_delta = 0.001, 
                                      patience = 5,
                                      restore_best_weights = TRUE)
```


### Checkpoints

Saving the current weights of the model at different points during training.
This callback is typically used in combination with callback_model_checkpoint, which lets you continually save the model during training (and, optionally, save only the current best model so far: the version of the model that achieved the best performance at the end of an epoch):

```{r}
save_cp <- callback_model_checkpoint("../../../keras_model_cps/checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5",
                                     save_best_only = TRUE)
```


### Adjusting the learning rate
Dynamically adjusting the value of certain parameters during training—Such as the learning rate of the optimizer.
The reduce-learning-rate-on-plateau callback
You can use this callback to reduce the learning rate when the validation loss has stopped improving. Reducing or increasing the learning rate in case of a loss plateau is is an effective strategy to get out of local minima during training. The following example uses callback_reduce_lr_on_plateau:

1 Monitors the model’s validation loss
2 Divides the learning rate by 10 when triggered
3 The callback is triggered after the validation loss has stopped improving for 10 epochs.

Adapting the learning rate for your stochastic gradient descent optimization procedure can increase performance and reduce training time.

Sometimes this is called learning rate annealing or adaptive learning rates. Here we will call this approach a learning rate schedule, were the default schedule is to use a constant learning rate to update network weights for each training epoch.

The simplest and perhaps most used adaptation of learning rate during training are techniques that reduce the learning rate over time. These have the benefit of making large changes at the beginning of the training procedure when larger learning rate values are used, and decreasing the learning rate such that a smaller rate and therefore smaller training updates are made to weights later in the training procedure.

This has the effect of quickly learning good weights early and fine tuning them later.

Two popular and easy to use learning rate schedules are as follows:

Decrease the learning rate gradually based on the epoch.
Decrease the learning rate using punctuated large drops at specific epochs.

Increase the initial learning rate. Because the learning rate will very likely decrease, start with a larger value to decrease from. A larger learning rate will result in a lot larger changes to the weights, at least in the beginning, allowing you to benefit from the fine tuning later.
Use a large momentum. Using a larger momentum value will help the optimization algorithm to continue to make updates in the right direction when your learning rate shrinks to small values.
Experiment with different schedules. It will not be clear which learning rate schedule to use so try a few with different configuration options and see what works best on your problem. Also try schedules that change exponentially and even schedules that respond to the accuracy of your model on the training or test datasets.

```{r}
lr_schedule <- function (epoch, lr) {
  
  if (epoch <= 10) {
    return(lr)
  } else {
    return(lr * tf$math$exp(-0.1))
  }

}

lr_schedule <- function (epoch, lr) {
  
  decay_rate = 0.1
  decay_step = 10
  
  if (epoch %% decay_step == 0) {
    lr * decay_rate
  }
  
  return(lr)

}

reduce_lr <- callback_learning_rate_scheduler(lr_schedule)
```

```{r}
reduce_lr_pt <- callback_reduce_lr_on_plateau(
  monitor = "val_loss",
  factor = 0.1,
  patience = 5,
  min_lr = 0.0000001,
  verbose = 1
)
```

### Terminate training on NaN

```{r}
terminate_na <- callback_terminate_on_naan()
```

### Logging

Logging training and validation metrics during training, or visualizing the representations learned by the model as they’re updated—The Keras progress bar that you’re familiar with is a callback!

```{r}
prog_bar <- callback_progbar_logger(count_mode = "steps")
```

### CSV logger

```{r}
logfile <- callback_csv_logger("../../../keras_model_cps/run/log.csv")
```

### Introduction to TensorBoard: the TensorFlow visualization framework

To do good research or develop good models, you need rich, frequent feedback about what’s going on inside your models during your experiments. That’s the point of running experiments: to get information about how well a model performs—as much information as possible. Making progress is an iterative process, or loop: you start with an idea and express it as an experiment, attempting to validate or invalidate your idea. You run this experiment and process the information it generates. This inspires your next idea. The more iterations of this loop you’re able to run, the more refined and powerful your ideas become. Keras helps you go from idea to experiment in the least possible time, and fast GPUs can help you get from experiment to result as quickly as possible. But what about processing the experiment results? That’s where Tensor-Board comes in.

This section introduces TensorBoard, a browser-based visualization tool that comes packaged with TensorFlow. Note that it’s only available for Keras models when you’re using Keras with the TensorFlow backend.

The key purpose of TensorBoard is to help you visually monitor everything that goes on inside your model during training. If you’re monitoring more information than just the model’s final loss, you can develop a clearer vision of what the model does and doesn’t do, and you can make progress more quickly. TensorBoard gives you access to several neat features, all in your browser:

Visually monitoring metrics during training
Visualizing your model architecture
Visualizing histograms of activations and gradients
Exploring embeddings in 3D
Let’s demonstrate these features on a simple example. You’ll train a 1D convnet on the IMDB sentiment-analysis task.

```{r}
tensorboard <- callback_tensorboard("../../../keras_model_cps/logs")
```

### Remote monitoring

```{r}
remote_monitor <- callback_remote_monitor(
  root = "http://localhost:9000",
  path = "../../../keras_model_cps/publish/epoch/end/",
  field = "data",
  headers = NULL,
  send_as_json = FALSE
)
```

### Writing your own callback

You can create a custom callback by creating a new R6 class that inherits from the KerasCallback class.

If you need to take a specific action during training that isn’t covered by one of the built-in callbacks, you can write your own callback. Callbacks are implemented by creating a new R6 class that inherits from the KerasCallback class. You can then implement any number of the following transparently named methods, which are called at various points during training:

1 Called at the start of every epoch
2 Called at the end of every epoch
3 Called right before processing each batch
4 Called right after processing each batch
5 Called at the start of training
6 Called at the end of training
These methods all are called with a logs argument, which is a named list containing information about the previous batch, epoch, or training run: training and validation metrics, and so on. Additionally, the callback has access to the following attributes:

self$model—Reference to the Keras model being trained
self$params—Named list with training parameters (verbosity, batch size, number of epochs, and so on)
Here’s a simple example that saves a list of losses over each batch during training:

1 Called at the end of every training batch
2 Accumulates losses from every batch in a list
3 Creates an instance of the callback
4 Attaches the callback to model training
5 Accumulated losses are now available from the callback instance.

```{r}
custom_cb <- callback_lambda(
  
  on_epoch_end = function(epoch, logs) {
    if (epoch %% 5 == 0) {
      cat("\nEpoch",  epoch, "end\n")
    }
  }
  
)
```

## Model training with callbacks

```{r}
# fit
model %>% 
  
  compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(lr = 0.0001, decay = 1e-6),
  metrics = "accuracy"
) %>% 
  
  fit_generator(
  # training data
  train_image_array_gen,
  
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = epochs, 
  
  # validation data
  validation_data = valid_image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size),
  
  # callbacks
  callbacks = list(
    early_stop,
    terminate_na,
    save_cp,
    logfile,
    tensorboard,
    #reduce_lr,
    #reduce_lr_pt,
    #remote_monitor,
    custom_cb
  )
)
```

```{}
Epoch 1/100
168/168 [==============================] - 20s 117ms/step - loss: 2.1054 - accuracy: 0.3278 - val_loss: 2.0478 - val_accuracy: 0.6059
Epoch 2/100
168/168 [==============================] - 14s 86ms/step - loss: 1.0841 - accuracy: 0.6374 - val_loss: 0.9194 - val_accuracy: 0.8924
Epoch 3/100
168/168 [==============================] - 10s 62ms/step - loss: 0.6357 - accuracy: 0.7905 - val_loss: 0.3312 - val_accuracy: 0.9240
Epoch 4/100
168/168 [==============================] - 11s 67ms/step - loss: 0.4122 - accuracy: 0.8650 - val_loss: 0.1618 - val_accuracy: 0.9692
Epoch 5/100
168/168 [==============================] - 11s 68ms/step - loss: 0.2736 - accuracy: 0.9162 - val_loss: 0.0921 - val_accuracy: 0.9822
Epoch 6/100
168/168 [==============================] - 13s 80ms/step - loss: 0.2064 - accuracy: 0.9376 - val_loss: 0.0798 - val_accuracy: 0.9805
Epoch 7/100
168/168 [==============================] - 11s 64ms/step - loss: 0.1446 - accuracy: 0.9536 - val_loss: 0.0687 - val_accuracy: 0.9818
Epoch 8/100
168/168 [==============================] - 11s 65ms/step - loss: 0.1128 - accuracy: 0.9668 - val_loss: 0.0366 - val_accuracy: 0.9896
Epoch 9/100
168/168 [==============================] - 10s 62ms/step - loss: 0.0839 - accuracy: 0.9752 - val_loss: 0.0376 - val_accuracy: 0.9852
Epoch 10/100
168/168 [==============================] - 11s 65ms/step - loss: 0.0761 - accuracy: 0.9782 - val_loss: 0.0340 - val_accuracy: 0.9891
Epoch 11/100
168/168 [==============================] - 10s 60ms/step - loss: 0.0631 - accuracy: 0.9823 - val_loss: 0.0351 - val_accuracy: 0.9878
Epoch 12/100
168/168 [==============================] - 11s 63ms/step - loss: 0.0499 - accuracy: 0.9849 - val_loss: 0.0155 - val_accuracy: 0.9965
Epoch 13/100
168/168 [==============================] - 10s 61ms/step - loss: 0.0434 - accuracy: 0.9888 - val_loss: 0.0174 - val_accuracy: 0.9948
Epoch 14/100
168/168 [==============================] - 13s 75ms/step - loss: 0.0360 - accuracy: 0.9907 - val_loss: 0.0170 - val_accuracy: 0.9926
Epoch 15/100
168/168 [==============================] - 12s 70ms/step - loss: 0.0339 - accuracy: 0.9892 - val_loss: 0.0148 - val_accuracy: 0.9944
Epoch 16/100
168/168 [==============================] - 15s 87ms/step - loss: 0.0299 - accuracy: 0.9922 - val_loss: 0.0186 - val_accuracy: 0.9926
Epoch 17/100
168/168 [==============================] - 12s 74ms/step - loss: 0.0252 - accuracy: 0.9931 - val_loss: 0.0099 - val_accuracy: 0.9978
Epoch 18/100
168/168 [==============================] - 10s 58ms/step - loss: 0.0238 - accuracy: 0.9940 - val_loss: 0.0185 - val_accuracy: 0.9909
Epoch 19/100
168/168 [==============================] - 10s 58ms/step - loss: 0.0220 - accuracy: 0.9935 - val_loss: 0.0063 - val_accuracy: 1.0000
Epoch 20/100
168/168 [==============================] - 10s 58ms/step - loss: 0.0233 - accuracy: 0.9922 - val_loss: 0.0108 - val_accuracy: 0.9957
Epoch 21/100
168/168 [==============================] - 10s 60ms/step - loss: 0.0173 - accuracy: 0.9957 - val_loss: 0.0308 - val_accuracy: 0.9887
Epoch 22/100
168/168 [==============================] - 10s 59ms/step - loss: 0.0162 - accuracy: 0.9953 - val_loss: 0.0161 - val_accuracy: 0.9939
Epoch 23/100
168/168 [==============================] - 10s 58ms/step - loss: 0.0160 - accuracy: 0.9953 - val_loss: 0.0060 - val_accuracy: 0.9974
Epoch 24/100
168/168 [==============================] - 10s 59ms/step - loss: 0.0176 - accuracy: 0.9944 - val_loss: 0.0055 - val_accuracy: 0.9983
```

```{r }
hist
```

```{r}
hist_csv <- readr::read_csv("../../../keras_model_cps/run/log.csv")

hist_csv %>%
  gather("x", "y", accuracy:val_loss) %>%
  mutate(y = as.numeric(y),
         metric = gsub("val_", "", x),
         set = case_when(
           grepl("val", x, fixed = TRUE) ~ "validation",
           TRUE ~ "training")) %>%
  ggplot(aes(x = epoch, y = y, color = set)) +
    facet_wrap(vars(metric), ncol = 1, scales = "free_y") +
    geom_line() +
    geom_point()
```

```{r eval=FALSE}
tensorboard("../../../keras_model_cps/logs")
```

```{}
Started TensorBoard at http://127.0.0.1:4302 
```

![](/img/cb_tensorboard_1.png)

![](/img/cb_tensorboard_2.png)

---

```{r echo=FALSE}
xfun::session_info('blogdown')
```

```{r eval=TRUE}
devtools::session_info()
```


