

  
    
  


  




  


  

<!DOCTYPE html>
<html lang="en-us">
  <head>
    
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Hugo 0.27 with theme Tranquilpeak 0.4.1-BETA">
    <title>Code for Workshop: Introduction to Machine Learning with R</title>
    <meta name="author" content="Dr. Shirin Glander">
    <meta name="keywords" content=", R">

    <link rel="icon" href="img/favicon.png">
    

    
    <meta name="description" content="These are the slides from my workshop: Introduction to Machine Learning with R which I gave at the University of Heidelberg, Germany on June 28th 2018. The entire code accompanying the workshop can be found below the video.
The workshop covered the basics of machine learning. With an example dataset I went through a standard machine learning workflow in R with the packages caret and h2o:
 reading in data exploratory data analysis missingness feature engineering training and test split model training with Random Forests, Gradient Boosting, Neural Nets, etc.">
    <meta property="og:description" content="These are the slides from my workshop: Introduction to Machine Learning with R which I gave at the University of Heidelberg, Germany on June 28th 2018. The entire code accompanying the workshop can be found below the video.
The workshop covered the basics of machine learning. With an example dataset I went through a standard machine learning workflow in R with the packages caret and h2o:
 reading in data exploratory data analysis missingness feature engineering training and test split model training with Random Forests, Gradient Boosting, Neural Nets, etc.">
    <meta property="og:type" content="blog">
    <meta property="og:title" content="Code for Workshop: Introduction to Machine Learning with R">
    <meta property="og:url" content="/2018/06/intro_to_ml_workshop_heidelberg/">
    <meta property="og:site_name" content="Shirin&#39;s playgRound">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Shirin&#39;s playgRound">
    <meta name="twitter:description" content="These are the slides from my workshop: Introduction to Machine Learning with R which I gave at the University of Heidelberg, Germany on June 28th 2018. The entire code accompanying the workshop can be found below the video.
The workshop covered the basics of machine learning. With an example dataset I went through a standard machine learning workflow in R with the packages caret and h2o:
 reading in data exploratory data analysis missingness feature engineering training and test split model training with Random Forests, Gradient Boosting, Neural Nets, etc.">
    
      <meta name="twitter:creator" content="@ShirinGlander">
    
    

    
    

    
      <meta property="og:image" content="//www.gravatar.com/avatar/7f7f818e55624edfef8aa93860a1a3d4?s=640">
    

    
      <meta property="og:image" content="https://shiring.github.io/netlify_images/ml_workshop_heidelberg.png">
    
    
    

    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="/css/style-hbs5om8csx9a8yrv5hnhpi2qqdv4ykuslajwbramhqxvleqbfklxgek50hye.min.css" />
    
    

    
      
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-86119417-1', 'auto');
ga('send', 'pageview');
</script>

    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="4">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="/">Shirin&#39;s playgRound</a>
  </div>
  
    
      <a class="header-right-picture "
         href="/#about">
    
    
    
      
        <img class="header-picture" src="//www.gravatar.com/avatar/7f7f818e55624edfef8aa93860a1a3d4?s=90" alt="Author&#39;s picture" />
      
    
    </a>
  
</header>

      <nav id="sidebar" data-behavior="4">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <h1 class="sidebar-profile-title">Code for Workshop: Introduction to Machine Learning with R</h1>
        <a href="https://blog.feedspot.com/r_programming_blogs/" rel="nofollow" title="R Programming Blogs"><img alt="R Programming Blogs" src="https://blog.feedspot.com/wp-content/uploads/2018/04/r_program_216px.png?x20694"/></a>
        <a href="/#about">
          <img class="sidebar-profile-picture" src="//www.gravatar.com/avatar/7f7f818e55624edfef8aa93860a1a3d4?s=110" alt="Author&#39;s picture" />
        </a>
        <h4 class="sidebar-profile-name">Dr. Shirin Glander</h4>
        
          <h5 class="sidebar-profile-bio">Biologist turned Bioinformatician turned Data Scientist</h5>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">Home</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/archives/">
    
      <i class="sidebar-button-icon fa fa-lg fa-archive"></i>
      
      <span class="sidebar-button-desc">Archives</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/categories/">
    
      <i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
      
      <span class="sidebar-button-desc">Categories</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/tags/">
    
      <i class="sidebar-button-icon fa fa-lg fa-tags"></i>
      
      <span class="sidebar-button-desc">Tags</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/page/about/">
    
      <i class="sidebar-button-icon fa fa-lg fa-question"></i>
      
      <span class="sidebar-button-desc">About</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/page/conferences_podcasts_webinars/">
    
      <i class="sidebar-button-icon fa fa-lg fa-bullhorn"></i>
      
      <span class="sidebar-button-desc">Hear me talk</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://shiring.github.io/">
    
      <i class="sidebar-button-icon fa fa-lg fa-archive"></i>
      
      <span class="sidebar-button-desc">My old R-blog</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.codecentric.de/kuenstliche-intelligenz/">
    
      <i class="sidebar-button-icon fa fa-lg fa-angle-double-right"></i>
      
      <span class="sidebar-button-desc">codecentric.ai</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/ShirinG">
    
      <i class="sidebar-button-icon fa fa-lg fa-github"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.linkedin.com/in/shirin-glander-01120881/">
    
      <i class="sidebar-button-icon fa fa-lg fa-linkedin"></i>
      
      <span class="sidebar-button-desc">LinkedIn</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://stackoverflow.com/users/6623620/shirin-glander">
    
      <i class="sidebar-button-icon fa fa-lg fa-stack-overflow"></i>
      
      <span class="sidebar-button-desc">Stack Overflow</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://twitter.com/ShirinGlander">
    
      <i class="sidebar-button-icon fa fa-lg fa-twitter"></i>
      
      <span class="sidebar-button-desc">Twitter</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.xing.com/profile/Shirin_Glander">
    
      <i class="sidebar-button-icon fa fa-lg fa-xing"></i>
      
      <span class="sidebar-button-desc">Xing</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.meetup.com/Munster-R-Users-Group">
    
      <i class="sidebar-button-icon fa fa-lg fa-meetup"></i>
      
      <span class="sidebar-button-desc">MünsteR</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.r-bloggers.com/">
    
      <i class="sidebar-button-icon fa fa-lg fa-book"></i>
      
      <span class="sidebar-button-desc">R-bloggers</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.r-users.com/">
    
      <i class="sidebar-button-icon fa fa-lg fa-book"></i>
      
      <span class="sidebar-button-desc">R-users</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="/index.xml">
    
      <i class="sidebar-button-icon fa fa-lg fa-rss"></i>
      
      <span class="sidebar-button-desc">RSS</span>
    </a>
  </li>


    </ul>
  </div>
</nav>

      

      <div id="main" data-behavior="4"
        class="
               hasCoverMetaOut
               ">
        <article class="post" itemscope itemType="http://schema.org/BlogPosting">
          
          
            <div class="post-header main-content-wrap text-center">
  
    <h1 class="post-title" itemprop="headline">
      Code for Workshop: Introduction to Machine Learning with R
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2018-06-29T00:00:00Z">
        
  June 29, 2018

      </time>
    
    
  
  
    <span>in</span>
    
      <a class="category-link" href="/categories/r">R</a>, 
    
      <a class="category-link" href="/categories/machine-learning">machine learning</a>
    
  


  </div>

</div>
          
          <div class="post-content markdown" itemprop="articleBody">
            <div class="main-content-wrap">
              <p>These are the slides from my workshop: Introduction to Machine Learning with R which I gave at the University of Heidelberg, Germany on June 28th 2018. The entire code accompanying the workshop can be found below the video.</p>
<p>The workshop covered the basics of machine learning. With an example dataset I went through a standard machine learning workflow in R with the packages caret and h2o:</p>
<ul>
<li>reading in data</li>
<li>exploratory data analysis</li>
<li>missingness</li>
<li>feature engineering</li>
<li>training and test split</li>
<li>model training with Random Forests, Gradient Boosting, Neural Nets, etc.</li>
<li>hyperparameter tuning</li>
</ul>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/lRX4QJ5TvxgWSv" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen>
</iframe>
<div style="margin-bottom:5px">
<strong> <a href="//www.slideshare.net/ShirinGlander/workshop-introduction-to-machine-learning-with-r" title="Workshop - Introduction to Machine Learning with R" target="_blank">Workshop - Introduction to Machine Learning with R</a> </strong> from <strong><a href="https://www.slideshare.net/ShirinGlander" target="_blank">Shirin Glander</a></strong>
</div>
<p><br></p>
<hr />
<div id="setup" class="section level2">
<h2>Setup</h2>
<p>All analyses are done in R using RStudio. For detailed session information including R version, operating system and package versions, see the <code>sessionInfo()</code> output at the end of this document.</p>
<p>All figures are produced with ggplot2.</p>
<ul>
<li>libraries</li>
</ul>
<pre class="r"><code>library(tidyverse) # for tidy data analysis
library(readr)     # for fast reading of input files
library(mice)      # mice package for Multivariate Imputation by Chained Equations (MICE)</code></pre>
<p><br></p>
</div>
<div id="data-preparation" class="section level2 tabset tabset-fade tabset-pills">
<h2>Data preparation</h2>
<div id="the-dataset" class="section level3">
<h3>The dataset</h3>
<p>The dataset I am using in these example analyses, is the <strong>Breast Cancer Wisconsin (Diagnostic) Dataset</strong>. The data was downloaded from the <a href="http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29">UC Irvine Machine Learning Repository</a>.</p>
<p>The first dataset looks at the predictor classes:</p>
<ul>
<li>malignant or</li>
<li>benign breast mass.</li>
</ul>
<p>The features characterise cell nucleus properties and were generated from image analysis of <a href="https://en.wikipedia.org/wiki/Fine-needle_aspiration">fine needle aspirates (FNA)</a> of breast masses:</p>
<ul>
<li>Sample ID (code number)</li>
<li>Clump thickness</li>
<li>Uniformity of cell size</li>
<li>Uniformity of cell shape</li>
<li>Marginal adhesion</li>
<li>Single epithelial cell size</li>
<li>Number of bare nuclei</li>
<li>Bland chromatin</li>
<li>Number of normal nuclei</li>
<li>Mitosis</li>
<li>Classes, i.e. diagnosis</li>
</ul>
<pre class="r"><code>bc_data &lt;- read_delim(&quot;datasets/breast-cancer-wisconsin.data.txt&quot;,
                      delim = &quot;,&quot;,
                      col_names = c(&quot;sample_code_number&quot;, 
                       &quot;clump_thickness&quot;, 
                       &quot;uniformity_of_cell_size&quot;, 
                       &quot;uniformity_of_cell_shape&quot;, 
                       &quot;marginal_adhesion&quot;, 
                       &quot;single_epithelial_cell_size&quot;, 
                       &quot;bare_nuclei&quot;, 
                       &quot;bland_chromatin&quot;, 
                       &quot;normal_nucleoli&quot;, 
                       &quot;mitosis&quot;, 
                       &quot;classes&quot;)) %&gt;%
  mutate(bare_nuclei = as.numeric(bare_nuclei),
         classes = ifelse(classes == &quot;2&quot;, &quot;benign&quot;,
                          ifelse(classes == &quot;4&quot;, &quot;malignant&quot;, NA)))</code></pre>
<pre class="r"><code>summary(bc_data)</code></pre>
<pre><code>##  sample_code_number clump_thickness  uniformity_of_cell_size
##  Min.   :   61634   Min.   : 1.000   Min.   : 1.000         
##  1st Qu.:  870688   1st Qu.: 2.000   1st Qu.: 1.000         
##  Median : 1171710   Median : 4.000   Median : 1.000         
##  Mean   : 1071704   Mean   : 4.418   Mean   : 3.134         
##  3rd Qu.: 1238298   3rd Qu.: 6.000   3rd Qu.: 5.000         
##  Max.   :13454352   Max.   :10.000   Max.   :10.000         
##                                                             
##  uniformity_of_cell_shape marginal_adhesion single_epithelial_cell_size
##  Min.   : 1.000           Min.   : 1.000    Min.   : 1.000             
##  1st Qu.: 1.000           1st Qu.: 1.000    1st Qu.: 2.000             
##  Median : 1.000           Median : 1.000    Median : 2.000             
##  Mean   : 3.207           Mean   : 2.807    Mean   : 3.216             
##  3rd Qu.: 5.000           3rd Qu.: 4.000    3rd Qu.: 4.000             
##  Max.   :10.000           Max.   :10.000    Max.   :10.000             
##                                                                        
##   bare_nuclei     bland_chromatin  normal_nucleoli     mitosis      
##  Min.   : 1.000   Min.   : 1.000   Min.   : 1.000   Min.   : 1.000  
##  1st Qu.: 1.000   1st Qu.: 2.000   1st Qu.: 1.000   1st Qu.: 1.000  
##  Median : 1.000   Median : 3.000   Median : 1.000   Median : 1.000  
##  Mean   : 3.545   Mean   : 3.438   Mean   : 2.867   Mean   : 1.589  
##  3rd Qu.: 6.000   3rd Qu.: 5.000   3rd Qu.: 4.000   3rd Qu.: 1.000  
##  Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :10.000  
##  NA&#39;s   :16                                                         
##    classes         
##  Length:699        
##  Class :character  
##  Mode  :character  
##                    
##                    
##                    
## </code></pre>
<p><br></p>
</div>
<div id="missing-data" class="section level3">
<h3>Missing data</h3>
<pre class="r"><code># how many NAs are in the data
md.pattern(bc_data, plot = FALSE)</code></pre>
<pre><code>##     sample_code_number clump_thickness uniformity_of_cell_size
## 683                  1               1                       1
## 16                   1               1                       1
##                      0               0                       0
##     uniformity_of_cell_shape marginal_adhesion single_epithelial_cell_size
## 683                        1                 1                           1
## 16                         1                 1                           1
##                            0                 0                           0
##     bland_chromatin normal_nucleoli mitosis classes bare_nuclei   
## 683               1               1       1       1           1  0
## 16                1               1       1       1           0  1
##                   0               0       0       0          16 16</code></pre>
<pre class="r"><code>bc_data &lt;- bc_data %&gt;%
  drop_na() %&gt;%
  select(classes, everything(), -sample_code_number)
head(bc_data)</code></pre>
<pre><code>## # A tibble: 6 x 10
##   classes   clump_thickness uniformity_of_cell_si… uniformity_of_cell_sha…
##   &lt;chr&gt;               &lt;int&gt;                  &lt;int&gt;                   &lt;int&gt;
## 1 benign                  5                      1                       1
## 2 benign                  5                      4                       4
## 3 benign                  3                      1                       1
## 4 benign                  6                      8                       8
## 5 benign                  4                      1                       1
## 6 malignant               8                     10                      10
## # ... with 6 more variables: marginal_adhesion &lt;int&gt;,
## #   single_epithelial_cell_size &lt;int&gt;, bare_nuclei &lt;dbl&gt;,
## #   bland_chromatin &lt;int&gt;, normal_nucleoli &lt;int&gt;, mitosis &lt;int&gt;</code></pre>
<p>Missing values can be imputed with the <em>mice</em> package.</p>
<p>More info and tutorial with code: <a href="https://shirinsplayground.netlify.com/2018/04/flu_prediction/" class="uri">https://shirinsplayground.netlify.com/2018/04/flu_prediction/</a></p>
<p><br></p>
</div>
<div id="data-exploration" class="section level3">
<h3>Data exploration</h3>
<ul>
<li>Response variable for classification</li>
</ul>
<pre class="r"><code>ggplot(bc_data, aes(x = classes, fill = classes)) +
  geom_bar()</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/response_classification-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>More info on dealing with unbalanced classes: <a href="https://shiring.github.io/machine_learning/2017/04/02/unbalanced" class="uri">https://shiring.github.io/machine_learning/2017/04/02/unbalanced</a></p>
<p><br></p>
<ul>
<li>Response variable for regression</li>
</ul>
<pre class="r"><code>ggplot(bc_data, aes(x = clump_thickness)) +
  geom_histogram(bins = 10)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/response_regression-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><br></p>
<ul>
<li>Features</li>
</ul>
<pre class="r"><code>gather(bc_data, x, y, clump_thickness:mitosis) %&gt;%
  ggplot(aes(x = y, color = classes, fill = classes)) +
    geom_density(alpha = 0.3) +
    facet_wrap( ~ x, scales = &quot;free&quot;, ncol = 3)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/features-1.png" width="768" style="display: block; margin: auto;" /></p>
<p><br></p>
<ul>
<li>Correlation graphs</li>
</ul>
<pre class="r"><code>co_mat_benign &lt;- filter(bc_data, classes == &quot;benign&quot;) %&gt;%
  select(-1) %&gt;%
  cor()

co_mat_malignant &lt;- filter(bc_data, classes == &quot;malignant&quot;) %&gt;%
  select(-1) %&gt;%
  cor()

library(igraph)
g_benign &lt;- graph.adjacency(co_mat_benign,
                         weighted = TRUE,
                         diag = FALSE,
                         mode = &quot;upper&quot;)

g_malignant &lt;- graph.adjacency(co_mat_malignant,
                         weighted = TRUE,
                         diag = FALSE,
                         mode = &quot;upper&quot;)


# http://kateto.net/networks-r-igraph

cut.off_b &lt;- mean(E(g_benign)$weight)
cut.off_m &lt;- mean(E(g_malignant)$weight)

g_benign_2 &lt;- delete_edges(g_benign, E(g_benign)[weight &lt; cut.off_b])
g_malignant_2 &lt;- delete_edges(g_malignant, E(g_malignant)[weight &lt; cut.off_m])

c_g_benign_2 &lt;- cluster_fast_greedy(g_benign_2) 
c_g_malignant_2 &lt;- cluster_fast_greedy(g_malignant_2) </code></pre>
<pre class="r"><code>par(mfrow = c(1,2))

plot(c_g_benign_2, g_benign_2,
     vertex.size = colSums(co_mat_benign) * 10,
     vertex.frame.color = NA, 
     vertex.label.color = &quot;black&quot;, 
     vertex.label.cex = 0.8,
     edge.width = E(g_benign_2)$weight * 15,
     layout = layout_with_fr(g_benign_2),
     main = &quot;Benign tumors&quot;)

plot(c_g_malignant_2, g_malignant_2,
     vertex.size = colSums(co_mat_malignant) * 10,
     vertex.frame.color = NA, 
     vertex.label.color = &quot;black&quot;, 
     vertex.label.cex = 0.8,
     edge.width = E(g_malignant_2)$weight * 15,
     layout = layout_with_fr(g_malignant_2),
     main = &quot;Malignant tumors&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/cor_graph-1.png" width="1152" /></p>
<p><br></p>
</div>
<div id="principal-component-analysis" class="section level3">
<h3>Principal Component Analysis</h3>
<pre class="r"><code>library(ellipse)

# perform pca and extract scores
pcaOutput &lt;- prcomp(as.matrix(bc_data[, -1]), scale = TRUE, center = TRUE)
pcaOutput2 &lt;- as.data.frame(pcaOutput$x)
  
# define groups for plotting
pcaOutput2$groups &lt;- bc_data$classes
  
centroids &lt;- aggregate(cbind(PC1, PC2) ~ groups, pcaOutput2, mean)

conf.rgn  &lt;- do.call(rbind, lapply(unique(pcaOutput2$groups), function(t)
  data.frame(groups = as.character(t),
             ellipse(cov(pcaOutput2[pcaOutput2$groups == t, 1:2]),
                   centre = as.matrix(centroids[centroids$groups == t, 2:3]),
                   level = 0.95),
             stringsAsFactors = FALSE)))
    
ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, group = groups, color = groups)) + 
    geom_polygon(data = conf.rgn, aes(fill = groups), alpha = 0.2) +
    geom_point(size = 2, alpha = 0.6) + 
    labs(color = &quot;&quot;,
         fill = &quot;&quot;) </code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/pca-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="multidimensional-scaling" class="section level3">
<h3>Multidimensional Scaling</h3>
<pre class="r"><code>select(bc_data, -1) %&gt;%
  dist() %&gt;%
  cmdscale %&gt;%
  as.data.frame() %&gt;%
  mutate(group = bc_data$classes) %&gt;%
  ggplot(aes(x = V1, y = V2, color = group)) +
    geom_point()</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/mds_plot-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="t-sne-dimensionality-reduction" class="section level3">
<h3>t-SNE dimensionality reduction</h3>
<pre class="r"><code>library(tsne)

select(bc_data, -1) %&gt;%
  dist() %&gt;%
  tsne() %&gt;%
  as.data.frame() %&gt;%
  mutate(group = bc_data$classes) %&gt;%
  ggplot(aes(x = V1, y = V2, color = group)) +
    geom_point()</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/tsne_plot-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
</div>
<div id="machine-learning-packages-for-r" class="section level2 tabset tabset-fade tabset-pills">
<h2>Machine Learning packages for R</h2>
<div id="caret" class="section level3">
<h3><a href="http://topepo.github.io/caret/index.html">caret</a></h3>
<pre class="r"><code># configure multicore
library(doParallel)
cl &lt;- makeCluster(detectCores())
registerDoParallel(cl)

library(caret)</code></pre>
<p><br></p>
<div id="training-validation-and-test-data" class="section level4">
<h4>Training, validation and test data</h4>
<pre class="r"><code>set.seed(42)
index &lt;- createDataPartition(bc_data$classes, p = 0.7, list = FALSE)
train_data &lt;- bc_data[index, ]
test_data  &lt;- bc_data[-index, ]</code></pre>
<pre class="r"><code>bind_rows(data.frame(group = &quot;train&quot;, train_data),
      data.frame(group = &quot;test&quot;, test_data)) %&gt;%
  gather(x, y, clump_thickness:mitosis) %&gt;%
  ggplot(aes(x = y, color = group, fill = group)) +
    geom_density(alpha = 0.3) +
    facet_wrap( ~ x, scales = &quot;free&quot;, ncol = 3)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/distribution-1.png" width="768" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="regression" class="section level4">
<h4>Regression</h4>
<pre class="r"><code>set.seed(42)
model_glm &lt;- caret::train(clump_thickness ~ .,
                          data = train_data,
                          method = &quot;glm&quot;,
                          preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                          trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))</code></pre>
<pre class="r"><code>model_glm</code></pre>
<pre><code>## Generalized Linear Model 
## 
## 479 samples
##   9 predictor
## 
## Pre-processing: scaled (9), centered (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 432, 431, 432, 431, 431, 431, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   1.972314  0.5254215  1.648832</code></pre>
<pre class="r"><code>predictions &lt;- predict(model_glm, test_data)</code></pre>
<pre class="r"><code># model_glm$finalModel$linear.predictors == model_glm$finalModel$fitted.values
data.frame(residuals = resid(model_glm),
           predictors = model_glm$finalModel$linear.predictors) %&gt;%
  ggplot(aes(x = predictors, y = residuals)) +
    geom_jitter() +
    geom_smooth(method = &quot;lm&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/residuals-1.png" width="480" style="display: block; margin: auto;" /></p>
<pre class="r"><code># y == train_data$clump_thickness
data.frame(residuals = resid(model_glm),
           y = model_glm$finalModel$y) %&gt;%
  ggplot(aes(x = y, y = residuals)) +
    geom_jitter() +
    geom_smooth(method = &quot;lm&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/residuals-2.png" width="480" style="display: block; margin: auto;" /></p>
<pre class="r"><code>data.frame(actual = test_data$clump_thickness,
           predicted = predictions) %&gt;%
  ggplot(aes(x = actual, y = predicted)) +
    geom_jitter() +
    geom_smooth(method = &quot;lm&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/regression_result-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="classification" class="section level4">
<h4>Classification</h4>
<div id="decision-trees" class="section level5">
<h5>Decision trees</h5>
<p><a href="https://cran.r-project.org/web/packages/rpart/rpart.pdf">rpart</a></p>
<pre class="r"><code>library(rpart)
library(rpart.plot)

set.seed(42)
fit &lt;- rpart(classes ~ .,
            data = train_data,
            method = &quot;class&quot;,
            control = rpart.control(xval = 10, 
                                    minbucket = 2, 
                                    cp = 0), 
             parms = list(split = &quot;information&quot;))

rpart.plot(fit, extra = 100)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/decision_tree-1.png" width="960" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
</div>
<div id="random-forests" class="section level4">
<h4>Random Forests</h4>
<p><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">Random Forests</a> predictions are based on the generation of multiple classification trees. They can be used for both, classification and regression tasks. Here, I show a classification task.</p>
<pre class="r"><code>set.seed(42)
model_rf &lt;- caret::train(classes ~ .,
                         data = train_data,
                         method = &quot;rf&quot;,
                         preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                         trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 5, 
                                                  repeats = 3, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))</code></pre>
<p>When you specify <code>savePredictions = TRUE</code>, you can access the cross-validation resuls with <code>model_rf$pred</code>.</p>
<pre class="r"><code>model_rf</code></pre>
<pre><code>## Random Forest 
## 
## 479 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: scaled (9), centered (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 432, 431, 431, 431, 431, 431, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##   2     0.9776753  0.9513499
##   5     0.9757957  0.9469999
##   9     0.9714200  0.9370285
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<pre class="r"><code>model_rf$finalModel$confusion</code></pre>
<pre><code>##           benign malignant class.error
## benign       304         7  0.02250804
## malignant      5       163  0.02976190</code></pre>
</div>
<div id="dealing-with-unbalanced-data" class="section level4">
<h4>Dealing with unbalanced data</h4>
<p>Luckily, caret makes it very easy to incorporate over- and under-sampling techniques with cross-validation resampling. We can simply add the sampling option to our trainControl and choose down for under- (also called down-) sampling. The rest stays the same as with our original model.</p>
<pre class="r"><code>set.seed(42)
model_rf_down &lt;- caret::train(classes ~ .,
                         data = train_data,
                         method = &quot;rf&quot;,
                         preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                         trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE,
                                                  sampling = &quot;down&quot;))</code></pre>
<pre class="r"><code>model_rf_down</code></pre>
<pre><code>## Random Forest 
## 
## 479 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: scaled (9), centered (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 432, 431, 431, 431, 431, 431, ... 
## Addtional sampling using down-sampling prior to pre-processing
## 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##   2     0.9797503  0.9563138
##   5     0.9741198  0.9438326
##   9     0.9699578  0.9346310
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<p><br></p>
</div>
<div id="feature-importance" class="section level4">
<h4>Feature Importance</h4>
<pre class="r"><code>imp &lt;- model_rf$finalModel$importance
imp[order(imp, decreasing = TRUE), ]</code></pre>
<pre><code>##     uniformity_of_cell_size    uniformity_of_cell_shape 
##                   43.936945                   39.840595 
##                 bare_nuclei             bland_chromatin 
##                   33.820345                   31.984813 
##             normal_nucleoli single_epithelial_cell_size 
##                   21.686039                   17.761202 
##             clump_thickness           marginal_adhesion 
##                   16.318817                    9.518437 
##                     mitosis 
##                    2.220633</code></pre>
<pre class="r"><code># estimate variable importance
importance &lt;- varImp(model_rf, scale = TRUE)
plot(importance)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/importance_rf-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><br></p>
<ul>
<li>predicting test data</li>
</ul>
<pre class="r"><code>confusionMatrix(predict(model_rf, test_data), as.factor(test_data$classes))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  benign malignant
##   benign       128         4
##   malignant      5        67
##                                           
##                Accuracy : 0.9559          
##                  95% CI : (0.9179, 0.9796)
##     No Information Rate : 0.652           
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.9031          
##  Mcnemar&#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.9624          
##             Specificity : 0.9437          
##          Pos Pred Value : 0.9697          
##          Neg Pred Value : 0.9306          
##              Prevalence : 0.6520          
##          Detection Rate : 0.6275          
##    Detection Prevalence : 0.6471          
##       Balanced Accuracy : 0.9530          
##                                           
##        &#39;Positive&#39; Class : benign          
## </code></pre>
<pre class="r"><code>results &lt;- data.frame(actual = test_data$classes,
                      predict(model_rf, test_data, type = &quot;prob&quot;))

results$prediction &lt;- ifelse(results$benign &gt; 0.5, &quot;benign&quot;,
                             ifelse(results$malignant &gt; 0.5, &quot;malignant&quot;, NA))

results$correct &lt;- ifelse(results$actual == results$prediction, TRUE, FALSE)

ggplot(results, aes(x = prediction, fill = correct)) +
  geom_bar(position = &quot;dodge&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/results_bar_rf-1.png" width="480" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(results, aes(x = prediction, y = benign, color = correct, shape = correct)) +
  geom_jitter(size = 3, alpha = 0.6)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/results_jitter_rf-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="extreme-gradient-boosting-trees" class="section level4">
<h4>Extreme gradient boosting trees</h4>
<p><a href="http://xgboost.readthedocs.io/en/latest/model.html">Extreme gradient boosting (XGBoost)</a> is a faster and improved implementation of <a href="https://en.wikipedia.org/wiki/Gradient_boosting">gradient boosting</a> for supervised learning.</p>
<blockquote>
<p>“XGBoost uses a more regularized model formalization to control over-fitting, which gives it better performance.” Tianqi Chen, developer of xgboost</p>
</blockquote>
<p>XGBoost is a tree ensemble model, which means the sum of predictions from a set of classification and regression trees (CART). In that, XGBoost is similar to Random Forests but it uses a different approach to model training. Can be used for classification and regression tasks. Here, I show a classification task.</p>
<pre class="r"><code>set.seed(42)
model_xgb &lt;- caret::train(classes ~ .,
                          data = train_data,
                          method = &quot;xgbTree&quot;,
                          preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                          trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 5, 
                                                  repeats = 3, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))</code></pre>
<pre class="r"><code>model_xgb</code></pre>
<pre><code>## eXtreme Gradient Boosting 
## 
## 479 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: scaled (9), centered (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 432, 431, 431, 431, 431, 431, ... 
## Resampling results across tuning parameters:
## 
##   eta  max_depth  colsample_bytree  subsample  nrounds  Accuracy 
##   0.3  1          0.6               0.50        50      0.9567788
##   0.3  1          0.6               0.50       100      0.9544912
##   0.3  1          0.6               0.50       150      0.9513572
##   0.3  1          0.6               0.75        50      0.9576164
##   0.3  1          0.6               0.75       100      0.9536448
##   0.3  1          0.6               0.75       150      0.9525987
##   0.3  1          0.6               1.00        50      0.9559409
##   0.3  1          0.6               1.00       100      0.9555242
##   0.3  1          0.6               1.00       150      0.9551031
##   0.3  1          0.8               0.50        50      0.9718588
##   0.3  1          0.8               0.50       100      0.9720583
##   0.3  1          0.8               0.50       150      0.9699879
##   0.3  1          0.8               0.75        50      0.9726964
##   0.3  1          0.8               0.75       100      0.9724664
##   0.3  1          0.8               0.75       150      0.9705868
##   0.3  1          0.8               1.00        50      0.9714202
##   0.3  1          0.8               1.00       100      0.9710035
##   0.3  1          0.8               1.00       150      0.9705866
##   0.3  2          0.6               0.50        50      0.9559448
##   0.3  2          0.6               0.50       100      0.9565397
##   0.3  2          0.6               0.50       150      0.9555063
##   0.3  2          0.6               0.75        50      0.9530150
##   0.3  2          0.6               0.75       100      0.9550985
##   0.3  2          0.6               0.75       150      0.9551070
##   0.3  2          0.6               1.00        50      0.9532320
##   0.3  2          0.6               1.00       100      0.9551072
##   0.3  2          0.6               1.00       150      0.9557237
##   0.3  2          0.8               0.50        50      0.9720583
##   0.3  2          0.8               0.50       100      0.9735166
##   0.3  2          0.8               0.50       150      0.9720540
##   0.3  2          0.8               0.75        50      0.9722494
##   0.3  2          0.8               0.75       100      0.9726703
##   0.3  2          0.8               0.75       150      0.9716374
##   0.3  2          0.8               1.00        50      0.9716327
##   0.3  2          0.8               1.00       100      0.9724622
##   0.3  2          0.8               1.00       150      0.9718416
##   0.3  3          0.6               0.50        50      0.9548905
##   0.3  3          0.6               0.50       100      0.9557237
##   0.3  3          0.6               0.50       150      0.9555198
##   0.3  3          0.6               0.75        50      0.9561404
##   0.3  3          0.6               0.75       100      0.9546820
##   0.3  3          0.6               0.75       150      0.9552982
##   0.3  3          0.6               1.00        50      0.9577983
##   0.3  3          0.6               1.00       100      0.9573819
##   0.3  3          0.6               1.00       150      0.9567655
##   0.3  3          0.8               0.50        50      0.9733131
##   0.3  3          0.8               0.50       100      0.9728829
##   0.3  3          0.8               0.50       150      0.9718499
##   0.3  3          0.8               0.75        50      0.9751879
##   0.3  3          0.8               0.75       100      0.9743546
##   0.3  3          0.8               0.75       150      0.9735212
##   0.3  3          0.8               1.00        50      0.9743372
##   0.3  3          0.8               1.00       100      0.9737122
##   0.3  3          0.8               1.00       150      0.9743461
##   0.4  1          0.6               0.50        50      0.9548861
##   0.4  1          0.6               0.50       100      0.9528290
##   0.4  1          0.6               0.50       150      0.9498772
##   0.4  1          0.6               0.75        50      0.9557239
##   0.4  1          0.6               0.75       100      0.9513529
##   0.4  1          0.6               0.75       150      0.9492779
##   0.4  1          0.6               1.00        50      0.9559365
##   0.4  1          0.6               1.00       100      0.9551031
##   0.4  1          0.6               1.00       150      0.9536361
##   0.4  1          0.8               0.50        50      0.9710164
##   0.4  1          0.8               0.50       100      0.9697577
##   0.4  1          0.8               0.50       150      0.9687074
##   0.4  1          0.8               0.75        50      0.9710122
##   0.4  1          0.8               0.75       100      0.9707996
##   0.4  1          0.8               0.75       150      0.9691455
##   0.4  1          0.8               1.00        50      0.9705911
##   0.4  1          0.8               1.00       100      0.9697446
##   0.4  1          0.8               1.00       150      0.9697576
##   0.4  2          0.6               0.50        50      0.9544866
##   0.4  2          0.6               0.50       100      0.9542694
##   0.4  2          0.6               0.50       150      0.9536357
##   0.4  2          0.6               0.75        50      0.9540611
##   0.4  2          0.6               0.75       100      0.9542694
##   0.4  2          0.6               0.75       150      0.9549033
##   0.4  2          0.6               1.00        50      0.9540653
##   0.4  2          0.6               1.00       100      0.9555239
##   0.4  2          0.6               1.00       150      0.9546818
##   0.4  2          0.8               0.50        50      0.9720670
##   0.4  2          0.8               0.50       100      0.9695629
##   0.4  2          0.8               0.50       150      0.9702006
##   0.4  2          0.8               0.75        50      0.9722627
##   0.4  2          0.8               0.75       100      0.9720500
##   0.4  2          0.8               0.75       150      0.9716289
##   0.4  2          0.8               1.00        50      0.9726705
##   0.4  2          0.8               1.00       100      0.9708042
##   0.4  2          0.8               1.00       150      0.9708129
##   0.4  3          0.6               0.50        50      0.9555150
##   0.4  3          0.6               0.50       100      0.9553021
##   0.4  3          0.6               0.50       150      0.9548943
##   0.4  3          0.6               0.75        50      0.9555281
##   0.4  3          0.6               0.75       100      0.9563662
##   0.4  3          0.6               0.75       150      0.9555324
##   0.4  3          0.6               1.00        50      0.9575900
##   0.4  3          0.6               1.00       100      0.9571735
##   0.4  3          0.6               1.00       150      0.9559104
##   0.4  3          0.8               0.50        50      0.9737255
##   0.4  3          0.8               0.50       100      0.9745501
##   0.4  3          0.8               0.50       150      0.9730874
##   0.4  3          0.8               0.75        50      0.9747539
##   0.4  3          0.8               0.75       100      0.9724664
##   0.4  3          0.8               0.75       150      0.9720498
##   0.4  3          0.8               1.00        50      0.9747539
##   0.4  3          0.8               1.00       100      0.9749624
##   0.4  3          0.8               1.00       150      0.9734996
##   Kappa    
##   0.9050828
##   0.8999999
##   0.8930637
##   0.9067208
##   0.8982284
##   0.8959903
##   0.9028825
##   0.9022543
##   0.9014018
##   0.9382467
##   0.9386326
##   0.9340573
##   0.9400323
##   0.9395968
##   0.9353783
##   0.9372262
##   0.9362148
##   0.9353247
##   0.9032270
##   0.9047203
##   0.9024465
##   0.8968511
##   0.9015282
##   0.9016169
##   0.8971329
##   0.9015111
##   0.9028614
##   0.9387022
##   0.9419143
##   0.9387792
##   0.9391933
##   0.9401872
##   0.9379714
##   0.9377309
##   0.9397601
##   0.9384827
##   0.9008861
##   0.9029797
##   0.9024531
##   0.9037859
##   0.9004226
##   0.9019909
##   0.9074584
##   0.9064701
##   0.9051441
##   0.9414031
##   0.9405025
##   0.9380734
##   0.9456856
##   0.9438986
##   0.9419994
##   0.9438642
##   0.9426000
##   0.9439780
##   0.9007223
##   0.8964381
##   0.8897615
##   0.9027951
##   0.8931520
##   0.8886910
##   0.9030461
##   0.9014362
##   0.8982364
##   0.9363059
##   0.9334254
##   0.9311383
##   0.9361883
##   0.9357131
##   0.9320657
##   0.9353688
##   0.9333607
##   0.9334467
##   0.8999756
##   0.8997888
##   0.8983861
##   0.8991356
##   0.8998960
##   0.9013529
##   0.8990428
##   0.9023340
##   0.9004889
##   0.9387165
##   0.9332663
##   0.9345567
##   0.9393855
##   0.9389455
##   0.9380863
##   0.9401366
##   0.9361847
##   0.9361724
##   0.9021263
##   0.9017938
##   0.9010613
##   0.9025263
##   0.9043436
##   0.9024744
##   0.9069828
##   0.9059579
##   0.9031829
##   0.9424523
##   0.9442537
##   0.9410193
##   0.9447486
##   0.9397683
##   0.9388701
##   0.9449064
##   0.9454375
##   0.9422358
## 
## Tuning parameter &#39;gamma&#39; was held constant at a value of 0
## 
## Tuning parameter &#39;min_child_weight&#39; was held constant at a value of 1
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were nrounds = 50, max_depth = 3,
##  eta = 0.3, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1
##  and subsample = 0.75.</code></pre>
<p><br></p>
<ul>
<li>Feature Importance</li>
</ul>
<pre class="r"><code>importance &lt;- varImp(model_xgb, scale = TRUE)
plot(importance)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/importance_xgb-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><br></p>
<ul>
<li>predicting test data</li>
</ul>
<pre class="r"><code>confusionMatrix(predict(model_xgb, test_data), as.factor(test_data$classes))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  benign malignant
##   benign       128         3
##   malignant      5        68
##                                           
##                Accuracy : 0.9608          
##                  95% CI : (0.9242, 0.9829)
##     No Information Rate : 0.652           
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.9142          
##  Mcnemar&#39;s Test P-Value : 0.7237          
##                                           
##             Sensitivity : 0.9624          
##             Specificity : 0.9577          
##          Pos Pred Value : 0.9771          
##          Neg Pred Value : 0.9315          
##              Prevalence : 0.6520          
##          Detection Rate : 0.6275          
##    Detection Prevalence : 0.6422          
##       Balanced Accuracy : 0.9601          
##                                           
##        &#39;Positive&#39; Class : benign          
## </code></pre>
<pre class="r"><code>results &lt;- data.frame(actual = test_data$classes,
                      predict(model_xgb, test_data, type = &quot;prob&quot;))

results$prediction &lt;- ifelse(results$benign &gt; 0.5, &quot;benign&quot;,
                             ifelse(results$malignant &gt; 0.5, &quot;malignant&quot;, NA))

results$correct &lt;- ifelse(results$actual == results$prediction, TRUE, FALSE)

ggplot(results, aes(x = prediction, fill = correct)) +
  geom_bar(position = &quot;dodge&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/results_bar_xgb-1.png" width="480" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(results, aes(x = prediction, y = benign, color = correct, shape = correct)) +
  geom_jitter(size = 3, alpha = 0.6)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/results_jitter_xgb-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="available-models-in-caret" class="section level2">
<h2>Available models in caret</h2>
<p><a href="https://topepo.github.io/caret/available-models.html" class="uri">https://topepo.github.io/caret/available-models.html</a></p>
<p><br></p>
<div id="feature-selection" class="section level4">
<h4>Feature Selection</h4>
<p>Performing feature selection on the whole dataset would lead to prediction bias, we therefore need to run the whole modeling process on the training data alone!</p>
<ul>
<li>Correlation</li>
</ul>
<p>Correlations between all features are calculated and visualised with the <em>corrplot</em> package. I am then removing all features with a correlation higher than 0.7, keeping the feature with the lower mean.</p>
<pre class="r"><code>library(corrplot)

# calculate correlation matrix
corMatMy &lt;- cor(train_data[, -1])
corrplot(corMatMy, order = &quot;hclust&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/corplot-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Apply correlation filter at 0.70,
highlyCor &lt;- colnames(train_data[, -1])[findCorrelation(corMatMy, cutoff = 0.7, verbose = TRUE)]</code></pre>
<pre><code>## Compare row 2  and column  3 with corr  0.908 
##   Means:  0.709 vs 0.594 so flagging column 2 
## Compare row 3  and column  7 with corr  0.749 
##   Means:  0.67 vs 0.569 so flagging column 3 
## All correlations &lt;= 0.7</code></pre>
<pre class="r"><code># which variables are flagged for removal?
highlyCor</code></pre>
<pre><code>## [1] &quot;uniformity_of_cell_size&quot;  &quot;uniformity_of_cell_shape&quot;</code></pre>
<pre class="r"><code>#then we remove these variables
train_data_cor &lt;- train_data[, which(!colnames(train_data) %in% highlyCor)]</code></pre>
<p><br></p>
<ul>
<li>Recursive Feature Elimination (RFE)</li>
</ul>
<p>Another way to choose features is with Recursive Feature Elimination. RFE uses a Random Forest algorithm to test combinations of features and rate each with an accuracy score. The combination with the highest score is usually preferential.</p>
<pre class="r"><code>set.seed(7)
results_rfe &lt;- rfe(x = train_data[, -1], 
                   y = as.factor(train_data$classes), 
                   sizes = c(1:9), 
                   rfeControl = rfeControl(functions = rfFuncs, method = &quot;cv&quot;, number = 10))</code></pre>
<pre class="r"><code># chosen features
predictors(results_rfe)</code></pre>
<pre><code>## [1] &quot;bare_nuclei&quot;                 &quot;clump_thickness&quot;            
## [3] &quot;uniformity_of_cell_size&quot;     &quot;uniformity_of_cell_shape&quot;   
## [5] &quot;bland_chromatin&quot;             &quot;normal_nucleoli&quot;            
## [7] &quot;marginal_adhesion&quot;           &quot;single_epithelial_cell_size&quot;</code></pre>
<pre class="r"><code>train_data_rfe &lt;- train_data[, c(1, which(colnames(train_data) %in% predictors(results_rfe)))]</code></pre>
<p><br></p>
<ul>
<li>Genetic Algorithm (GA)</li>
</ul>
<p>The Genetic Algorithm (GA) has been developed based on evolutionary principles of natural selection: It aims to optimize a population of individuals with a given set of genotypes by modeling selection over time. In each generation (i.e. iteration), each individual’s fitness is calculated based on their genotypes. Then, the fittest individuals are chosen to produce the next generation. This subsequent generation of individuals will have genotypes resulting from (re-) combinations of the parental alleles. These new genotypes will again determine each individual’s fitness. This selection process is iterated for a specified number of generations and (ideally) leads to fixation of the fittest alleles in the gene pool.</p>
<p>This concept of optimization can be applied to non-evolutionary models as well, like feature selection processes in machine learning.</p>
<pre class="r"><code>set.seed(27)
model_ga &lt;- gafs(x = train_data[, -1], 
                 y = as.factor(train_data$classes),
                 iters = 10, # generations of algorithm
                 popSize = 10, # population size for each generation
                 levels = c(&quot;malignant&quot;, &quot;benign&quot;),
                 gafsControl = gafsControl(functions = rfGA, # Assess fitness with RF
                                           method = &quot;cv&quot;,    # 10 fold cross validation
                                           genParallel = TRUE, # Use parallel programming
                                           allowParallel = TRUE))</code></pre>
<pre class="r"><code>plot(model_ga) # Plot mean fitness (AUC) by generation</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-38-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>train_data_ga &lt;- train_data[, c(1, which(colnames(train_data) %in% model_ga$ga$final))]</code></pre>
<p><br></p>
</div>
<div id="hyperparameter-tuning-with-caret" class="section level3">
<h3>Hyperparameter tuning with caret</h3>
<ul>
<li><p>Cartesian Grid</p></li>
<li><p>mtry: Number of variables randomly sampled as candidates at each split.</p></li>
</ul>
<pre class="r"><code>set.seed(42)
grid &lt;- expand.grid(mtry = c(1:10))

model_rf_tune_man &lt;- caret::train(classes ~ .,
                         data = train_data,
                         method = &quot;rf&quot;,
                         preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                         trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE),
                         tuneGrid = grid)</code></pre>
<pre class="r"><code>model_rf_tune_man</code></pre>
<pre><code>## Random Forest 
## 
## 479 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: scaled (9), centered (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 432, 431, 431, 431, 431, 431, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    1    0.9785044  0.9532161
##    2    0.9772586  0.9504377
##    3    0.9774625  0.9508246
##    4    0.9766333  0.9488778
##    5    0.9753789  0.9460274
##    6    0.9737078  0.9422613
##    7    0.9730957  0.9408547
##    8    0.9714155  0.9371611
##    9    0.9718280  0.9380578
##   10    0.9718280  0.9380135
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 1.</code></pre>
<pre class="r"><code>plot(model_rf_tune_man)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-42-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
<ul>
<li>Random Search</li>
</ul>
<pre class="r"><code>set.seed(42)
model_rf_tune_auto &lt;- caret::train(classes ~ .,
                         data = train_data,
                         method = &quot;rf&quot;,
                         preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                         trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE,
                                                  search = &quot;random&quot;),
                         tuneGrid = grid,
                         tuneLength = 15)</code></pre>
<pre class="r"><code>model_rf_tune_auto</code></pre>
<pre><code>## Random Forest 
## 
## 479 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: scaled (9), centered (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 432, 431, 431, 431, 431, 431, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    1    0.9785044  0.9532161
##    2    0.9772586  0.9504377
##    3    0.9774625  0.9508246
##    4    0.9766333  0.9488778
##    5    0.9753789  0.9460274
##    6    0.9737078  0.9422613
##    7    0.9730957  0.9408547
##    8    0.9714155  0.9371611
##    9    0.9718280  0.9380578
##   10    0.9718280  0.9380135
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 1.</code></pre>
<pre class="r"><code>plot(model_rf_tune_auto)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-46-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="grid-search-with-h2o" class="section level3">
<h3>Grid search with h2o</h3>
<p>The R package h2o provides a convenient interface to <a href="http://www.h2o.ai/h2o/">H2O</a>, which is an open-source machine learning and deep learning platform. H2O distributes a wide range of common machine learning algorithms for classification, regression and deep learning.</p>
<pre class="r"><code>library(h2o)
h2o.init(nthreads = -1)</code></pre>
<pre><code>##  Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         3 days 4 minutes 
##     H2O cluster timezone:       Europe/Berlin 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.20.0.2 
##     H2O cluster version age:    16 days  
##     H2O cluster name:           H2O_started_from_R_shiringlander_jrj894 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   3.27 GB 
##     H2O cluster total cores:    8 
##     H2O cluster allowed cores:  8 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.0 (2018-04-23)</code></pre>
<pre class="r"><code>h2o.no_progress()

bc_data_hf &lt;- as.h2o(bc_data)</code></pre>
<pre class="r"><code>h2o.describe(bc_data_hf) %&gt;%
  gather(x, y, Zeros:Sigma) %&gt;%
  mutate(group = ifelse(x %in% c(&quot;Min&quot;, &quot;Max&quot;, &quot;Mean&quot;), &quot;min, mean, max&quot;, 
                        ifelse(x %in% c(&quot;NegInf&quot;, &quot;PosInf&quot;), &quot;Inf&quot;, &quot;sigma, zeros&quot;))) %&gt;% 
  ggplot(aes(x = Label, y = as.numeric(y), color = x)) +
    geom_point(size = 4, alpha = 0.6) +
    scale_color_brewer(palette = &quot;Set1&quot;) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    facet_grid(group ~ ., scales = &quot;free&quot;) +
    labs(x = &quot;Feature&quot;,
         y = &quot;Value&quot;,
         color = &quot;&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/h2o_describe-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>library(reshape2) # for melting

bc_data_hf[, 1] &lt;- h2o.asfactor(bc_data_hf[, 1])

cor &lt;- h2o.cor(bc_data_hf)
rownames(cor) &lt;- colnames(cor)

melt(cor) %&gt;%
  mutate(Var2 = rep(rownames(cor), nrow(cor))) %&gt;%
  mutate(Var2 = factor(Var2, levels = colnames(cor))) %&gt;%
  mutate(variable = factor(variable, levels = colnames(cor))) %&gt;%
  ggplot(aes(x = variable, y = Var2, fill = value)) + 
    geom_tile(width = 0.9, height = 0.9) +
    scale_fill_gradient2(low = &quot;white&quot;, high = &quot;red&quot;, name = &quot;Cor.&quot;) +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
    labs(x = &quot;&quot;, 
         y = &quot;&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/corr_plot-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
<div id="training-validation-and-test-data-1" class="section level4">
<h4>Training, validation and test data</h4>
<pre class="r"><code>splits &lt;- h2o.splitFrame(bc_data_hf, 
                         ratios = c(0.7, 0.15), 
                         seed = 1)

train &lt;- splits[[1]]
valid &lt;- splits[[2]]
test &lt;- splits[[3]]

response &lt;- &quot;classes&quot;
features &lt;- setdiff(colnames(train), response)</code></pre>
<pre class="r"><code>summary(as.factor(train$classes), exact_quantiles = TRUE)</code></pre>
<pre><code>##  classes       
##  benign   :313 
##  malignant:167</code></pre>
<pre class="r"><code>summary(as.factor(valid$classes), exact_quantiles = TRUE)</code></pre>
<pre><code>##  classes      
##  benign   :64 
##  malignant:38</code></pre>
<pre class="r"><code>summary(as.factor(test$classes), exact_quantiles = TRUE)</code></pre>
<pre><code>##  classes      
##  benign   :67 
##  malignant:34</code></pre>
<pre class="r"><code>pca &lt;- h2o.prcomp(training_frame = train,
           x = features,
           validation_frame = valid,
           transform = &quot;NORMALIZE&quot;,
           impute_missing = TRUE,
           k = 3,
           seed = 42)

eigenvec &lt;- as.data.frame(pca@model$eigenvectors)
eigenvec$label &lt;- features

library(ggrepel)
ggplot(eigenvec, aes(x = pc1, y = pc2, label = label)) +
  geom_point(color = &quot;navy&quot;, alpha = 0.7) +
  geom_text_repel()</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/pca_features-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="classification-1" class="section level4">
<h4>Classification</h4>
<div id="random-forest" class="section level5">
<h5>Random Forest</h5>
<pre class="r"><code>hyper_params &lt;- list(
                     ntrees = c(25, 50, 75, 100),
                     max_depth = c(10, 20, 30),
                     min_rows = c(1, 3, 5)
                     )

search_criteria &lt;- list(
                        strategy = &quot;RandomDiscrete&quot;, 
                        max_models = 50,
                        max_runtime_secs = 360,
                        stopping_rounds = 5,          
                        stopping_metric = &quot;AUC&quot;,      
                        stopping_tolerance = 0.0005,
                        seed = 42
                        )</code></pre>
<pre class="r"><code>rf_grid &lt;- h2o.grid(algorithm = &quot;randomForest&quot;, # h2o.randomForest, 
                                                # alternatively h2o.gbm 
                                                # for Gradient boosting trees
                    x = features,
                    y = response,
                    grid_id = &quot;rf_grid&quot;,
                    training_frame = train,
                    validation_frame = valid,
                    nfolds = 25,                           
                    fold_assignment = &quot;Stratified&quot;,
                    hyper_params = hyper_params,
                    search_criteria = search_criteria,
                    seed = 42
                    )</code></pre>
<pre class="r"><code># performance metrics where smaller is better -&gt; order with decreasing = FALSE
sort_options_1 &lt;- c(&quot;mean_per_class_error&quot;, &quot;mse&quot;, &quot;err&quot;, &quot;logloss&quot;)

for (sort_by_1 in sort_options_1) {
  
  grid &lt;- h2o.getGrid(&quot;rf_grid&quot;, sort_by = sort_by_1, decreasing = FALSE)
  
  model_ids &lt;- grid@model_ids
  best_model &lt;- h2o.getModel(model_ids[[1]])
  
  h2o.saveModel(best_model, path=&quot;models&quot;, force = TRUE)
  
}


# performance metrics where bigger is better -&gt; order with decreasing = TRUE
sort_options_2 &lt;- c(&quot;auc&quot;, &quot;precision&quot;, &quot;accuracy&quot;, &quot;recall&quot;, &quot;specificity&quot;)

for (sort_by_2 in sort_options_2) {
  
  grid &lt;- h2o.getGrid(&quot;rf_grid&quot;, sort_by = sort_by_2, decreasing = TRUE)
  
  model_ids &lt;- grid@model_ids
  best_model &lt;- h2o.getModel(model_ids[[1]])
  
  h2o.saveModel(best_model, path = &quot;models&quot;, force = TRUE)
  
}</code></pre>
<pre class="r"><code>files &lt;- list.files(path = &quot;models&quot;)</code></pre>
<pre class="r"><code>rf_models &lt;- files[grep(&quot;rf_grid_model&quot;, files)]

for (model_id in rf_models) {
  
  path &lt;- paste0(getwd(), &quot;/models/&quot;, model_id)
  best_model &lt;- h2o.loadModel(path)
  mse_auc_test &lt;- data.frame(model_id = model_id, 
                             mse = h2o.mse(h2o.performance(best_model, test)),
                             auc = h2o.auc(h2o.performance(best_model, test)))
  
  if (model_id == rf_models[[1]]) {
    
    mse_auc_test_comb &lt;- mse_auc_test
    
  } else {
    
    mse_auc_test_comb &lt;- rbind(mse_auc_test_comb, mse_auc_test)
    
  }
}</code></pre>
<pre class="r"><code>mse_auc_test_comb %&gt;%
  gather(x, y, mse:auc) %&gt;%
  ggplot(aes(x = model_id, y = y, fill = model_id)) +
    facet_grid(x ~ ., scales = &quot;free&quot;) +
    geom_bar(stat = &quot;identity&quot;, alpha = 0.8, position = &quot;dodge&quot;) +
    scale_fill_brewer(palette = &quot;Set1&quot;) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
          plot.margin = unit(c(0.5, 0, 0, 1.5), &quot;cm&quot;)) +
    labs(x = &quot;&quot;, y = &quot;value&quot;, fill = &quot;&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/auc_mse-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>for (model_id in rf_models) {
  
  best_model &lt;- h2o.getModel(model_id)
  
  finalRf_predictions &lt;- data.frame(model_id = rep(best_model@model_id, 
                                                   nrow(test)),
                                    actual = as.vector(test$classes), 
                                    as.data.frame(h2o.predict(object = best_model, 
                                                              newdata = test)))
  
  finalRf_predictions$accurate &lt;- ifelse(finalRf_predictions$actual == 
                                           finalRf_predictions$predict, 
                                         &quot;yes&quot;, &quot;no&quot;)
  
  finalRf_predictions$predict_stringent &lt;- ifelse(finalRf_predictions$benign &gt; 0.8, 
                                                  &quot;benign&quot;, 
                                                  ifelse(finalRf_predictions$malignant 
                                                         &gt; 0.8, &quot;malignant&quot;, &quot;uncertain&quot;))
  
  finalRf_predictions$accurate_stringent &lt;- ifelse(finalRf_predictions$actual == 
                                                     finalRf_predictions$predict_stringent, &quot;yes&quot;, 
                                         ifelse(finalRf_predictions$predict_stringent == 
                                                  &quot;uncertain&quot;, &quot;na&quot;, &quot;no&quot;))
  
  if (model_id == rf_models[[1]]) {
    
    finalRf_predictions_comb &lt;- finalRf_predictions
    
  } else {
    
    finalRf_predictions_comb &lt;- rbind(finalRf_predictions_comb, finalRf_predictions)
    
  }
}</code></pre>
<pre class="r"><code>finalRf_predictions_comb %&gt;%
  ggplot(aes(x = actual, fill = accurate)) +
    geom_bar(position = &quot;dodge&quot;) +
    scale_fill_brewer(palette = &quot;Set1&quot;) +
    facet_wrap(~ model_id, ncol = 2) +
    labs(fill = &quot;Were\npredictions\naccurate?&quot;,
         title = &quot;Default predictions&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/final_predictions_rf-1.png" width="864" style="display: block; margin: auto;" /></p>
<pre class="r"><code>finalRf_predictions_comb %&gt;%
  subset(accurate_stringent != &quot;na&quot;) %&gt;%
  ggplot(aes(x = actual, fill = accurate_stringent)) +
    geom_bar(position = &quot;dodge&quot;) +
    scale_fill_brewer(palette = &quot;Set1&quot;) +
    facet_wrap(~ model_id, ncol = 2) +
    labs(fill = &quot;Were\npredictions\naccurate?&quot;,
         title = &quot;Stringent predictions&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/final_predictions_rf-2.png" width="864" style="display: block; margin: auto;" /></p>
<pre class="r"><code>rf_model &lt;- h2o.loadModel(&quot;models/rf_grid_model_0&quot;)</code></pre>
<pre class="r"><code>h2o.varimp_plot(rf_model)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-60-1.png" width="672" /></p>
<pre class="r"><code>#h2o.varimp(rf_model)</code></pre>
<pre class="r"><code>h2o.mean_per_class_error(rf_model, train = TRUE, valid = TRUE, xval = TRUE)</code></pre>
<pre><code>##      train      valid       xval 
## 0.02196246 0.02343750 0.02515735</code></pre>
<pre class="r"><code>h2o.confusionMatrix(rf_model, valid = TRUE)</code></pre>
<pre><code>## Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.533333333333333:
##           benign malignant    Error    Rate
## benign        61         3 0.046875   =3/64
## malignant      0        38 0.000000   =0/38
## Totals        61        41 0.029412  =3/102</code></pre>
<pre class="r"><code>plot(rf_model,
     timestep = &quot;number_of_trees&quot;,
     metric = &quot;classification_error&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-63-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plot(rf_model,
     timestep = &quot;number_of_trees&quot;,
     metric = &quot;logloss&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-64-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plot(rf_model,
     timestep = &quot;number_of_trees&quot;,
     metric = &quot;AUC&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-65-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plot(rf_model,
     timestep = &quot;number_of_trees&quot;,
     metric = &quot;rmse&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-66-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>h2o.auc(rf_model, train = TRUE)</code></pre>
<pre><code>## [1] 0.9907214</code></pre>
<pre class="r"><code>h2o.auc(rf_model, valid = TRUE)</code></pre>
<pre><code>## [1] 0.9829359</code></pre>
<pre class="r"><code>h2o.auc(rf_model, xval = TRUE)</code></pre>
<pre><code>## [1] 0.9903005</code></pre>
<pre class="r"><code>perf &lt;- h2o.performance(rf_model, test)
perf</code></pre>
<pre><code>## H2OBinomialMetrics: drf
## 
## MSE:  0.03258482
## RMSE:  0.1805127
## LogLoss:  0.1072519
## Mean Per-Class Error:  0.02985075
## AUC:  0.9916594
## Gini:  0.9833187
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##           benign malignant    Error    Rate
## benign        63         4 0.059701   =4/67
## malignant      0        34 0.000000   =0/34
## Totals        63        38 0.039604  =4/101
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.306667 0.944444  18
## 2                       max f2  0.306667 0.977011  18
## 3                 max f0point5  0.720000 0.933735  13
## 4                 max accuracy  0.533333 0.960396  16
## 5                max precision  1.000000 1.000000   0
## 6                   max recall  0.306667 1.000000  18
## 7              max specificity  1.000000 1.000000   0
## 8             max absolute_mcc  0.306667 0.917235  18
## 9   max min_per_class_accuracy  0.533333 0.955224  16
## 10 max mean_per_class_accuracy  0.306667 0.970149  18
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`</code></pre>
<pre class="r"><code>plot(perf)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/auc_curve-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>perf@metrics$thresholds_and_metric_scores %&gt;%
  ggplot(aes(x = fpr, y = tpr)) +
    geom_point() +
    geom_line() +
    geom_abline(slope = 1, intercept = 0) +
    labs(x = &quot;False Positive Rate&quot;,
         y = &quot;True Positive Rate&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-69-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>h2o.logloss(perf)</code></pre>
<pre><code>## [1] 0.1072519</code></pre>
<pre class="r"><code>h2o.mse(perf)</code></pre>
<pre><code>## [1] 0.03258482</code></pre>
<pre class="r"><code>h2o.auc(perf)</code></pre>
<pre><code>## [1] 0.9916594</code></pre>
<pre class="r"><code>head(h2o.metric(perf))</code></pre>
<pre><code>## Metrics for Thresholds: Binomial metrics as a function of classification thresholds
##   threshold       f1       f2 f0point5 accuracy precision   recall
## 1  1.000000 0.583333 0.466667 0.777778 0.801980  1.000000 0.411765
## 2  0.986667 0.666667 0.555556 0.833333 0.831683  1.000000 0.500000
## 3  0.973333 0.716981 0.612903 0.863636 0.851485  1.000000 0.558824
## 4  0.960000 0.740741 0.641026 0.877193 0.861386  1.000000 0.588235
## 5  0.946667 0.763636 0.668790 0.889831 0.871287  1.000000 0.617647
## 6  0.920000 0.807018 0.723270 0.912698 0.891089  1.000000 0.676471
##   specificity absolute_mcc min_per_class_accuracy mean_per_class_accuracy
## 1    1.000000     0.563122               0.411765                0.705882
## 2    1.000000     0.631514               0.500000                0.750000
## 3    1.000000     0.675722               0.558824                0.779412
## 4    1.000000     0.697542               0.588235                0.794118
## 5    1.000000     0.719221               0.617647                0.808824
## 6    1.000000     0.762280               0.676471                0.838235
##   tns fns fps tps      tnr      fnr      fpr      tpr idx
## 1  67  20   0  14 1.000000 0.588235 0.000000 0.411765   0
## 2  67  17   0  17 1.000000 0.500000 0.000000 0.500000   1
## 3  67  15   0  19 1.000000 0.441176 0.000000 0.558824   2
## 4  67  14   0  20 1.000000 0.411765 0.000000 0.588235   3
## 5  67  13   0  21 1.000000 0.382353 0.000000 0.617647   4
## 6  67  11   0  23 1.000000 0.323529 0.000000 0.676471   5</code></pre>
<pre class="r"><code>finalRf_predictions &lt;- data.frame(actual = as.vector(test$classes), 
                                  as.data.frame(h2o.predict(object = rf_model, 
                                                            newdata = test)))

finalRf_predictions$accurate &lt;- ifelse(finalRf_predictions$actual == 
                                         finalRf_predictions$predict, &quot;yes&quot;, &quot;no&quot;)

finalRf_predictions$predict_stringent &lt;- ifelse(finalRf_predictions$benign &gt; 0.8, &quot;benign&quot;, 
                                                ifelse(finalRf_predictions$malignant 
                                                       &gt; 0.8, &quot;malignant&quot;, &quot;uncertain&quot;))
finalRf_predictions$accurate_stringent &lt;- ifelse(finalRf_predictions$actual == 
                                                   finalRf_predictions$predict_stringent, &quot;yes&quot;, 
                                       ifelse(finalRf_predictions$predict_stringent == 
                                                &quot;uncertain&quot;, &quot;na&quot;, &quot;no&quot;))

finalRf_predictions %&gt;%
  group_by(actual, predict) %&gt;%
  dplyr::summarise(n = n())</code></pre>
<pre><code>## # A tibble: 4 x 3
## # Groups:   actual [?]
##   actual    predict       n
##   &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;
## 1 benign    benign       64
## 2 benign    malignant     3
## 3 malignant benign        1
## 4 malignant malignant    33</code></pre>
<pre class="r"><code>finalRf_predictions %&gt;%
  group_by(actual, predict_stringent) %&gt;%
  dplyr::summarise(n = n())</code></pre>
<pre><code>## # A tibble: 5 x 3
## # Groups:   actual [?]
##   actual    predict_stringent     n
##   &lt;fct&gt;     &lt;chr&gt;             &lt;int&gt;
## 1 benign    benign               62
## 2 benign    malignant             2
## 3 benign    uncertain             3
## 4 malignant malignant            29
## 5 malignant uncertain             5</code></pre>
<pre class="r"><code>finalRf_predictions %&gt;%
  ggplot(aes(x = actual, fill = accurate)) +
    geom_bar(position = &quot;dodge&quot;) +
    scale_fill_brewer(palette = &quot;Set1&quot;) +
    labs(fill = &quot;Were\npredictions\naccurate?&quot;,
         title = &quot;Default predictions&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/default_vs_stringent-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>finalRf_predictions %&gt;%
  subset(accurate_stringent != &quot;na&quot;) %&gt;%
  ggplot(aes(x = actual, fill = accurate_stringent)) +
    geom_bar(position = &quot;dodge&quot;) +
    scale_fill_brewer(palette = &quot;Set1&quot;) +
    labs(fill = &quot;Were\npredictions\naccurate?&quot;,
         title = &quot;Stringent predictions&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/default_vs_stringent-2.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>df &lt;- finalRf_predictions[, c(1, 3, 4)]

thresholds &lt;- seq(from = 0, to = 1, by = 0.1)

prop_table &lt;- data.frame(threshold = thresholds, prop_true_b = NA, prop_true_m = NA)

for (threshold in thresholds) {
  pred &lt;- ifelse(df$benign &gt; threshold, &quot;benign&quot;, &quot;malignant&quot;)
  pred_t &lt;- ifelse(pred == df$actual, TRUE, FALSE)
  
  group &lt;- data.frame(df, &quot;pred&quot; = pred_t) %&gt;%
  group_by(actual, pred) %&gt;%
  dplyr::summarise(n = n())
  
  group_b &lt;- filter(group, actual == &quot;benign&quot;)
  
  prop_b &lt;- sum(filter(group_b, pred == TRUE)$n) / sum(group_b$n)
  prop_table[prop_table$threshold == threshold, &quot;prop_true_b&quot;] &lt;- prop_b
  
  group_m &lt;- filter(group, actual == &quot;malignant&quot;)
  
  prop_m &lt;- sum(filter(group_m, pred == TRUE)$n) / sum(group_m$n)
  prop_table[prop_table$threshold == threshold, &quot;prop_true_m&quot;] &lt;- prop_m
}

prop_table %&gt;%
  gather(x, y, prop_true_b:prop_true_m) %&gt;%
  ggplot(aes(x = threshold, y = y, color = x)) +
    geom_point() +
    geom_line() +
    scale_color_brewer(palette = &quot;Set1&quot;) +
    labs(y = &quot;proportion of true predictions&quot;,
         color = &quot;b: benign cases\nm: malignant cases&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/prop_table-1.png" width="576" style="display: block; margin: auto;" /></p>
<hr />
<p>If you are interested in more machine learning posts, check out the category listing for <strong>machine_learning</strong> on my blog - <a href="https://shirinsplayground.netlify.com/categories/#posts-list-machine-learning" class="uri">https://shirinsplayground.netlify.com/categories/#posts-list-machine-learning</a> - <a href="https://shiring.github.io/categories.html#machine_learning-ref" class="uri">https://shiring.github.io/categories.html#machine_learning-ref</a></p>
<hr />
<p><br></p>
<pre class="r"><code>stopCluster(cl)
h2o.shutdown()</code></pre>
<pre><code>## Are you sure you want to shutdown the H2O instance running at http://localhost:54321/ (Y/N)?</code></pre>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.0 (2018-04-23)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.5
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods  
## [8] base     
## 
## other attached packages:
##  [1] ggrepel_0.8.0     reshape2_1.4.3    h2o_3.20.0.2     
##  [4] corrplot_0.84     caret_6.0-80      doParallel_1.0.11
##  [7] iterators_1.0.9   foreach_1.4.4     ellipse_0.4.1    
## [10] igraph_1.2.1      bindrcpp_0.2.2    mice_3.1.0       
## [13] lattice_0.20-35   forcats_0.3.0     stringr_1.3.1    
## [16] dplyr_0.7.5       purrr_0.2.5       readr_1.1.1      
## [19] tidyr_0.8.1       tibble_1.4.2      ggplot2_2.2.1    
## [22] tidyverse_1.2.1  
## 
## loaded via a namespace (and not attached):
##  [1] minqa_1.2.4         colorspace_1.3-2    class_7.3-14       
##  [4] rprojroot_1.3-2     pls_2.6-0           rstudioapi_0.7     
##  [7] DRR_0.0.3           prodlim_2018.04.18  lubridate_1.7.4    
## [10] xml2_1.2.0          codetools_0.2-15    splines_3.5.0      
## [13] mnormt_1.5-5        robustbase_0.93-1   knitr_1.20         
## [16] RcppRoll_0.3.0      jsonlite_1.5        nloptr_1.0.4       
## [19] broom_0.4.4         ddalpha_1.3.4       kernlab_0.9-26     
## [22] sfsmisc_1.1-2       compiler_3.5.0      httr_1.3.1         
## [25] backports_1.1.2     assertthat_0.2.0    Matrix_1.2-14      
## [28] lazyeval_0.2.1      cli_1.0.0           htmltools_0.3.6    
## [31] tools_3.5.0         gtable_0.2.0        glue_1.2.0         
## [34] Rcpp_0.12.17        cellranger_1.1.0    nlme_3.1-137       
## [37] blogdown_0.6        psych_1.8.4         timeDate_3043.102  
## [40] xfun_0.2            gower_0.1.2         lme4_1.1-17        
## [43] rvest_0.3.2         pan_1.4             DEoptimR_1.0-8     
## [46] MASS_7.3-50         scales_0.5.0        ipred_0.9-6        
## [49] hms_0.4.2           RColorBrewer_1.1-2  yaml_2.1.19        
## [52] rpart_4.1-13        stringi_1.2.3       randomForest_4.6-14
## [55] e1071_1.6-8         lava_1.6.1          geometry_0.3-6     
## [58] bitops_1.0-6        rlang_0.2.1         pkgconfig_2.0.1    
## [61] evaluate_0.10.1     bindr_0.1.1         recipes_0.1.3      
## [64] labeling_0.3        CVST_0.2-2          tidyselect_0.2.4   
## [67] plyr_1.8.4          magrittr_1.5        bookdown_0.7       
## [70] R6_2.2.2            mitml_0.3-5         dimRed_0.1.0       
## [73] pillar_1.2.3        haven_1.1.1         foreign_0.8-70     
## [76] withr_2.1.2         RCurl_1.95-4.10     survival_2.42-3    
## [79] abind_1.4-5         nnet_7.3-12         modelr_0.1.2       
## [82] crayon_1.3.4        jomo_2.6-2          xgboost_0.71.2     
## [85] utf8_1.1.4          rmarkdown_1.10      grid_3.5.0         
## [88] readxl_1.1.0        data.table_1.11.4   ModelMetrics_1.1.0 
## [91] digest_0.6.15       stats4_3.5.0        munsell_0.5.0      
## [94] magic_1.5-8</code></pre>
</div>
</div>
</div>
</div>

              
            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
                
                
                  <div class="post-footer-tags">
                    <span class="text-color-light text-small">TAGGED IN</span><br/>
                    
  <a class="tag tag--primary tag--small" href="//tags/r/">R</a>

  <a class="tag tag--primary tag--small" href="//tags/machine-learning/">machine learning</a>

  <a class="tag tag--primary tag--small" href="//tags/caret/">caret</a>

  <a class="tag tag--primary tag--small" href="//tags/h2o/">h2o</a>

  <a class="tag tag--primary tag--small" href="//tags/random-forest/">random forest</a>

  <a class="tag tag--primary tag--small" href="//tags/gradient-boosting/">gradient boosting</a>

  <a class="tag tag--primary tag--small" href="//tags/neural-nets/">neural nets</a>

                  </div>
                
              
            
            
  <div class="post-actions-wrap">
      <nav >
        <ul class="post-actions post-action-nav">
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="/2018/06/googlelanguager/" data-tooltip="Addendum: Text-to-Speech with the googleLanguageR package">
              
                <i class="fa fa-angle-left"></i>
                <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
              </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="/2018/06/text_to_speech_r/" data-tooltip="Text-to-speech with R">
              
                <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                <i class="fa fa-angle-right"></i>
              </a>
            </li>
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=/2018/06/intro_to_ml_workshop_heidelberg/">
              <i class="fa fa-facebook-official"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=/2018/06/intro_to_ml_workshop_heidelberg/">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=/2018/06/intro_to_ml_workshop_heidelberg/">
              <i class="fa fa-google-plus"></i>
            </a>
          </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  </div>


            
              
                <div id="disqus_thread">
  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
              
            
          </div>
        </article>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2018 Dr. Shirin Glander. All Rights Reserved
  </span>
</footer>

      </div>
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
        
  <div class="post-actions-wrap">
      <nav >
        <ul class="post-actions post-action-nav">
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="/2018/06/googlelanguager/" data-tooltip="Addendum: Text-to-Speech with the googleLanguageR package">
              
                <i class="fa fa-angle-left"></i>
                <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
              </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="/2018/06/text_to_speech_r/" data-tooltip="Text-to-speech with R">
              
                <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                <i class="fa fa-angle-right"></i>
              </a>
            </li>
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=/2018/06/intro_to_ml_workshop_heidelberg/">
              <i class="fa fa-facebook-official"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=/2018/06/intro_to_ml_workshop_heidelberg/">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://plus.google.com/share?url=/2018/06/intro_to_ml_workshop_heidelberg/">
              <i class="fa fa-google-plus"></i>
            </a>
          </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  </div>


      </div>
      <div id="share-options-bar" class="share-options-bar" data-behavior="4">
  <i id="btn-close-shareoptions" class="fa fa-close"></i>
  <ul class="share-options">
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://www.facebook.com/sharer/sharer.php?u=%2F2018%2F06%2Fintro_to_ml_workshop_heidelberg%2F">
          <i class="fa fa-facebook-official"></i><span>Share on Facebook</span>
        </a>
      </li>
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://twitter.com/intent/tweet?text=%2F2018%2F06%2Fintro_to_ml_workshop_heidelberg%2F">
          <i class="fa fa-twitter"></i><span>Share on Twitter</span>
        </a>
      </li>
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://plus.google.com/share?url=%2F2018%2F06%2Fintro_to_ml_workshop_heidelberg%2F">
          <i class="fa fa-google-plus"></i><span>Share on Google&#43;</span>
        </a>
      </li>
    
  </ul>
</div>
<div id="share-options-mask" class="share-options-mask"></div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="//www.gravatar.com/avatar/7f7f818e55624edfef8aa93860a1a3d4?s=110" alt="Author&#39;s picture" />
    
    <h4 id="about-card-name">Dr. Shirin Glander</h4>
    
      <div id="about-card-bio">Biologist turned Bioinformatician turned Data Scientist</div>
    
    
      <div id="about-card-job">
        <i class="fa fa-briefcase"></i>
        <br/>
        Data Scientist
      </div>
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        Münster, Germany
      </div>
    
  </div>
</div>

    <div id="algolia-search-modal" class="modal-container">
  <div class="modal">
    <div class="modal-header">
      <span class="close-button"><i class="fa fa-close"></i></span>
      <a href="https://algolia.com" target="_blank" rel="noopener" class="searchby-algolia text-color-light link-unstyled">
        <span class="searchby-algolia-text text-color-light text-small">by</span>
        <img class="searchby-algolia-logo" src="https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg">
      </a>
      <i class="search-icon fa fa-search"></i>
      <form id="algolia-search-form">
        <input type="text" id="algolia-search-input" name="search"
          class="form-control input--large search-input" placeholder="Search" />
      </form>
    </div>
    <div class="modal-body">
      <div class="no-result text-color-light text-center">no post found</div>
      <div class="results">
        
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2018/12/customer_churn_code/">
                <h3 class="media-heading">Code for case study - Customer Churn with Keras/TensorFlow and H2O</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Dec 12, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">This is code that accompanies a book chapter on customer churn that I have written for the German dpunkt Verlag. The book is in German and will probably appear in February: https://www.dpunkt.de/buecher/13208/9783864906107-data-science.html.
The code you find below can be used to recreate all figures and analyses from this book chapter. Because the content is exclusively for the book, my descriptions around the code had to be minimal. But I’m sure, you can get the gist, even without the book.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2018/12/trust_in_ml_slides_ix/">
                <h3 class="media-heading">Trust in ML models. Slides from TWiML &amp; AI EMEA Meetup &#43; iX Articles</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Dec 12, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Update: There is now a recording of the meetup up on YouTube.
 Here you find my slides the TWiML &amp; AI EMEA Meetup about Trust in ML models, where I presented the Anchors paper by Carlos Guestrin et al..
  I have also just written two articles for the German IT magazin iX about the same topic of Explaining Black-Box Machine Learning Models:
 A short article in the iX 12/2018</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2018/11/how_cnns_learn/">
                <h3 class="media-heading">How do Convolutional Neural Nets (CNNs) learn? &#43; Keras example</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Nov 11, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">As with the other videos from our codecentric.ai Bootcamp (Random Forests, Neural Nets &amp; Gradient Boosting), I am again sharing an English version of the script (plus R code) for this most recent addition on How Convolutional Neural Nets work.
In this lesson, I am going to explain how computers learn to see; meaning, how do they learn to recognize images or object on images? One of the most commonly used approaches to teach computers “vision” are Convolutional Neural Nets.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2018/11/ml_basics_gbm/">
                <h3 class="media-heading">Machine Learning Basics - Gradient Boosting &amp; XGBoost</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Nov 11, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">In a recent video, I covered Random Forests and Neural Nets as part of the codecentric.ai Bootcamp.
In the most recent video, I covered Gradient Boosting and XGBoost.
You can find the video on YouTube and the slides on slides.com. Both are again in German with code examples in Python.
But below, you find the English version of the content, plus code examples in R for caret, xgboost and h2o. :-)</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2018/11/slides_demystifying_dl/">
                <h3 class="media-heading">Slides from my talks about Demystifying Big Data and Deep Learning (and how to get started)</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Nov 11, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">On November 7th, Uwe Friedrichsen and I gave our talk from the JAX conference 2018: Deep Learning - a Primer again at the W-JAX in Munich.
A few weeks before, I gave a similar talk at two events about Demystifying Big Data and Deep Learning (and how to get started).
Here are the two very similar presentations from these talks:
    </div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2018/11/keras_fruits_crossvalidation/">
                <h3 class="media-heading">How to use cross-validation with the image data generator in Keras and TensorFlow</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Nov 11, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">I’ve been using keras and TensorFlow for a while now - and love its simplicity and straight-forward way to modeling. As part of the latest update to my Workshop about deep learning with R and keras I’ve added a new example analysis:
 https://shirinsplayground.netlify.com/2018/06/keras_fruits/
library(keras) library(tidyverse) ## ── Attaching packages ────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.0 ✔ purrr 0.2.5 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.7 ## ✔ tidyr 0.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2018/11/twimlai_meetup/">
                <h3 class="media-heading">TWIMLAI European Online Meetup about Trust in Predictions of ML Models</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Nov 11, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">At the upcoming This week in machine learning and AI European online Meetup, I’ll be presenting and leading a discussion about the Anchors paper, the next generation of machine learning interpretability tools. Come and join the fun! :-)
 Date: Tuesday 4th December 2018 Time: 19:00 PM CET/CEST  Join: https://twimlai.com/meetups/trust-in-predictions-of-ml-models/
 </div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2018/11/text_classification_keras_data_prep/">
                <h3 class="media-heading">How to prepare data for NLP (text classification) with Keras and TensorFlow</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Nov 11, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">In the past, I have written and taught quite a bit about image classification with Keras (e.g. here). Text classification isn’t too different in terms of using the Keras principles to train a sequential or function model. You can even use Convolutional Neural Nets (CNNs) for text classification.
What is very different, however, is how to prepare raw text data for modeling. When you look at the IMDB example from the Deep Learning with R Book, you get a great explanation of how to train the model.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2018/11/neural_nets_explained/">
                <h3 class="media-heading">&#39;How do neural nets learn?&#39; A step by step explanation using the H2O Deep Learning algorithm.</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Nov 11, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">In my last blogpost about Random Forests I introduced the codecentric.ai Bootcamp. The next part I published was about Neural Networks and Deep Learning. Every video of our bootcamp will have example code and tasks to promote hands-on learning. While the practical parts of the bootcamp will be using Python, below you will find the English R version of this Neural Nets Practical Example, where I explain how neural nets learn and how the concepts and techniques translate to training neural nets in R with the H2O Deep Learning function.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="/2018/10/ml_basics_rf/">
                <h3 class="media-heading">Machine Learning Basics - Random Forest</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Oct 10, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">A few colleagues of mine and I from codecentric.ai are currently working on developing a free online course about machine learning and deep learning. As part of this course, I am developing a series of videos about machine learning basics - the first video in this series was about Random Forests.
You can find the video on YouTube but as of now, it is only available in German. Same goes for the slides, which are also currently German only.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
      </div>
    </div>
    <div class="modal-footer">
      <p class="results-count text-medium"
         data-message-zero="no post found"
         data-message-one="1 post found"
         data-message-other="{n} posts found">
         73 posts found
      </p>
    </div>
  </div>
</div>
    
  
    
    <div id="cover" style="background-image:url('/images/autumn-2789234_1920.jpg');"></div>
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js" integrity="sha256-IFHWFEbU2/+wNycDECKgjIRSirRNIDp2acEB5fvdVRU=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js" integrity="sha256-+mpyNVJsNt4rVXCw0F+pAOiB3YxmHgrbJsx4ecPuUaI=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.js" integrity="sha256-vMxgR/7FtLovVA+IPrR7+xTgIgARH7y9VZQnmmi0HDI=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.js" integrity="sha256-N0qFUh7/9vLvia87dDndewmsgsyYoNkdA212tPc+2NI=" crossorigin="anonymous"></script>


<script src="/js/script-kr8dyqj6rb6ortyib7whfo9x9p6td6zo8t1v4fdz4ecx5kwybsdlmk1slygn.min.js"></script>


<script>
$(document).ready(function() {
  hljs.configure({ classPrefix: '', useBR: false });
  $('pre.code-highlight, pre > code').each(function(i, block) {
    if (!$(this).hasClass('codeblock')) {
      $(this).addClass('codeblock');
    }
    hljs.highlightBlock(block);
  });
});
</script>

  
    
      <script>
        var disqus_config = function () {
          this.page.url = '\/2018\/06\/intro_to_ml_workshop_heidelberg\/';
          
            this.page.identifier = '\/2018\/06\/intro_to_ml_workshop_heidelberg\/'
          
        };
        (function() {
          
          
          if (window.location.hostname == "localhost") {
            return;
          }
          var d = document, s = d.createElement('script');
          var disqus_shortname = 'shirinsplayground';
          s.src = '//' + disqus_shortname + '.disqus.com/embed.js';

          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
        })();
      </script>
    
  



    
  </body>
</html>

