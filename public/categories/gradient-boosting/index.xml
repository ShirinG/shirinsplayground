<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>gradient boosting on Shirin&#39;s playgRound</title>
    <link>https://shirinsplayground.netlify.app/categories/gradient-boosting/</link>
    <description>Recent content in gradient boosting on Shirin&#39;s playgRound</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 Nov 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://shirinsplayground.netlify.app/categories/gradient-boosting/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Machine Learning Basics - Gradient Boosting &amp; XGBoost</title>
      <link>https://shirinsplayground.netlify.app/2018/11/ml_basics_gbm/</link>
      <pubDate>Thu, 29 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/11/ml_basics_gbm/</guid>
      <description>In a recent video, I covered Random Forests and Neural Nets as part of the codecentric.ai Bootcamp.
In the most recent video, I covered Gradient Boosting and XGBoost.
You can find the video on YouTube and the slides on slides.com. Both are again in German with code examples in Python.
But below, you find the English version of the content, plus code examples in R for caret, xgboost and h2o.</description>
    </item>
    
  </channel>
</rss>
