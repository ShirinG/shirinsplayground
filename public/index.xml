<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Shirin&#39;s playgRound</title>
  <link href="/index.xml" rel="self"/>
  <link href="/"/>
  <updated>2018-04-22T00:00:00+00:00</updated>
  <id>/</id>
  <author>
    <name>Dr. Shirin Glander</name>
  </author>
  <generator>Hugo -- gohugo.io</generator>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #121: Reproducibility and the Philosophy of Data with Clare Gollnick]]></title>
    <link href="/2018/04/twimlai121/"/>
    <id>/2018/04/twimlai121/</id>
    <published>2018-04-22T00:00:00+00:00</published>
    <updated>2018-04-22T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Reproducibility and the Philosophy of Data with Clare Gollnick</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai121.jpg" alt="Sketchnotes from TWiMLAI talk #121: Reproducibility and the Philosophy of Data with Clare Gollnick" />
<p class="caption">Sketchnotes from TWiMLAI talk #121: Reproducibility and the Philosophy of Data with Clare Gollnick</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-121-reproducibility-philosophy-data-clare-gollnick/">here</a>.</p>
<blockquote>
<p>In this episode, i’m joined by Clare Gollnick, CTO of Terbium Labs, to discuss her thoughts on the “reproducibility crisis” currently haunting the scientific landscape. For a little background, a “Nature” survey in 2016 showed that more than 70% of researchers have tried and failed to reproduce another scientist’s experiments, and more than half have failed to reproduce their own experiments. Clare gives us her take on the situation, and how it applies to data science, along with some great nuggets about the philosophy of data and a few interesting use cases as well. We also cover her thoughts on Bayesian vs Frequentist techniques and while we’re at it, the Vim vs Emacs debate. No, actually I’m just kidding on that last one. But this was indeed a very fun conversation that I think you’ll enjoy! <a href="https://twimlai.com/twiml-talk-121-reproducibility-philosophy-data-clare-gollnick/" class="uri">https://twimlai.com/twiml-talk-121-reproducibility-philosophy-data-clare-gollnick/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Update: Can we predict flu outcome with Machine Learning in R?]]></title>
    <link href="/2018/04/flu_prediction/"/>
    <id>/2018/04/flu_prediction/</id>
    <published>2018-04-22T00:00:00+00:00</published>
    <updated>2018-04-22T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Since I migrated my blog from <a href="shiring.github.io">Github Pages</a> to <a href="www.shirin-glander.de">blogdown and Netlify</a>, I wanted to start migrating (most of) my old posts too - and use that opportunity to update them and make sure the code still works.</p>
<p>Here I am updating my very first machine learning post from 27 Nov 2016: <a href="https://shiring.github.io/machine_learning/2016/11/27/flu_outcome_ML_post">Can we predict flu deaths with Machine Learning and R?</a>. Changes are marked as <strong>bold</strong> comments.</p>
<p>The main changes I made are:</p>
<ul>
<li>using the <code>tidyverse</code> more consistently throughout the analysis</li>
<li>focusing on comparing multiple imputations from the <code>mice</code> package, rather than comparing different algorithms</li>
<li>using <code>purrr</code>, <code>map()</code>, <code>nest()</code> and <code>unnest()</code> to model and predict the machine learning algorithm over the different imputed datasets</li>
</ul>
<hr />
<p>Among the many nice R packages containing data collections is the <a href="https://mran.microsoft.com/web/packages/outbreaks/outbreaks.pdf">outbreaks</a> package. It contains a dataset on epidemics and among them is data from the 2013 outbreak of <a href="http://www.who.int/influenza/human_animal_interface/faq_H7N9/en/">influenza A H7N9</a> in <a href="http://www.who.int/influenza/human_animal_interface/influenza_h7n9/ChinaH7N9JointMissionReport2013u.pdf?ua=1">China</a> as analysed by Kucharski et al. (2014):</p>
<blockquote>
<p>A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. PLOS Currents Outbreaks. Mar 7, edition 1. doi: 10.1371/currents.outbreaks.e1473d9bfc99d080ca242139a06c455f.</p>
</blockquote>
<blockquote>
<p>A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Data from: Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. Dryad Digital Repository. <a href="http://dx.doi.org/10.5061/dryad.2g43n" class="uri">http://dx.doi.org/10.5061/dryad.2g43n</a>.</p>
</blockquote>
<p>I will be using their data as an example to show how to use Machine Learning algorithms for predicting disease outcome.</p>
<p><br></p>
<pre class="r"><code>library(outbreaks)
library(tidyverse)
library(plyr)
library(mice)
library(caret)
library(purrr)</code></pre>
<div id="the-data" class="section level1">
<h1>The data</h1>
<p>The dataset contains case ID, date of onset, date of hospitalization, date of outcome, gender, age, province and of course outcome: Death or Recovery.</p>
<div id="pre-processing" class="section level2">
<h2>Pre-processing</h2>
<p><strong>Change: variable names (i.e. column names) have been renamed, dots have been replaced with underscores, letters are all lower case now.</strong></p>
<p><strong>Change: I am using the tidyverse notation more consistently.</strong></p>
<p>First, I’m doing some preprocessing, including:</p>
<ul>
<li>renaming missing data as NA</li>
<li>adding an ID column</li>
<li>setting column types</li>
<li>gathering date columns</li>
<li>changing factor names of dates (to make them look nicer in plots) and of <code>province</code> (to combine provinces with few cases)</li>
</ul>
<pre class="r"><code>fluH7N9_china_2013$age[which(fluH7N9_china_2013$age == &quot;?&quot;)] &lt;- NA
fluH7N9_china_2013_gather &lt;- fluH7N9_china_2013 %&gt;%
  mutate(case_id = paste(&quot;case&quot;, case_id, sep = &quot;_&quot;),
         age = as.numeric(age)) %&gt;%
  gather(Group, Date, date_of_onset:date_of_outcome) %&gt;%
  mutate(Group = as.factor(mapvalues(Group, from = c(&quot;date_of_onset&quot;, &quot;date_of_hospitalisation&quot;, &quot;date_of_outcome&quot;), 
          to = c(&quot;date of onset&quot;, &quot;date of hospitalisation&quot;, &quot;date of outcome&quot;))),
         province = mapvalues(province, from = c(&quot;Anhui&quot;, &quot;Beijing&quot;, &quot;Fujian&quot;, &quot;Guangdong&quot;, &quot;Hebei&quot;, &quot;Henan&quot;, &quot;Hunan&quot;, &quot;Jiangxi&quot;, &quot;Shandong&quot;, &quot;Taiwan&quot;), to = rep(&quot;Other&quot;, 10)))</code></pre>
<p>I’m also</p>
<ul>
<li>adding a third gender level for unknown gender</li>
</ul>
<pre class="r"><code>levels(fluH7N9_china_2013_gather$gender) &lt;- c(levels(fluH7N9_china_2013_gather$gender), &quot;unknown&quot;)
fluH7N9_china_2013_gather$gender[is.na(fluH7N9_china_2013_gather$gender)] &lt;- &quot;unknown&quot;
head(fluH7N9_china_2013_gather)</code></pre>
<pre><code>##   case_id outcome gender age province         Group       Date
## 1  case_1   Death      m  58 Shanghai date of onset 2013-02-19
## 2  case_2   Death      m   7 Shanghai date of onset 2013-02-27
## 3  case_3   Death      f  11    Other date of onset 2013-03-09
## 4  case_4    &lt;NA&gt;      f  18  Jiangsu date of onset 2013-03-19
## 5  case_5 Recover      f  20  Jiangsu date of onset 2013-03-19
## 6  case_6   Death      f   9  Jiangsu date of onset 2013-03-21</code></pre>
<p>For plotting, I am defining a custom <code>ggplot2</code> theme:</p>
<pre class="r"><code>my_theme &lt;- function(base_size = 12, base_family = &quot;sans&quot;){
  theme_minimal(base_size = base_size, base_family = base_family) +
  theme(
    axis.text = element_text(size = 12),
    axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5),
    axis.title = element_text(size = 14),
    panel.grid.major = element_line(color = &quot;grey&quot;),
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = &quot;aliceblue&quot;),
    strip.background = element_rect(fill = &quot;lightgrey&quot;, color = &quot;grey&quot;, size = 1),
    strip.text = element_text(face = &quot;bold&quot;, size = 12, color = &quot;black&quot;),
    legend.position = &quot;bottom&quot;,
    legend.justification = &quot;top&quot;, 
    legend.box = &quot;horizontal&quot;,
    legend.box.background = element_rect(colour = &quot;grey50&quot;),
    legend.background = element_blank(),
    panel.border = element_rect(color = &quot;grey&quot;, fill = NA, size = 0.5)
  )
}</code></pre>
<p>And use that theme to visualize the data:</p>
<pre class="r"><code>ggplot(data = fluH7N9_china_2013_gather, aes(x = Date, y = age, fill = outcome)) +
  stat_density2d(aes(alpha = ..level..), geom = &quot;polygon&quot;) +
  geom_jitter(aes(color = outcome, shape = gender), size = 1.5) +
  geom_rug(aes(color = outcome)) +
  scale_y_continuous(limits = c(0, 90)) +
  labs(
    fill = &quot;Outcome&quot;,
    color = &quot;Outcome&quot;,
    alpha = &quot;Level&quot;,
    shape = &quot;Gender&quot;,
    x = &quot;Date in 2013&quot;,
    y = &quot;Age&quot;,
    title = &quot;2013 Influenza A H7N9 cases in China&quot;,
    subtitle = &quot;Dataset from &#39;outbreaks&#39; package (Kucharski et al. 2014)&quot;,
    caption = &quot;&quot;
  ) +
  facet_grid(Group ~ province) +
  my_theme() +
  scale_shape_manual(values = c(15, 16, 17)) +
  scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) +
  scale_fill_brewer(palette=&quot;Set1&quot;)</code></pre>
<p><img src="/post/2018-04-22_flu_outcome_ML_post_files/figure-html/main-1.png" width="1152" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(data = fluH7N9_china_2013_gather, aes(x = Date, y = age, color = outcome)) +
  geom_point(aes(color = outcome, shape = gender), size = 1.5, alpha = 0.6) +
  geom_path(aes(group = case_id)) +
  facet_wrap( ~ province, ncol = 2) +
  my_theme() +
  scale_shape_manual(values = c(15, 16, 17)) +
  scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) +
  scale_fill_brewer(palette=&quot;Set1&quot;) +
  labs(
    color = &quot;Outcome&quot;,
    shape = &quot;Gender&quot;,
    x = &quot;Date in 2013&quot;,
    y = &quot;Age&quot;,
    title = &quot;2013 Influenza A H7N9 cases in China&quot;,
    subtitle = &quot;Dataset from &#39;outbreaks&#39; package (Kucharski et al. 2014)&quot;,
    caption = &quot;\nTime from onset of flu to outcome.&quot;
  )</code></pre>
<p><img src="/post/2018-04-22_flu_outcome_ML_post_files/figure-html/unnamed-chunk-5-1.png" width="768" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
</div>
<div id="features" class="section level1">
<h1>Features</h1>
<p>In machine learning-speak features are what we call the variables used for model training. Using the right features dramatically influences the accuracy and success of your model.</p>
<p>For this example, I am keeping age, but I am also generating new features from the date information and converting gender and province into numerical values.</p>
<pre class="r"><code>dataset &lt;- fluH7N9_china_2013 %&gt;%
  mutate(hospital = as.factor(ifelse(is.na(date_of_hospitalisation), 0, 1)),
         gender_f = as.factor(ifelse(gender == &quot;f&quot;, 1, 0)),
         province_Jiangsu = as.factor(ifelse(province == &quot;Jiangsu&quot;, 1, 0)),
         province_Shanghai = as.factor(ifelse(province == &quot;Shanghai&quot;, 1, 0)),
         province_Zhejiang = as.factor(ifelse(province == &quot;Zhejiang&quot;, 1, 0)),
         province_other = as.factor(ifelse(province == &quot;Zhejiang&quot; | province == &quot;Jiangsu&quot; | province == &quot;Shanghai&quot;, 0, 1)),
         days_onset_to_outcome = as.numeric(as.character(gsub(&quot; days&quot;, &quot;&quot;,
                                      as.Date(as.character(date_of_outcome), format = &quot;%Y-%m-%d&quot;) - 
                                        as.Date(as.character(date_of_onset), format = &quot;%Y-%m-%d&quot;)))),
         days_onset_to_hospital = as.numeric(as.character(gsub(&quot; days&quot;, &quot;&quot;,
                                      as.Date(as.character(date_of_hospitalisation), format = &quot;%Y-%m-%d&quot;) - 
                                        as.Date(as.character(date_of_onset), format = &quot;%Y-%m-%d&quot;)))),
         age = age,
         early_onset = as.factor(ifelse(date_of_onset &lt; summary(fluH7N9_china_2013$date_of_onset)[[3]], 1, 0)),
         early_outcome = as.factor(ifelse(date_of_outcome &lt; summary(fluH7N9_china_2013$date_of_outcome)[[3]], 1, 0))) %&gt;%
  subset(select = -c(2:4, 6, 8))
rownames(dataset) &lt;- dataset$case_id
dataset[, -2] &lt;- as.numeric(as.matrix(dataset[, -2]))
head(dataset)</code></pre>
<pre><code>##   case_id outcome age hospital gender_f province_Jiangsu province_Shanghai
## 1       1   Death  87        0        0                0                 1
## 2       2   Death  27        1        0                0                 1
## 3       3   Death  35        1        1                0                 0
## 4       4    &lt;NA&gt;  45        1        1                1                 0
## 5       5 Recover  48        1        1                1                 0
## 6       6   Death  32        1        1                1                 0
##   province_Zhejiang province_other days_onset_to_outcome
## 1                 0              0                    13
## 2                 0              0                    11
## 3                 0              1                    31
## 4                 0              0                    NA
## 5                 0              0                    57
## 6                 0              0                    36
##   days_onset_to_hospital early_onset early_outcome
## 1                     NA           1             1
## 2                      4           1             1
## 3                     10           1             1
## 4                      8           1            NA
## 5                     11           1             0
## 6                      7           1             1</code></pre>
<pre class="r"><code>summary(dataset$outcome)</code></pre>
<pre><code>##   Death Recover    NA&#39;s 
##      32      47      57</code></pre>
<p><br></p>
<div id="imputing-missing-values" class="section level2">
<h2>Imputing missing values</h2>
<p>I am using the <a href="https://gerkovink.github.io/miceVignettes/Ad_hoc_and_mice/Ad_hoc_methods.html"><code>mice</code> package for imputing missing values</a></p>
<p><strong>Note: Since publishing this blogpost I learned that the idea behind using <code>mice</code> is to compare different imputations to see how stable they are, instead of picking one imputed set as fixed for the remainder of the analysis. Therefore, I changed the focus of this post a little bit: in the old post I compared many different algorithms and their outcome; in this updated version I am only showing the Random Forest algorithm and focus on comparing the different imputed datasets. I am ignoring feature importance and feature plots because nothing changed compared to the old post.</strong></p>
<p><br></p>
<ul>
<li><code>md.pattern()</code> shows the pattern of missingness in the data:</li>
</ul>
<pre class="r"><code>md.pattern(dataset)</code></pre>
<pre><code>##    case_id hospital province_Jiangsu province_Shanghai province_Zhejiang
## 42       1        1                1                 1                 1
## 27       1        1                1                 1                 1
##  2       1        1                1                 1                 1
##  2       1        1                1                 1                 1
## 18       1        1                1                 1                 1
##  1       1        1                1                 1                 1
## 36       1        1                1                 1                 1
##  3       1        1                1                 1                 1
##  3       1        1                1                 1                 1
##  2       1        1                1                 1                 1
##          0        0                0                 0                 0
##    province_other age gender_f early_onset outcome early_outcome
## 42              1   1        1           1       1             1
## 27              1   1        1           1       1             1
##  2              1   1        1           1       1             0
##  2              1   1        1           0       1             1
## 18              1   1        1           1       0             0
##  1              1   1        1           1       1             0
## 36              1   1        1           1       0             0
##  3              1   1        1           0       1             0
##  3              1   1        1           0       0             0
##  2              1   0        0           0       1             0
##                 0   2        2          10      57            65
##    days_onset_to_outcome days_onset_to_hospital    
## 42                     1                      1   0
## 27                     1                      0   1
##  2                     0                      1   2
##  2                     0                      0   3
## 18                     0                      1   3
##  1                     0                      0   3
## 36                     0                      0   4
##  3                     0                      0   4
##  3                     0                      0   5
##  2                     0                      0   6
##                       67                     74 277</code></pre>
<ul>
<li><code>mice()</code> generates the imputations</li>
</ul>
<pre class="r"><code>dataset_impute &lt;- mice(data = dataset[, -2],  print = FALSE)</code></pre>
<ul>
<li>by default, <code>mice()</code> calculates five (m = 5) imputed data sets</li>
<li>we can combine them all in one output with the <code>complete(&quot;long&quot;)</code> function</li>
<li>I did not want to impute missing values in the <code>outcome</code> column, so I have to merge it back in with the imputed data</li>
</ul>
<pre class="r"><code>datasets_complete &lt;- right_join(dataset[, c(1, 2)], 
                           complete(dataset_impute, &quot;long&quot;),
                           by = &quot;case_id&quot;) %&gt;%
  select(-.id)</code></pre>
<pre class="r"><code>head(datasets_complete)</code></pre>
<pre><code>##   case_id outcome .imp age hospital gender_f province_Jiangsu
## 1       1   Death    1  87        0        0                0
## 2       2   Death    1  27        1        0                0
## 3       3   Death    1  35        1        1                0
## 4       4    &lt;NA&gt;    1  45        1        1                1
## 5       5 Recover    1  48        1        1                1
## 6       6   Death    1  32        1        1                1
##   province_Shanghai province_Zhejiang province_other days_onset_to_outcome
## 1                 1                 0              0                    13
## 2                 1                 0              0                    11
## 3                 0                 0              1                    31
## 4                 0                 0              0                    22
## 5                 0                 0              0                    57
## 6                 0                 0              0                    36
##   days_onset_to_hospital early_onset early_outcome
## 1                      1           1             1
## 2                      4           1             1
## 3                     10           1             1
## 4                      8           1             1
## 5                     11           1             0
## 6                      7           1             1</code></pre>
<p>Let’s compare the distributions of the five different imputed datasets:</p>
<pre class="r"><code>datasets_complete %&gt;%
  gather(x, y, age:early_outcome) %&gt;%
  ggplot(aes(x = y, fill = .imp, color = .imp)) +
    facet_wrap(~ x, ncol = 3, scales = &quot;free&quot;) +
    geom_density(alpha = 0.4) +
    scale_fill_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) +
    scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) +
    my_theme()</code></pre>
<p><img src="/post/2018-04-22_flu_outcome_ML_post_files/figure-html/unnamed-chunk-12-1.png" width="768" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
</div>
<div id="test-train-and-validation-data-sets" class="section level1">
<h1>Test, train and validation data sets</h1>
<p>Now, we can go ahead with machine learning!</p>
<p>The dataset contains a few missing values in the <code>outcome</code> column; those will be the test set used for final predictions (see the old blog post for this).</p>
<pre class="r"><code>train_index &lt;- which(is.na(datasets_complete$outcome))
train_data &lt;- datasets_complete[-train_index, ]
test_data  &lt;- datasets_complete[train_index, -2]</code></pre>
<p>The remainder of the data will be used for modeling. Here, I am splitting the data into 70% training and 30% test data.</p>
<p>Because I want to model each imputed dataset separately, I am using the <code>nest()</code> and <code>map()</code> functions.</p>
<pre class="r"><code>set.seed(42)
val_data &lt;- train_data %&gt;%
  group_by(.imp) %&gt;%
  nest() %&gt;%
  mutate(val_index = map(data, ~ createDataPartition(.$outcome, p = 0.7, list = FALSE)),
         val_train_data = map2(data, val_index, ~ .x[.y, ]),
         val_test_data = map2(data, val_index, ~ .x[-.y, ]))</code></pre>
<p><br></p>
</div>
<div id="machine-learning-algorithms" class="section level1">
<h1>Machine Learning algorithms</h1>
<div id="random-forest" class="section level2">
<h2>Random Forest</h2>
<p>To make the code tidier, I am first defining the modeling function with the parameters I want.</p>
<pre class="r"><code>model_function &lt;- function(df) {
  caret::train(outcome ~ .,
               data = df,
               method = &quot;rf&quot;,
               preProcess = c(&quot;scale&quot;, &quot;center&quot;),
               trControl = trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 3, verboseIter = FALSE))
}</code></pre>
<p>Next, I am using the nested tibble from before to <code>map()</code> the model function, predict the outcome and calculate confusion matrices.</p>
<pre class="r"><code>set.seed(42)
val_data_model &lt;- val_data %&gt;%
  mutate(model = map(val_train_data, ~ model_function(.x)),
         predict = map2(model, val_test_data, ~ data.frame(prediction = predict(.x, .y[, -2]))),
         predict_prob = map2(model, val_test_data, ~ data.frame(outcome = .y[, 2],
                                                                prediction = predict(.x, .y[, -2], type = &quot;prob&quot;))),
         confusion_matrix = map2(val_test_data, predict, ~ confusionMatrix(.x$outcome, .y$prediction)),
         confusion_matrix_tbl = map(confusion_matrix, ~ as.tibble(.x$table)))</code></pre>
<p><br></p>
</div>
<div id="comparing-accuracy-of-models" class="section level2">
<h2>Comparing accuracy of models</h2>
<p>To compare how the different imputations did, I am plotting</p>
<ul>
<li>the confusion matrices:</li>
</ul>
<pre class="r"><code>val_data_model %&gt;%
  unnest(confusion_matrix_tbl) %&gt;%
  ggplot(aes(x = Prediction, y = Reference, fill = n)) +
    facet_wrap(~ .imp, ncol = 5, scales = &quot;free&quot;) +
    geom_tile() +
    scale_fill_viridis_c() +
    my_theme()</code></pre>
<p><img src="/post/2018-04-22_flu_outcome_ML_post_files/figure-html/unnamed-chunk-17-1.png" width="1440" style="display: block; margin: auto;" /></p>
<ul>
<li>and the prediction probabilities for correct and wrong predictions:</li>
</ul>
<pre class="r"><code>val_data_model %&gt;%
  unnest(predict_prob) %&gt;%
  gather(x, y, prediction.Death:prediction.Recover) %&gt;%
  ggplot(aes(x = x, y = y, fill = outcome)) +
    facet_wrap(~ .imp, ncol = 5, scales = &quot;free&quot;) +
    geom_boxplot() +
    scale_fill_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) +
    my_theme()</code></pre>
<p><img src="/post/2018-04-22_flu_outcome_ML_post_files/figure-html/unnamed-chunk-18-1.png" width="1440" style="display: block; margin: auto;" /></p>
<p>Hope, you found that example interesting and helpful!</p>
<p><br></p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.4
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] methods   stats     graphics  grDevices utils     datasets  base     
## 
## other attached packages:
##  [1] bindrcpp_0.2       caret_6.0-78       mice_2.46.0       
##  [4] lattice_0.20-35    plyr_1.8.4         forcats_0.3.0     
##  [7] stringr_1.3.0      dplyr_0.7.4        purrr_0.2.4       
## [10] readr_1.1.1        tidyr_0.8.0        tibble_1.4.2      
## [13] ggplot2_2.2.1.9000 tidyverse_1.2.1    outbreaks_1.3.0   
## 
## loaded via a namespace (and not attached):
##  [1] nlme_3.1-131.1      lubridate_1.7.3     dimRed_0.1.0       
##  [4] RColorBrewer_1.1-2  httr_1.3.1          rprojroot_1.3-2    
##  [7] tools_3.4.3         backports_1.1.2     R6_2.2.2           
## [10] rpart_4.1-13        lazyeval_0.2.1      colorspace_1.3-2   
## [13] nnet_7.3-12         withr_2.1.1.9000    tidyselect_0.2.4   
## [16] mnormt_1.5-5        compiler_3.4.3      cli_1.0.0          
## [19] rvest_0.3.2         xml2_1.2.0          labeling_0.3       
## [22] bookdown_0.7        scales_0.5.0.9000   sfsmisc_1.1-1      
## [25] DEoptimR_1.0-8      psych_1.7.8         robustbase_0.92-8  
## [28] randomForest_4.6-12 digest_0.6.15       foreign_0.8-69     
## [31] rmarkdown_1.8       pkgconfig_2.0.1     htmltools_0.3.6    
## [34] rlang_0.2.0.9000    readxl_1.0.0        ddalpha_1.3.1.1    
## [37] rstudioapi_0.7      bindr_0.1           jsonlite_1.5       
## [40] ModelMetrics_1.1.0  magrittr_1.5        Matrix_1.2-12      
## [43] Rcpp_0.12.15        munsell_0.4.3       stringi_1.1.6      
## [46] yaml_2.1.17         MASS_7.3-49         recipes_0.1.2      
## [49] grid_3.4.3          parallel_3.4.3      crayon_1.3.4       
## [52] haven_1.1.1         splines_3.4.3       hms_0.4.1          
## [55] knitr_1.20          pillar_1.2.1        reshape2_1.4.3     
## [58] codetools_0.2-15    stats4_3.4.3        CVST_0.2-1         
## [61] glue_1.2.0          evaluate_0.10.1     blogdown_0.5       
## [64] modelr_0.1.1        foreach_1.4.4       cellranger_1.1.0   
## [67] gtable_0.2.0        kernlab_0.9-25      assertthat_0.2.0   
## [70] DRR_0.0.3           xfun_0.1            gower_0.1.2        
## [73] prodlim_1.6.1       broom_0.4.3         e1071_1.6-8        
## [76] class_7.3-14        survival_2.41-3     viridisLite_0.3.0  
## [79] timeDate_3043.102   RcppRoll_0.2.2      iterators_1.0.9    
## [82] lava_1.6            ipred_0.9-6</code></pre>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Look, something shiny: How to use R Shiny to make Münster traffic data accessible. Join MünsteR for our next meetup!]]></title>
    <link href="/2018/04/meetup_june18/"/>
    <id>/2018/04/meetup_june18/</id>
    <published>2018-04-19T00:00:00+00:00</published>
    <updated>2018-04-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://shiring.github.io/r_users_group/2017/05/20/map.png" />

</div>
<p>In our <a href="http://meetu.ps/e/DVFrp/w54bW/f">next MünsteR R-user group meetup</a> on <strong>Monday, June 11th, 2018</strong> Thomas Kluth and Thorben Jensen will give a talk titled <strong>Look, something shiny: How to use R Shiny to make Münster traffic data accessible</strong>. You can RSVP here: <a href="http://meetu.ps/e/F7zDN/w54bW/f" class="uri">http://meetu.ps/e/F7zDN/w54bW/f</a></p>
<blockquote>
<p>About a year ago, we stumbled upon rich datasets on <em>traffic dynamics</em> of Münster: count data of bikes, cars, and bus passengers of high resolution. Since that day we have been crunching, modeling, and visualizing it. To involve local stakeholders and NGOs (e.g., the <a href="http://fahrradstadt.ms">IG Fahrradstadt Münster</a>), we found the R Shiny framework to be very useful.</p>
</blockquote>
<blockquote>
<p>Shiny is probably the fastest way to take your R projects online. According to <a href="https://shiny.rstudio.com/">RStudio</a>, it “is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage or embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions.”</p>
</blockquote>
<blockquote>
<p>We would like to introduce Shiny to you using the following topics:</p>
</blockquote>
<ol style="list-style-type: decimal">
<li>Why we love Shiny, and why you should, too</li>
<li>How Shiny works, from code to browser</li>
<li>How to deploy your R Shiny project with Docker</li>
<li>Our <a href="https://traffics.codeformuenster.org">Shiny project traffic dynamics</a></li>
<li>Plotting results of Bayesian and frequentist models within Shiny</li>
<li>Group discussion: what else to present with Shiny?</li>
</ol>
<blockquote>
<p>You will not have to bring much previous knowledge to our talk. A basic understanding of how R code works will take you far. The part about statistical modeling will be as intuitive as possible. Overall, we will try to <em>keep it simple and shiny</em>.</p>
</blockquote>
<blockquote>
<p>All parts of our talk will be connected to traffic data for Münster. We look forward to your feedback and ideas for more analyses. You find our <em>traffic dynamics</em> projects on <a href="https://github.com/codeformuenster">Code for Münster’s github page</a>.</p>
</blockquote>
<div id="about-the-speakers" class="section level2">
<h2>About the speakers</h2>
<p>The speakers Thomas Kluth and Thorben Jensen are members of <a href="http://codeformuenster.org">Code for Münster</a>. We meet each Tuesday at 18:30 at the Dreiklang bar to make our city a better place by coding. New coders are always welcome!</p>
<p>Thomas Kluth has studied Computer Science in Münster and Bremen. During his currently ongoing (close-to-be-finished) Linguistics PhD in Bielefeld, he models human cognitive behavior. Using computational cognitive models, he aims to link spatial language use with perceptual mechanisms such as visual attention. The statistical analysis of empirical data convinced him to use R and Bayesian modeling to explain almost everything. He is looking forward to applying his skill set to real-world domains for creating a sustainable future.</p>
<p>Thorben Jensen has studied, designed, and implemented predictive models since more than 10 years. After studying modeling and computer science in 5 countries, he graduated from the PhD program at Delft University of Technology. His PhD thesis and other publications propose increased use of optimization methods and automation when building simulations with software agents. When consulting clients on Data Science, he enjoys making predictions intuitive with R Shiny.</p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[HH Data Science Meetup slides: Explaining complex machine learning models with LIME]]></title>
    <link href="/2018/04/hh_datascience_meetup_2018_slides/"/>
    <id>/2018/04/hh_datascience_meetup_2018_slides/</id>
    <published>2018-04-18T00:00:00+00:00</published>
    <updated>2018-04-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>On April 12th, 2018 I gave a talk about <strong>Explaining complex machine learning models with LIME</strong> at the <a href="https://www.meetup.com/Hamburg-Data-Science-Meetup/events/krrqhpyxfbkc/">Hamburg Data Science Meetup</a> - so if you’re intersted: the slides can be found here: <a href="https://www.slideshare.net/ShirinGlander/hh-data-science-meetup-explaining-complex-machine-learning-models-with-lime-94218890" class="uri">https://www.slideshare.net/ShirinGlander/hh-data-science-meetup-explaining-complex-machine-learning-models-with-lime-94218890</a></p>
<blockquote>
<p>Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex machine learning models are essentially black boxes and too complicated to understand, we need to use approximations to get a better sense of how they work. One such approach is LIME, which stands for Local Interpretable Model-agnostic Explanations and is a tool that helps understand and explain the decisions made by complex machine learning models.</p>
</blockquote>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/3CI6SYAltdniw" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; display: block; margin: auto; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen>
</iframe>
<div style="margin-bottom:5px">
<strong> <a href="//www.slideshare.net/ShirinGlander/hh-data-science-meetup-explaining-complex-machine-learning-models-with-lime-94218890" title="HH Data Science Meetup: Explaining complex machine learning models with LIME" target="_blank">HH Data Science Meetup: Explaining complex machine learning models with LIME</a> </strong> from <strong><a href="https://www.slideshare.net/ShirinGlander" target="_blank">Shirin Glander</a></strong>
</div>
<p>– slide deck was produced with <a href="www.beautiful.ai">beautiful.ai</a> –</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #124: Systems and Software for Machine Learning at Scale with Jeff Dean]]></title>
    <link href="/2018/04/twimlai124/"/>
    <id>/2018/04/twimlai124/</id>
    <published>2018-04-18T00:00:00+00:00</published>
    <updated>2018-04-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Systems and Software for Machine Learning at Scale with Jeff Dean</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai124.jpg" alt="Sketchnotes from TWiMLAI talk #124: Systems and Software for Machine Learning at Scale with Jeff Dean" />
<p class="caption">Sketchnotes from TWiMLAI talk #124: Systems and Software for Machine Learning at Scale with Jeff Dean</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-124-systems-software-machine-learning-scale-jeff-dean/">here</a>.</p>
<blockquote>
<p>In this episode I’m joined by Jeff Dean, Google Senior Fellow and head of the company’s deep learning research team Google Brain, who I had a chance to sit down with last week at the Googleplex in Mountain View. As you’ll hear, I was very excited for this interview, because so many of Jeff’s contributions since he started at Google in ‘99 have touched my life and work. In our conversation, Jeff and I dig into a bunch of the core machine learning innovations we’ve seen from Google. Of course we discuss TensorFlow, and its origins and evolution at Google. We also explore AI acceleration hardware, including TPU v1, v2 and future directions from Google and the broader market in this area. We talk through the machine learning toolchain, including some things that Googlers might take for granted, and where the recently announced Cloud AutoML fits in. We also discuss Google’s process for mapping problems across a variety of domains to deep learning, and much, much more. This was definitely one of my favorite conversations, and I’m pumped to be able to share it with you. <a href="https://twimlai.com/twiml-talk-124-systems-software-machine-learning-scale-jeff-dean/" class="uri">https://twimlai.com/twiml-talk-124-systems-software-machine-learning-scale-jeff-dean/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Meetup slides: Introducing Deep Learning with Keras]]></title>
    <link href="/2018/04/ruhrpy_meetup_2018_slides/"/>
    <id>/2018/04/ruhrpy_meetup_2018_slides/</id>
    <published>2018-04-11T00:00:00+00:00</published>
    <updated>2018-04-11T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>On April 4th, 2018 I gave a talk about <strong>Deep Learning with Keras</strong> at the Ruhr.Py Meetup in Essen, Germany. The talk was not specific to Python, though - so if you’re intersted: the slides can be found here: <a href="https://www.slideshare.net/ShirinGlander/ruhrpy-introducing-deep-learning-with-keras-and-python" class="uri">https://www.slideshare.net/ShirinGlander/ruhrpy-introducing-deep-learning-with-keras-and-python</a></p>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/cZz1j6qtlwm62l" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; display: block; margin: auto; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen>
</iframe>
<div style="margin-bottom:5px">
<strong> <a href="//www.slideshare.net/ShirinGlander/ruhrpy-introducing-deep-learning-with-keras-and-python" title="Ruhr.PY - Introducing Deep Learning with Keras and Python" target="_blank">Ruhr.PY - Introducing Deep Learning with Keras and Python</a> </strong> von <strong><a href="//www.slideshare.net/ShirinGlander" target="_blank">Shirin Glander</a></strong>
</div>
<p>There is also a video recording of my talk, which you can see here: <a href="https://youtu.be/Q8hVXnpEPmc" class="uri">https://youtu.be/Q8hVXnpEPmc</a></p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Q8hVXnpEPmc?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Join MünsteR for our next meetup on deep learning with Keras and R]]></title>
    <link href="/2018/03/meetup_april18/"/>
    <id>/2018/03/meetup_april18/</id>
    <published>2018-03-28T00:00:00+00:00</published>
    <updated>2018-03-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://shiring.github.io/r_users_group/2017/05/20/map.png" />

</div>
<p>In our <a href="http://meetu.ps/e/DVFrp/w54bW/f">next MünsteR R-user group meetup</a> on <strong>Tuesday, April 17th, 2018</strong> Kai Lichtenberg will talk about deep learning with Keras. You can RSVP here: <a href="http://meetu.ps/e/DDY1B/w54bW/f" class="uri">http://meetu.ps/e/DDY1B/w54bW/f</a></p>
<blockquote>
<p>Although neural networks have been around for quite a while now, deep learning really just took of a few years ago. It pretty much all started when Alex Krizhevsky and Geoffrey Hinton utterly crushed classic image recognition in the 2012 ImageNet Large Scale Visual Recognition Challenge by implementing a deep neural network with CUDA on graphics cards. A lot has changed since that time: The toolchain to do deep learning has rapidly evolved into API’s with a very high level of abstraction. Nowadays everyone can train complex neural networks with billions of free parameters. Just last year RStudio announced the Keras for R package. Keras is a high level neural network API that makes it really easy to define the architecture of a neural network. In this talk we will rush through an explanation of convolutional neural networks for image recognition, learn how easy it has become to do production ready deep learning with the use of docker and why R’s syntax is even better suited to define a neural network than python’s. (Hint: Probably this is not a pipe ;-)</p>
</blockquote>
<p>Kai Lichtenberg is a PhD student in the Bosch PhD Program working on models to predict the reliability of components in drive trains by leveraging the ever more available high dimensional data in the era of the Internet of Things. Coming from the field of mechanical engineering and technical reliability (which has a lot to do with stochastic processes) he found his destination in data science. As a long time computer enthusiast he is always keen to use the newest technologies. To bear the pain of getting a toolchain up and running is probably his super power.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[My upcoming meetup talks about Deep Learning with Keras and explaining complex Machine Learning Models with LIME]]></title>
    <link href="/2018/03/meetup_talk_ruhrpy_april_18/"/>
    <id>/2018/03/meetup_talk_ruhrpy_april_18/</id>
    <published>2018-03-28T00:00:00+00:00</published>
    <updated>2018-03-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I’ll be talking about Deep Learning with Keras in R and Python at the following upcoming meetup:</p>
<ul>
<li><a href="https://www.meetup.com/Ruhr-py/events/248093628/">Ruhr.Py 2018</a> on Wednesday, April 4th</li>
</ul>
<blockquote>
<p>Introducing Deep Learning with Keras and Python Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. In this talk we build, train and visualize a Model using Python and Keras - all interactive with Jupyter Notebooks!</p>
</blockquote>
<hr />
<p>And I’ll be talking about explaining complex Machine Learning Models with LIME at this upcoming meetup:</p>
<ul>
<li><a href="https://www.meetup.com/Hamburg-Data-Science-Meetup/events/244145443/">Data Science Meetup Hamburg</a> on Thursday, April 12th</li>
</ul>
<blockquote>
<p>Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex machine learning models are essentially black boxes and too complicated to understand, we need to use approximations to get a better sense of how they work. One such approach is LIME, which stands for Local Interpretable Model-agnostic Explanations and is a tool that helps understand and explain the decisions made by complex machine learning models.</p>
</blockquote>
<blockquote>
<p>Dr. Shirin Glander is Data Scientist at codecentric AG. She has received a PhD in Bioinformatics and applies methods of analysis and visualization from different areas - for instance, machine learning, classical statistics, text mining, etc. -to extract and leverage information from data.</p>
</blockquote>
<div class="figure">
<img src="https://secure.meetupstatic.com/s/img/5455565085016210254/logo/svg/logo--script.svg" />

</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[US Roadtrip Location data mapping]]></title>
    <link href="/2018/03/roadtrip_2018/"/>
    <id>/2018/03/roadtrip_2018/</id>
    <published>2018-03-27T00:00:00+00:00</published>
    <updated>2018-03-27T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<pre class="r"><code>library(jsonlite)
system.time(x &lt;- fromJSON(&quot;~/Documents/Github/Data/Takeout/Standortverlauf/Standortverlauf.json&quot;))</code></pre>
<pre class="r"><code># extracting the locations dataframe
loc = x$locations

# converting time column from posix milliseconds into a readable time scale
loc$time = as.POSIXct(as.numeric(x$locations$timestampMs)/1000, origin = &quot;1970-01-01&quot;)

# converting longitude and latitude from E7 to GPS coordinates
loc$lat = loc$latitudeE7 / 1e7
loc$lon = loc$longitudeE7 / 1e7</code></pre>
<pre class="r"><code>usa &lt;- get_map(location = &#39;USA&#39;, zoom = 5)</code></pre>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #115: Scaling Machine Learning at Uber with Mike Del Balso]]></title>
    <link href="/2018/03/twimlai115/"/>
    <id>/2018/03/twimlai115/</id>
    <published>2018-03-07T00:00:00+00:00</published>
    <updated>2018-03-07T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Scaling Machine Learning at Uber with Mike Del Balso</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai115.jpg" alt="Sketchnotes from TWiMLAI talk #115: Scaling Machine Learning at Uber with Mike Del Balso" />
<p class="caption">Sketchnotes from TWiMLAI talk #115: Scaling Machine Learning at Uber with Mike Del Balso</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-115-scaling-machine-learning-uber-mike-del-balso/">here</a>.</p>
<blockquote>
<p>In this episode, I speak with Mike Del Balso, Product Manager for Machine Learning Platforms at Uber. Mike and I sat down last fall at the Georgian Partners Portfolio conference to discuss his presentation “Finding success with machine learning in your company.” In our discussion, Mike shares some great advice for organizations looking to get value out of machine learning. He also details some of the pitfalls companies run into, such as not have proper infrastructure in place for maintenance and monitoring, not managing their expectations, and not putting the right tools in place for data science and development teams. On this last point, we touch on the Michelangelo platform, which Uber uses internally to build, deploy and maintain ML systems at scale, and the open source distributed TensorFlow system they’ve created, Horovod. This was a very insightful interview, so get your notepad ready! <a href="https://twimlai.com/twiml-talk-115-scaling-machine-learning-uber-mike-del-balso/" class="uri">https://twimlai.com/twiml-talk-115-scaling-machine-learning-uber-mike-del-balso/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Another Game of Thrones network analysis - this time with tidygraph and ggraph]]></title>
    <link href="/2018/03/got_network/"/>
    <id>/2018/03/got_network/</id>
    <published>2018-03-04T00:00:00+00:00</published>
    <updated>2018-03-04T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>A while back, I did <a href="https://shiring.github.io/networks/2017/05/15/got_final">an analysis of the family network of major characters from the A Song of Ice and Fire books and the Game of Thrones TV show</a>. In that analysis I found out that House Stark (specifically Ned and Sansa) and House Lannister (especially Tyrion) are the most important family connections in Game of Thrones; they also connect many of the story lines and are central parts of the narrative.</p>
<p>In that old post, I used <code>igraph</code> for plotting and calculating network metrics.</p>
<p>But there are two packages that integrate network analysis much more nicely with the <code>tidyverse</code>: <code>tidygraph</code> and <code>ggraph</code>. These, I am going to show how to use for analyzing yet another network of characters from <strong>A Song of Ice and Fire</strong> / <strong>Game of Thrones</strong> (to be correct, this new network here is strictly based on the <strong>A Song of Ice and Fire</strong> books, NOT on the TV show).</p>
<div id="what-can-network-analysis-tell-us" class="section level2">
<h2>What can network analysis tell us?</h2>
<p>Network analysis can e.g. be used to explore relationships in social or professional networks. In such cases, we would typically ask questions like:</p>
<ul>
<li>How many connections does each person have?</li>
<li>Who is the most connected (i.e. influential or “important”) person?</li>
<li>Are there clusters of tightly connected people?</li>
<li>Are there a few key players that connect clusters of people?</li>
<li>etc.</li>
</ul>
<p>These answers can give us a lot of information about the patterns of how people interact.</p>
<p>So, how do we find out who the most important characters are in this network? We consider a character “important” if he has connections to many other characters. There are a few network properties, that tell us more about this, like node centrality and which characters are key-players in the books.</p>
<p><strong>A word of caution before you read on: BEWARE of SPOILERS for all books!</strong></p>
<div class="figure">
<img src="https://shirinsplayground.netlify.com/post/2018-03-03_got_network_files/figure-html/unnamed-chunk-18-1.png" alt="A Song of Ice and Fire character network across all five books; find out how I made it by following the code below…" />
<p class="caption">A Song of Ice and Fire character network across all five books; find out how I made it by following the code below…</p>
</div>
<pre class="r"><code>library(readr)     # fast reading of csv files
library(tidyverse) # tidy data analysis
library(tidygraph) # tidy graph analysis
library(ggraph)    # for plotting</code></pre>
</div>
<div id="the-data" class="section level2">
<h2>The Data</h2>
<p>I obtained the following data by cloning <a href="https://github.com/mathbeveridge/asoiaf">this Github repository</a> from Andrew Beveridge:</p>
<blockquote>
<p>Character Interaction Networks for George R. R. Martin’s “A Song of Ice and Fire” saga These networks were created by connecting two characters whenever their names (or nicknames) appeared within 15 words of one another in one of the books in “A Song of Ice and Fire.” The edge weight corresponds to the number of interactions. You can use this data to explore the dynamics of the Seven Kingdoms using network science techniques. For example, community detection finds coherent plotlines. Centrality measures uncover the multiple ways in which characters play important roles in the saga.</p>
</blockquote>
<p>Andrew already did a great job analyzing these character networks and you can read all his conclusions on his site <a href="https://networkofthrones.wordpress.com" class="uri">https://networkofthrones.wordpress.com</a>. Here, I don’t aim to replicate his analyses but I want to show how you could do this or similar analyses with <code>tidygraph</code> and <code>ggraph</code>. Thus, I am also not going to use all of his node and edge files.</p>
<pre class="r"><code>path &lt;- &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data/&quot;
files &lt;- list.files(path = path, full.names = TRUE)
files</code></pre>
<pre><code>##  [1] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-all-edges.csv&quot;   
##  [2] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-all-nodes.csv&quot;   
##  [3] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book1-edges.csv&quot; 
##  [4] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book1-nodes.csv&quot; 
##  [5] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book2-edges.csv&quot; 
##  [6] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book2-nodes.csv&quot; 
##  [7] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book3-edges.csv&quot; 
##  [8] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book3-nodes.csv&quot; 
##  [9] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book4-edges.csv&quot; 
## [10] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book4-nodes.csv&quot; 
## [11] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book45-edges.csv&quot;
## [12] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book45-nodes.csv&quot;
## [13] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book5-edges.csv&quot; 
## [14] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book5-nodes.csv&quot;</code></pre>
</div>
<div id="characters-across-all-books" class="section level2">
<h2>Characters across all books</h2>
<p>The first data set I am going to use are the character interactions in all five books. I am not using the node files here, because I find the edge names sufficient for this demonstration. If you wanted to have nice name labels, you could use the node files.</p>
<pre class="r"><code>cooc_all_edges &lt;- read_csv(files[1])</code></pre>
<p>Because there are so many characters in the books, many of them minor, I am subsetting the data to the 100 characters with the most interactions across all books. The edges are undirected, therefore there are no redundant Source-Target combinations; because of this, I gathered Source and Target data before summing up the weights.</p>
<pre class="r"><code>main_ch &lt;- cooc_all_edges %&gt;%
  select(-Type) %&gt;%
  gather(x, name, Source:Target) %&gt;%
  group_by(name) %&gt;%
  summarise(sum_weight = sum(weight)) %&gt;%
  ungroup()

main_ch_l &lt;- main_ch %&gt;%
  arrange(desc(sum_weight)) %&gt;%
  top_n(100, sum_weight)
main_ch_l</code></pre>
<pre><code>## # A tibble: 100 x 2
##    name               sum_weight
##    &lt;chr&gt;                   &lt;int&gt;
##  1 Tyrion-Lannister         2873
##  2 Jon-Snow                 2757
##  3 Cersei-Lannister         2232
##  4 Joffrey-Baratheon        1762
##  5 Eddard-Stark             1649
##  6 Daenerys-Targaryen       1608
##  7 Jaime-Lannister          1569
##  8 Sansa-Stark              1547
##  9 Bran-Stark               1508
## 10 Robert-Baratheon         1488
## # ... with 90 more rows</code></pre>
<pre class="r"><code>cooc_all_f &lt;- cooc_all_edges %&gt;%
  filter(Source %in% main_ch_l$name &amp; Target %in% main_ch_l$name)</code></pre>
</div>
<div id="tidygraph-and-ggraph" class="section level2">
<h2>tidygraph and ggraph</h2>
<p>Both <code>tidygraph</code> and <code>ggraph</code> have been developed by <a href="https://www.data-imaginist.com">Thomas Lin Pedersen</a>:</p>
<blockquote>
<p>With tidygraph I set out to make it easier to get your data into a graph and perform common transformations on it, but the aim has expanded since its inception. The goal of tidygraph is to empower the user to formulate complex questions regarding relational data as simple steps, thus enabling them to retrieve insights directly from the data itself. The central idea this all boils down to is this: you don’t have to plot a network to understand it. While I absolutely love the field of network visualisation, it is in many ways overused in data science — especially when it comes to extracting knowledge from a network. Just as you don’t need a plot to tell you which car in a dataset is the fastest, you don’t need a plot to tell you which pair of friends are the closest. What you do need, instead of a plot, is a tool that allow you to formulate your question into a logic sequence of operations. For many people in the world of rectangular data, this tool is increasingly dplyr (and friends), and I do hope that tidygraph can take on the same role in the world of relational data. <a href="https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/" class="uri">https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/</a></p>
</blockquote>
<p>The first step is to convert our edge table into a <code>tbl_graph</code> object structure. Here, we use the <code>as_tbl_graph()</code> function from <code>tidygraph</code>; it can take many different types of input data, like <code>data.frame</code>, <code>matrix</code>, <code>dendrogram</code>, <code>igraph</code>, etc.</p>
<blockquote>
<p>Underneath the hood of tidygraph lies the well-oiled machinery of igraph, ensuring efficient graph manipulation. Rather than keeping the node and edge data in a list and creating igraph objects on the fly when needed, tidygraph subclasses igraph with the tbl_graph class and simply exposes it in a tidy manner. This ensures that all your beloved algorithms that expects igraph objects still works with tbl_graph objects. Further, tidygraph is very careful not to override any of igraphs exports so the two packages can coexist quite happily. <a href="https://www.data-imaginist.com/2017/introducing-tidygraph/" class="uri">https://www.data-imaginist.com/2017/introducing-tidygraph/</a></p>
</blockquote>
<p>A central aspect of <code>tidygraph</code> is that you can directly manipulate node and edge data from this <code>tbl_graph</code> object by <strong>activating</strong> nodes or edges. When we first create a <code>tbl_graph</code> object, the nodes will be activated. We can then directly calculate node or edge metrics, like centrality, using <code>tidyverse</code> functions.</p>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE)</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Node Data: 100 x 1 (active)
##   name                           
##   &lt;chr&gt;                          
## 1 Aemon-Targaryen-(Maester-Aemon)
## 2 Aeron-Greyjoy                  
## 3 Aerys-II-Targaryen             
## 4 Alliser-Thorne                 
## 5 Arianne-Martell                
## 6 Arya-Stark                     
## # ... with 94 more rows
## #
## # Edge Data: 798 x 5
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1     1     4 Undirected    43      7
## 2     1    13 Undirected    44      4
## 3     1    28 Undirected    52      3
## # ... with 795 more rows</code></pre>
<p>We can change that with the <code>activate()</code> function. We can now, for example, remove multiple edges. When you are using RStudio, start typing <code>?edge_is_</code> and wait for the autocomplete function to show you what else is possible (or go to the <code>tidygraph</code> manual).</p>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(edges) %&gt;%
  filter(!edge_is_multiple())</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Edge Data: 798 x 5 (active)
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1     1     4 Undirected    43      7
## 2     1    13 Undirected    44      4
## 3     1    28 Undirected    52      3
## 4     1    32 Undirected    53     20
## 5     1    34 Undirected    54      5
## 6     1    41 Undirected    56      5
## # ... with 792 more rows
## #
## # Node Data: 100 x 1
##   name                           
##   &lt;chr&gt;                          
## 1 Aemon-Targaryen-(Maester-Aemon)
## 2 Aeron-Greyjoy                  
## 3 Aerys-II-Targaryen             
## # ... with 97 more rows</code></pre>
<div id="node-ranking" class="section level3">
<h3>Node ranking</h3>
<blockquote>
<p>Often, especially when visualising networks with certain layouts, the order in which the nodes appear will have a huge influence on the insight you can get out (e.g. matrix plots and arc diagrams). The node_rank_*() family of algorithms have been introduced to provide different ways of sorting nodes so that closely related nodes are positionally close. As there is often not a single correct answer to this endeavor, there’s a lot of different algorithms that may provide different insights into your network. Many of them are based on the seriation package, and the vignette provided therein serves as a nice introduction to the different algorithms. <a href="https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/" class="uri">https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/</a></p>
</blockquote>
<p>There are many options for node ranking (go to <code>?node_rank</code> for a full list); let’s try out <strong>Minimize hamiltonian path length using a travelling salesperson solver</strong>.</p>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(nodes) %&gt;% 
  mutate(n_rank_trv = node_rank_traveller()) %&gt;%
  arrange(n_rank_trv)</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Node Data: 100 x 2 (active)
##   name            n_rank_trv
##   &lt;chr&gt;                &lt;int&gt;
## 1 Arianne-Martell          1
## 2 Doran-Martell            2
## 3 Oberyn-Martell           3
## 4 Tywin-Lannister          4
## 5 Varys                    5
## 6 Shae                     6
## # ... with 94 more rows
## #
## # Edge Data: 798 x 5
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1    41    46 Undirected    43      7
## 2    39    41 Undirected    44      4
## 3    41    43 Undirected    52      3
## # ... with 795 more rows</code></pre>
</div>
<div id="centrality" class="section level3">
<h3>Centrality</h3>
<p>Centrality describes the number of edges that are in- or outgoing to/from nodes. High centrality networks have few nodes with many connections, low centrality networks have many nodes with similar numbers of edges. The centrality of a node measures the importance of it in the network.</p>
<blockquote>
<p>This version adds 19(!) new ways to define the notion of centrality along with a manual version where you can mix and match different distance measures and summation strategies opening up the world to even more centrality scores. All of this wealth of centrality comes from the netrankr package that provides a framework for defining and calculating centrality scores. If you use centrality measures somewhere in your analysis I cannot recommend the vignettes provided by netrankr enough as they provide a fundamental intuition about the nature of such measures and how they can/should be used. <a href="https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/" class="uri">https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/</a></p>
</blockquote>
<p>Again, type <code>?centrality</code> for an overview about all possible centrality measures you can use. Let’s try out <code>centrality_degree()</code>.</p>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(nodes) %&gt;% 
  mutate(neighbors = centrality_degree()) %&gt;%
  arrange(-neighbors)</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Node Data: 100 x 2 (active)
##   name              neighbors
##   &lt;chr&gt;                 &lt;dbl&gt;
## 1 Tyrion-Lannister        54.
## 2 Cersei-Lannister        49.
## 3 Joffrey-Baratheon       49.
## 4 Robert-Baratheon        47.
## 5 Jaime-Lannister         45.
## 6 Sansa-Stark             44.
## # ... with 94 more rows
## #
## # Edge Data: 798 x 5
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1    41    42 Undirected    43      7
## 2    41    60 Undirected    44      4
## 3    41    63 Undirected    52      3
## # ... with 795 more rows</code></pre>
</div>
<div id="grouping-and-clustering" class="section level3">
<h3>Grouping and clustering</h3>
<blockquote>
<p>Another common operation is to group nodes based on the graph topology, sometimes referred to as community detection based on its commonality in social network analysis. All clustering algorithms from igraph is available in tidygraph using the group_* prefix. All of these functions return an integer vector with nodes (or edges) sharing the same integer being grouped together. <a href="https://www.data-imaginist.com/2017/introducing-tidygraph/" class="uri">https://www.data-imaginist.com/2017/introducing-tidygraph/</a></p>
</blockquote>
<p>We can use <code>?group_graph</code> for an overview about all possible ways to cluster and group nodes. Here I am using <code>group_infomap()</code>: <strong>Group nodes by minimizing description length using</strong>.</p>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(nodes) %&gt;% 
  mutate(group = group_infomap()) %&gt;%
  arrange(-group)</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Node Data: 100 x 2 (active)
##   name              group
##   &lt;chr&gt;             &lt;int&gt;
## 1 Arianne-Martell       7
## 2 Doran-Martell         7
## 3 Davos-Seaworth        6
## 4 Melisandre            6
## 5 Selyse-Florent        6
## 6 Stannis-Baratheon     6
## # ... with 94 more rows
## #
## # Edge Data: 798 x 5
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1    32    33 Undirected    43      7
## 2    32    34 Undirected    44      4
## 3    32    36 Undirected    52      3
## # ... with 795 more rows</code></pre>
</div>
<div id="querying-node-types" class="section level3">
<h3>Querying node types</h3>
<p>We can also query different node types (<code>?node_types</code> gives us a list of options):</p>
<blockquote>
<p>These functions all lets the user query whether each node is of a certain type. All of the functions returns a logical vector indicating whether the node is of the type in question. Do note that the types are not mutually exclusive and that nodes can thus be of multiple types.</p>
</blockquote>
<p>Here, I am trying out <code>node_is_center()</code> (does the node have the minimal eccentricity in the graph) and <code>node_is_keyplayer()</code> to identify the top 10 key-players in the network. You can read more about the <code>node_is_keyplayer()</code> function in the manual for the <code>influenceR</code> package:</p>
<blockquote>
<p>The “Key Player” family of node importance algorithms (Borgatti 2006) involves the selection of a metric of node importance and a combinatorial optimization strategy to choose the set S of vertices of size k that maximize that metric. This function implements KPP-Pos, a metric intended to identify k nodes which optimize resource diffusion through the net … <a href="https://cran.r-project.org/web/packages/influenceR/" class="uri">https://cran.r-project.org/web/packages/influenceR/</a></p>
</blockquote>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(nodes) %&gt;% 
  mutate(center = node_is_center(),
         keyplayer = node_is_keyplayer(k = 10))</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Node Data: 100 x 3 (active)
##   name                            center keyplayer
##   &lt;chr&gt;                           &lt;lgl&gt;  &lt;lgl&gt;    
## 1 Aemon-Targaryen-(Maester-Aemon) FALSE  FALSE    
## 2 Aeron-Greyjoy                   FALSE  FALSE    
## 3 Aerys-II-Targaryen              FALSE  FALSE    
## 4 Alliser-Thorne                  FALSE  FALSE    
## 5 Arianne-Martell                 FALSE  FALSE    
## 6 Arya-Stark                      FALSE  TRUE     
## # ... with 94 more rows
## #
## # Edge Data: 798 x 5
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1     1     4 Undirected    43      7
## 2     1    13 Undirected    44      4
## 3     1    28 Undirected    52      3
## # ... with 795 more rows</code></pre>
</div>
</div>
<div id="node-pairs" class="section level2">
<h2>Node pairs</h2>
<blockquote>
<p>Some statistics are a measure between two nodes, such as distance or similarity between nodes. In a tidy context one of the ends must always be the node defined by the row, while the other can be any other node. All of the node pair functions are prefixed with node_* and ends with _from/_to if the measure is not symmetric and _with if it is; e.g. there’s both a node_max_flow_to() and node_max_flow_from() function while only a single node_cocitation_with() function. The other part of the node pair can be specified as an integer vector that will get recycled if needed, or a logical vector which will get recycled and converted to indexes with which(). This means that output from node type functions can be used directly in the calls. <a href="https://www.data-imaginist.com/2017/introducing-tidygraph/" class="uri">https://www.data-imaginist.com/2017/introducing-tidygraph/</a></p>
</blockquote>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(nodes) %&gt;% 
  mutate(dist_to_center = node_distance_to(node_is_center()))</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Node Data: 100 x 2 (active)
##   name                            dist_to_center
##   &lt;chr&gt;                                    &lt;dbl&gt;
## 1 Aemon-Targaryen-(Maester-Aemon)             1.
## 2 Aeron-Greyjoy                               2.
## 3 Aerys-II-Targaryen                          1.
## 4 Alliser-Thorne                              1.
## 5 Arianne-Martell                             2.
## 6 Arya-Stark                                  1.
## # ... with 94 more rows
## #
## # Edge Data: 798 x 5
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1     1     4 Undirected    43      7
## 2     1    13 Undirected    44      4
## 3     1    28 Undirected    52      3
## # ... with 795 more rows</code></pre>
<div id="edge-betweenness" class="section level3">
<h3>Edge betweenness</h3>
<p>Similarly to node metrics, we can calculate all kinds of edge metrics. Betweenness, for example, describes the shortest paths between nodes. More about what you can do with edges can be found with <code>?edge_types</code> and in the <a href="https://cran.r-project.org/web/packages/tidygraph/tidygraph.pdf">tidygraph manual</a>.</p>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(edges) %&gt;% 
  mutate(centrality_e = centrality_edge_betweenness())</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Edge Data: 798 x 6 (active)
##    from    to Type          id weight centrality_e
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;        &lt;dbl&gt;
## 1     1     4 Undirected    43      7         1.00
## 2     1    13 Undirected    44      4        30.2 
## 3     1    28 Undirected    52      3        42.1 
## 4     1    32 Undirected    53     20         0.  
## 5     1    34 Undirected    54      5        35.2 
## 6     1    41 Undirected    56      5        18.9 
## # ... with 792 more rows
## #
## # Node Data: 100 x 1
##   name                           
##   &lt;chr&gt;                          
## 1 Aemon-Targaryen-(Maester-Aemon)
## 2 Aeron-Greyjoy                  
## 3 Aerys-II-Targaryen             
## # ... with 97 more rows</code></pre>
</div>
</div>
<div id="the-complete-code" class="section level2">
<h2>The complete code</h2>
<p>Now let’s combine what we’ve done above in true tidyverse fashion:</p>
<pre class="r"><code>cooc_all_f_graph &lt;- as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  mutate(n_rank_trv = node_rank_traveller(),
         neighbors = centrality_degree(),
         group = group_infomap(),
         center = node_is_center(),
         dist_to_center = node_distance_to(node_is_center()),
         keyplayer = node_is_keyplayer(k = 10)) %&gt;%
  activate(edges) %&gt;% 
  filter(!edge_is_multiple()) %&gt;%
  mutate(centrality_e = centrality_edge_betweenness())</code></pre>
<p>We can also convert our active node or edge table back to a <code>tibble</code>:</p>
<pre class="r"><code>cooc_all_f_graph %&gt;%
  activate(nodes) %&gt;% # %N&gt;%
  as.tibble()</code></pre>
<pre><code>## # A tibble: 100 x 7
##    name         n_rank_trv neighbors group center dist_to_center keyplayer
##    &lt;chr&gt;             &lt;int&gt;     &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt;           &lt;dbl&gt; &lt;lgl&gt;    
##  1 Aemon-Targa…         48       13.     2 FALSE              1. FALSE    
##  2 Aeron-Greyj…         94        5.     5 FALSE              2. FALSE    
##  3 Aerys-II-Ta…         74       12.     1 FALSE              1. FALSE    
##  4 Alliser-Tho…         50       13.     2 FALSE              1. FALSE    
##  5 Arianne-Mar…         85        4.     7 FALSE              2. FALSE    
##  6 Arya-Stark           63       37.     1 FALSE              1. FALSE    
##  7 Asha-Greyjoy         93        7.     5 FALSE              1. FALSE    
##  8 Balon-Greyj…         92       11.     5 FALSE              2. FALSE    
##  9 Barristan-S…         83       23.     3 FALSE              1. FALSE    
## 10 Belwas               77        6.     3 FALSE              2. FALSE    
## # ... with 90 more rows</code></pre>
<pre class="r"><code>cooc_all_f_graph %&gt;%
  activate(edges) %&gt;% # %E&gt;%
  as.tibble()</code></pre>
<pre><code>## # A tibble: 798 x 6
##     from    to Type          id weight centrality_e
##    &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;        &lt;dbl&gt;
##  1     1     4 Undirected    43      7         1.00
##  2     1    13 Undirected    44      4        30.2 
##  3     1    28 Undirected    52      3        42.1 
##  4     1    32 Undirected    53     20         0.  
##  5     1    34 Undirected    54      5        35.2 
##  6     1    41 Undirected    56      5        18.9 
##  7     1    42 Undirected    57     25         0.  
##  8     1    48 Undirected    58    110         0.  
##  9     1    58 Undirected    60      5        24.5 
## 10     1    71 Undirected    62      5        17.0 
## # ... with 788 more rows</code></pre>
<div id="plotting-with-ggraph" class="section level3">
<h3>Plotting with ggraph</h3>
<p>For plotting our graph object, we can make good use of the <code>ggraph</code> package:</p>
<blockquote>
<p>ggraph is an extension of ggplot2 aimed at supporting relational data structures such as networks, graphs, and trees. While it builds upon the foundation of ggplot2 and its API it comes with its own self-contained set of geoms, facets, etc., as well as adding the concept of layouts to the grammar. <a href="https://github.com/thomasp85/ggraph" class="uri">https://github.com/thomasp85/ggraph</a></p>
</blockquote>
<p>First, I am going to define a layout. There are lots of <a href="https://www.data-imaginist.com/2017/ggraph-introduction-layouts/">options for layouts</a>, here I am using a <a href="http://igraph.org/r/doc/layout_with_fr.html">Fruchterman-Reingold</a> algorithm.</p>
<pre class="r"><code>layout &lt;- create_layout(cooc_all_f_graph, 
                        layout = &quot;fr&quot;)</code></pre>
<p>The rest works like any <code>ggplot2</code> function call, just that we use special geoms for our network, like <code>geom_edge_density()</code> to draw a shadow where the edge density is higher, <code>geom_edge_link()</code> to connect edges with a straight line, <code>geom_node_point()</code> to draw node points and <code>geom_node_text()</code> to draw the labels. More options can be found <a href="https://github.com/thomasp85/ggraph">here</a>.</p>
<p>Here are three options of plotting the network with the metrics we just calculated:</p>
<pre class="r"><code>ggraph(layout) + 
    geom_edge_density(aes(fill = weight)) +
    geom_edge_link(aes(width = weight), alpha = 0.2) + 
    geom_node_point(aes(color = factor(group)), size = 10) +
    geom_node_text(aes(label = name), size = 8, repel = TRUE) +
    scale_color_brewer(palette = &quot;Set1&quot;) +
    theme_graph() +
    labs(title = &quot;A Song of Ice and Fire character network&quot;,
         subtitle = &quot;Nodes are colored by group&quot;)</code></pre>
<p><img src="/post/2018-03-03_got_network_files/figure-html/unnamed-chunk-18-1.png" width="2880" /></p>
<p>Interestingly, many of the groups reflect the narrative perfectly: the men from the Night’s Watch are grouped together with the Wildlings, Stannis, Davos, Selyse and Melisandre form another group, the Greyjoys, Bran’s group in Winterfell before they left for the North, Dany and her squad and the Martells (except for Quentyn, who “belongs” to Dany - just like in the books ;-)). The big group around the remaining characters is the only one that’s not split up very well.</p>
<p>For the next graphs, I want specific colors form the <code>RColorBrewer</code> palette “Set1”:</p>
<pre class="r"><code>cols &lt;- RColorBrewer::brewer.pal(3, &quot;Set1&quot;)</code></pre>
<pre class="r"><code>ggraph(layout) + 
    geom_edge_density(aes(fill = weight)) +
    geom_edge_link(aes(width = weight), alpha = 0.2) + 
    geom_node_point(aes(color = factor(center), size = dist_to_center)) +
    geom_node_text(aes(label = name), size = 8, repel = TRUE) +
    scale_colour_manual(values = c(cols[2], cols[1])) +
    theme_graph() +
    labs(title = &quot;A Song of Ice and Fire character network&quot;,
         subtitle = &quot;Nodes are colored by centeredness&quot;)</code></pre>
<p><img src="/post/2018-03-03_got_network_files/figure-html/unnamed-chunk-20-1.png" width="2880" /></p>
<p>In the next graph I plotted the center-most characters in red and the distance to center as node size. The two center characters across all books are Robert Baratheon and Tyrion Lannister. I had not expected Robert, since he dies pretty much right away but I guess he and his rebellion following Lyanna’s “abduction” is the main trigger for most of what happens in the books, so why not… And that Tyrion is the best character (and George RR Martin’s favorite) is a given, anyways! ;-)</p>
</div>
</div>
<div id="characters-devided-by-books" class="section level2">
<h2>Characters devided by books</h2>
<p>The second data set I am going to use is a comparison of character interactions in the five books.</p>
<p><strong>A little node on the side:</strong> My original plan was to loop over the separate edge files for each book, concatenate them together with the information from which book they are and then plot them via faceting. This turned out to be a bad solution because I wanted to show the different key-players in each of the five books. So, instead of using one joined graph, I created separate graphs for every book and used the <code>bind_graphs()</code> and <code>facet_nodes()</code> functions to plot them together.</p>
<pre class="r"><code>for (i in 1:5) {
  cooc &lt;- read_csv(paste0(&quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book&quot;, i, &quot;-edges.csv&quot;)) %&gt;%
    mutate(book = paste0(&quot;book_&quot;, i)) %&gt;%
    filter(Source %in% main_ch_l$name &amp; Target %in% main_ch_l$name)
  
  assign(paste0(&quot;coocs_book_&quot;, i), cooc)
}</code></pre>
<p>The concepts are the same as above, here I want to know the key-players in each book:</p>
<pre class="r"><code>cooc_books_1_graph &lt;- as_tbl_graph(coocs_book_1, directed = FALSE) %&gt;%
  mutate(book = &quot;Book 1: A Game of Thrones&quot;,
         keyplayer = node_is_keyplayer(k = 10))

cooc_books_2_graph &lt;- as_tbl_graph(coocs_book_2, directed = FALSE) %&gt;%
  mutate(book = &quot;Book 2: A Clash of Kings&quot;,
         keyplayer = node_is_keyplayer(k = 10))

cooc_books_3_graph &lt;- as_tbl_graph(coocs_book_3, directed = FALSE) %&gt;%
  mutate(book = &quot;Book 3: A Storm of Swords&quot;,
         keyplayer = node_is_keyplayer(k = 10))

cooc_books_4_graph &lt;- as_tbl_graph(coocs_book_4, directed = FALSE) %&gt;%
  mutate(book = &quot;Book 4: A Feast for Crows&quot;,
         keyplayer = node_is_keyplayer(k = 10))

cooc_books_5_graph &lt;- as_tbl_graph(coocs_book_5, directed = FALSE) %&gt;%
  mutate(book = &quot;Book 5: A Dance with Dragons&quot;,
         keyplayer = node_is_keyplayer(k = 10))</code></pre>
<p>And let’s combine and plot the key-players:</p>
<pre class="r"><code>cooc_books_1_graph %&gt;% 
  bind_graphs(cooc_books_2_graph)  %&gt;%
  bind_graphs(cooc_books_3_graph)  %&gt;%
  bind_graphs(cooc_books_4_graph)  %&gt;%
  bind_graphs(cooc_books_5_graph)  %&gt;%
  ggraph(layout = &quot;fr&quot;) + 
    facet_nodes( ~ book, scales = &quot;free&quot;, ncol = 1) +
    geom_edge_density(aes(fill = weight)) +
    geom_edge_link(aes(edge_width = weight), alpha = 0.2) + 
    geom_node_point(aes(color = factor(keyplayer)), size = 3) +
    geom_node_text(aes(label = name), color = &quot;black&quot;, size = 3, repel = TRUE) +
    theme_graph() +
    scale_colour_manual(values = c(cols[2], cols[1]))</code></pre>
<p><img src="/post/2018-03-03_got_network_files/figure-html/unnamed-chunk-23-1.png" width="960" /></p>
<p>The networks and key-players of the five different books also offer a few surprises but also a lot that reflects the narrative quite well. I’m not going to go into details here as that would go a bit too far for an R-related blog - but if you are interested in in-depth discussions about the books, email me… ;-)</p>
</div>
<div id="more-info" class="section level2">
<h2>More info</h2>
<p>You can find more info about</p>
<ul>
<li><code>tidygraph</code> <a href="https://cran.r-project.org/web/packages/tidygraph">here</a></li>
<li><code>ggraph</code> <a href="https://cran.r-project.org/web/packages/ggraph">here</a></li>
<li><code>influenceR</code> <a href="https://cran.r-project.org/web/packages/influenceR">here</a></li>
<li>and DataCamp has a Python project for the same data set <a href="https://www.datacamp.com/projects/76?utm_campaign=broadcast&amp;utm_medium=broadcast_8&amp;utm_source=main">here</a></li>
</ul>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.3
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] methods   stats     graphics  grDevices utils     datasets  base     
## 
## other attached packages:
##  [1] bindrcpp_0.2       ggraph_1.0.1       tidygraph_1.1.0   
##  [4] forcats_0.3.0      stringr_1.3.0      dplyr_0.7.4       
##  [7] purrr_0.2.4        tidyr_0.8.0        tibble_1.4.2      
## [10] ggplot2_2.2.1.9000 tidyverse_1.2.1    readr_1.1.1       
## 
## loaded via a namespace (and not attached):
##  [1] nlme_3.1-131.1     bitops_1.0-6       lubridate_1.7.3   
##  [4] RColorBrewer_1.1-2 httr_1.3.1         prabclus_2.2-6    
##  [7] rprojroot_1.3-2    tools_3.4.3        backports_1.1.2   
## [10] utf8_1.1.3         R6_2.2.2           KernSmooth_2.23-15
## [13] lazyeval_0.2.1     colorspace_1.3-2   trimcluster_0.1-2 
## [16] nnet_7.3-12        withr_2.1.1.9000   tidyselect_0.2.4  
## [19] gridExtra_2.3      mnormt_1.5-5       compiler_3.4.3    
## [22] cli_1.0.0          rvest_0.3.2        TSP_1.1-5         
## [25] influenceR_0.1.0   xml2_1.2.0         labeling_0.3      
## [28] bookdown_0.7       diptest_0.75-7     caTools_1.17.1    
## [31] scales_0.5.0.9000  DEoptimR_1.0-8     robustbase_0.92-8 
## [34] mvtnorm_1.0-7      psych_1.7.8        digest_0.6.15     
## [37] foreign_0.8-69     rmarkdown_1.8      pkgconfig_2.0.1   
## [40] htmltools_0.3.6    rlang_0.2.0.9000   readxl_1.0.0      
## [43] rstudioapi_0.7     bindr_0.1          jsonlite_1.5      
## [46] mclust_5.4         gtools_3.5.0       dendextend_1.7.0  
## [49] magrittr_1.5       modeltools_0.2-21  Rcpp_0.12.15      
## [52] munsell_0.4.3      viridis_0.5.0      stringi_1.1.6     
## [55] whisker_0.3-2      yaml_2.1.17        MASS_7.3-49       
## [58] flexmix_2.3-14     gplots_3.0.1       plyr_1.8.4        
## [61] grid_3.4.3         parallel_3.4.3     gdata_2.18.0      
## [64] ggrepel_0.7.0      crayon_1.3.4       udunits2_0.13     
## [67] lattice_0.20-35    haven_1.1.1        hms_0.4.1         
## [70] knitr_1.20         pillar_1.2.1       igraph_1.1.2      
## [73] fpc_2.1-11         stats4_3.4.3       reshape2_1.4.3    
## [76] codetools_0.2-15   glue_1.2.0         gclus_1.3.1       
## [79] evaluate_0.10.1    blogdown_0.5       modelr_0.1.1      
## [82] tweenr_0.1.5       foreach_1.4.4      cellranger_1.1.0  
## [85] gtable_0.2.0       kernlab_0.9-25     assertthat_0.2.0  
## [88] xfun_0.1           ggforce_0.1.1      broom_0.4.3       
## [91] class_7.3-14       viridisLite_0.3.0  seriation_1.2-3   
## [94] iterators_1.0.9    registry_0.5       units_0.5-1       
## [97] cluster_2.0.6</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #111: Learning “Common Sense” and Physical Concepts with Roland Memisevic]]></title>
    <link href="/2018/02/twimlai111/"/>
    <id>/2018/02/twimlai111/</id>
    <published>2018-02-19T00:00:00+00:00</published>
    <updated>2018-02-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Learning “Common Sense” and Physical Concepts with Roland Memisevic</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai111.jpg" alt="Sketchnotes from TWiMLAI talk #111: Learning Common Sense and Physical Concepts with Roland Memisevic" />
<p class="caption">Sketchnotes from TWiMLAI talk #111: Learning “Common Sense” and Physical Concepts with Roland Memisevic</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-111-learning-common-sense-physical-concepts-roland-memisevic/">here</a>.</p>
<blockquote>
<p>In today’s episode, I’m joined by Roland Memisevic, co-founder, CEO, and chief scientist at Twenty Billion Neurons. Roland joined me at the RE•WORK Deep Learning Summit in Montreal to discuss the work his company is doing to train deep neural networks to understand physical actions. In our conversation, we dig into video analysis and understanding, including how data-rich video can help us develop what Roland calls comparative understanding, or AI “common sense”. We briefly touch on the implications of AI/ML systems having comparative understanding, and how Roland and his team are addressing problems like getting properly labeled training data. <a href="https://twimlai.com/twiml-talk-111-learning-common-sense-physical-concepts-roland-memisevic/" class="uri">https://twimlai.com/twiml-talk-111-learning-common-sense-physical-concepts-roland-memisevic/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[April 12th &amp; 13th in Hamburg: Workshop on Deep Learning with Keras and TensorFlow in R]]></title>
    <link href="/2018/02/deep_learning_keras_tensorflow_18_04/"/>
    <id>/2018/02/deep_learning_keras_tensorflow_18_04/</id>
    <published>2018-02-06T00:00:00+00:00</published>
    <updated>2018-02-06T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Registration is now open for my 1.5-day <a href="https://www.eventbrite.de/e/workshop-deep-learning-mit-keras-und-tensorflow-tickets-42710095044?utm-medium=discovery&amp;utm-campaign=social&amp;utm-content=attendeeshare&amp;aff=escb&amp;utm-source=cp&amp;utm-term=listing">workshop on deep learning with Keras and TensorFlow using R</a>.</p>
<p>It will take place on <strong>April 12th and 13th</strong> in <strong>Hamburg, Germany</strong>.</p>
<p><a href="https://blog.codecentric.de/en/2018/02/deep-learning-workshop-bei-der-codecentric-ag-solingen/">You can read about one participant’s experience in my last workshop:</a></p>
<blockquote>
<p>Big Data – a buzz word you can find everywhere these days, from nerdy blogs to scientific research papers and even in the news. But how does Big Data Analysis work, exactly? In order to find that out, I attended the workshop on “Deep Learning with Keras and TensorFlow”. On a stormy Thursday afternoon, we arrived at the modern and light-flooded codecentric AG headquarters. There, we met performance expert Dieter Dirkes and Data Scientist Dr. Shirin Glander. In the following two days, Shirin gave us a hands-on introduction into the secrets of Deep Learning and helped us to program our first Neural Net. After a short round of introduction of the participants, it became clear that many different areas and domains are interested in Deep Learning: geologists want to classify (satellite) images, energy providers want to analyse time-series, insurers want to predict numbers and I – a humanities major – want to classify text. And codecentric employees were also interested in getting to know the possibilities of Deep Learning, so that a third of the participants were employees from the company itself.</p>
</blockquote>
<p><a href="https://blog.codecentric.de/en/2018/02/deep-learning-workshop-bei-der-codecentric-ag-solingen/">Continue reading…</a></p>
<p>In my workshop, you will learn</p>
<ul>
<li>the basics of deep learning</li>
<li>what cross-entropy and loss is</li>
<li>about activation functions</li>
<li>how to optimize weights and biases with backpropagation and gradient descent</li>
<li>how to build (deep) neural networks with Keras and TensorFlow</li>
<li>how to save and load models and model weights</li>
<li>how to visualize models with TensorBoard</li>
<li>how to make predictions on test data</li>
</ul>
<p>My slides and all material is in English, so I’m flexible with the language. If it’s all German participants it’ll be in German, if some prefer English it’ll be English or a mix of German and English so that everybody understands. :-)</p>
<p><a href="https://www.eventbrite.de/e/workshop-deep-learning-mit-keras-und-tensorflow-tickets-42710095044?utm-medium=discovery&amp;utm-campaign=social&amp;utm-content=attendeeshare&amp;aff=escb&amp;utm-source=cp&amp;utm-term=listing">Tickets can be booked via eventbrite</a>.</p>
<p><br></p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/keras_workshop_april18.png" />

</div>
<p>Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. Keras is very convenient for fast and easy prototyping of neural networks. It is highly modular and very flexible, so that you can build basically any type of neural network you want. It supports convolutional neural networks and recurrent neural networks, as well as combinations of both. Due to its layer structure, it is highly extensible and can run on CPU or GPU.</p>
<p>The <code>keras</code> R package provides an interface to the Python library of Keras, just as the tensorflow package provides an interface to TensorFlow. Basically, R creates a conda instance and runs Keras it it, while you can still use all the functionalities of R for plotting, etc. Almost all function names are the same, so models can easily be recreated in Python for deployment.</p>
<p><br></p>
<div class="figure">
<img src="https://blog.keras.io/img/keras-tensorflow-logo.jpg" />

</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Conferences, webinars, podcasts and the likes]]></title>
    <link href="/page/conferences_podcasts_webinars/"/>
    <id>/page/conferences_podcasts_webinars/</id>
    <published>2018-02-01T16:06:06+02:00</published>
    <updated>2018-02-01T16:06:06+02:00</updated>
    <content type="html"><![CDATA[

<p>Here, you can find a list of all the talks I gave at conferences, webinars, podcasts, workshops, and all the other places you can and could hear me talk. :-)</p>

<p><img src="https://secure.meetupstatic.com/s/img/5455565085016210254/logo/svg/logo--script.svg" alt="" /></p>

<h2 id="workshops-i-am-giving">Workshops I am giving</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2017/11/deep_learning_keras_tensorflow/">Workshop on Deep Learning with Keras and TensorFlow in R</a></li>
</ul>

<blockquote>
<p>I offer a workshop on deep learning with Keras and TensorFlow using R.
Date and place depend on who and how many people are interested, so please contact me either directly or via the workshop page: <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/">https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/</a> (the description is in German but I also offer to give the workshop in English).</p>
</blockquote>

<hr />

<p><br></p>

<h2 id="upcoming-talks-webinars-podcasts-etc">Upcoming talks, webinars, podcasts, etc.</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/02/m3_2018/">Announcing my talk about explainability of machine learning models at Minds Mastering Machines Conference</a></li>
</ul>

<blockquote>
<p>On Wednesday, April 25th 2018 I am going to talk about explainability of machine learning models at the Minds Mastering Machines conference in Cologne.</p>
</blockquote>

<p><br></p>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/01/jax2018/">JAX 2018 talk announcement: Deep Learning - a Primer</a></li>
</ul>

<blockquote>
<p>Deep Learning is one of the &ldquo;hot&rdquo; topics in the AI area – a lot of hype, a lot of inflated expectation, but also quite some impressive success stories. As some AI experts already predict that Deep Learning will become &ldquo;Software 2.0&rdquo;, it might be a good time to have a closer look at the topic. In this session I will try to give a comprehensive overview of Deep Learning. We will start with a bit of history and some theoretical foundations that we will use to create a little Deep Learning taxonomy. Then we will have a look at current and upcoming application areas: Where can we apply Deep Learning successfully and what does it differentiate from other approaches? Afterwards we will examine the ecosystem: Which tools and libraries are available? What are their strengths and weaknesses? And to complete the session, we will look into some practical code examples and the typical pitfalls of Deep Learning. After this session you will have a much better idea of the why, what and how of Deep Learning, including if and how you might want to apply it to your own work. <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/">https://jax.de/big-data-machine-learning/deep-learning-a-primer/</a></p>
</blockquote>

<hr />

<p><br></p>

<h2 id="past-talks-webinars-podcasts-etc">Past talks, webinars, podcasts, etc.</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/03/meetup_talk_ruhrpy_april_18/">I talked about explaining complex Machine Learning Models with LIME</a> at this meetup: <a href="https://www.meetup.com/Hamburg-Data-Science-Meetup/events/244145443/">Data Science Meetup Hamburg</a> on Thursday, April 12th 2018</li>
</ul>

<p>Slides can be found here: <a href="https://shirinsplayground.netlify.com/2018/04/hh_datascience_meetup_2018_slides/">https://shirinsplayground.netlify.com/2018/04/hh_datascience_meetup_2018_slides/</a></p>

<blockquote>
<p>Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex machine learning models are essentially black boxes and too complicated to understand, we need to use approximations to get a better sense of how they work. One such approach is LIME, which stands for Local Interpretable Model-agnostic Explanations and is a tool that helps understand and explain the decisions made by complex machine learning models.
Dr. Shirin Glander is Data Scientist at codecentric AG. She has received a PhD in Bioinformatics and applies methods of analysis and visualization from different areas - for instance, machine learning, classical statistics, text mining, etc. -to extract and leverage information from data.</p>
</blockquote>

<p><br></p>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/03/meetup_talk_ruhrpy_april_18/">I talk about Deep Learning with Keras in R and Python</a> at this meetup: <a href="https://www.meetup.com/Ruhr-py/events/248093628/">Ruhr.Py 2018</a> on Wednesday, April 4th 2018</li>
</ul>

<blockquote>
<p>Introducing Deep Learning with Keras and Python
Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. In this talk we build, train and visualize a Model using Python and Keras - all interactive with Jupyter Notebooks!</p>
</blockquote>

<p>Slides can be found here: <a href="https://shirinsplayground.netlify.com/2018/04/ruhrpy_meetup_2018_slides/">https://shirinsplayground.netlify.com/2018/04/ruhrpy_meetup_2018_slides/</a></p>

<p><br></p>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/02/herr_mies_wills_wissen/">I talked about machine learning with Daniel Mies (Podcast in German, though)</a></li>
</ul>

<blockquote>
<p>In January 2018 I was interviewed for a tech podcast where I talked about machine learning, neural nets, why I love R and Rstudio and how I became a Data Scientist.</p>
</blockquote>

<p><br></p>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/">Explaining Predictions of Machine Learning Models with LIME - Münster Data Science Meetup</a></li>
</ul>

<blockquote>
<p>In December 2017 I talked about Explaining Predictions of Machine Learning Models with LIME at the Münster Data Science Meetup.</p>
</blockquote>

<p><br></p>

<ul>
<li><a href="https://shiring.github.io/blogging/2017/09/20/webinar_biology_to_data_science">From Biology to Industry. A Blogger’s Journey to Data Science</a></li>
</ul>

<blockquote>
<p>In September 2017 I gave a webinar for the Applied Epidemiology Didactic of the University of Wisconsin - Madison titled “From Biology to Industry. A Blogger’s Journey to Data Science.”
I talked about how blogging about R and Data Science helped me become a Data Scientist. I also gave a short introduction to Machine Learning, Big Data and Neural Networks.</p>
</blockquote>

<p><br></p>

<ul>
<li><a href="https://shiring.github.io/machine_learning/2017/03/31/webinar_code">Building meaningful machine learning models for disease prediction</a></li>
</ul>

<blockquote>
<p>In March 2017 I gave a webinar for the ISDS R Group about my work on building machine-learning models to predict the course of different diseases. I went over building a model, evaluating its performance, and answering or addressing different disease related questions using machine learning. My talk covered the theory of machine learning as it is applied using R.</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Announcing my talk about explainability of machine learning models at Minds Mastering Machines conference]]></title>
    <link href="/2018/02/m3_2018/"/>
    <id>/2018/02/m3_2018/</id>
    <published>2018-02-01T00:00:00+00:00</published>
    <updated>2018-02-01T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>On Wednesday, April 25th 2018 I am going to <a href="https://www.m3-konferenz.de/veranstaltung-6274-erkl%C3%A4rbarkeit-von-machine-learning%3A-wie-k%C3%B6nnen-wir-vertrauen-in-komplexe-modelle-schaffen.html?id=6274">talk about explainability of machine learning models at the Minds Mastering Machines conference in Cologne</a>. The conference will be in German, though.</p>
<div class="figure">
<img src="https://www.m3-konferenz.de/common/images/konferenzen/m3.png" />

</div>
<blockquote>
<p>ERKLÄRBARKEIT VON MACHINE LEARNING: WIE KÖNNEN WIR VERTRAUEN IN KOMPLEXE MODELLE SCHAFFEN?</p>
</blockquote>
<blockquote>
<p>Mit Machine-Learning getroffene Entscheidungen sind inhärent schwierig – wenn nicht gar unmöglich – nachzuvollziehen. Die Komplexität einiger der besten Modelle, wie Neuronale Netzwerke, ist genau das, was sie so erfolgreich macht. Aber es macht sie gleichzeitig zu einer Black Box. Das kann problematisch sein, denn Geschäftsführer oder Vorstände werden weniger geneigt sein einer Entscheidung zu vertrauen und nach ihr zu handeln, wenn sie sie nicht verstehen.</p>
</blockquote>
<blockquote>
<p>Local Interpretable Model-Agnostic Explanations (LIME) ist ein Versuch, diese komplexen Modelle zumindest teilweise nachvollziehbar zu machen. In diesem Vortrag erkläre ich das Prinzip und zeige Anwendungsbeispiele von LIME.</p>
</blockquote>
<blockquote>
<p>Vorkenntnisse Grundkenntnisse Machine Learning &amp; Statistik</p>
</blockquote>
<blockquote>
<p>Lernziele * Einblick in Möglichkeit, die komplexe Modelle erklärbar machen * Vertrauen in Entscheidungen durch Machine Learning schaffen</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[I talk about machine learning with Daniel Mies (Podcast in German, though)]]></title>
    <link href="/2018/02/herr_mies_wills_wissen/"/>
    <id>/2018/02/herr_mies_wills_wissen/</id>
    <published>2018-02-01T00:00:00+00:00</published>
    <updated>2018-02-01T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>For those of you out there who speak German:</p>
<p>I was interviewed for a tech podcast where I talked about machine learning, neural nets, why I love R and Rstudio and how I became a Data Scientist.</p>
<p>You can download and listen to the podcast here: <a href="https://mies.me/2018/01/31/hmww17-machine-learning-mit-dr-shirin-glander/" class="uri">https://mies.me/2018/01/31/hmww17-machine-learning-mit-dr-shirin-glander/</a></p>
<div class="figure">
<img src="https://mies.me/wp-content/cache/podlove/09/cad1c2bcc3b506410d277c27cc12fb/herr-mies-wills-wissen_500x500.png" />

</div>
<blockquote>
<p>In der aktuellen Episode gibt Dr. Shirin Glander (Twitter, Homepage) uns ein paar Einblicke in das Thema Machine Learning. Wir klären zunächst, was Machine Learning ist und welche Möglichkeiten es bietet bevor wir etwas mehr in die Tiefe gehen. Wir beginnen mit Neuronalen Netzen und Entscheidungsbäumen und wie sich diese unterschieden. Hier kommen wir natürlich auch nicht an Supervised Learning, Unsupervised Learning und Reinforcement Learning vorbei. Wichtig bei der Arbeit mit Machine Learning sind die verwendeten Daten: Hier beginnt man mit Testdaten und Trainingsdaten, welche man mit Hilfe von Feature Engineering für die jeweilige Aufgabe optimieren kann. Shirin erzählt, wie sie mit Daten arbeitet und wie sie die richtigen Algorithmen findet. Eine wichtige Rolle spielen hier R und R Studio, welches sich besonders für statistische Analysen eignet. Gerade die Visualisierung der Daten ist hier hilfreich um selbige besser zu verstehen. Aber auch die Möglichkeiten Reports zu erzeugen und beispielsweise als PDF zu exportieren überzeugen. Wenn ihr R für Machine Learning einsetzen wollt, solltet ihr Euch auch caret ansehen. Shirin organisiert übrigens auch MünsteR, die R Users group in Münster. Wenn ihr Euch näher mit Machine Learning beschäftigen wollt, solltet ihr Euch Datacamp oder Coursera ansehen. Wenn ihr Euch für R interessiert schaut Euch die R Bloggers an Am Ende sprechen wir auch noch kurz über Deep Dreaming. Den passenden Generator hierfür, findet ihr unter deepdreamgenerator.com.</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[JAX 2018 talk announcement: Deep Learning - a Primer]]></title>
    <link href="/2018/01/jax2018/"/>
    <id>/2018/01/jax2018/</id>
    <published>2018-01-30T00:00:00+00:00</published>
    <updated>2018-01-30T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I am happy to announce that on Tuesday, April 24th 2018 Uwe Friedrichsen and I will give a talk about <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/">Deep Learning - a Primer</a> at the JAX conference in Mainz, Germany.</p>
<blockquote>
<p>Deep Learning is one of the “hot” topics in the AI area – a lot of hype, a lot of inflated expectation, but also quite some impressive success stories. As some AI experts already predict that Deep Learning will become “Software 2.0”, it might be a good time to have a closer look at the topic. In this session I will try to give a comprehensive overview of Deep Learning. We will start with a bit of history and some theoretical foundations that we will use to create a little Deep Learning taxonomy. Then we will have a look at current and upcoming application areas: Where can we apply Deep Learning successfully and what does it differentiate from other approaches? Afterwards we will examine the ecosystem: Which tools and libraries are available? What are their strengths and weaknesses? And to complete the session, we will look into some practical code examples and the typical pitfalls of Deep Learning. After this session you will have a much better idea of the why, what and how of Deep Learning, including if and how you might want to apply it to your own work. <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/" class="uri">https://jax.de/big-data-machine-learning/deep-learning-a-primer/</a></p>
</blockquote>
<div class="figure">
<img src="https://pbs.twimg.com/media/DUt3SXyUQAE3TOv.jpg" alt="https://twitter.com/jaxcon/status/957990506331557890" />
<p class="caption"><a href="https://twitter.com/jaxcon/status/957990506331557890" class="uri">https://twitter.com/jaxcon/status/957990506331557890</a></p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #94: Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley]]></title>
    <link href="/2018/01/twimlai94/"/>
    <id>/2018/01/twimlai94/</id>
    <published>2018-01-28T00:00:00+00:00</published>
    <updated>2018-01-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai94.jpg" alt="Sketchnotes from TWiMLAI talk #94: Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley" />
<p class="caption">Sketchnotes from TWiMLAI talk #94: Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-94-neuroevolution-evolving-novel-neural-network-architectures-kenneth-stanley/">here</a>.</p>
<blockquote>
<p>Kenneth studied under TWiML Talk #47 guest Risto Miikkulainen at UT Austin, and joined Uber AI Labs after Geometric Intelligence , the company he co-founded with Gary Marcus and others, was acquired in late 2016. Kenneth’s research focus is what he calls Neuroevolution, applies the idea of genetic algorithms to the challenge of evolving neural network architectures. In this conversation, we discuss the Neuroevolution of Augmenting Topologies (or NEAT) paper that Kenneth authored along with Risto, which won the 2017 International Society for Artificial Life’s Award for Outstanding Paper of the Decade 2002 – 2012. We also cover some of the extensions to that approach he’s created since, including, HyperNEAT, which can efficiently evolve very large networks with connectivity patterns that look more like those of the human and that are generally much larger than what prior approaches to neural learning could produce, and novelty search, an approach which unlike most evolutionary algorithms has no defined objective, but rather simply searches for novel behaviors. We also cover concepts like “Complexification” and “Deception”, biology vs computation including differences and similarities, and some of his other work including his book, and NERO, a video game complete with Real-time Neuroevolution. This is a meaty “Nerd Alert” interview that I think you’ll really enjoy. <a href="https://twimlai.com/twiml-talk-94-neuroevolution-evolving-novel-neural-network-architectures-kenneth-stanley/" class="uri">https://twimlai.com/twiml-talk-94-neuroevolution-evolving-novel-neural-network-architectures-kenneth-stanley/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Join MünsteR for our next meetup on obtaining functional implications of gene expression data with R]]></title>
    <link href="/2018/01/meetup_march18/"/>
    <id>/2018/01/meetup_march18/</id>
    <published>2018-01-24T00:00:00+00:00</published>
    <updated>2018-01-24T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://shiring.github.io/r_users_group/2017/05/20/map.png" />

</div>
<p>In our <a href="http://meetu.ps/e/DDY1B/w54bW/f">next MünsteR R-user group meetup</a> on <strong>March 5th, 2018</strong> Frank Rühle will talk about bioinformatics and how to analyse genome data.</p>
<p>You can RSVP here: <a href="http://meetu.ps/e/DDY1B/w54bW/f" class="uri">http://meetu.ps/e/DDY1B/w54bW/f</a></p>
<blockquote>
<p>Next-Generation sequencing and array-based technologies provided a plethora of gene expression data in the public genomics databases. But how to get meaningful information and functional implications out of this vast amount of data? Various R-packages have been published by the Bioconductor user community for distinct kinds of analysis strategies. Here, several approaches will be presented for functional gene annotation, gene enrichment analysis and co-expression network analysis. A collection of wrapper functions for streamlined analysis of expression data can be found at: <a href="https://github.com/frankRuehle/systemsbio" class="uri">https://github.com/frankRuehle/systemsbio</a>.</p>
</blockquote>
<p>Dr. Frank Rühle is a post-doctoral research fellow in the group of genetic epidemiology at the Institute of Human Genetics at the University of Münster. As biologist with focus on computational biology he aims at identifying genomic biomarker for complex cardiovascular diseases by analyzing multiomics data with respect to a systems biology view. Further research interests include the functions of long non-coding RNAs and their impact on gene regulation in heart-related phenotypes.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #92: Learning State Representations with Yael Niv]]></title>
    <link href="/2018/01/twimlai92/"/>
    <id>/2018/01/twimlai92/</id>
    <published>2018-01-19T00:00:00+00:00</published>
    <updated>2018-01-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about Learning State Representations with Yael Niv: <a href="https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/" class="uri">https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/</a></p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai92.jpg" alt="Sketchnotes from TWiMLAI talk #92: Learning State Representations with Yael Niv" />
<p class="caption">Sketchnotes from TWiMLAI talk #92: Learning State Representations with Yael Niv</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/">here</a>.</p>
<blockquote>
<p>In this interview Yael and I explore the relationship between neuroscience and machine learning. In particular, we discusses the importance of state representations in human learning, some of her experimental results in this area, and how a better understanding of representation learning can lead to insights into machine learning problems such as reinforcement and transfer learning. Did I mention this was a nerd alert show? I really enjoyed this interview and I know you will too. Be sure to send over any thoughts or feedback via the show notes page. <a href="https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/" class="uri">https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[How to make your machine learning model available as an API with the plumber package]]></title>
    <link href="/2018/01/plumber/"/>
    <id>/2018/01/plumber/</id>
    <published>2018-01-16T00:00:00+00:00</published>
    <updated>2018-01-16T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://d33wubrfki0l68.cloudfront.net/03f3d9156c39a8043be42caeee1507d43d832472/8d968/components/images/plumber.png" />

</div>
<p>The <strong>plumber</strong> package for R makes it easy to expose existing R code as a webservice via an API (<a href="https://www.rplumber.io/" class="uri">https://www.rplumber.io/</a>, Trestle Technology, LLC 2017).</p>
<p>You take an existing R script and make it accessible with <code>plumber</code> by simply adding a few lines of comments. If you have worked with Roxygen before, e.g. when building a package, you will already be familiar with the core concepts. If not, here are the most important things to know:</p>
<ul>
<li>you define the output or endpoint</li>
<li>you can add additional annotation to customize your input, output and other functionalities of your API</li>
<li>you can define every input parameter that will go into your function</li>
<li>every such annotation will begin with either <code>#'</code> or <code>#*</code></li>
</ul>
<p>With this setup, we can take a trained machine learning model and make it available as an API. With this API, other programs can access it and use it to make predictions.</p>
<div id="what-are-apis-and-webservices" class="section level2">
<h2>What are APIs and webservices?</h2>
<p>With <code>plumber</code>, we can build so called <strong>HTTP APIs</strong>. HTTP stands for Hypertext Transfer Protocol and is used to transmit information on the web; API stands for Application Programming Interface and governs the connection between some software and underlying applications. Software can then communicate via HTTP APIs. This way, our R script can be called from other software, even if the other program is not written in R and we have built a tool for machine-to-machine communication, i.e. a webservice.</p>
</div>
<div id="how-to-convert-your-r-script-into-an-api-with-plumber" class="section level2">
<h2>How to convert your R script into an API with plumber</h2>
<div id="training-and-saving-a-model" class="section level3">
<h3>Training and saving a model</h3>
<p>Let’s say we have trained a machine learning model as in <a href="https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/">this post about LIME</a>. I loaded a data set on chronic kidney disease, did some preprocessing (converting categorical features into dummy variables, scaling and centering), split it into training and test data and trained a Random Forest model with <code>caret</code>. We can use this trained model to make predictions for one test case with the following code:</p>
<pre class="r"><code>library(tidyverse)

# load test and train data
load(&quot;../../data/test_data.RData&quot;)
load(&quot;../../data/train_data.RData&quot;)

# load model
load(&quot;../../data/model_rf.RData&quot;)

# take first test case for prediction
input_data &lt;- test_data[1, ] %&gt;%
  select(-class)

# predict test case using model
pred &lt;- predict(model_rf, input_data)
cat(&quot;----------------\nTest case predicted to be&quot;, as.character(pred), &quot;\n----------------&quot;)</code></pre>
<pre><code>## ----------------
## Test case predicted to be ckd 
## ----------------</code></pre>
</div>
<div id="the-input" class="section level3">
<h3>The input</h3>
<p>For our API to work, we need to define the input, in our case the features of the test data. When we look at the model object, we see that it expects the following parameters:</p>
<pre class="r"><code>var_names &lt;- model_rf$finalModel$xNames
var_names</code></pre>
<pre><code>##  [1] &quot;age&quot;            &quot;bp&quot;             &quot;sg_1.005&quot;       &quot;sg_1.010&quot;      
##  [5] &quot;sg_1.015&quot;       &quot;sg_1.020&quot;       &quot;sg_1.025&quot;       &quot;al_0&quot;          
##  [9] &quot;al_1&quot;           &quot;al_2&quot;           &quot;al_3&quot;           &quot;al_4&quot;          
## [13] &quot;al_5&quot;           &quot;su_0&quot;           &quot;su_1&quot;           &quot;su_2&quot;          
## [17] &quot;su_3&quot;           &quot;su_4&quot;           &quot;su_5&quot;           &quot;rbc_normal&quot;    
## [21] &quot;rbc_abnormal&quot;   &quot;pc_normal&quot;      &quot;pc_abnormal&quot;    &quot;pcc_present&quot;   
## [25] &quot;pcc_notpresent&quot; &quot;ba_present&quot;     &quot;ba_notpresent&quot;  &quot;bgr&quot;           
## [29] &quot;bu&quot;             &quot;sc&quot;             &quot;sod&quot;            &quot;pot&quot;           
## [33] &quot;hemo&quot;           &quot;pcv&quot;            &quot;wbcc&quot;           &quot;rbcc&quot;          
## [37] &quot;htn_yes&quot;        &quot;htn_no&quot;         &quot;dm_yes&quot;         &quot;dm_no&quot;         
## [41] &quot;cad_yes&quot;        &quot;cad_no&quot;         &quot;appet_good&quot;     &quot;appet_poor&quot;    
## [45] &quot;pe_yes&quot;         &quot;pe_no&quot;          &quot;ane_yes&quot;        &quot;ane_no&quot;</code></pre>
<p>Good practice is to write the input parameter definition into you <a href="https://swagger.io/swagger-ui/">API Swagger UI</a>, but the code would work without these annotations. We define the parameters by annotating them with name and description in our R-script using <code>@parameter</code>. For this purpose, I want to know the type and min/max values for each of my variables in the training data. Because categorical data has been converted to dummy variables and then scaled and centered, these values will all be numeric and between 0 and 1 in this example. If I would build this script for a real case, I’d use the raw data as input and add a preprocessing function to my script, though!</p>
<pre class="r"><code># show parameter definition for the first three features
for (i in 1:3) {
# if you wanted to see it for all features, use
#for (i in 1:length(var_names)) {
  var &lt;- var_names[i]
  train_data_subs &lt;- train_data[, which(colnames(train_data) == var)]
  type &lt;- class(train_data_subs)
  
  if (type == &quot;numeric&quot;) {
    min &lt;- min(train_data_subs)
    max &lt;- max(train_data_subs)
  }
  
  cat(&quot;Variable:&quot;, var, &quot;is of type:&quot;, type, &quot;\n&quot;,
      &quot;Min value in training data =&quot;, min, &quot;\n&quot;,
      &quot;Max value in training data =&quot;, max, &quot;\n----------\n&quot;)

}</code></pre>
<pre><code>## Variable: age is of type: numeric 
##  Min value in training data = 0 
##  Max value in training data = 0.9777778 
## ----------
## Variable: bp is of type: numeric 
##  Min value in training data = 0 
##  Max value in training data = 0.7222222 
## ----------
## Variable: sg_1.005 is of type: numeric 
##  Min value in training data = 0 
##  Max value in training data = 1 
## ----------</code></pre>
<blockquote>
<p>Unless otherwise instructed, all parameters passed into plumber endpoints from query strings or dynamic paths will be character strings. <a href="https://www.rplumber.io/docs/routing-and-input.html#typed-dynamic-routes" class="uri">https://www.rplumber.io/docs/routing-and-input.html#typed-dynamic-routes</a></p>
</blockquote>
<p>This means that we need to convert numeric values before we process them further. Or we can define the parameter type explicitly, e.g. by writing <code>variable_1:numeric</code> if we want to specifiy that <em>variable_1</em> is supposed to be numeric.</p>
<p>To make sure that the model will perform as expected, it is also advisable to add a few validation functions. Here, I will validate</p>
<ul>
<li>whether every parameter is numeric/integer by checking for NAs (which would have resulted from <code>as.numeric()</code>/<code>as.integer()</code> applied to data of character type)</li>
<li>whether every parameter is between 0 and 1</li>
</ul>
<p>In order for <code>plumber</code> to work with our input, it needs to be part of the HTTP request, which can then be routed to our R function. The <a href="https://www.rplumber.io/docs/routing-and-input.html#query-strings">plumber documentation</a> describes how to use query strings as inputs. But in our case, manually writing query strings is not practical because we have so many parameters. Of course, there are programs that let us generate query strings but the easiest way to format the input from a line of data I found is to use JSON.</p>
<p>The <code>toJSON()</code> function from the <code>rjson</code> package converts our input line to JSON format:</p>
<pre class="r"><code>library(rjson)
test_case_json &lt;- toJSON(input_data)
cat(test_case_json)</code></pre>
<pre><code>## {&quot;age&quot;:0.511111111111111,&quot;bp&quot;:0.111111111111111,&quot;sg_1.005&quot;:1,&quot;sg_1.010&quot;:0,&quot;sg_1.015&quot;:0,&quot;sg_1.020&quot;:0,&quot;sg_1.025&quot;:0,&quot;al_0&quot;:0,&quot;al_1&quot;:0,&quot;al_2&quot;:0,&quot;al_3&quot;:0,&quot;al_4&quot;:1,&quot;al_5&quot;:0,&quot;su_0&quot;:1,&quot;su_1&quot;:0,&quot;su_2&quot;:0,&quot;su_3&quot;:0,&quot;su_4&quot;:0,&quot;su_5&quot;:0,&quot;rbc_normal&quot;:1,&quot;rbc_abnormal&quot;:0,&quot;pc_normal&quot;:0,&quot;pc_abnormal&quot;:1,&quot;pcc_present&quot;:1,&quot;pcc_notpresent&quot;:0,&quot;ba_present&quot;:0,&quot;ba_notpresent&quot;:1,&quot;bgr&quot;:0.193877551020408,&quot;bu&quot;:0.139386189258312,&quot;sc&quot;:0.0447368421052632,&quot;sod&quot;:0.653374233128834,&quot;pot&quot;:0,&quot;hemo&quot;:0.455056179775281,&quot;pcv&quot;:0.425925925925926,&quot;wbcc&quot;:0.170454545454545,&quot;rbcc&quot;:0.225,&quot;htn_yes&quot;:1,&quot;htn_no&quot;:0,&quot;dm_yes&quot;:0,&quot;dm_no&quot;:1,&quot;cad_yes&quot;:0,&quot;cad_no&quot;:1,&quot;appet_good&quot;:0,&quot;appet_poor&quot;:1,&quot;pe_yes&quot;:1,&quot;pe_no&quot;:0,&quot;ane_yes&quot;:1,&quot;ane_no&quot;:0}</code></pre>
</div>
<div id="defining-the-endpoint-and-output" class="section level3">
<h3>Defining the endpoint and output</h3>
<p>In order to convert this very simple script into an API, we need to define the endpoint(s). Endpoints will return an output, in our case it will return the output of the <code>predict()</code> function pasted into a line of text (e.g. “Test case predicted to be ckd”). Here, we want to have the predictions returned, so we annotate the entire function with <code>@get</code>. This endpoint in the API will get a custom name, so that we can call it later; here we call it <code>predict</code> and therefore write <code>#' @get /predict</code>.</p>
<blockquote>
<p>According to the design of the HTTP specification, GET (along with HEAD) requests are used only to read data and not change it. Therefore, when used this way, they are considered safe. That is, they can be called without risk of data modification or corruption — calling it once has the same effect as calling it 10 times, or none at all. Additionally, GET (and HEAD) is idempotent, which means that making multiple identical requests ends up having the same result as a single request. <a href="http://www.restapitutorial.com/lessons/httpmethods.html" class="uri">http://www.restapitutorial.com/lessons/httpmethods.html</a></p>
</blockquote>
<p>In this case, we could also consider using <code>@post</code> to avoid caching issues, but for this example I’ll leave it as <code>@get</code>.</p>
<blockquote>
<p>The POST verb is most-often utilized to <strong>create</strong> new resources. In particular, it’s used to create subordinate resources. That is, subordinate to some other (e.g. parent) resource. In other words, when creating a new resource, POST to the parent and the service takes care of associating the new resource with the parent, assigning an ID (new resource URI), etc. On successful creation, return HTTP status 201, returning a Location header with a link to the newly-created resource with the 201 HTTP status. POST is neither safe nor idempotent. It is therefore recommended for non-idempotent resource requests. Making two identical POST requests will most-likely result in two resources containing the same information. <a href="http://www.restapitutorial.com/lessons/httpmethods.html" class="uri">http://www.restapitutorial.com/lessons/httpmethods.html</a></p>
</blockquote>
<p>We can also customize the output. Keep in mind though, that the output should be <a href="https://www.rplumber.io/docs/rendering-and-output.html#serializers">“serialized”</a>. By default, the output will be in JSON format. Here, I want to have a text output, so I’ll specify <code>@html</code> without html formatting specifications, although I could add them if I wanted to display the text on a website. If we were to <a href="https://www.rplumber.io/docs/runtime.html#external-data-store">store the data in a database</a>, however, this would not be a good idea. In that case, it would be better to output the result as a JSON object.</p>
</div>
<div id="logging-with-filters" class="section level3">
<h3>Logging with filters</h3>
<p>It is also useful to provide some sort of logging for your API. Here, I am using the simple example from the <a href="https://www.rplumber.io/docs/routing-and-input.html#filters">plumber documentation</a> that uses filters and output the logs to the console or your API server logs. You could also <a href="https://www.rplumber.io/docs/runtime.html#file-system">write your logging output to a file</a>. In production, it would be better to use a real logging setup that stores information about each request, e.g. the time stamp, whether any errors or warnings occurred, etc. The <code>forward()</code> part of the logging function passes control on to the next handler in the pipeline, here our predict function.</p>
</div>
<div id="running-the-plumber-script" class="section level3">
<h3>Running the plumber script</h3>
<p>We need to save the entire script with annotations as an <em>.R</em> file as seen below. The regular comments <code>#</code> describe what each section does.</p>
<pre><code># script name:
# plumber.R

# set API title and description to show up in http://localhost:8000/__swagger__/

#&#39; @apiTitle Run predictions for Chronic Kidney Disease with Random Forest Model
#&#39; @apiDescription This API takes as patient data on Chronic Kidney Disease and returns a prediction whether the lab values
#&#39; indicate Chronic Kidney Disease (ckd) or not (notckd).
#&#39; For details on how the model is built, see https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/
#&#39; For further explanations of this plumber function, see https://shirinsplayground.netlify.com/2018/01/plumber/

# load model
# this path would have to be adapted if you would deploy this
load(&quot;/Users/shiringlander/Documents/Github/shirinsplayground/data/model_rf.RData&quot;)

#&#39; Log system time, request method and HTTP user agent of the incoming request
#&#39; @filter logger
function(req){
  cat(&quot;System time:&quot;, as.character(Sys.time()), &quot;\n&quot;,
      &quot;Request method:&quot;, req$REQUEST_METHOD, req$PATH_INFO, &quot;\n&quot;,
      &quot;HTTP user agent:&quot;, req$HTTP_USER_AGENT, &quot;@&quot;, req$REMOTE_ADDR, &quot;\n&quot;)
  plumber::forward()
}

# core function follows below:
# define parameters with type and description
# name endpoint
# return output as html/text
# specify 200 (okay) return

#&#39; predict Chronic Kidney Disease of test case with Random Forest model
#&#39; @param age:numeric The age of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param bp:numeric The blood pressure of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param sg_1.005:int The urinary specific gravity of the patient, integer (1: sg = 1.005, otherwise 0)
#&#39; @param sg_1.010:int The urinary specific gravity of the patient, integer (1: sg = 1.010, otherwise 0)
#&#39; @param sg_1.015:int The urinary specific gravity of the patient, integer (1: sg = 1.015, otherwise 0)
#&#39; @param sg_1.020:int The urinary specific gravity of the patient, integer (1: sg = 1.020, otherwise 0)
#&#39; @param sg_1.025:int The urinary specific gravity of the patient, integer (1: sg = 1.025, otherwise 0)
#&#39; @param al_0:int The urine albumin level of the patient, integer (1: al = 0, otherwise 0)
#&#39; @param al_1:int The urine albumin level of the patient, integer (1: al = 1, otherwise 0)
#&#39; @param al_2:int The urine albumin level of the patient, integer (1: al = 2, otherwise 0)
#&#39; @param al_3:int The urine albumin level of the patient, integer (1: al = 3, otherwise 0)
#&#39; @param al_4:int The urine albumin level of the patient, integer (1: al = 4, otherwise 0)
#&#39; @param al_5:int The urine albumin level of the patient, integer (1: al = 5, otherwise 0)
#&#39; @param su_0:int The sugar level of the patient, integer (1: su = 0, otherwise 0)
#&#39; @param su_1:int The sugar level of the patient, integer (1: su = 1, otherwise 0)
#&#39; @param su_2:int The sugar level of the patient, integer (1: su = 2, otherwise 0)
#&#39; @param su_3:int The sugar level of the patient, integer (1: su = 3, otherwise 0)
#&#39; @param su_4:int The sugar level of the patient, integer (1: su = 4, otherwise 0)
#&#39; @param su_5:int The sugar level of the patient, integer (1: su = 5, otherwise 0)
#&#39; @param rbc_normal:int The red blood cell count of the patient, integer (1: rbc = normal, otherwise 0)
#&#39; @param rbc_abnormal:int The red blood cell count of the patient, integer (1: rbc = abnormal, otherwise 0)
#&#39; @param pc_normal:int The pus cell level of the patient, integer (1: pc = normal, otherwise 0)
#&#39; @param pc_abnormal:int The pus cell level of the patient, integer (1: pc = abnormal, otherwise 0)
#&#39; @param pcc_present:int The puc cell clumps status of the patient, integer (1: pcc = present, otherwise 0)
#&#39; @param pcc_notpresent:int The puc cell clumps status of the patient, integer (1: pcc = notpresent, otherwise 0)
#&#39; @param ba_present:int The bacteria status of the patient, integer (1: ba = present, otherwise 0)
#&#39; @param ba_notpresent:int The bacteria status of the patient, integer (1: ba = notpresent, otherwise 0)
#&#39; @param bgr:numeric The blood glucose random level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param bu:numeric The blood urea level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param sc:numeric The serum creatinine level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param sod:numeric The sodium level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param pot:numeric The potassium level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param hemo:numeric The hemoglobin level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param pcv:numeric The packed cell volume of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param wbcc:numeric The white blood cell count of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param rbcc:numeric The red blood cell count of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param htn_yes:int The hypertension status of the patient, integer (1: htn = yes, otherwise 0)
#&#39; @param htn_no:int The hypertension status of the patient, integer (1: htn = no, otherwise 0)
#&#39; @param dm_yes:int The diabetes mellitus status of the patient, integer (1: dm = yes, otherwise 0)
#&#39; @param dm_no:int The diabetes mellitus status of the patient, integer (1: dm = no, otherwise 0)
#&#39; @param cad_yes:int The coronary artery disease status of the patient, integer (1: cad = yes, otherwise 0)
#&#39; @param cad_no:int The coronary artery disease status of the patient, integer (1: cad = no, otherwise 0)
#&#39; @param appet_good:int The appetite of the patient, integer (1: appet = good, otherwise 0)
#&#39; @param appet_poor:int The appetite of the patient, integer (1: appet = poor, otherwise 0)
#&#39; @param pe_yes:int The pedal edema status of the patient, integer (1: pe = yes, otherwise 0)
#&#39; @param pe_no:int The pedal edema status of the patient, integer (1: pe = no, otherwise 0)
#&#39; @param ane_yes:int The anemia status of the patient, integer (1: ane = yes, otherwise 0)
#&#39; @param ane_no:int The anemia status of the patient, integer (1: ane = no, otherwise 0)
#&#39; @get /predict
#&#39; @html
#&#39; @response 200 Returns the class (ckd or notckd) prediction from the Random Forest model; ckd = Chronic Kidney Disease
calculate_prediction &lt;- function(age, bp, sg_1.005, sg_1.010, sg_1.015, sg_1.020, sg_1.025, al_0, al_1, al_2, 
                                al_3, al_4, al_5, su_0, su_1, su_2, su_3, su_4, su_5, rbc_normal, rbc_abnormal, pc_normal, pc_abnormal,
                                pcc_present, pcc_notpresent, ba_present, ba_notpresent, bgr, bu, sc, sod, pot, hemo, pcv, 
                                wbcc, rbcc, htn_yes, htn_no, dm_yes, dm_no, cad_yes, cad_no, appet_good, appet_poor, pe_yes, pe_no, 
                                ane_yes, ane_no) {
  
  # make data frame from numeric parameters
  input_data_num &lt;&lt;- data.frame(age, bp, bgr, bu, sc, sod, pot, hemo, pcv, wbcc, rbcc,
                     stringsAsFactors = FALSE)
  # and make sure they really are numeric
  input_data_num &lt;&lt;- as.data.frame(t(sapply(input_data_num, as.numeric)))
  
  # make data frame from (binary) integer parameters
  input_data_int &lt;&lt;- data.frame(sg_1.005, sg_1.010, sg_1.015, sg_1.020, sg_1.025, al_0, al_1, al_2, 
                                al_3, al_4, al_5, su_0, su_1, su_2, su_3, su_4, su_5, rbc_normal, rbc_abnormal, pc_normal, pc_abnormal,
                                pcc_present, pcc_notpresent, ba_present, ba_notpresent, htn_yes, htn_no, dm_yes, dm_no, 
                                cad_yes, cad_no, appet_good, appet_poor, pe_yes, pe_no, ane_yes, ane_no,
                                stringsAsFactors = FALSE)
  # and make sure they really are numeric
  input_data_int &lt;&lt;- as.data.frame(t(sapply(input_data_int, as.integer)))
  # combine into one data frame
  input_data &lt;&lt;- as.data.frame(cbind(input_data_num, input_data_int))
  
  # validation for parameter
  if (any(is.na(input_data))) {
    res$status &lt;- 400
    res$body &lt;- &quot;Parameters have to be numeric or integers&quot;
  }
  
  if (any(input_data &lt; 0) || any(input_data &gt; 1)) {
    res$status &lt;- 400
    res$body &lt;- &quot;Parameters have to be between 0 and 1&quot;
  }

  # predict and return result
  pred_rf &lt;&lt;- predict(model_rf, input_data)
  paste(&quot;----------------\nTest case predicted to be&quot;, as.character(pred_rf), &quot;\n----------------\n&quot;)
}
</code></pre>
<p>Note that I am using the “double-assignment” operator <code>&lt;&lt;-</code> in my function, because I want to make sure that objects are overwritten at the top level (i.e. globally). This would have been relevant had I set a global parameter, but to show it the example, I decided to use it here as well.</p>
<p>We can now call our script with the <code>plumb()</code> function, run it with <code>run()</code> and open it on port 800. Calling <code>plumb()</code> creates an environment in which all our functions are evaluated.</p>
<pre class="r"><code>library(plumber)</code></pre>
<pre class="r"><code>r &lt;- plumb(&quot;/Users/shiringlander/Documents/Github/shirinsplayground/static/scripts/plumber.R&quot;)
r$run(port = 8000)</code></pre>
<p>We will now see the following message in our R console:</p>
<pre><code>Starting server to listen on port 8000
Running the swagger UI at http://127.0.0.1:8000/__swagger__/</code></pre>
<p>If you go to *<a href="http://localhost:8000/__swagger__/*" class="uri">http://localhost:8000/__swagger__/*</a>, you could now try out the function by manually choosing values for all the parameters we defined in the script.</p>
<p><img src="https://shiring.github.io/netlify_images/swagger1.png" alt="http://localhost:8000/__swagger__/" /> … <img src="https://shiring.github.io/netlify_images/swagger2.png" alt="http://localhost:8000/__swagger__/ continued" /></p>
<p>Because we annotated the <code>calculate_prediction()</code> function in our script with <code>#' @get /predict</code> we can access it via *<a href="http://localhost:8000/predict*" class="uri">http://localhost:8000/predict*</a>. But because we have no input specified as of yet, we will only see an error on this site. So, we still need to put our JSON formatted input into the function. To do this, we can use <a href="https://en.wikipedia.org/wiki/CURL"><em>curl</em></a> from the terminal and feed in the JSON string from above. If you are using RStudio in the latest version, you have a handy terminal window open in your working directory. You find it right next to the Console.</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/terminal_rstudio.png" alt="Terminal in RStudio" />
<p class="caption">Terminal in RStudio</p>
</div>
<pre><code>curl -H &quot;Content-Type: application/json&quot; -X GET -d &#39;{&quot;age&quot;:0.511111111111111,&quot;bp&quot;:0.111111111111111,&quot;sg_1.005&quot;:1,&quot;sg_1.010&quot;:0,&quot;sg_1.015&quot;:0,&quot;sg_1.020&quot;:0,&quot;sg_1.025&quot;:0,&quot;al_0&quot;:0,&quot;al_1&quot;:0,&quot;al_2&quot;:0,&quot;al_3&quot;:0,&quot;al_4&quot;:1,&quot;al_5&quot;:0,&quot;su_0&quot;:1,&quot;su_1&quot;:0,&quot;su_2&quot;:0,&quot;su_3&quot;:0,&quot;su_4&quot;:0,&quot;su_5&quot;:0,&quot;rbc_normal&quot;:1,&quot;rbc_abnormal&quot;:0,&quot;pc_normal&quot;:0,&quot;pc_abnormal&quot;:1,&quot;pcc_present&quot;:1,&quot;pcc_notpresent&quot;:0,&quot;ba_present&quot;:0,&quot;ba_notpresent&quot;:1,&quot;bgr&quot;:0.193877551020408,&quot;bu&quot;:0.139386189258312,&quot;sc&quot;:0.0447368421052632,&quot;sod&quot;:0.653374233128834,&quot;pot&quot;:0,&quot;hemo&quot;:0.455056179775281,&quot;pcv&quot;:0.425925925925926,&quot;wbcc&quot;:0.170454545454545,&quot;rbcc&quot;:0.225,&quot;htn_yes&quot;:1,&quot;htn_no&quot;:0,&quot;dm_yes&quot;:0,&quot;dm_no&quot;:1,&quot;cad_yes&quot;:0,&quot;cad_no&quot;:1,&quot;appet_good&quot;:0,&quot;appet_poor&quot;:1,&quot;pe_yes&quot;:1,&quot;pe_no&quot;:0,&quot;ane_yes&quot;:1,&quot;ane_no&quot;:0}&#39; &quot;http://localhost:8000/predict&quot;</code></pre>
<blockquote>
<p><strong>-H</strong> defines an extra header to include in the request when sending HTTP to a server (<a href="https://curl.haxx.se/docs/manpage.html#-H" class="uri">https://curl.haxx.se/docs/manpage.html#-H</a>).</p>
</blockquote>
<blockquote>
<p><strong>-X</strong> pecifies a custom request method to use when communicating with the HTTP server (<a href="https://curl.haxx.se/docs/manpage.html#-X" class="uri">https://curl.haxx.se/docs/manpage.html#-X</a>).</p>
</blockquote>
<blockquote>
<p><strong>-d</strong> sends the specified data in a request to the HTTP server, in the same way that a browser does when a user has filled in an HTML form and presses the submit button. This will cause curl to pass the data to the server using the content-type application/x-www-form-urlencoded (<a href="https://curl.haxx.se/docs/manpage.html#-d" class="uri">https://curl.haxx.se/docs/manpage.html#-d</a>).</p>
</blockquote>
<p>This will return the following output:</p>
<ul>
<li><code>cat()</code> outputs to the R console if you use R interactively; if you use R on a server, it will be included in the server logs.</li>
</ul>
<pre><code>System time: 2018-01-15 13:34:32 
 Request method: GET /predict 
 HTTP user agent: curl/7.54.0 @ 127.0.0.1 </code></pre>
<ul>
<li><code>paste</code> outputs to the terminal</li>
</ul>
<pre><code>----------------
Test case predicted to be ckd 
----------------</code></pre>
</div>
<div id="security" class="section level3">
<h3>Security</h3>
<p>This example shows a pretty simply R-script API. But if you plan on deploying your API to production, you should consider the <a href="https://www.rplumber.io/docs/security.html">security section of the plumber documentation</a>. It give additional information about how you can make your code (more) secure.</p>
</div>
<div id="finalize" class="section level3">
<h3>Finalize</h3>
<p>If you wanted to deploy this API you would need to <a href="https://www.rplumber.io/docs/hosting.html">host</a> it, i.e. provide the model and run an R environment with plumber, ideally on a server. A good way to do this, would be to package everything in a <a href="https://www.rplumber.io/docs/hosting.html#docker">Docker</a> container and run this. Docker will ensure that you have a working snapshot of the system settings, R and package versions that won’t change. For more information on dockerizing your API, check out <a href="https://hub.docker.com/r/trestletech/plumber/" class="uri">https://hub.docker.com/r/trestletech/plumber/</a>.</p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.3
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] methods   stats     graphics  grDevices utils     datasets  base     
## 
## other attached packages:
##  [1] plumber_0.4.4      rjson_0.2.15       forcats_0.3.0     
##  [4] stringr_1.3.0      dplyr_0.7.4        purrr_0.2.4       
##  [7] readr_1.1.1        tidyr_0.8.0        tibble_1.4.2      
## [10] ggplot2_2.2.1.9000 tidyverse_1.2.1   
## 
## loaded via a namespace (and not attached):
##  [1] nlme_3.1-131.1      lubridate_1.7.3     dimRed_0.1.0       
##  [4] httr_1.3.1          rprojroot_1.3-2     tools_3.4.3        
##  [7] backports_1.1.2     R6_2.2.2            rpart_4.1-13       
## [10] lazyeval_0.2.1      colorspace_1.3-2    nnet_7.3-12        
## [13] withr_2.1.1.9000    tidyselect_0.2.4    mnormt_1.5-5       
## [16] compiler_3.4.3      cli_1.0.0           rvest_0.3.2        
## [19] xml2_1.2.0          bookdown_0.7        scales_0.5.0.9000  
## [22] sfsmisc_1.1-1       DEoptimR_1.0-8      psych_1.7.8        
## [25] robustbase_0.92-8   randomForest_4.6-12 digest_0.6.15      
## [28] foreign_0.8-69      rmarkdown_1.8       pkgconfig_2.0.1    
## [31] htmltools_0.3.6     rlang_0.2.0.9000    readxl_1.0.0       
## [34] ddalpha_1.3.1.1     rstudioapi_0.7      bindr_0.1          
## [37] jsonlite_1.5        ModelMetrics_1.1.0  magrittr_1.5       
## [40] Matrix_1.2-12       Rcpp_0.12.15        munsell_0.4.3      
## [43] stringi_1.1.6       yaml_2.1.17         MASS_7.3-49        
## [46] plyr_1.8.4          recipes_0.1.2       grid_3.4.3         
## [49] parallel_3.4.3      crayon_1.3.4        lattice_0.20-35    
## [52] haven_1.1.1         splines_3.4.3       hms_0.4.1          
## [55] knitr_1.20          pillar_1.2.1        reshape2_1.4.3     
## [58] codetools_0.2-15    stats4_3.4.3        CVST_0.2-1         
## [61] glue_1.2.0          evaluate_0.10.1     blogdown_0.5       
## [64] modelr_0.1.1        httpuv_1.3.6.1      foreach_1.4.4      
## [67] cellranger_1.1.0    gtable_0.2.0        kernlab_0.9-25     
## [70] assertthat_0.2.0    DRR_0.0.3           xfun_0.1           
## [73] gower_0.1.2         prodlim_1.6.1       broom_0.4.3        
## [76] class_7.3-14        survival_2.41-3     timeDate_3043.102  
## [79] RcppRoll_0.2.2      iterators_1.0.9     bindrcpp_0.2       
## [82] lava_1.6            caret_6.0-78        ipred_0.9-6</code></pre>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #91: Philosophy of Intelligence with Matthew Crosby]]></title>
    <link href="/2018/01/twimlai91/"/>
    <id>/2018/01/twimlai91/</id>
    <published>2018-01-14T00:00:00+00:00</published>
    <updated>2018-01-14T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about Philosophy of Intelligence with Matthew Crosby: <a href="https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/" class="uri">https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/</a></p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai91.jpg" alt="Sketchnotes from TWiMLAI talk #92: Philosophy of Intelligence with Matthew Crosby" />
<p class="caption">Sketchnotes from TWiMLAI talk #92: Philosophy of Intelligence with Matthew Crosby</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-91-philosophy-intelligence-matthew-crosby/">here</a>.</p>
<blockquote>
<p>This week on the podcast we’re featuring a series of conversations from the NIPs conference in Long Beach, California. I attended a bunch of talks and learned a ton, organized an impromptu roundtable on Building AI Products, and met a bunch of great people, including some former TWiML Talk guests.This time around i’m joined by Matthew Crosby, a researcher at Imperial College London, working on the Kinds of Intelligence Project. Matthew joined me after the NIPS Symposium of the same name, an event that brought researchers from a variety of disciplines together towards three aims: a broader perspective of the possible types of intelligence beyond human intelligence, better measurements of intelligence, and a more purposeful analysis of where progress should be made in AI to best benefit society. Matthew’s research explores intelligence from a philosophical perspective, exploring ideas like predictive processing and controlled hallucination, and how these theories of intelligence impact the way we approach creating artificial intelligence. This was a very interesting conversation, i’m sure you’ll enjoy. <a href="https://twimlai.com/twiml-talk-91-philosophy-intelligence-matthew-crosby/" class="uri">https://twimlai.com/twiml-talk-91-philosophy-intelligence-matthew-crosby/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Looking beyond accuracy to improve trust in machine learning]]></title>
    <link href="/2018/01/looking_beyond_accuracy_to_improve_trust_in_ml/"/>
    <id>/2018/01/looking_beyond_accuracy_to_improve_trust_in_ml/</id>
    <published>2018-01-10T00:00:00+00:00</published>
    <updated>2018-01-10T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I have written another blogpost about <a href="https://blog.codecentric.de/en/2018/01/look-beyond-accuracy-improve-trust-machine-learning/">Looking beyond accuracy to improve trust in machine learning</a> at my company <a href="https://blog.codecentric.de/en/">codecentric</a>&rsquo;s blog:</p>

<blockquote>
<p>Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex machine learning models are essentially black boxes and too complicated to understand, we need to use approximations to get a better sense of how they work. One such approach is LIME, which stands for Local Interpretable Model-agnostic Explanations and is a tool that helps understand and explain the decisions made by complex machine learning models.</p>
</blockquote>

<p>Continue reading at <a href="https://blog.codecentric.de/en/2018/01/look-beyond-accuracy-improve-trust-machine-learning/">https://blog.codecentric.de/en/2018/01/look-beyond-accuracy-improve-trust-machine-learning/</a>&hellip;</p>

<p>Links to the entire example code and more info are given at the end of the blog post.</p>

<p>The blog post is <a href="https://blog.codecentric.de/2018/01/vertrauen-und-vorurteile-maschinellem-lernen/">also available in German</a>.</p>

<p><img src="https://blog.codecentric.de/files/2018/01/lime_output_figure.png" alt="" /></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[TWiMLAI talk 88 sketchnotes: Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru]]></title>
    <link href="/2018/01/twimlai88_sketchnotes/"/>
    <id>/2018/01/twimlai88_sketchnotes/</id>
    <published>2018-01-10T00:00:00+00:00</published>
    <updated>2018-01-10T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes taken from the <a href="https://twimlai.com/twiml-talk-88-using-deep-learning-google-street-view-estimate-demographics-timnit-gebru/">“This week in Machine Learning &amp; AI” podcast number 88 about Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru</a>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai88_sketchnotes_vhjzac.jpg" alt="Sketchnotes from TWiMLAI talk #88: Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru" />
<p class="caption">Sketchnotes from TWiMLAI talk #88: Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru</p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Registration now open for workshop on Deep Learning with Keras and TensorFlow using R]]></title>
    <link href="/2017/12/keras_sketchnotes/"/>
    <id>/2017/12/keras_sketchnotes/</id>
    <published>2017-12-20T00:00:00+00:00</published>
    <updated>2017-12-20T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Recently, I announced <a href="https://shirinsplayground.netlify.com/2017/11/deep_learning_keras_tensorflow/">my workshop on Deep Learning with Keras and TensorFlow</a>.</p>
<p>The next dates for it are <strong>January 18th and 19th</strong> in Solingen, Germany.</p>
<p>You can register now by following this link: <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow" class="uri">https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow</a></p>
<p>If any non-German-speaking people want to attend, I’m happy to give the course in English!</p>
<p><a href="mailto:shirin.glander@codecentric.de">Contact me if you have further questions.</a></p>
<hr />
<p>As a little bonus, I am also sharing my sketch notes from a Podcast I listened to when first getting into Keras:</p>
<ul>
<li><a href="https://softwareengineeringdaily.com/2016/01/29/deep-learning-and-keras-with-francois-chollet/">Software Engineering Daily with Francois Chollet</a></li>
</ul>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/keras_sketchnotes_lgrnvo.jpg" alt="Sketchnotes: Software Engineering Daily - Podcast from Jan 29th 2016" />
<p class="caption">Sketchnotes: Software Engineering Daily - Podcast from Jan 29th 2016</p>
</div>
<p>Links from the notes:</p>
<ul>
<li><a href="https://keras.io/">Keras for Python</a></li>
<li><a href="https://keras.rstudio.com/">Keras for R</a></li>
<li><a href="https://github.com/maxpumperla/elephas">elephas</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Explaining Predictions of Machine Learning Models with LIME - Münster Data Science Meetup]]></title>
    <link href="/2017/12/lime_sketchnotes/"/>
    <id>/2017/12/lime_sketchnotes/</id>
    <published>2017-12-12T00:00:00+00:00</published>
    <updated>2017-12-12T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="slides-from-munster-data-science-meetup" class="section level2">
<h2>Slides from Münster Data Science Meetup</h2>
<p><a href="https://shiring.github.io/netlify_images/lime_meetup_slides_wvsh6s.pdf">These are my slides</a> from the <a href="https://www.meetup.com/Data-Science-Meetup-Muenster/events/244173239/">Münster Data Science Meetup on December 12th, 2017</a>.</p>
<pre class="r"><code>knitr::include_url(&quot;https://shiring.github.io/netlify_images/lime_meetup_slides_wvsh6s.pdf&quot;)</code></pre>
<iframe src="https://shiring.github.io/netlify_images/lime_meetup_slides_wvsh6s.pdf" width="672" height="400px">
</iframe>
<p><br></p>
<p>My sketchnotes were collected from these two podcasts:</p>
<ul>
<li><a href="https://twimlai.com/twiml-talk-7-carlos-guestrin-explaining-predictions-machine-learning-models/" class="uri">https://twimlai.com/twiml-talk-7-carlos-guestrin-explaining-predictions-machine-learning-models/</a></li>
<li><a href="https://dataskeptic.com/blog/episodes/2016/trusting-machine-learning-models-with-lime" class="uri">https://dataskeptic.com/blog/episodes/2016/trusting-machine-learning-models-with-lime</a></li>
</ul>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/lime_sketchnotes_guq6u5.jpg" alt="Sketchnotes: TWiML Talk #7 with Carlos Guestrin – Explaining the Predictions of Machine Learning Models &amp; Data Skeptic Podcast - Trusting Machine Learning Models with Lime" />
<p class="caption">Sketchnotes: TWiML Talk #7 with Carlos Guestrin – Explaining the Predictions of Machine Learning Models &amp; Data Skeptic Podcast - Trusting Machine Learning Models with Lime</p>
</div>
<hr />
</div>
<div id="example-code" class="section level2">
<h2>Example Code</h2>
<ul>
<li>the following libraries were loaded:</li>
</ul>
<pre class="r"><code>library(tidyverse)  # for tidy data analysis
library(farff)      # for reading arff file
library(missForest) # for imputing missing values
library(dummies)    # for creating dummy variables
library(caret)      # for modeling
library(lime)       # for explaining predictions</code></pre>
<div id="data" class="section level3">
<h3>Data</h3>
<p>The Chronic Kidney Disease dataset was downloaded from UC Irvine’s Machine Learning repository: <a href="http://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease" class="uri">http://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease</a></p>
<pre class="r"><code>data_file &lt;- file.path(&quot;path/to/chronic_kidney_disease_full.arff&quot;)</code></pre>
<ul>
<li>load data with the <code>farff</code> package</li>
</ul>
<pre class="r"><code>data &lt;- readARFF(data_file)</code></pre>
<div id="features" class="section level4">
<h4>Features</h4>
<ul>
<li>age - age</li>
<li>bp - blood pressure</li>
<li>sg - specific gravity</li>
<li>al - albumin</li>
<li>su - sugar</li>
<li>rbc - red blood cells</li>
<li>pc - pus cell</li>
<li>pcc - pus cell clumps</li>
<li>ba - bacteria</li>
<li>bgr - blood glucose random</li>
<li>bu - blood urea</li>
<li>sc - serum creatinine</li>
<li>sod - sodium</li>
<li>pot - potassium</li>
<li>hemo - hemoglobin</li>
<li>pcv - packed cell volume</li>
<li>wc - white blood cell count</li>
<li>rc - red blood cell count</li>
<li>htn - hypertension</li>
<li>dm - diabetes mellitus</li>
<li>cad - coronary artery disease</li>
<li>appet - appetite</li>
<li>pe - pedal edema</li>
<li>ane - anemia</li>
<li>class - class</li>
</ul>
</div>
</div>
<div id="missing-data" class="section level3">
<h3>Missing data</h3>
<ul>
<li>impute missing data with Nonparametric Missing Value Imputation using Random Forest (<code>missForest</code> package)</li>
</ul>
<pre class="r"><code>data_imp &lt;- missForest(data)</code></pre>
</div>
<div id="one-hot-encoding" class="section level3">
<h3>One-hot encoding</h3>
<ul>
<li>create dummy variables (<code>caret::dummy.data.frame()</code>)</li>
<li>scale and center</li>
</ul>
<pre class="r"><code>data_imp_final &lt;- data_imp$ximp
data_dummy &lt;- dummy.data.frame(dplyr::select(data_imp_final, -class), sep = &quot;_&quot;)
data &lt;- cbind(dplyr::select(data_imp_final, class), scale(data_dummy, 
                                                   center = apply(data_dummy, 2, min),
                                                   scale = apply(data_dummy, 2, max)))</code></pre>
</div>
<div id="modeling" class="section level3">
<h3>Modeling</h3>
<pre class="r"><code># training and test set
set.seed(42)
index &lt;- createDataPartition(data$class, p = 0.9, list = FALSE)
train_data &lt;- data[index, ]
test_data  &lt;- data[-index, ]

# modeling
model_rf &lt;- caret::train(class ~ .,
  data = train_data,
  method = &quot;rf&quot;, # random forest
  trControl = trainControl(method = &quot;repeatedcv&quot;, 
       number = 10, 
       repeats = 5, 
       verboseIter = FALSE))</code></pre>
<pre class="r"><code>model_rf</code></pre>
<pre><code>## Random Forest 
## 
## 360 samples
##  48 predictor
##   2 classes: &#39;ckd&#39;, &#39;notckd&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 324, 324, 324, 324, 325, 324, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    2    0.9922647  0.9838466
##   25    0.9917392  0.9826070
##   48    0.9872930  0.9729881
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<pre class="r"><code># predictions
pred &lt;- data.frame(sample_id = 1:nrow(test_data), predict(model_rf, test_data, type = &quot;prob&quot;), actual = test_data$class) %&gt;%
  mutate(prediction = colnames(.)[2:3][apply(.[, 2:3], 1, which.max)], correct = ifelse(actual == prediction, &quot;correct&quot;, &quot;wrong&quot;))

confusionMatrix(pred$actual, pred$prediction)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction ckd notckd
##     ckd     23      2
##     notckd   0     15
##                                           
##                Accuracy : 0.95            
##                  95% CI : (0.8308, 0.9939)
##     No Information Rate : 0.575           
##     P-Value [Acc &gt; NIR] : 1.113e-07       
##                                           
##                   Kappa : 0.8961          
##  Mcnemar&#39;s Test P-Value : 0.4795          
##                                           
##             Sensitivity : 1.0000          
##             Specificity : 0.8824          
##          Pos Pred Value : 0.9200          
##          Neg Pred Value : 1.0000          
##              Prevalence : 0.5750          
##          Detection Rate : 0.5750          
##    Detection Prevalence : 0.6250          
##       Balanced Accuracy : 0.9412          
##                                           
##        &#39;Positive&#39; Class : ckd             
## </code></pre>
</div>
<div id="lime" class="section level3">
<h3>LIME</h3>
<ul>
<li>LIME needs data without response variable</li>
</ul>
<pre class="r"><code>train_x &lt;- dplyr::select(train_data, -class)
test_x &lt;- dplyr::select(test_data, -class)

train_y &lt;- dplyr::select(train_data, class)
test_y &lt;- dplyr::select(test_data, class)</code></pre>
<ul>
<li>build explainer</li>
</ul>
<pre class="r"><code>explainer &lt;- lime(train_x, model_rf, n_bins = 5, quantile_bins = TRUE)</code></pre>
<ul>
<li>run <code>explain()</code> function</li>
</ul>
<pre class="r"><code>explanation_df &lt;- lime::explain(test_x, explainer, n_labels = 1, n_features = 8, n_permutations = 1000, feature_select = &quot;forward_selection&quot;)</code></pre>
<ul>
<li>model reliability</li>
</ul>
<pre class="r"><code>explanation_df %&gt;%
  ggplot(aes(x = model_r2, fill = label)) +
    geom_density(alpha = 0.5)</code></pre>
<p><img src="/post/2017-12-12_lime_sketchnotes_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<ul>
<li>plot explanations</li>
</ul>
<pre class="r"><code>plot_features(explanation_df[1:24, ], ncol = 1)</code></pre>
<p><img src="/post/2017-12-12_lime_sketchnotes_files/figure-html/unnamed-chunk-15-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="session-info" class="section level3">
<h3>Session Info</h3>
<pre><code>## Session info -------------------------------------------------------------</code></pre>
<pre><code>##  setting  value                       
##  version  R version 3.4.3 (2017-11-30)
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  de_DE.UTF-8                 
##  tz       Europe/Berlin               
##  date     2018-03-04</code></pre>
<pre><code>## Packages -----------------------------------------------------------------</code></pre>
<pre><code>##  package      * version    date       source                            
##  assertthat     0.2.0      2017-04-11 CRAN (R 3.4.0)                    
##  backports      1.1.2      2017-12-13 CRAN (R 3.4.3)                    
##  base         * 3.4.3      2017-12-07 local                             
##  BBmisc         1.11       2017-03-10 CRAN (R 3.4.0)                    
##  bindr          0.1        2016-11-13 CRAN (R 3.4.0)                    
##  bindrcpp     * 0.2        2017-06-17 CRAN (R 3.4.0)                    
##  blogdown       0.5        2018-01-24 CRAN (R 3.4.3)                    
##  bookdown       0.7        2018-02-18 CRAN (R 3.4.3)                    
##  broom          0.4.3      2017-11-20 CRAN (R 3.4.2)                    
##  caret        * 6.0-78     2017-12-10 CRAN (R 3.4.3)                    
##  cellranger     1.1.0      2016-07-27 CRAN (R 3.4.0)                    
##  checkmate      1.8.5      2017-10-24 CRAN (R 3.4.2)                    
##  class          7.3-14     2015-08-30 CRAN (R 3.4.3)                    
##  cli            1.0.0      2017-11-05 CRAN (R 3.4.2)                    
##  codetools      0.2-15     2016-10-05 CRAN (R 3.4.3)                    
##  colorspace     1.3-2      2016-12-14 CRAN (R 3.4.0)                    
##  compiler       3.4.3      2017-12-07 local                             
##  crayon         1.3.4      2017-09-16 CRAN (R 3.4.1)                    
##  CVST           0.2-1      2013-12-10 CRAN (R 3.4.0)                    
##  datasets     * 3.4.3      2017-12-07 local                             
##  ddalpha        1.3.1.1    2018-02-02 CRAN (R 3.4.3)                    
##  DEoptimR       1.0-8      2016-11-19 CRAN (R 3.4.0)                    
##  devtools       1.13.5     2018-02-18 CRAN (R 3.4.3)                    
##  digest         0.6.15     2018-01-28 CRAN (R 3.4.3)                    
##  dimRed         0.1.0      2017-05-04 CRAN (R 3.4.0)                    
##  dplyr        * 0.7.4      2017-09-28 CRAN (R 3.4.2)                    
##  DRR            0.0.3      2018-01-06 CRAN (R 3.4.3)                    
##  dummies      * 1.5.6      2012-06-14 CRAN (R 3.4.0)                    
##  e1071          1.6-8      2017-02-02 CRAN (R 3.4.0)                    
##  evaluate       0.10.1     2017-06-24 CRAN (R 3.4.1)                    
##  farff        * 1.0        2016-09-11 CRAN (R 3.4.0)                    
##  forcats      * 0.3.0      2018-02-19 CRAN (R 3.4.3)                    
##  foreach      * 1.4.4      2017-12-12 CRAN (R 3.4.3)                    
##  foreign        0.8-69     2017-06-22 CRAN (R 3.4.3)                    
##  ggplot2      * 2.2.1.9000 2018-02-28 Github (thomasp85/ggplot2@7859a29)
##  glmnet         2.0-13     2017-09-22 CRAN (R 3.4.2)                    
##  glue           1.2.0      2017-10-29 CRAN (R 3.4.2)                    
##  gower          0.1.2      2017-02-23 CRAN (R 3.4.0)                    
##  graphics     * 3.4.3      2017-12-07 local                             
##  grDevices    * 3.4.3      2017-12-07 local                             
##  grid           3.4.3      2017-12-07 local                             
##  gtable         0.2.0      2016-02-26 CRAN (R 3.4.0)                    
##  haven          1.1.1      2018-01-18 CRAN (R 3.4.3)                    
##  highr          0.6        2016-05-09 CRAN (R 3.4.0)                    
##  hms            0.4.1      2018-01-24 CRAN (R 3.4.3)                    
##  htmltools      0.3.6      2017-04-28 CRAN (R 3.4.0)                    
##  htmlwidgets    1.0        2018-01-20 CRAN (R 3.4.3)                    
##  httpuv         1.3.6.1    2018-02-28 CRAN (R 3.4.3)                    
##  httr           1.3.1      2017-08-20 CRAN (R 3.4.1)                    
##  ipred          0.9-6      2017-03-01 CRAN (R 3.4.0)                    
##  iterators    * 1.0.9      2017-12-12 CRAN (R 3.4.3)                    
##  itertools    * 0.1-3      2014-03-12 CRAN (R 3.4.0)                    
##  jsonlite       1.5        2017-06-01 CRAN (R 3.4.0)                    
##  kernlab        0.9-25     2016-10-03 CRAN (R 3.4.0)                    
##  knitr          1.20       2018-02-20 CRAN (R 3.4.3)                    
##  labeling       0.3        2014-08-23 CRAN (R 3.4.0)                    
##  lattice      * 0.20-35    2017-03-25 CRAN (R 3.4.3)                    
##  lava           1.6        2018-01-13 CRAN (R 3.4.3)                    
##  lazyeval       0.2.1      2017-10-29 CRAN (R 3.4.2)                    
##  lime         * 0.3.1      2017-11-24 CRAN (R 3.4.3)                    
##  lubridate      1.7.3      2018-02-27 CRAN (R 3.4.3)                    
##  magrittr       1.5        2014-11-22 CRAN (R 3.4.0)                    
##  MASS           7.3-49     2018-02-23 CRAN (R 3.4.3)                    
##  Matrix         1.2-12     2017-11-20 CRAN (R 3.4.3)                    
##  memoise        1.1.0      2017-04-21 CRAN (R 3.4.0)                    
##  methods      * 3.4.3      2017-12-07 local                             
##  mime           0.5        2016-07-07 CRAN (R 3.4.0)                    
##  missForest   * 1.4        2013-12-31 CRAN (R 3.4.0)                    
##  mnormt         1.5-5      2016-10-15 CRAN (R 3.4.0)                    
##  ModelMetrics   1.1.0      2016-08-26 CRAN (R 3.4.0)                    
##  modelr         0.1.1      2017-07-24 CRAN (R 3.4.1)                    
##  munsell        0.4.3      2016-02-13 CRAN (R 3.4.0)                    
##  nlme           3.1-131.1  2018-02-16 CRAN (R 3.4.3)                    
##  nnet           7.3-12     2016-02-02 CRAN (R 3.4.3)                    
##  parallel       3.4.3      2017-12-07 local                             
##  pillar         1.2.1      2018-02-27 CRAN (R 3.4.3)                    
##  pkgconfig      2.0.1      2017-03-21 CRAN (R 3.4.0)                    
##  plyr           1.8.4      2016-06-08 CRAN (R 3.4.0)                    
##  prodlim        1.6.1      2017-03-06 CRAN (R 3.4.0)                    
##  psych          1.7.8      2017-09-09 CRAN (R 3.4.1)                    
##  purrr        * 0.2.4      2017-10-18 CRAN (R 3.4.2)                    
##  R6             2.2.2      2017-06-17 CRAN (R 3.4.0)                    
##  randomForest * 4.6-12     2015-10-07 CRAN (R 3.4.0)                    
##  Rcpp           0.12.15    2018-01-20 CRAN (R 3.4.3)                    
##  RcppRoll       0.2.2      2015-04-05 CRAN (R 3.4.0)                    
##  readr        * 1.1.1      2017-05-16 CRAN (R 3.4.0)                    
##  readxl         1.0.0      2017-04-18 CRAN (R 3.4.0)                    
##  recipes        0.1.2      2018-01-11 CRAN (R 3.4.3)                    
##  reshape2       1.4.3      2017-12-11 CRAN (R 3.4.3)                    
##  rlang          0.2.0.9000 2018-02-28 Github (tidyverse/rlang@9ea33dd)  
##  rmarkdown      1.8        2017-11-17 CRAN (R 3.4.2)                    
##  robustbase     0.92-8     2017-11-01 CRAN (R 3.4.2)                    
##  rpart          4.1-13     2018-02-23 CRAN (R 3.4.3)                    
##  rprojroot      1.3-2      2018-01-03 CRAN (R 3.4.3)                    
##  rstudioapi     0.7        2017-09-07 CRAN (R 3.4.1)                    
##  rvest          0.3.2      2016-06-17 CRAN (R 3.4.0)                    
##  scales         0.5.0.9000 2018-02-28 Github (hadley/scales@d767915)    
##  sfsmisc        1.1-1      2017-06-08 CRAN (R 3.4.0)                    
##  shiny          1.0.5      2017-08-23 CRAN (R 3.4.1)                    
##  shinythemes    1.1.1      2016-10-12 CRAN (R 3.4.0)                    
##  splines        3.4.3      2017-12-07 local                             
##  stats        * 3.4.3      2017-12-07 local                             
##  stats4         3.4.3      2017-12-07 local                             
##  stringdist     0.9.4.6    2017-07-31 CRAN (R 3.4.1)                    
##  stringi        1.1.6      2017-11-17 CRAN (R 3.4.2)                    
##  stringr      * 1.3.0      2018-02-19 CRAN (R 3.4.3)                    
##  survival       2.41-3     2017-04-04 CRAN (R 3.4.3)                    
##  tibble       * 1.4.2      2018-01-22 CRAN (R 3.4.3)                    
##  tidyr        * 0.8.0      2018-01-29 CRAN (R 3.4.3)                    
##  tidyselect     0.2.4      2018-02-26 CRAN (R 3.4.3)                    
##  tidyverse    * 1.2.1      2017-11-14 CRAN (R 3.4.2)                    
##  timeDate       3043.102   2018-02-21 CRAN (R 3.4.3)                    
##  tools          3.4.3      2017-12-07 local                             
##  utils        * 3.4.3      2017-12-07 local                             
##  withr          2.1.1.9000 2018-02-28 Github (jimhester/withr@5d05571)  
##  xfun           0.1        2018-01-22 CRAN (R 3.4.3)                    
##  xml2           1.2.0      2018-01-24 CRAN (R 3.4.3)                    
##  xtable         1.8-2      2016-02-05 CRAN (R 3.4.0)                    
##  yaml           2.1.17     2018-02-27 CRAN (R 3.4.3)</code></pre>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[MICE (Multiple Imputation by Chained Equations) in R - sketchnotes from MünsteR Meetup]]></title>
    <link href="/2017/11/mice_sketchnotes/"/>
    <id>/2017/11/mice_sketchnotes/</id>
    <published>2017-11-28T00:00:00+00:00</published>
    <updated>2017-11-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Last night, the <a href="http://meetu.ps/c/3ffGL/w54bW/f">MünsteR R user-group</a> had <a href="https://www.meetup.com/Munster-R-Users-Group/events/243388360/">another great meetup</a>:</p>
<p>Karin Groothuis-Oudshoorn, Assistant Professor at the University of Twente, presented her R package <code>mice</code> about Multivariate Imputation by Chained Equations.</p>
<p>It was a very interesting talk and here are my sketchnotes that I took during it:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/mice_sketchnote_gxjsgc.jpg" alt="MICE talk sketchnotes" />
<p class="caption">MICE talk sketchnotes</p>
</div>
<p>Here is the link to the paper referenced in my notes: <a href="https://www.jstatsoft.org/article/view/v045i03" class="uri">https://www.jstatsoft.org/article/view/v045i03</a></p>
<blockquote>
<p>“The mice package implements a method to deal with missing data. The package creates multiple imputations (replacement values) for multivariate missing data. The method is based on Fully Conditional Specification, where each incomplete variable is imputed by a separate model. The MICE algorithm can impute mixes of continuous, binary, unordered categorical and ordered categorical data. In addition, MICE can impute continuous two-level data, and maintain consistency between imputations by means of passive imputation. Many diagnostic plots are implemented to inspect the quality of the imputations.”&quot; (<a href="https://cran.r-project.org/web/packages/mice/README.html" class="uri">https://cran.r-project.org/web/packages/mice/README.html</a>)</p>
</blockquote>
<p>For more information on the package go to <a href="http://stefvanbuuren.github.io/mice/" class="uri">http://stefvanbuuren.github.io/mice/</a>.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Workshop on Deep Learning with Keras and TensorFlow in R]]></title>
    <link href="/2017/11/deep_learning_keras_tensorflow/"/>
    <id>/2017/11/deep_learning_keras_tensorflow/</id>
    <published>2017-11-20T00:00:00+00:00</published>
    <updated>2017-11-20T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>You can now book me and my 1-day workshop on <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/">deep learning with Keras and TensorFlow using R</a>.</p>
<p>In my workshop, you will learn</p>
<ul>
<li>the basics of deep learning</li>
<li>what cross-entropy and loss is</li>
<li>about activation functions</li>
<li>how to optimize weights and biases with backpropagation and gradient descent</li>
<li>how to build (deep) neural networks with Keras and TensorFlow</li>
<li>how to save and load models and model weights</li>
<li>how to visualize models with TensorBoard</li>
<li>how to make predictions on test data</li>
</ul>
<p>Date and place depend on who and how many people are interested, so please contact me either directly or via the workshop page: <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/" class="uri">https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/</a> (the description is in German but I also offer to give the workshop in English).</p>
<p><br></p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/mlp_r7pv7z.jpg" alt="Neural Network with three densely connected hidden layers" />
<p class="caption">Neural Network with three densely connected hidden layers</p>
</div>
<p>Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. Keras is very convenient for fast and easy prototyping of neural networks. It is highly modular and very flexible, so that you can build basically any type of neural network you want. It supports convolutional neural networks and recurrent neural networks, as well as combinations of both. Due to its layer structure, it is highly extensible and can run on CPU or GPU.</p>
<p>The <code>keras</code> R package provides an interface to the Python library of Keras, just as the tensorflow package provides an interface to TensorFlow. Basically, R creates a conda instance and runs Keras it it, while you can still use all the functionalities of R for plotting, etc. Almost all function names are the same, so models can easily be recreated in Python for deployment.</p>
<p><br></p>
<div class="figure">
<img src="https://blog.keras.io/img/keras-tensorflow-logo.jpg" />

</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[How to combine point and boxplots in timeline charts with ggplot2 facets]]></title>
    <link href="/2017/11/combine_point_boxplot_ggplot/"/>
    <id>/2017/11/combine_point_boxplot_ggplot/</id>
    <published>2017-11-18T00:00:00+00:00</published>
    <updated>2017-11-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>In a recent project, I was looking to plot data from different variables along the same time axis. The difficulty was, that some of these variables I wanted to have as point plots, while others I wanted as box-plots.</p>
<p>Because I work with the tidyverse, I wanted to produce these plots with ggplot2. Faceting was the obvious first step but it took me quite a while to figure out how to best combine facets with point plots (where I have one value per time point) with and box-plots (where I have multiple values per time point).</p>
<p>The reason why this isn’t trivial is that box plots require groups or factors on the x-axis, while points can be plotted over a continuous range of x-values. If your alarm bells are ringing right now, you are absolutely right: before you try to combine plots with different x-axis properties, you should think long and hard whether this is an accurate representation of the data and if its a good idea to do so! Here, I had multiple values per time point for one variable and I wanted to make the median + variation explicitly clear, while also showing the continuous changes of other variables over the same range of time.</p>
<p>So, I am writing this short tutorial here in hopes that it saves the next person trying to do something similar from spending an entire morning on stackoverflow. ;-)</p>
<p>For this demonstration, I am creating some fake data:</p>
<pre class="r"><code>library(tidyverse)
dates &lt;- seq(as.POSIXct(&quot;2017-10-01 07:00&quot;), as.POSIXct(&quot;2017-10-01 10:30&quot;), by = 180) # 180 seconds == 3 minutes
fake_data &lt;- data.frame(time = dates,
                        var1_1 = runif(length(dates)),
                        var1_2 = runif(length(dates)),
                        var1_3 = runif(length(dates)),
                        var2 = runif(length(dates))) %&gt;%
  sample_frac(size = 0.33)
head(fake_data)</code></pre>
<pre><code>##                   time    var1_1     var1_2    var1_3      var2
## 53 2017-10-01 09:36:00 0.1984466 0.04188588 0.3464290 0.5583152
## 38 2017-10-01 08:51:00 0.7057563 0.22779574 0.2382634 0.7954228
## 34 2017-10-01 08:39:00 0.5239017 0.75149639 0.3942653 0.4908962
## 13 2017-10-01 07:36:00 0.8838265 0.22154491 0.9388426 0.2199565
## 6  2017-10-01 07:15:00 0.7729077 0.45930575 0.1749513 0.6661809
## 36 2017-10-01 08:45:00 0.9434972 0.40341611 0.4926815 0.8377908</code></pre>
<p>Here, variable 1 (<code>var1</code>) has three measurements per time point, while variable 2 (<code>var2</code>) has one.</p>
<p>First, for plotting with ggplot2 we want our data in a tidy long format. I also add another column for faceting that groups the variables from <code>var1</code> together.</p>
<pre class="r"><code>fake_data_long &lt;- fake_data %&gt;%
  gather(x, y, var1_1:var2) %&gt;%
  mutate(facet = ifelse(x %in% c(&quot;var1_1&quot;, &quot;var1_2&quot;, &quot;var1_3&quot;), &quot;var1&quot;, x))
head(fake_data_long)</code></pre>
<pre><code>##                  time      x         y facet
## 1 2017-10-01 09:36:00 var1_1 0.1984466  var1
## 2 2017-10-01 08:51:00 var1_1 0.7057563  var1
## 3 2017-10-01 08:39:00 var1_1 0.5239017  var1
## 4 2017-10-01 07:36:00 var1_1 0.8838265  var1
## 5 2017-10-01 07:15:00 var1_1 0.7729077  var1
## 6 2017-10-01 08:45:00 var1_1 0.9434972  var1</code></pre>
<p>Now, we can plot this the following way:</p>
<ul>
<li>facet by variable</li>
<li>subset data to facets for point plots and give aesthetics in <code>geom_point()</code></li>
<li>subset data to facets for box plots and give aesthetics in <code>geom_boxplot()</code>. Here we also need to set the <code>group</code> aesthetic; if we don’t specifically give that, we will get a plot with one big box, instead of a box for every time point.</li>
</ul>
<pre class="r"><code>fake_data_long %&gt;%
  ggplot() +
    facet_grid(facet ~ ., scales = &quot;free&quot;) +
    geom_point(data = subset(fake_data_long, facet == &quot;var2&quot;), 
               aes(x = time, y = y),
               size = 1) +
    geom_line(data = subset(fake_data_long, facet == &quot;var2&quot;), 
               aes(x = time, y = y)) +
    geom_boxplot(data = subset(fake_data_long, facet == &quot;var1&quot;), 
               aes(x = time, y = y, group = time))</code></pre>
<p><img src="/post/2017-11-18-combine_point_boxplot_ggplot_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.3
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] methods   stats     graphics  grDevices utils     datasets  base     
## 
## other attached packages:
##  [1] bindrcpp_0.2       forcats_0.3.0      stringr_1.3.0     
##  [4] dplyr_0.7.4        purrr_0.2.4        readr_1.1.1       
##  [7] tidyr_0.8.0        tibble_1.4.2       ggplot2_2.2.1.9000
## [10] tidyverse_1.2.1   
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_0.2.4  xfun_0.1          reshape2_1.4.3   
##  [4] haven_1.1.1       lattice_0.20-35   colorspace_1.3-2 
##  [7] htmltools_0.3.6   yaml_2.1.17       rlang_0.2.0.9000 
## [10] pillar_1.2.1      withr_2.1.1.9000  foreign_0.8-69   
## [13] glue_1.2.0        modelr_0.1.1      readxl_1.0.0     
## [16] bindr_0.1         plyr_1.8.4        munsell_0.4.3    
## [19] blogdown_0.5      gtable_0.2.0      cellranger_1.1.0 
## [22] rvest_0.3.2       psych_1.7.8       evaluate_0.10.1  
## [25] labeling_0.3      knitr_1.20        parallel_3.4.3   
## [28] broom_0.4.3       Rcpp_0.12.15      backports_1.1.2  
## [31] scales_0.5.0.9000 jsonlite_1.5      mnormt_1.5-5     
## [34] hms_0.4.1         digest_0.6.15     stringi_1.1.6    
## [37] bookdown_0.7      grid_3.4.3        rprojroot_1.3-2  
## [40] cli_1.0.0         tools_3.4.3       magrittr_1.5     
## [43] lazyeval_0.2.1    crayon_1.3.4      pkgconfig_2.0.1  
## [46] xml2_1.2.0        lubridate_1.7.3   assertthat_0.2.0 
## [49] rmarkdown_1.8     httr_1.3.1        rstudioapi_0.7   
## [52] R6_2.2.2          nlme_3.1-131.1    compiler_3.4.3</code></pre>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Explore Predictive Maintenance with flexdashboard]]></title>
    <link href="/2017/11/predictive_maintenance_dashboard/"/>
    <id>/2017/11/predictive_maintenance_dashboard/</id>
    <published>2017-11-02T00:00:00+00:00</published>
    <updated>2017-11-02T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I have written the following post about <a href="https://blog.codecentric.de/en/2017/11/explore-predictive-maintenance-flexdashboard/">Predictive Maintenance and flexdashboard</a> at my company <a href="https://blog.codecentric.de/en/">codecentric</a>’s blog:</p>
<blockquote>
<p>Predictive Maintenance is an increasingly popular strategy associated with Industry 4.0; it uses advanced analytics and machine learning to optimize machine costs and output (see Google Trends plot below). A common use-case for Predictive Maintenance is to proactively monitor machines, so as to predict when a check-up is needed to reduce failure and maximize performance. In contrast to traditional maintenance, where each machine has to undergo regular routine check-ups, Predictive Maintenance can save costs and reduce downtime. A machine learning approach to such a problem would be to analyze machine failure over time to train a supervised classification model that predicts failure. Data from sensors and weather information is often used as features in modeling.</p>
</blockquote>
<blockquote>
<p>…</p>
</blockquote>
<blockquote>
<p>With flexdashboard RStudio provides a great way to create interactive dashboards with R. It is an easy and very fast way to present analyses or create story maps. Here, I have used it to demonstrate different analysis techniques for Predictive Maintenance. It uses Shiny run-time to create interactive content.</p>
</blockquote>
<blockquote>
<p>…</p>
</blockquote>
<p>Continue reading at <a href="https://blog.codecentric.de/en/2017/11/explore-predictive-maintenance-flexdashboard/" class="uri">https://blog.codecentric.de/en/2017/11/explore-predictive-maintenance-flexdashboard/</a></p>
<div class="figure">
<img src="https://blog.codecentric.de/files/2017/10/dashboard_screenshot.png" />

</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Blockchain &amp; distributed ML - my report from the data2day conference]]></title>
    <link href="/2017/09/data2day/"/>
    <id>/2017/09/data2day/</id>
    <published>2017-09-28T00:00:00+00:00</published>
    <updated>2017-09-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://www.data2day.de/common/images/konferenzen/data2day2017.svg" />

</div>
<p>Yesterday and today I attended the <a href="www.data2day.de">data2day</a>, a conference about Big Data, Machine Learning and Data Science in Heidelberg, Germany. Topics and workshops covered a range of topics surrounding (big) data analysis and Machine Learning, like Deep Learning, Reinforcement Learning, TensorFlow applications, etc. Distributed systems and scalability were a major part of a lot of the talks as well, reflecting the growing desire to build bigger and more complex models that can’t (or would take too long to) run on a single computer. Most of the application examples were built in Python but one talk by Andreas Prawitt was specifically titled “Using R for Predictive Maintenance: an example from the TRUMPF Laser GmbH”. I also saw quite a few graphs that were obviously made with ggplot!</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="de" dir="ltr">
Guten Morgen auf der <a href="https://twitter.com/data2day"><span class="citation">@data2day</span></a> Kommt uns doch mal am Stand besuchen :-) <a href="https://t.co/YK46ACdNj9">pic.twitter.com/YK46ACdNj9</a>
</p>
— codecentric AG (<span class="citation">@codecentric</span>) <a href="https://twitter.com/codecentric/status/912928993279606784">September 27, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><br></p>
<p>The keynote lecture on Wednesday about <strong>Blockchains for AI</strong> was given by Trent McConaghy. <a href="https://www.sitepen.com/blog/2017/09/21/blockchain-basics/">Blockchain technology</a> is based on a decentralized system of storing and validating data and changes in data. It experiences a huge hype at the moment but it is only starting to gain track in Data Science and Machine Learning as well. I therefore found it a very fitting topic for the keynote lecture! Trent and his colleagues at <a href="www.bigchaindb.com">BigchainDB</a> are implementing an “internet-scale blockchain database for the world” - the Interplanetary Database (IPDB).</p>
<blockquote>
<p>“IPDB is a blockchain database that offers decentralized control, immutability and the creation and trading of digital assets. […] As a database for the world, IPDB offers decentralized control, strong governance and universal accessibility. IPDB relies on “caretaker” organizations around the world, who share responsibility for managing the network and governing the IPDB Foundation. Anyone in the world will be able to use IPDB. […]” <a href="https://blog.bigchaindb.com/ipdb-announced-as-public-planetary-scale-blockchain-database-7a363824fc14" class="uri">https://blog.bigchaindb.com/ipdb-announced-as-public-planetary-scale-blockchain-database-7a363824fc14</a></p>
</blockquote>
<p>He presented a number of examples where blockchain technology for decentralized data storage/access can be beneficial to Machine Learning and AI, like exchanging data from self-driving cars, of online market places and for generating art with computers. You can learn more about him <a href="https://blog.oceanprotocol.com/from-ai-to-blockchain-to-data-meet-ocean-f210ff460465">here</a>:</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="en" dir="ltr">
It's always been about the data.<br>Announcing Ocean.<a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a> <a href="https://twitter.com/hashtag/Blockchain?src=hash&amp;ref_src=twsrc%5Etfw">#Blockchain</a> <a href="https://twitter.com/oceanprotocol?ref_src=twsrc%5Etfw"><span class="citation">@OceanProtocol</span></a><a href="https://t.co/Do4XNn3ucN">https://t.co/Do4XNn3ucN</a>
</p>
— Trent McConaghy (<span class="citation">@trentmc0</span>) <a href="https://twitter.com/trentmc0/status/909793166416662528?ref_src=twsrc%5Etfw">September 18, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><br></p>
<p>The other talks were a mix of high- and low-level topics: from introductions to machine learning, Apache Spark and data analysis with Python to (distributed) data streaming with Kappa architecture or Apache Kafka, containerization with Docker and Kubernetes, data archiving with Apache Cassandra, relevance tuning with Solr and much more. While I spent most of the time at my company’s conference stand, I did hear three of the talks. I summarize each of them below…</p>
<p><br></p>
<ol style="list-style-type: decimal">
<li><strong>Scalable Machine Learning with Apache Spark for Fraud Detection</strong></li>
</ol>
<p>In this first talk I heard, Dr. Patrick Baier and Dr. Stanimir Dragiev presented their work at <a href="www.zalando.de/">Zalando</a>. They built a scalable machine learning framework with Apache Spark, Scala and AWS to assess and predict fraud in online transactions. <a href="www.zalando.de/">Zalando</a> is a German online store that sells clothes, shoes and accessories. Normally, they allow registered customers to buy via invoice, i.e. they receive their ordered items before they pay them. This leaves them vulnerable to fraud where item are not paid for. The goal of their data science team is to use customer and basket data to obtain a probability score for how likely a transaction is going to be fraudulent. High-risk payment options, like invoice, can then be disabled in transactions with high fraud probability. To build and run such machine learning models, the Zalando data science team uses a combination of Spark, Scala, R, AWS, SQL, Python, Docker, etc. In their workflow, they use a combination of static and dynamic features, imputing missing values and building a decision model. In order to scale their modeling workflow to process more requests, use more data in training, update models more frequently and/or run more models, they described a workflow that uses Spark, Scala and Amazon Web Services (AWS). Spark’s machine learning library can be used for modeling and scaled horizontally by increasing the number of clusters on which to run the models. Scala provides multi-threading functionality and AWS is used for storing data in S3 and extending computation power depending on changing needs. Finally, they include a model inspector into their workflow to assure comparability of training and test data and check that the model is behaving as expected. Problems that they are dealing with include highly unbalanced data (which is getting even worse the better their models work as they keep reducing the number of fraud cases), delayed labeling due to the long process of the transactions, seasonality in data.</p>
<p><br></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Sparse Data: Don’t Mind the Gap!</strong></li>
</ol>
<p>In this talk, my colleagues from <a href="www.codecentric.de">codecentric</a> Dr. Daniel Pape and Dr. Michael Plümacher showed an example from ad targeting of how to deal with sparse data. Sparse data occurs in many areas, e.g. as rare events over a long period of time or in areas where there are many items and few occurrences per item, like in recommender systems or in natural language processing (NLP). In ad targeting, the measure of success is the rate of the click-through rate (CRT): this is the number of clicks on a given advertisement displayed to a user on a website divided by the total number of advertisements, or impressions. Because financial revenue comes from a high CTR, advertisements should be placed in a way that maximizes their chance of being clicked, i.e. we want to recommend advertisements for specific users that match their interests or are of actual relevance. Sparsity come into play with ad targeting because the number of clicks is very low compared to two metrics: a) from all the potential ads that a user could see, only a small proportion is actually shown to her/him and b) of the ads that a user sees, she/he only clicks on very few. This means that, a CTR matrix of advertisements x targets will have very few combinations that have been clicked (the mean CTR is 0.003) and contain many missing values. The approach they took was to impute the missing values and predict for each target/user the most similar ads from the imputed CTR matrix. This approach worked well for a reasonably large data set but it didn’t perform so well with smaller (and therefore even sparser) data. They then talked about alternative approaches, like grouping users and/or ads into groups in order to reduce the sparsity of the data. Their take-home messages were that 1) there is no one-size-fits-all solution, what works depends on the context and 2) if the underlying data is of bad quality, the results will be sub-optimal - no matter how sophisticated the model.</p>
<p><br></p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Distributed TensorFlow with Kubernetes</strong></li>
</ol>
<p>In the third talk, another colleague of mine from <a href="www.codecentric.de">codecentric</a>, Jakob Karalus, explained in detail how to set up a distributed machine learning modelling set-up with <a href="https://www.tensorflow.org/">TensorFlow</a> and <a href="https://kubernetes.io/">Kubernetes</a>. TensorFlow is used to build neural networks in a graph-based manner. Distributed and parallel machine learning can be necessary when training big neural networks with a lot of training data, very deep neural networks, with complex parameters, grid search for hyper-parameter tuning, etc. A good way to build neural networks in a controlled and stable environment is to use <a href="https://www.docker.com/">Docker</a> containers. Kubernetes is a container orchestration tool that can set up distribution of nodes from our TensorFlow modeling container. Setting up this distributed system is quite complex, though and Jakob recommended to try to stay on one CPU/GPU as long as possible.</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="de" dir="ltr">
Tag 2 auf der <a href="https://twitter.com/data2day?ref_src=twsrc%5Etfw"><span class="citation">@data2day</span></a> kommt am Stand vorbei, wir haben noch ein paar T-Shirts und Softwerker für euch :-) <a href="https://t.co/xyG8Leg3lF">pic.twitter.com/xyG8Leg3lF</a>
</p>
— codecentric AG (<span class="citation">@codecentric</span>) <a href="https://twitter.com/codecentric/status/913301091755941888?ref_src=twsrc%5Etfw">September 28, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="de" dir="ltr">
Verteiltes Deep Learning mit TensorFlow und Kubernetes - <a href="https://twitter.com/krallistic"><span class="citation">@krallistic</span></a> auf der <a href="https://twitter.com/data2day"><span class="citation">@data2day</span></a> <a href="https://t.co/5AGJdhL5U1">pic.twitter.com/5AGJdhL5U1</a>
</p>
— codecentric AG (<span class="citation">@codecentric</span>) <a href="https://twitter.com/codecentric/status/913041395128111105">September 27, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[From Biology to Industry. A Blogger’s Journey to Data Science.]]></title>
    <link href="/2017/09/from-biology-to-industry.-a-bloggers-journey-to-data-science./"/>
    <id>/2017/09/from-biology-to-industry.-a-bloggers-journey-to-data-science./</id>
    <published>2017-09-20T00:00:00+00:00</published>
    <updated>2017-09-20T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Today, I have given a webinar for the Applied Epidemiology Didactic of the University of Wisconsin - Madison titled “From Biology to Industry. A Blogger’s Journey to Data Science.”</p>
<p>I talked about how blogging about R and Data Science helped me become a Data Scientist. I also gave a short introduction to Machine Learning, Big Data and Neural Networks.</p>
<p>My slides can be found here: <a href="https://www.slideshare.net/ShirinGlander/from-biology-to-industry-a-bloggers-journey-to-data-science" class="uri">https://www.slideshare.net/ShirinGlander/from-biology-to-industry-a-bloggers-journey-to-data-science</a></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Why I use R for Data Science - An Ode to R]]></title>
    <link href="/2017/09/ode_to_r/"/>
    <id>/2017/09/ode_to_r/</id>
    <published>2017-09-19T00:00:00+00:00</published>
    <updated>2017-09-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Working in Data Science, I often feel like I have to justify using R over Python. And while I do use Python for running scripts in production, I am much more comfortable with the R environment. Basically, whenever I can, I use R for prototyping, testing, visualizing and teaching. But because personal gut-feeling preference isn’t a very good reason to give to (scientifically minded) people, I’ve thought a lot about the pros and cons of using R. This is what I came up with why I still prefer R…</p>
<p><em>Disclaimer:</em> I have “grown up” with R and I’m much more familiar with it, so I admit that I am quite biased in my assessment. If you think I’m not doing other languages justice, I’ll be happy to hear your pros and cons!</p>
<p><br></p>
<ol style="list-style-type: decimal">
<li><p>First of, <a href="https://www.r-project.org/">R</a> is an <a href="https://cran.r-project.org/">open-source, cross-platform</a> language, so it’s free to use by any- and everybody. This in itself doesn’t make it special, though, because so are other languages, like Python.</p></li>
<li><p>It is an established language, so that there are lots and lots of packages for basically every type of analysis you can think of. You find packages for <a href="https://www.analyticsvidhya.com/blog/2015/08/list-r-packages-data-analysis/">data analysis</a>, <a href="http://www.kdnuggets.com/2017/02/top-r-packages-machine-learning.html">machine learning</a>, <a href="https://support.rstudio.com/hc/en-us/articles/201057987-Quick-list-of-useful-R-packages">visualization</a>, <a href="https://www.computerworld.com/article/2921176/business-intelligence/great-r-packages-for-data-import-wrangling-visualization.html">data wrangling</a>, <a href="https://cran.r-project.org/web/views/Spatial.html">spatial analysis</a>, <a href="https://www.bioconductor.org/">bioinformatics</a> and much more. But, same as with Python, this plethora of packages can sometimes make things a bit confusing: you would often need to test and compare several similar packages in order to find the best one.</p></li>
<li><p>Most of the packages are of very high quality. And when a package is on <a href="https://cran.r-project.org/web/packages/available_packages_by_name.html">CRAN</a> or <a href="https://www.bioconductor.org/">Bioconductor</a> (as most are), you can be sure that it has been checked, that you will get proper documentation and that you won’t have problems with installation, dependencies, etc. In my experience, R package and function documentation generally tends to be better than, say, of Python packages.</p></li>
<li><p>R’s graphics capabilities are superior to any other I know. Especially <a href="http://ggplot2.org/">ggplot2</a> with all its <a href="http://www.ggplot2-exts.org/">extensions</a> provides a structured, yet powerful set of tools for producing <a href="http://www.r-graph-gallery.com/portfolio/ggplot2-package/">high-quality publication-ready graphs and figures</a>. Moreover, ggplot2 is part of the <a href="https://www.tidyverse.org/">tidyverse</a> and works well with <a href="https://cran.r-project.org/web/packages/broom/vignettes/broom.html">broom</a>. This has made data wrangling and analysis much more convenient and structured and structured for me.</p></li>
<li><p>The suite of tools around <a href="https://www.rstudio.com/">R Studio</a> make it perfect for documenting data analysis workflows and for teaching. You can provide easy instructions for installation and <a href="http://rmarkdown.rstudio.com/">R Markdown</a> files for your students to follow along. Everybody is going to use the same system. In Python, you are always dealing with questions like version 2 vs version 3, Spyder vs Jupyter Notebook, pip vs conda, etc. <a href="https://www.rstudio.com/products/rpackages/">Everything around R Studio</a> is very well maintained and comes with extensive documentation and detailed tutorials. You find add-ins for <a href="https://support.rstudio.com/hc/en-us/articles/200532077-Version-Control-with-Git-and-SVN">version control</a>, <a href="http://shiny.rstudio.com/">Shiny</a> apps, writing books or other documents (<a href="https://bookdown.org/yihui/bookdown/">bookdown</a>) and you can write presentations directly in R Markdown, including code + output and everything as <a href="http://rmarkdown.rstudio.com/beamer_presentation_format.html">LaTeX beamer presentations</a>, <a href="http://rmarkdown.rstudio.com/ioslides_presentation_format.html">ioslides</a> or <a href="http://rmarkdown.rstudio.com/revealjs_presentation_format.html">reveal.js</a>. You can also create <a href="http://rmarkdown.rstudio.com/flexdashboard/">Dashboards</a>, include interactive <a href="http://rmarkdown.rstudio.com/developer_html_widgets.html">HTML widgets</a> and you can even build your blog (as this one is) with <a href="https://bookdown.org/yihui/blogdown/">blogdown</a> conveniently from within RStudio!</p></li>
<li><p>If you are looking for advanced functionality, it is very likely that somebody has already written a package for it. There are packages that allow you to access <a href="https://spark.rstudio.com/">Spark</a>, <a href="https://cran.r-project.org/web/packages/h2o/index.html">H2O</a>, <a href="https://ropensci.org/tutorials/elastic_tutorial.html">elasticsearch</a>, <a href="https://tensorflow.rstudio.com/">TensorFlow</a>, <a href="https://tensorflow.rstudio.com/keras/">Keras</a>, <a href="https://ropensci.org/blog/blog/2016/11/16/tesseract">tesseract</a>, and so many more with no hassle at all. And you can even run <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/system2.html">bash</a>, <a href="https://github.com/rstudio/reticulate">Python</a> from within R!</p></li>
<li><p>There is a big - and very active - community! This is one of the things I most enjoy about working with R. You can find many high-quality <a href="https://cran.r-project.org/manuals.html">manuals</a>, <a href="https://cran.r-project.org/other-docs.html">resources</a> and tutorials for all kinds of topics. Most of them provided free of charge by people who often dedicate their spare time to help others. The same goes for asking questions on <a href="https://stackoverflow.com/questions/tagged/r">Stack Overflow</a>, putting up issues on <a href="https://github.com/">Github</a> or <a href="https://groups.google.com/forum/#!forum/r-help-archive">Google groups</a>: usually you will get several answers within a short period of time (from my experience minutes to hours). What other community is so supportive and so helpful!? But for most things, you wouldn’t even need to ask for help because many of the packages come with absolutely amazing vignettes, that describe the functions and workflows in a detailed, yet easy to understand way. If that’s not enough, you will very likely find additional tutorials on <a href="https://www.r-bloggers.com/">R-bloggers</a>, a site maintained by Tal Galili that aggregates hundreds of R-blogs. There are several <a href="https://www.r-project.org/conferences.html">R Conferences</a>, like the <a href="https://user2018.r-project.org/">useR</a>, <a href="https://ropensci.org/community/events.html">rOpenSci Unconference</a> and many <a href="https://jumpingrivers.github.io/meetingsR/r-user-groups.html">R-user groups</a> all around the globe.</p></li>
</ol>
<p>I can’t stress enough how much I appreciate all the people who are involved in the R-community; who write packages, tutorials, blogs, who share information, provide support and who think about how to make data analysis easy, more convenient and - dare I say - fun!</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/circle-159252_1280_mfs0ku.png" alt="Community is everything!" />
<p class="caption">Community is everything!</p>
</div>
<p>The main drawbacks I experience with R are that scripts tends to be harder to deploy than Python (<a href="https://www.microsoft.com/en-us/cloud-platform/r-server">R-server</a> might be a solution, but I don’t know enough about it to really judge). Dealing with memory, space and security issues is often difficult in R. But there has already been a vast improvement over the last months/years, so I’m sure we will see development there in the future…</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Welcome to my page!]]></title>
    <link href="/page/about/"/>
    <id>/page/about/</id>
    <published>2017-09-12T16:06:06+02:00</published>
    <updated>2017-09-12T16:06:06+02:00</updated>
    <content type="html"><![CDATA[

<p><img src="/img/Bewerbungsfoto_klein.jpg" alt="" /></p>

<p>I&rsquo;m Shirin, a biologist turned bioinformatician turned data scientist.</p>

<p>I&rsquo;m especially interested in machine learning and data visualization. While I am using R most every day at work, I wanted to have an incentive to regularly explore other types of analyses and other types of data that I don&rsquo;t normally work with. I have also very often benefited from other people&rsquo;s published code in that it gave me ideas for my own work; and I hope that sharing my own analyses will inspire others as much as I often am by what can be be done with data.  It&rsquo;s amazing to me what can be learned from analyzing and visualizing data!</p>

<p>My tool of choice for data analysis so far has been R. I also organize the <a href="https://shiring.github.io/r_users_group/2017/05/20/muenster_r_user_group">MünsteR R-users group on meetup.com</a>.</p>

<p><img src="http://res.cloudinary.com/shiring/image/upload/v1511852499/my_story_wml3zm.png" alt="My journey to Data Science" /></p>

<p>I love dancing and used to do competitive ballroom and latin dancing. Even though I don&rsquo;t have time for that anymore, I still enjoy teaching &ldquo;social dances&rdquo; once a week with the Hochschulsport (university sports courses).</p>

<p>I created the R package <a href="https://github.com/ShirinG/exprAnalysis">exprAnalysis</a>, designed to streamline my RNA-seq data analysis pipeline. It is available via Github. Instructions for installation and usage can be found <a href="https://shiring.github.io/rna-seq/microarray/2016/09/28/exprAnalysis">here</a>.</p>

<p>This blog will showcase some of the analyses I have been doing with different data sets (all freely available). I will also host teaching materials for students to access in conjunction with R courses I am giving.</p>

<hr />

<h2 id="contact-me">Contact me:</h2>

<ul>
<li><a href="https://www.codecentric.de/team/shirin-glander/">Codecentric AG</a></li>
<li><a href="mailto:shirin.glander@gmail.com">Email</a></li>
<li><a href="http://www.xing.com/profile/Shirin_Glander">Xing</a></li>
<li><a href="http://de.linkedin.com/in/shirin-glander-01120881">Linkedin</a></li>
<li><a href="http://twitter.com/ShirinGlander">Twitter</a></li>
</ul>

<hr />

<p>Also check out <a href="http://www.R-bloggers.com">R-bloggers</a> for lots of cool R stuff!</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Moving my blog to blogdown]]></title>
    <link href="/2017/09/moving-my-blog-to-blogdown/"/>
    <id>/2017/09/moving-my-blog-to-blogdown/</id>
    <published>2017-09-12T00:00:00+00:00</published>
    <updated>2017-09-12T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>It’s been a long time coming but I finally moved my blog from Jekyll/Bootstrap on Github pages to blogdown, Hugo and <a href="https://www.netlify.com/">Netlify</a>! Moreover, I also now have my own domain name <a href="https://www.shirin-glander.de">www.shirin-glander.de</a>. :-)</p>
<p>I followed the <a href="https://bookdown.org/yihui/blogdown/">blogdown ebook</a> to set up my blog. I chose Thibaud Leprêtre’s <a href="https://themes.gohugo.io/hugo-tranquilpeak-theme/">tranquilpeak theme</a>. It looks much more polished than my old blog.</p>
<p>My old blog will remain where it is, so that all the links that are out there will still work (and I don’t have to go through the hassle of migrating all my posts to my new site). You find a link to my old site in the sidebar.</p>
<p><br></p>
<hr />
<p>Just to test that everything works, I run the example code:</p>
<div id="r-markdown" class="section level1">
<h1>R Markdown</h1>
<p>This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <a href="http://rmarkdown.rstudio.com" class="uri">http://rmarkdown.rstudio.com</a>.</p>
<p>You can embed an R code chunk like this:</p>
<pre class="r"><code>summary(cars)
##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00
fit &lt;- lm(dist ~ speed, data = cars)
fit
## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Coefficients:
## (Intercept)        speed  
##     -17.579        3.932</code></pre>
</div>
<div id="including-plots" class="section level1">
<h1>Including Plots</h1>
<p>You can also embed plots. See Figure <a href="#fig:pie">1</a> for example:</p>
<pre class="r"><code>par(mar = c(0, 1, 0, 1))
pie(
  c(280, 60, 20),
  c(&#39;Sky&#39;, &#39;Sunny side of pyramid&#39;, &#39;Shady side of pyramid&#39;),
  col = c(&#39;#0292D8&#39;, &#39;#F7EA39&#39;, &#39;#C4B632&#39;),
  init.angle = -50, border = NA
)</code></pre>
<div class="figure"><span id="fig:pie"></span>
<img src="/post/2017-09-12-moving-my-blog-to-blogdown_files/figure-html/pie-1.png" alt="A fancy pie chart." width="672" />
<p class="caption">
Figure 1: A fancy pie chart.
</p>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Data Science for Fraud Detection]]></title>
    <link href="/2017/09/data-science-fraud-detection/"/>
    <id>/2017/09/data-science-fraud-detection/</id>
    <published>2017-09-06T00:00:00+00:00</published>
    <updated>2017-09-06T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I have written the following post about <a href="https://blog.codecentric.de/en/2017/09/data-science-fraud-detection/">Data Science for Fraud Detection</a> at my company <a href="https://blog.codecentric.de/en/">codecentric</a>&rsquo;s blog:</p>

<blockquote>
<p>Fraud can be defined as “the crime of getting money by deceiving people” (Cambridge Dictionary); it is as old as humanity: whenever two parties exchange goods or conduct business there is the potential for one party scamming the other. With an ever increasing use of the internet for shopping, banking, filing insurance claims, etc. these businesses have become targets of fraud in a whole new dimension. Fraud has become a major problem in e-commerce and a lot of resources are being invested to recognize and prevent it.</p>

<p>Traditional approaches to identifying fraud have been rule-based. This means that hard and fast rules for flagging a transaction as fraudulent have to be established manually and in advance. But this system isn’t flexible and inevitably results in an arms-race between the seller’s fraud detection system and criminals finding ways to circumnavigate these rules. The modern alternative is to leverage the vast amounts of Big Data that can be collected from online transactions and model it in a way that allows us to flag or predict fraud in future transactions. For this, Data Science and Machine Learning techniques, like Deep Neural Networks (DNNs), are the obvious solution!</p>

<p>Here, I am going to show an example of how Data Science techniques can be used to identify fraud in financial transactions. I will offer some insights into the inner workings of fraud analysis, aimed at non-experts to understand.</p>
</blockquote>

<p>Continue reading at <a href="https://blog.codecentric.de/en/2017/09/data-science-fraud-detection/">https://blog.codecentric.de/en/2017/09/data-science-fraud-detection/</a>&hellip;</p>

<p>The blog post is <a href="https://blog.codecentric.de/2017/09/fraud-analyse-mit-data-science-techniken/">also available in German</a>.</p>

<p><img src="https://shiring.github.io/netlify_images/r_mse_gklfsi.png" alt="" /></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Migrating from GitHub to GitLab with RStudio (Tutorial)]]></title>
    <link href="/2017/09/migrating-github-gitlab/"/>
    <id>/2017/09/migrating-github-gitlab/</id>
    <published>2017-09-04T00:00:00+00:00</published>
    <updated>2017-09-04T00:00:00+00:00</updated>
    <content type="html"><![CDATA[

<h2 id="github-vs-gitlab">GitHub vs. GitLab</h2>

<p>Git is a distributed implementation of version control. Many people have written very eloquently about why it is a good idea to use version control, not only if you collaborate in a team but also if you work on your own; one example is <a href="https://support.rstudio.com/hc/en-us/articles/200532077?version=1.0.153&amp;mode=desktop">this article from RStudio&rsquo;s Support pages</a>.</p>

<p>In short, its main feature is that version control allows you to keep track of the changes you make to your code. It will also keep a history of all the changes you have made in the past and allows you to go back to specific versions if you made a major mistake. And Git makes collaborating on the same code very easy.</p>

<p>Most R packages are also hosted on <a href="https://github.com/">GitHub</a>. You can check out their R code in the repositories if you want to get a deeper understanding of the functions, you can install the latest development versions of packages or install packages that are not on CRAN. The issue tracker function of GitHub also makes it easy to report and respond to issues/problems with your code.</p>

<h3 id="why-would-you-want-to-leave-github">Why would you want to leave GitHub?</h3>

<p>Public repositories are free on GitHub but you need to pay for private repos (if you are a student or work in academia, you <a href="https://education.github.com/discount_requests/new">get private repos for free</a>). Since I switched from academia to industry lately and no longer fulfil these criteria, all my private repos would have to be switched to public in the future. Here, GitLab is a great alternative!</p>

<p><a href="https://gitlab.com/">GitLab</a> offers very similar functionalities as GitHub. There are <a href="https://www.slant.co/versus/532/4860/~github_vs_gitlab">many pros and cons for using GitHub versus GitLab</a> but for me, the selling point was that GitLab offers unlimited private projects and collaborators in its free plan.</p>

<p><br></p>

<h1 id="tutorial">Tutorial</h1>

<p>Migrating from GitHub to <a href="https://gitlab.com/">GitLab</a> with RStudio is very easy! Here, I will show how I migrated my GitHub repositories of R projects, that I work with from within RStudio, to GitLab.</p>

<p><img src="https://shiring.github.io/netlify_images/GitLab_logo_yej6ht.png" alt="" /></p>

<p>Beware, that ALL code snippets below show Terminal code (they are NOT from the R console)!</p>

<p><br></p>

<h2 id="migrating-existing-repositories">Migrating existing repositories</h2>

<p>You first need to set up your GitLab account (you can login with your GitHub account) and connect your old GitHub account. Under <a href="https://gitlab.com/profile/account">Settings &amp;Account</a>, you will find &ldquo;Social sign-in&rdquo;; here click on &ldquo;Connect&rdquo; next to the GitHub symbol (if you signed in with your GitHub account, it will already be connected).</p>

<p>Once you have done this, you can import all your GitHub repositories to GitLab. To do this, you first need to create a new project. Click on the drop-down arrow next to the plus sign in the top-right corner and select &ldquo;New project&rdquo;. This will open the following window:</p>

<p><img src="https://shiring.github.io/netlify_images/Bildschirmfoto1_yuc7gb.png" alt="" /></p>

<p>Here, choose &ldquo;Import project from GitHub&rdquo; and choose the repositories you want to import.</p>

<p>If you go into one of your repositories, GitLab will show you a message at the top of the site that tells you that you need to add an SSH key. The SSH key is used for secure communication between the GitLab server and your computer when you want to share information, like push/pull commits.</p>

<p><img src="https://shiring.github.io/netlify_images/Bildschirmfoto2_diwetw.png" alt="" /></p>

<p>If you already work with GitHub on your computer, you will have an SSH key set up and you can <a href="https://gitlab.com/profile/keys">copy your public SSH key to GitLab</a>. Follow the instructions <a href="https://gitlab.com/help/ssh/README">here</a>.</p>

<p>Here is how you do it on a Mac:</p>

<ol>
<li>Look for your public key and copy it to the clipboard</li>
</ol>

<!-- -->

<pre><code>cat ~/.ssh/id_rsa.pub
pbcopy &lt; ~/.ssh/id_rsa.pub
</code></pre>

<p>Then paste it into the respective field <a href="https://gitlab.com/profile/keys">here</a>.</p>

<p>The next step is to change the remote URL for pushing/pulling your project from RStudio. In your Git window (tab next to &ldquo;Environment&rdquo; and &ldquo;History&rdquo; for me), click on Settings and &ldquo;Shell&rdquo;.</p>

<p><img src="https://shiring.github.io/netlify_images/Bildschirmfoto3_ydklnw.png" alt="" /></p>

<p>Then write in the shell window that opened:</p>

<pre><code>git remote set-url origin git@&lt;GITLABHOST&gt;:&lt;ORGNAME&gt;/&lt;REPO&gt;.git
</code></pre>

<p>You can copy the link in the navigation bar of your repo on GitLab.</p>

<p><img src="https://shiring.github.io/netlify_images/Bildschirmfoto4_dheikm.png" alt="" /></p>

<p>Check that you now have the correct new gitlab path by going to &ldquo;Tools&rdquo;, &ldquo;Project Options&rdquo; and &ldquo;Git/SVN&rdquo;.</p>

<p>Also check your SSH key configuration with:</p>

<pre><code>ssh -T git@&lt;GITLABHOST&gt;
</code></pre>

<p>If you get the following message</p>

<pre><code>The authenticity of host 'gitlab.com (52.167.219.168)' can't be established.
ECDSA key fingerprint is ...
Are you sure you want to continue connecting (yes/no)?
</code></pre>

<p>type &ldquo;yes&rdquo; (and enter passphrase if prompted).</p>

<p>If everything is okay, you now get a message saying <code>Welcome to GitLab!</code></p>

<p>Now, you can commit, push and pull from within RStudio just as you have done before!</p>

<p><br></p>

<h2 id="in-case-of-problems-with-pushing-pulling">In case of problems with pushing/pulling</h2>

<p>In my case, I migrated both, my private as well as my company&rsquo;s GitHub repos to GitLab. While my private repos could be migrated without a hitch, migrating my company&rsquo;s repos was a bit more tricky (because they had additional security settings, I assume).</p>

<p>Here is how I solved this problem with my company&rsquo;s repos:</p>

<p>I have protected my SSH key with a passphrase. When pushing or pulling commits via the shell with <code>git pull</code> and <code>git push origin master</code>, I am prompted to enter my passphrase and everything works fine. Pushing/pulling from within RStudio, however, threw an error:</p>

<pre><code>ssh_askpass: exec(/usr/X11R6/bin/ssh-askpass): No such file or directory
Permission denied (publickey).
fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.
</code></pre>

<p>I am using a MacBook Pro with MacOS Sierra version 10.12.6, so this might not be an issue with another operating system.</p>

<p>The following solution worked for me:</p>

<ol>
<li>Add your SSH key</li>
</ol>

<!-- -->

<pre><code>ssh-add ~/.ssh/id_rsa
</code></pre>

<ol>
<li>And reinstall <a href="https://vscode-eastus.azurewebsites.net/docs/setup/mac">VS Code</a></li>
</ol>

<p>Now I could commit, push and pull from within RStudio just as before!</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Social Network Analysis and Topic Modeling of codecentric’s Twitter friends and followers]]></title>
    <link href="/2017/07/twitter-analysis-codecentric/"/>
    <id>/2017/07/twitter-analysis-codecentric/</id>
    <published>2017-07-28T00:00:00+00:00</published>
    <updated>2017-07-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I have written the following post about <a href="https://blog.codecentric.de/en/2017/07/combining-social-network-analysis-topic-modeling-characterize-codecentrics-twitter-friends-followers/">Social Network Analysis and Topic Modeling of codecentric&rsquo;s Twitter friends and followers</a> for <a href="https://blog.codecentric.de/en/">codecentric</a>&rsquo;s blog:</p>

<blockquote>
<p>Recently, Matthias Radtke has written a very nice blog post on Topic Modeling of the codecentric Blog Articles, where he is giving a comprehensive introduction to Topic Modeling. In this article I am showing a real-world example of how we can use Data Science to gain insights from text data and social network analysis.</p>

<p>I am using publicly available Twitter data to characterize codecentric&rsquo;s friends and followers for identifying the most &ldquo;influential&rdquo; followers and using text analysis tools like sentiment analysis to characterize their interests from their user descriptions, performing Social Network Analysis on friends, followers and a subset of second degree connections to identify key players who will be able to pass on information to a wide reach of other users and combing this network analysis with topic modeling to identify meta-groups with similar interests.</p>

<p>Knowing the interests and social network positions of our followers allows us to identify key users who are likely to retweet posts that fall within their range of interests and who will reach a wide audience.</p>

<p>&hellip;</p>
</blockquote>

<p>Continue reading at <a href="https://blog.codecentric.de/en/2017/07/combining-social-network-analysis-topic-modeling-characterize-codecentrics-twitter-friends-followers/">https://blog.codecentric.de/en/2017/07/combining-social-network-analysis-topic-modeling-characterize-codecentrics-twitter-friends-followers/</a>&hellip;</p>

<p>The entire analysis has been done in R 3.4.0 and you can find my code on <a href="https://github.com/ShirinG/blog_posts_prep/blob/master/twitter/twitter_codecentric.Rmd">Github</a>.</p>

<p><img src="https://shiring.github.io/netlify_images/twitter_net_topics_lnu3j9.png" alt="" /></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Find all my other posts on my old website!]]></title>
    <link href="/2017/07/find-all-my-other-posts-on-my-old-website/"/>
    <id>/2017/07/find-all-my-other-posts-on-my-old-website/</id>
    <published>2017-07-01T00:00:00+00:00</published>
    <updated>2017-07-01T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>For all my other posts, see my old website:
<a href="https://shiring.github.io">shiring.github.io</a></p>
]]></content>
  </entry>
</feed>