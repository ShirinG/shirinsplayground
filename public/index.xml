<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Shirin&#39;s playgRound</title>
  <link href="/index.xml" rel="self"/>
  <link href="/"/>
  <updated>2018-09-17T00:00:00+00:00</updated>
  <id>/</id>
  <author>
    <name>Dr. Shirin Glander</name>
  </author>
  <generator>Hugo -- gohugo.io</generator>
  <entry>
    <title type="html"><![CDATA[Code for case study - Customer Churn with Keras/TensorFlow and H2O]]></title>
    <link href="/2018/09/customer_churn_code/"/>
    <id>/2018/09/customer_churn_code/</id>
    <published>2018-09-17T00:00:00+00:00</published>
    <updated>2018-09-17T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>This is code that accompanies a <a href="https://www.dpunkt.de/buecher/13208/9783864906107-data-science.html">book</a> chapter on customer churn that I have written for dpunkt Verlag. The book is in German and will probably appear in December: <a href="https://www.dpunkt.de/buecher/13208/9783864906107-data-science.html" class="uri">https://www.dpunkt.de/buecher/13208/9783864906107-data-science.html</a>.</p>
<div id="inspiration-sources" class="section level1">
<h1>Inspiration &amp; Sources</h1>
<p>Thank you to the following people for providing excellent code examples about customer churn:</p>
<ul>
<li>Matt Dancho: <a href="http://www.business-science.io/business/2017/11/28/customer_churn_analysis_keras.html" class="uri">http://www.business-science.io/business/2017/11/28/customer_churn_analysis_keras.html</a></li>
<li>JJ Allaire: <a href="https://github.com/rstudio/keras-customer-churn" class="uri">https://github.com/rstudio/keras-customer-churn</a></li>
<li>Susan Li: <a href="https://towardsdatascience.com/predict-customer-churn-with-r-9e62357d47b4" class="uri">https://towardsdatascience.com/predict-customer-churn-with-r-9e62357d47b4</a></li>
<li>John Sullivan: <a href="https://jtsulliv.github.io/churn-eda/" class="uri">https://jtsulliv.github.io/churn-eda/</a></li>
</ul>
</div>
<div id="setup" class="section level1">
<h1>Setup</h1>
<p>All analyses are done in R using RStudio. For detailed session information including R version, operating system and package versions, see the <code>sessionInfo()</code> output at the end of this document.</p>
<p>All figures are produced with ggplot2.</p>
<ul>
<li>libraries</li>
</ul>
<pre class="r"><code># Load libraries
library(tidyverse) # for tidy data analysis
library(readr)     # for fast reading of input files
library(caret)     # for convenient splitting
library(mice)      # mice package for Multivariate Imputation by Chained Equations (MICE)
library(keras)     # for neural nets
library(lime)      # for explaining neural nets
library(rsample)   # for splitting training and test data
library(recipes)   # for preprocessing
library(yardstick) # for evaluation
library(ggthemes)  # for additional plotting themes
library(corrplot)  # for correlation

theme_set(theme_minimal())</code></pre>
<pre class="r"><code># Install Keras if you have not installed it before
# follow instructions if you haven&#39;t installed TensorFlow
install_keras()</code></pre>
<p><br></p>
</div>
<div id="data-preparation" class="section level1 tabset tabset-fade tabset-pills">
<h1>Data preparation</h1>
<div id="the-dataset" class="section level2">
<h2>The dataset</h2>
<p>The Telco Customer Churn data set is the same one that Matt Dancho used in his post (see above). It was downloaded from <a href="https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/">IBM Watson</a></p>
<pre class="r"><code>churn_data_raw &lt;- read_csv(&quot;WA_Fn-UseC_-Telco-Customer-Churn.csv&quot;)</code></pre>
<pre class="r"><code>head(churn_data_raw)</code></pre>
<pre><code>## # A tibble: 6 x 21
##   customerID gender SeniorCitizen Partner Dependents tenure PhoneService
##   &lt;chr&gt;      &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;int&gt; &lt;chr&gt;       
## 1 7590-VHVEG Female             0 Yes     No              1 No          
## 2 5575-GNVDE Male               0 No      No             34 Yes         
## 3 3668-QPYBK Male               0 No      No              2 Yes         
## 4 7795-CFOCW Male               0 No      No             45 No          
## 5 9237-HQITU Female             0 No      No              2 Yes         
## 6 9305-CDSKC Female             0 No      No              8 Yes         
## # ... with 14 more variables: MultipleLines &lt;chr&gt;, InternetService &lt;chr&gt;,
## #   OnlineSecurity &lt;chr&gt;, OnlineBackup &lt;chr&gt;, DeviceProtection &lt;chr&gt;,
## #   TechSupport &lt;chr&gt;, StreamingTV &lt;chr&gt;, StreamingMovies &lt;chr&gt;,
## #   Contract &lt;chr&gt;, PaperlessBilling &lt;chr&gt;, PaymentMethod &lt;chr&gt;,
## #   MonthlyCharges &lt;dbl&gt;, TotalCharges &lt;dbl&gt;, Churn &lt;chr&gt;</code></pre>
<pre class="r"><code>dim(churn_data_raw)</code></pre>
<pre><code>## [1] 7043   21</code></pre>
<div id="eda" class="section level3">
<h3>EDA</h3>
<ul>
<li>proportion of churn</li>
</ul>
<pre class="r"><code>churn_data_raw %&gt;%
  count(Churn)</code></pre>
<pre><code>## # A tibble: 2 x 2
##   Churn     n
##   &lt;chr&gt; &lt;int&gt;
## 1 No     5174
## 2 Yes    1869</code></pre>
<ul>
<li>plot categorical features</li>
</ul>
<pre class="r"><code>churn_data_raw %&gt;%
  mutate(SeniorCitizen = as.character(SeniorCitizen)) %&gt;%
  select(-customerID) %&gt;%
  select_if(is.character) %&gt;%
  select(Churn, everything()) %&gt;%
  gather(x, y, gender:PaymentMethod) %&gt;%
  count(Churn, x, y) %&gt;%
  ggplot(aes(x = y, y = n, fill = Churn, color = Churn)) +
    facet_wrap(~ x, ncol = 4, scales = &quot;free&quot;) +
    geom_bar(stat = &quot;identity&quot;, alpha = 0.5) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
          legend.position = &quot;top&quot;) +
    scale_color_tableau() +
    scale_fill_tableau()</code></pre>
<p><img src="/post/2018-09-17_customer_churn_code_files/figure-html/eda_chr-1.png" width="1152" /></p>
<ul>
<li>plot numerical features</li>
</ul>
<pre class="r"><code>churn_data_raw %&gt;%
  select(-customerID) %&gt;%
  #select_if(is.numeric) %&gt;%
  select(Churn, MonthlyCharges, tenure, TotalCharges) %&gt;%
  gather(x, y, MonthlyCharges:TotalCharges) %&gt;%
  ggplot(aes(x = y, fill = Churn, color = Churn)) +
    facet_wrap(~ x, ncol = 3, scales = &quot;free&quot;) +
    geom_density(alpha = 0.5) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
          legend.position = &quot;top&quot;) +
    scale_color_tableau() +
    scale_fill_tableau()</code></pre>
<p><img src="/post/2018-09-17_customer_churn_code_files/figure-html/eda_num-1.png" width="1152" /></p>
<pre class="r"><code>churn_data &lt;- churn_data_raw %&gt;%
  select(-customerID)</code></pre>
</div>
<div id="dealing-with-missing-values" class="section level3">
<h3>Dealing with missing values</h3>
<pre class="r"><code>md.pattern(churn_data, plot = FALSE)</code></pre>
<pre><code>##      gender SeniorCitizen Partner Dependents tenure PhoneService
## 7032      1             1       1          1      1            1
## 11        1             1       1          1      1            1
##           0             0       0          0      0            0
##      MultipleLines InternetService OnlineSecurity OnlineBackup
## 7032             1               1              1            1
## 11               1               1              1            1
##                  0               0              0            0
##      DeviceProtection TechSupport StreamingTV StreamingMovies Contract
## 7032                1           1           1               1        1
## 11                  1           1           1               1        1
##                     0           0           0               0        0
##      PaperlessBilling PaymentMethod MonthlyCharges Churn TotalCharges   
## 7032                1             1              1     1            1  0
## 11                  1             1              1     1            0  1
##                     0             0              0     0           11 11</code></pre>
<pre class="r"><code>imp &lt;- mice(data = churn_data,  print = FALSE)
train_data_impute &lt;- complete(imp, &quot;long&quot;)</code></pre>
<pre class="r"><code>churn_data &lt;- churn_data %&gt;%
  drop_na()</code></pre>
</div>
</div>
<div id="training-and-test-split" class="section level2">
<h2>Training and test split</h2>
<pre class="r"><code>set.seed(42)
index &lt;- createDataPartition(churn_data$Churn, p = 0.75, list = FALSE)</code></pre>
<pre class="r"><code>train_data &lt;- churn_data[index, ]
test_data  &lt;- churn_data[-index, ]

index2 &lt;- createDataPartition(train_data$Churn, p = 0.8, list = FALSE)

train_data &lt;- train_data[index2, ]
valid_data &lt;- train_data[-index2, ]</code></pre>
<pre class="r"><code>nrow(train_data)</code></pre>
<pre><code>## [1] 4221</code></pre>
<pre class="r"><code>nrow(valid_data)</code></pre>
<pre><code>## [1] 848</code></pre>
<pre class="r"><code>nrow(test_data)</code></pre>
<pre><code>## [1] 1757</code></pre>
</div>
<div id="pre-processing" class="section level2">
<h2>Pre-Processing</h2>
<blockquote>
<p>A recipe is a description of what steps should be applied to a data set in order to get it ready for data analysis.</p>
</blockquote>
<pre class="r"><code>recipe_churn &lt;- recipe(Churn ~ ., train_data) %&gt;%
  step_dummy(all_nominal(), -all_outcomes()) %&gt;%
  step_center(all_predictors(), -all_outcomes()) %&gt;%
  step_scale(all_predictors(), -all_outcomes()) %&gt;%
  prep(data = train_data)</code></pre>
<pre class="r"><code>train_data &lt;- bake(recipe_churn, newdata = train_data) %&gt;%
  select(Churn, everything())

valid_data &lt;- bake(recipe_churn, newdata = valid_data) %&gt;%
  select(Churn, everything())

test_data &lt;- bake(recipe_churn, newdata = test_data) %&gt;%
  select(Churn, everything())</code></pre>
<pre class="r"><code>train_y_drop &lt;- to_categorical(as.integer(train_data$Churn) - 1, 2)
colnames(train_y_drop) &lt;- c(&quot;No&quot;, &quot;Yes&quot;)

valid_y_drop &lt;- to_categorical(as.integer(valid_data$Churn) - 1, 2)
colnames(valid_y_drop) &lt;- c(&quot;No&quot;, &quot;Yes&quot;)

test_y_drop &lt;- to_categorical(as.integer(test_data$Churn) - 1, 2)
colnames(test_y_drop) &lt;- c(&quot;No&quot;, &quot;Yes&quot;)</code></pre>
<pre class="r"><code># if training with binary crossentropy
train_y_drop &lt;- train_y_drop[, 2, drop = FALSE]
head(train_y_drop)</code></pre>
<pre><code>##      Yes
## [1,]   1
## [2,]   1
## [3,]   0
## [4,]   1
## [5,]   0
## [6,]   0</code></pre>
<pre class="r"><code>valid_y_drop &lt;- valid_y_drop[, 2, drop = FALSE]
test_y_drop &lt;- test_y_drop[, 2, drop = FALSE]</code></pre>
<pre class="r"><code>train_data_bk &lt;- select(train_data, -Churn)
head(train_data_bk)</code></pre>
<pre><code>## # A tibble: 6 x 30
##   SeniorCitizen tenure MonthlyCharges TotalCharges gender_Male Partner_Yes
##           &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;
## 1        -0.439 -1.23          -0.356       -0.956       0.973      -0.965
## 2        -0.439 -1.23           0.207       -0.937      -1.03       -0.965
## 3        -0.439 -0.904         -1.16        -0.870      -1.03       -0.965
## 4        -0.439 -0.167          1.35         0.356      -1.03        1.04 
## 5        -0.439  1.22          -0.279        0.554       0.973      -0.965
## 6        -0.439 -0.781         -0.487       -0.742       0.973       1.04 
## # ... with 24 more variables: Dependents_Yes &lt;dbl&gt;,
## #   PhoneService_Yes &lt;dbl&gt;, MultipleLines_No.phone.service &lt;dbl&gt;,
## #   MultipleLines_Yes &lt;dbl&gt;, InternetService_Fiber.optic &lt;dbl&gt;,
## #   InternetService_No &lt;dbl&gt;, OnlineSecurity_No.internet.service &lt;dbl&gt;,
## #   OnlineSecurity_Yes &lt;dbl&gt;, OnlineBackup_No.internet.service &lt;dbl&gt;,
## #   OnlineBackup_Yes &lt;dbl&gt;, DeviceProtection_No.internet.service &lt;dbl&gt;,
## #   DeviceProtection_Yes &lt;dbl&gt;, TechSupport_No.internet.service &lt;dbl&gt;,
## #   TechSupport_Yes &lt;dbl&gt;, StreamingTV_No.internet.service &lt;dbl&gt;,
## #   StreamingTV_Yes &lt;dbl&gt;, StreamingMovies_No.internet.service &lt;dbl&gt;,
## #   StreamingMovies_Yes &lt;dbl&gt;, Contract_One.year &lt;dbl&gt;,
## #   Contract_Two.year &lt;dbl&gt;, PaperlessBilling_Yes &lt;dbl&gt;,
## #   PaymentMethod_Credit.card..automatic. &lt;dbl&gt;,
## #   PaymentMethod_Electronic.check &lt;dbl&gt;, PaymentMethod_Mailed.check &lt;dbl&gt;</code></pre>
<pre class="r"><code>valid_data_bk &lt;- select(valid_data, -Churn)
test_data_bk &lt;- select(test_data, -Churn)</code></pre>
<pre class="r"><code>train_data$Churn &lt;- ifelse(train_data$Churn == &quot;Yes&quot;, 1, 0)
valid_data$Churn &lt;- ifelse(valid_data$Churn == &quot;Yes&quot;, 1, 0)
test_data$Churn &lt;- ifelse(test_data$Churn == &quot;Yes&quot;, 1, 0)</code></pre>
<pre class="r"><code># alternative - not run
train_data_impute_bk &lt;- bake(recipe_churn, newdata = train_data_impute) %&gt;%
  select(Churn, everything())
train_data_impute_bk &lt;- cbind(train_data_impute$.imp, train_data_impute_bk)</code></pre>
</div>
<div id="modeling-with-keras" class="section level2">
<h2>Modeling with keras</h2>
<pre class="r"><code>model_keras &lt;- keras_model_sequential()

model_keras %&gt;% 
  layer_dense(units = 32, kernel_initializer = &quot;uniform&quot;, activation = &quot;relu&quot;, 
              input_shape = ncol(train_data_bk)) %&gt;% 
  layer_dropout(rate = 0.2) %&gt;%
  
  layer_dense(units = 16, kernel_initializer = &quot;uniform&quot;, activation = &quot;relu&quot;) %&gt;% 
  layer_dropout(rate = 0.2) %&gt;%
  
  layer_dense(units = 8, kernel_initializer = &quot;uniform&quot;, activation = &quot;relu&quot;) %&gt;% 
  layer_dropout(rate = 0.2) %&gt;%

  layer_dense(units = 1,
              kernel_initializer = &quot;uniform&quot;, activation = &quot;sigmoid&quot;) %&gt;%
  
  compile(
        optimizer = &#39;adamax&#39;,
        loss      = &#39;binary_crossentropy&#39;,
        metrics   = c(&quot;binary_accuracy&quot;, &quot;mse&quot;)
    )

summary(model_keras)</code></pre>
<pre><code>## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## dense_1 (Dense)                  (None, 32)                    992         
## ___________________________________________________________________________
## dropout_1 (Dropout)              (None, 32)                    0           
## ___________________________________________________________________________
## dense_2 (Dense)                  (None, 16)                    528         
## ___________________________________________________________________________
## dropout_2 (Dropout)              (None, 16)                    0           
## ___________________________________________________________________________
## dense_3 (Dense)                  (None, 8)                     136         
## ___________________________________________________________________________
## dropout_3 (Dropout)              (None, 8)                     0           
## ___________________________________________________________________________
## dense_4 (Dense)                  (None, 1)                     9           
## ===========================================================================
## Total params: 1,665
## Trainable params: 1,665
## Non-trainable params: 0
## ___________________________________________________________________________</code></pre>
<pre class="r"><code>fit_keras &lt;- fit(model_keras, 
    x = as.matrix(train_data_bk), 
    y = train_y_drop,
    batch_size = 32, 
    epochs = 20,
    #validation_split = 0.30,
    validation_data = list(as.matrix(valid_data_bk), valid_y_drop),
    verbose = 2
    )</code></pre>
<pre class="r"><code>plot(fit_keras) +
  scale_color_tableau() +
  scale_fill_tableau()</code></pre>
<p><img src="/post/2018-09-17_customer_churn_code_files/figure-html/plot_fit_keras-1.png" width="960" /></p>
</div>
<div id="evaluation" class="section level2">
<h2>Evaluation</h2>
<pre class="r"><code>pred_classes_test &lt;- predict_classes(object = model_keras, x = as.matrix(test_data_bk))
pred_proba_test  &lt;- predict_proba(object = model_keras, x = as.matrix(test_data_bk))</code></pre>
<pre class="r"><code>test_results &lt;- tibble(
  actual_yes = as.factor(as.vector(test_y_drop)),
  pred_classes_test = as.factor(as.vector(pred_classes_test)),
  Yes = as.vector(pred_proba_test), 
  No = 1 - as.vector(pred_proba_test))
head(test_results)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   actual_yes pred_classes_test    Yes    No
##   &lt;fct&gt;      &lt;fct&gt;              &lt;dbl&gt; &lt;dbl&gt;
## 1 0          0                 0.441  0.559
## 2 0          0                 0.0326 0.967
## 3 1          1                 0.658  0.342
## 4 0          0                 0.454  0.546
## 5 1          0                 0.198  0.802
## 6 1          0                 0.367  0.633</code></pre>
<pre class="r"><code>test_results %&gt;% 
  conf_mat(actual_yes, pred_classes_test)</code></pre>
<pre><code>##           Truth
## Prediction    0    1
##          0 1174  228
##          1  116  239</code></pre>
<pre class="r"><code>test_results %&gt;% 
  metrics(actual_yes, pred_classes_test)</code></pre>
<pre><code>## # A tibble: 1 x 1
##   accuracy
##      &lt;dbl&gt;
## 1    0.804</code></pre>
<pre class="r"><code>test_results %&gt;% 
  roc_auc(actual_yes, Yes)</code></pre>
<pre><code>## [1] 0.8390908</code></pre>
<pre class="r"><code>tibble(
    precision = test_results %&gt;% yardstick::precision(actual_yes, pred_classes_test),
    recall    = test_results %&gt;% yardstick::recall(actual_yes, pred_classes_test)
)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   precision recall
##       &lt;dbl&gt;  &lt;dbl&gt;
## 1     0.837  0.910</code></pre>
<pre class="r"><code># F1-Statistic
test_results %&gt;% yardstick::f_meas(actual_yes, pred_classes_test, beta = 1)</code></pre>
<pre><code>## [1] 0.872214</code></pre>
</div>
<div id="h2o" class="section level2">
<h2>H2O</h2>
<pre class="r"><code>library(h2o)
h2o.init(nthreads = -1)</code></pre>
<pre><code>## 
## H2O is not running yet, starting it now...
## 
## Note:  In case of errors look at the following log files:
##     /var/folders/5j/v30zfr7s14qfhqwqdmqmpxw80000gn/T//Rtmpnak4Ye/h2o_shiringlander_started_from_r.out
##     /var/folders/5j/v30zfr7s14qfhqwqdmqmpxw80000gn/T//Rtmpnak4Ye/h2o_shiringlander_started_from_r.err
## 
## 
## Starting H2O JVM and connecting: .. Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         3 seconds 573 milliseconds 
##     H2O cluster timezone:       Europe/Berlin 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.20.0.2 
##     H2O cluster version age:    2 months and 5 days  
##     H2O cluster name:           H2O_started_from_R_shiringlander_zvk734 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   3.56 GB 
##     H2O cluster total cores:    8 
##     H2O cluster allowed cores:  8 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.1 (2018-07-02)</code></pre>
<pre class="r"><code>h2o.no_progress()

train_data_hf &lt;- as.h2o(train_data)
test &lt;- as.h2o(test_data)</code></pre>
<pre class="r"><code>splits &lt;- h2o.splitFrame(train_data_hf, 
                         ratios = c(0.8), 
                         seed = 1)

train &lt;- splits[[1]]
valid &lt;- splits[[2]]

response &lt;- &quot;Churn&quot;
features &lt;- setdiff(colnames(train), response)

# For binary classification, response should be a factor
train[, response] &lt;- as.factor(train[, response])
valid[, response] &lt;- as.factor(valid[, response])
test[, response] &lt;- as.factor(test[, response])</code></pre>
<pre class="r"><code>summary(train$Churn, exact_quantiles = TRUE)</code></pre>
<pre><code>##  Churn  
##  0:2491 
##  1: 900</code></pre>
<pre class="r"><code>summary(valid$Churn, exact_quantiles = TRUE)</code></pre>
<pre><code>##  Churn 
##  0:608 
##  1:222</code></pre>
<pre class="r"><code>summary(test$Churn, exact_quantiles = TRUE)</code></pre>
<pre><code>##  Churn  
##  0:1290 
##  1: 467</code></pre>
<blockquote>
<p>“During model training, you might find that the majority of your data belongs in a single class. For example, consider a binary classification model that has 100 rows, with 80 rows labeled as class 1 and the remaining 20 rows labeled as class 2. This is a common scenario, given that machine learning attempts to predict class 1 with the highest accuracy. It can also be an example of an imbalanced dataset, in this case, with a ratio of 4:1. The balance_classes option can be used to balance the class distribution. When enabled, H2O will either undersample the majority classes or oversample the minority classes. Note that the resulting model will also correct the final probabilities (“undo the sampling”) using a monotonic transform, so the predicted probabilities of the first model will differ from a second model. However, because AUC only cares about ordering, it won’t be affected. If this option is enabled, then you can also specify a value for the class_sampling_factors and max_after_balance_size options.” <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/balance_classes.html" class="uri">http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/balance_classes.html</a></p>
</blockquote>
<p>The model was trained with <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html">AutoML</a>. The best model was a Best Of Family Stacked Ensemble.</p>
<pre class="r"><code>aml &lt;- h2o.automl(x = features, 
                  y = response,
                  training_frame = train,
                  validation_frame = valid,
                  balance_classes = TRUE,
                  max_runtime_secs = 30)

# View the AutoML Leaderboard
lb &lt;- aml@leaderboard
lb</code></pre>
<pre><code>##                                                model_id       auc
## 1    StackedEnsemble_AllModels_0_AutoML_20180821_101739 0.8384489
## 2 StackedEnsemble_BestOfFamily_0_AutoML_20180821_101739 0.8380784
## 3             GBM_grid_0_AutoML_20180821_101739_model_4 0.8358176
## 4             GLM_grid_0_AutoML_20180821_101739_model_0 0.8341097
## 5             GBM_grid_0_AutoML_20180821_101739_model_0 0.8333280
## 6             GBM_grid_0_AutoML_20180821_101739_model_1 0.8286081
##     logloss mean_per_class_error      rmse       mse
## 1 0.4322100            0.2415552 0.3739274 0.1398217
## 2 0.4321532            0.2350863 0.3739190 0.1398154
## 3 0.4282520            0.2375987 0.3736850 0.1396405
## 4 0.4293852            0.2402290 0.3740019 0.1398774
## 5 0.4346053            0.2420070 0.3769994 0.1421285
## 6 0.4426769            0.2438496 0.3794211 0.1439604
## 
## [13 rows x 6 columns]</code></pre>
<pre class="r"><code>best_model &lt;- aml@leader
best_model</code></pre>
<pre><code>## Model Details:
## ==============
## 
## H2OBinomialModel: stackedensemble
## Model ID:  StackedEnsemble_AllModels_0_AutoML_20180821_101739 
## NULL
## 
## 
## H2OBinomialMetrics: stackedensemble
## ** Reported on training data. **
## 
## MSE:  0.1235994
## RMSE:  0.351567
## LogLoss:  0.3893192
## Mean Per-Class Error:  0.2106229
## AUC:  0.8791347
## Gini:  0.7582693
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##           0    1    Error       Rate
## 0      1998  493 0.197912  =493/2491
## 1       201  699 0.223333   =201/900
## Totals 2199 1192 0.204659  =694/3391
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.274083 0.668260 248
## 2                       max f2  0.160300 0.781640 305
## 3                 max f0point5  0.480116 0.677030 158
## 4                 max accuracy  0.480116 0.824830 158
## 5                max precision  0.875054 1.000000   0
## 6                   max recall  0.065367 1.000000 386
## 7              max specificity  0.875054 1.000000   0
## 8             max absolute_mcc  0.215980 0.537548 277
## 9   max min_per_class_accuracy  0.256202 0.787635 256
## 10 max mean_per_class_accuracy  0.205745 0.800347 283
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`
## H2OBinomialMetrics: stackedensemble
## ** Reported on validation data. **
## 
## MSE:  0.1269186
## RMSE:  0.3562564
## LogLoss:  0.4030923
## Mean Per-Class Error:  0.2144381
## AUC:  0.8607345
## Gini:  0.721469
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##          0   1    Error      Rate
## 0      528  80 0.131579   =80/608
## 1       66 156 0.297297   =66/222
## Totals 594 236 0.175904  =146/830
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.344425 0.681223 181
## 2                       max f2  0.182311 0.756735 263
## 3                 max f0point5  0.555273 0.684597 114
## 4                 max accuracy  0.463244 0.825301 145
## 5                max precision  0.855358 1.000000   0
## 6                   max recall  0.059345 1.000000 397
## 7              max specificity  0.855358 1.000000   0
## 8             max absolute_mcc  0.344425 0.560414 181
## 9   max min_per_class_accuracy  0.232010 0.774775 226
## 10 max mean_per_class_accuracy  0.318849 0.786955 191
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`
## H2OBinomialMetrics: stackedensemble
## ** Reported on cross-validation data. **
## ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **
## 
## MSE:  0.1398217
## RMSE:  0.3739274
## LogLoss:  0.43221
## Mean Per-Class Error:  0.2415552
## AUC:  0.8384489
## Gini:  0.6768977
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##           0    1    Error       Rate
## 0      1938  553 0.221999  =553/2491
## 1       235  665 0.261111   =235/900
## Totals 2173 1218 0.232380  =788/3391
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.261737 0.627951 248
## 2                       max f2  0.139831 0.750376 316
## 3                 max f0point5  0.501019 0.625000 145
## 4                 max accuracy  0.501019 0.800944 145
## 5                max precision  0.873181 1.000000   0
## 6                   max recall  0.055355 1.000000 399
## 7              max specificity  0.873181 1.000000   0
## 8             max absolute_mcc  0.186677 0.475864 287
## 9   max min_per_class_accuracy  0.237597 0.757527 260
## 10 max mean_per_class_accuracy  0.186677 0.767161 287
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`</code></pre>
<pre class="r"><code>pred &lt;- h2o.predict(best_model, test[, -1])</code></pre>
<pre class="r"><code>h2o.mean_per_class_error(best_model, train = TRUE, valid = TRUE, xval = TRUE)</code></pre>
<pre><code>##     train     valid      xval 
## 0.2106229 0.2144381 0.2415552</code></pre>
<pre class="r"><code>h2o.confusionMatrix(best_model, valid = TRUE)</code></pre>
<pre><code>## Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.344424598963745:
##          0   1    Error      Rate
## 0      528  80 0.131579   =80/608
## 1       66 156 0.297297   =66/222
## Totals 594 236 0.175904  =146/830</code></pre>
<pre class="r"><code>h2o.auc(best_model, train = TRUE)</code></pre>
<pre><code>## [1] 0.8791347</code></pre>
<pre class="r"><code>h2o.auc(best_model, valid = TRUE)</code></pre>
<pre><code>## [1] 0.8607345</code></pre>
<pre class="r"><code>h2o.auc(best_model, xval = TRUE)</code></pre>
<pre><code>## [1] 0.8384489</code></pre>
<pre class="r"><code>perf &lt;- h2o.performance(best_model, test)
h2o.confusionMatrix(perf)</code></pre>
<pre><code>## Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.316913182517402:
##           0   1    Error       Rate
## 0      1070 220 0.170543  =220/1290
## 1       152 315 0.325482   =152/467
## Totals 1222 535 0.211725  =372/1757</code></pre>
<pre class="r"><code>plot(perf)</code></pre>
<pre class="r"><code>h2o.logloss(perf)</code></pre>
<pre><code>## [1] 0.4249711</code></pre>
<pre class="r"><code>h2o.mse(perf)</code></pre>
<pre><code>## [1] 0.1369139</code></pre>
<pre class="r"><code>h2o.auc(perf)</code></pre>
<pre><code>## [1] 0.845558</code></pre>
<pre class="r"><code>metrics &lt;- as.data.frame(h2o.metric(perf))
head(metrics)</code></pre>
<pre><code>##   threshold          f1          f2   f0point5  accuracy precision
## 1 0.8604557 0.004273504 0.002675227 0.01061571 0.7347752 1.0000000
## 2 0.8572855 0.008528785 0.005347594 0.02105263 0.7353443 1.0000000
## 3 0.8517721 0.012765957 0.008017103 0.03131524 0.7359135 1.0000000
## 4 0.8485592 0.021186441 0.013347571 0.05133470 0.7370518 1.0000000
## 5 0.8469450 0.025316456 0.016000000 0.06060606 0.7370518 0.8571429
## 6 0.8451180 0.033613445 0.021310602 0.07952286 0.7381901 0.8888889
##        recall specificity absolute_mcc min_per_class_accuracy
## 1 0.002141328   1.0000000   0.03966196            0.002141328
## 2 0.004282655   1.0000000   0.05610646            0.004282655
## 3 0.006423983   1.0000000   0.06873568            0.006423983
## 4 0.010706638   1.0000000   0.08878802            0.010706638
## 5 0.012847966   0.9992248   0.08466273            0.012847966
## 6 0.017130621   0.9992248   0.10120995            0.017130621
##   mean_per_class_accuracy  tns fns fps tps       tnr       fnr
## 1               0.5010707 1290 466   0   1 1.0000000 0.9978587
## 2               0.5021413 1290 465   0   2 1.0000000 0.9957173
## 3               0.5032120 1290 464   0   3 1.0000000 0.9935760
## 4               0.5053533 1290 462   0   5 1.0000000 0.9892934
## 5               0.5060364 1289 461   1   6 0.9992248 0.9871520
## 6               0.5081777 1289 459   1   8 0.9992248 0.9828694
##            fpr         tpr idx
## 1 0.0000000000 0.002141328   0
## 2 0.0000000000 0.004282655   1
## 3 0.0000000000 0.006423983   2
## 4 0.0000000000 0.010706638   3
## 5 0.0007751938 0.012847966   4
## 6 0.0007751938 0.017130621   5</code></pre>
<pre class="r"><code>metrics %&gt;%
  gather(x, y, f1:tpr) %&gt;%
  ggplot(aes(x = threshold, y = y, group = x)) +
    facet_wrap(~ x, ncol = 2, scales = &quot;free&quot;) +
    geom_line()</code></pre>
<p><img src="/post/2018-09-17_customer_churn_code_files/figure-html/metrics-1.png" width="768" /></p>
<pre class="r"><code>threshold &lt;- metrics[order(-metrics$accuracy), &quot;threshold&quot;][1]

finalRf_predictions &lt;- data.frame(actual = as.vector(test$Churn), 
                                  as.data.frame(h2o.predict(object = best_model, 
                                                            newdata = test)))

finalRf_predictions$accurate &lt;- ifelse(finalRf_predictions$actual == 
                                         finalRf_predictions$predict, &quot;ja&quot;, &quot;nein&quot;)

finalRf_predictions$predict_stringent &lt;- ifelse(finalRf_predictions$p1 &gt; threshold, 1, 
                                                ifelse(finalRf_predictions$p0 &gt; threshold, 0, &quot;unsicher&quot;))
finalRf_predictions$accurate_stringent &lt;- ifelse(finalRf_predictions$actual == 
                                                   finalRf_predictions$predict_stringent, &quot;ja&quot;, 
                                       ifelse(finalRf_predictions$predict_stringent == 
                                                &quot;unsicher&quot;, &quot;unsicher&quot;, &quot;nein&quot;))

finalRf_predictions %&gt;%
  group_by(actual, predict) %&gt;%
  dplyr::summarise(n = n())</code></pre>
<pre><code>## # A tibble: 4 x 3
## # Groups:   actual [?]
##   actual predict     n
##   &lt;fct&gt;  &lt;fct&gt;   &lt;int&gt;
## 1 0      0        1090
## 2 0      1         200
## 3 1      0         164
## 4 1      1         303</code></pre>
<pre class="r"><code>finalRf_predictions %&gt;%
  group_by(actual, predict_stringent) %&gt;%
  dplyr::summarise(n = n())</code></pre>
<pre><code>## # A tibble: 4 x 3
## # Groups:   actual [?]
##   actual predict_stringent     n
##   &lt;fct&gt;  &lt;chr&gt;             &lt;int&gt;
## 1 0      0                  1161
## 2 0      1                   129
## 3 1      0                   210
## 4 1      1                   257</code></pre>
<pre class="r"><code>finalRf_predictions %&gt;%
  gather(x, y, accurate, accurate_stringent) %&gt;%
  mutate(x = ifelse(x == &quot;accurate&quot;, &quot;Default Schwelle: 0.5&quot;, 
                    paste(&quot;Angepasste Schwelle:&quot;, round(threshold, digits = 2)))) %&gt;%
  ggplot(aes(x = actual, fill = y)) +
    facet_grid(~ x) +
    geom_bar(position = &quot;dodge&quot;) +
    scale_fill_tableau()</code></pre>
<p><img src="/post/2018-09-17_customer_churn_code_files/figure-html/default_vs_stringent-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>df &lt;- finalRf_predictions[, c(1, 3, 4)]

thresholds &lt;- seq(from = 0, to = 1, by = 0.1)

prop_table &lt;- data.frame(threshold = thresholds, 
                         prop_p0_true = NA, prop_p0_false = NA,
                         prop_p1_true = NA, prop_p1_false = NA)

for (threshold in thresholds) {

  pred_1 &lt;- ifelse(df$p1 &gt; threshold, 1, 0)
  pred_1_t &lt;- ifelse(pred_1 == df$actual, TRUE, FALSE)
  
  group &lt;- data.frame(df, 
                      &quot;pred_true&quot; = pred_1_t) %&gt;%
    group_by(actual, pred_true) %&gt;%
    dplyr::summarise(n = n())
  
  group_p0 &lt;- filter(group, actual == &quot;0&quot;)
  
  prop_p0_t &lt;- sum(filter(group_p0, pred_true == TRUE)$n) / sum(group_p0$n)
  prop_p0_f &lt;- sum(filter(group_p0, pred_true == FALSE)$n) / sum(group_p0$n)
  prop_table[prop_table$threshold == threshold, &quot;prop_p0_true&quot;] &lt;- prop_p0_t
  prop_table[prop_table$threshold == threshold, &quot;prop_p0_false&quot;] &lt;- prop_p0_f
  
  group_p1 &lt;- filter(group, actual == &quot;1&quot;)
  
  prop_p1_t &lt;- sum(filter(group_p1, pred_true == TRUE)$n) / sum(group_p1$n)
  prop_p1_f &lt;- sum(filter(group_p1, pred_true == FALSE)$n) / sum(group_p1$n)
  prop_table[prop_table$threshold == threshold, &quot;prop_p1_true&quot;] &lt;- prop_p1_t
  prop_table[prop_table$threshold == threshold, &quot;prop_p1_false&quot;] &lt;- prop_p1_f
}</code></pre>
<pre class="r"><code>prop_table %&gt;%
  gather(x, y, prop_p0_true, prop_p1_true) %&gt;%
  rename(Schwellenwert = threshold) %&gt;%
  mutate(x = ifelse(x == &quot;prop_p0_true&quot;, &quot;prop true p0&quot;,
         &quot;prop true p1&quot;)) %&gt;%
  ggplot(aes(x = Schwellenwert, y = y, color = x)) +
    geom_point() +
    geom_line() +
    scale_color_tableau()</code></pre>
<p><img src="/post/2018-09-17_customer_churn_code_files/figure-html/prop_table-1.png" width="576" style="display: block; margin: auto;" /></p>
<div id="cost-calculation" class="section level3">
<h3>Cost calculation</h3>
<p>Let’s assume that</p>
<ol style="list-style-type: decimal">
<li>a marketing campaign + employee time will cost the company 1000€ per year for every customer that is included in the campaign.</li>
<li>the annual average revenue per customer is 2000€ (in more complex scenarios customers could be further divided into revenue groups to calculate how “valuable” they are and how harmful loosing them would be)</li>
<li>investing into unnecessary marketing doesn’t cause churn by itself (i.e. a customer who isn’t going to churn isn’t reacting negatively to the add campaign - which could happen in more complex scenarios).</li>
<li>without a customer churn model the company would target half of their customer (by chance) for ad-campaigns</li>
<li>without a customer churn model the company would lose about 25% of their customers to churn</li>
</ol>
<p>This would mean that compared to no intervention we would have</p>
<ul>
<li>prop_p0_true == customers who were correctly predicted to not churn did not cost anything (no marketing money was spent): +/-0€</li>
<li>prop_p0_false == customers that did not churn who are predicted to churn will be an empty investment: +/-0€ - 1500€</li>
<li>prop_p1_false == customer that were predicted to stay but churned: -2000€</li>
<li>prop_p1_true == customers that were correctly predicted to churn:</li>
<li>let’s say 100% of those could be kept by investing into marketing: +2000€ -1500€</li>
<li>let’s say 50% could be kept by investing into marketing: +2000€ * 0.5 -1500€</li>
</ul>
<pre class="r"><code># baseline

revenue &lt;- 2000
cost &lt;- 1000

customers_churn &lt;- filter(test_data, Churn == 1)
customers_churn_n &lt;- nrow(customers_churn)

customers_no_churn &lt;- filter(filter(test_data, Churn == 0))
customers_no_churn_n &lt;- nrow(customers_no_churn)

customers &lt;- customers_churn_n + customers_no_churn_n

ad_target_rate &lt;- 0.5
ad_cost_default &lt;- customers * ad_target_rate * cost

churn_rate_default &lt;- customers_churn_n / customers_no_churn_n
ann_revenue_default &lt;- customers_no_churn_n * revenue

net_win_default &lt;- ann_revenue_default - ad_cost_default
net_win_default</code></pre>
<pre><code>## [1] 1701500</code></pre>
<p>How much can we revenue gain from predicting customer churn:</p>
<pre class="r"><code>conversion &lt;- 0.7

net_win_table &lt;- prop_table %&gt;%
  mutate(prop_p0_true_X = prop_p0_true * customers_no_churn_n * revenue,
         prop_p0_false_X = prop_p0_false * customers_no_churn_n * (revenue -cost),
         prop_p1_false_X = prop_p1_false * customers_churn_n * 0,
         prop_p1_true_X = prop_p1_true * customers_churn_n * ((revenue * conversion) - cost)) %&gt;%
  group_by(threshold) %&gt;%
  summarise(net_win = sum(prop_p0_true_X + prop_p0_false_X + prop_p1_false_X + prop_p1_true_X),
            net_win_compared = net_win - net_win_default) %&gt;%
  arrange(-net_win_compared)

net_win_table</code></pre>
<pre><code>## # A tibble: 11 x 3
##    threshold net_win net_win_compared
##        &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;
##  1       0.8 2590800           889300
##  2       0.7 2589000           887500
##  3       0.9 2580000           878500
##  4       1   2580000           878500
##  5       0.6 2578800           877300
##  6       0.5 2559800           858300
##  7       0.4 2521400           819900
##  8       0.3 2465800           764300
##  9       0.2 2347200           645700
## 10       0.1 2116200           414700
## 11       0   1476800          -224700</code></pre>
</div>
</div>
<div id="lime" class="section level2">
<h2>LIME</h2>
<pre class="r"><code>Xtrain &lt;- as.data.frame(train)
Xtest &lt;- as.data.frame(test)

# run lime() on training set
explainer &lt;- lime::lime(x = Xtrain, 
                        model = best_model)

# run explain() on the explainer
explanation &lt;- lime::explain(x = Xtest[1:9, ], 
                             explainer = explainer, 
                             n_labels = 1,
                             n_features = 4,
                             kernel_width = 0.5)</code></pre>
<pre class="r"><code>plot_explanations(explanation)</code></pre>
<p><img src="/post/2018-09-17_customer_churn_code_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
<pre class="r"><code>explanation %&gt;%
  plot_features(ncol = 3)</code></pre>
<p><img src="/post/2018-09-17_customer_churn_code_files/figure-html/unnamed-chunk-49-1.png" width="1248" /></p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] h2o_3.20.0.2    bindrcpp_0.2.2  corrplot_0.84   ggthemes_4.0.0 
##  [5] yardstick_0.0.1 recipes_0.1.3   rsample_0.0.2   broom_0.5.0    
##  [9] lime_0.4.0      keras_2.1.6     mice_3.3.0      caret_6.0-80   
## [13] lattice_0.20-35 forcats_0.3.0   stringr_1.3.1   dplyr_0.7.6    
## [17] purrr_0.2.5     readr_1.1.1     tidyr_0.8.1     tibble_1.4.2   
## [21] ggplot2_3.0.0   tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##   [1] minqa_1.2.4        colorspace_1.3-2   class_7.3-14      
##   [4] rprojroot_1.3-2    pls_2.6-0          base64enc_0.1-3   
##   [7] rstudioapi_0.7     DRR_0.0.3          fansi_0.2.3       
##  [10] prodlim_2018.04.18 lubridate_1.7.4    xml2_1.2.0        
##  [13] codetools_0.2-15   splines_3.5.1      robustbase_0.93-2 
##  [16] knitr_1.20         shinythemes_1.1.1  RcppRoll_0.3.0    
##  [19] zeallot_0.1.0      jsonlite_1.5       nloptr_1.0.4      
##  [22] pROC_1.12.1        ddalpha_1.3.4      kernlab_0.9-26    
##  [25] tfruns_1.3         sfsmisc_1.1-2      shiny_1.1.0       
##  [28] compiler_3.5.1     httr_1.3.1         backports_1.1.2   
##  [31] assertthat_0.2.0   Matrix_1.2-14      lazyeval_0.2.1    
##  [34] cli_1.0.0          later_0.7.3        htmltools_0.3.6   
##  [37] tools_3.5.1        gtable_0.2.0       glue_1.3.0        
##  [40] reshape2_1.4.3     Rcpp_0.12.18       cellranger_1.1.0  
##  [43] nlme_3.1-137       blogdown_0.8       iterators_1.0.10  
##  [46] timeDate_3043.102  gower_0.1.2        xfun_0.3          
##  [49] lme4_1.1-17        rvest_0.3.2        mime_0.5          
##  [52] stringdist_0.9.5.1 pan_1.6            DEoptimR_1.0-8    
##  [55] MLmetrics_1.1.1    MASS_7.3-50        scales_0.5.0      
##  [58] ipred_0.9-6        promises_1.0.1     hms_0.4.2         
##  [61] parallel_3.5.1     yaml_2.2.0         reticulate_1.9    
##  [64] rpart_4.1-13       stringi_1.2.4      tensorflow_1.8    
##  [67] foreach_1.4.4      lava_1.6.2         geometry_0.3-6    
##  [70] bitops_1.0-6       rlang_0.2.1        pkgconfig_2.0.1   
##  [73] evaluate_0.11      bindr_0.1.1        labeling_0.3      
##  [76] htmlwidgets_1.2    CVST_0.2-2         tidyselect_0.2.4  
##  [79] plyr_1.8.4         magrittr_1.5       bookdown_0.7      
##  [82] R6_2.2.2           magick_1.9         mitml_0.3-6       
##  [85] dimRed_0.1.0       pillar_1.3.0       haven_1.1.2       
##  [88] whisker_0.3-2      withr_2.1.2        RCurl_1.95-4.11   
##  [91] survival_2.42-6    abind_1.4-5        nnet_7.3-12       
##  [94] modelr_0.1.2       crayon_1.3.4       jomo_2.6-2        
##  [97] utf8_1.1.4         rmarkdown_1.10     grid_3.5.1        
## [100] readxl_1.1.0       ModelMetrics_1.1.0 digest_0.6.15     
## [103] xtable_1.8-2       httpuv_1.4.5       stats4_3.5.1      
## [106] munsell_0.5.0      glmnet_2.0-16      magic_1.5-8</code></pre>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Slides from my SAP webinar: Explaining Keras Image Classification Models with LIME]]></title>
    <link href="/2018/08/sap_webinar_slides/"/>
    <id>/2018/08/sap_webinar_slides/</id>
    <published>2018-08-21T00:00:00+00:00</published>
    <updated>2018-08-21T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Here I am sharing the slides for a webinar I gave for SAP about <strong>Explaining Keras Image Classification Models with LIME</strong>.</p>
<p>Slides can be found here: <a href="https://www.slideshare.net/ShirinGlander/sap-webinar-explaining-keras-image-classification-models-with-lime" class="uri">https://www.slideshare.net/ShirinGlander/sap-webinar-explaining-keras-image-classification-models-with-lime</a></p>
<blockquote>
<p>Keras is a high-level open-source deep learning framework that by default works on top of TensorFlow. Keras is minimalistic, efficient and highly flexible because it works with a modular layer system to define, compile and fit neural networks. It has been written in Python but can also be used from within R. Because the underlying backend can be changed from TensorFlow to Theano and CNTK (with more options being developed right now) it is designed to be framework-independent. Models can be trained on CPU or GPU, locally or in the cloud.</p>
</blockquote>
<blockquote>
<p>I will show an example how to build an image classifier with Keras. We’ll be using a convolutional neural net to classify fruits in images. But that’s not all! We not only want to judge our black-box model based on accuracy and loss measures - we want to get a better understanding of how the model works. We will use an algorithm called LIME (local interpretable model-agnostic explanations) to find out what part of the different test images contributed most strongly to the classification that was made by our model. I will introduce LIME and explain how it works. And finally, I will show how to apply LIME to the image classifier we built before, as well as to a pretrained Imagenet model.</p>
</blockquote>
<blockquote>
<p>You will get:</p>
</blockquote>
<ul>
<li>an introduction to Keras</li>
<li>an overview about deep learning and neural nets</li>
<li>a demo how to build an image classifier with Keras</li>
<li>an introduction to explaining black box models, specifically to the LIME algorithm</li>
<li>a demo how to apply LIME to explain the predictions of our own Keras image classifier, as well as of a pretrained Imagenet</li>
</ul>
<blockquote>
<p>Further Information:</p>
</blockquote>
<ul>
<li><a href="www.shirin-glander.de" class="uri">www.shirin-glander.de</a></li>
<li><a href="https://blog.codecentric.de/author/shirin-glander/" class="uri">https://blog.codecentric.de/author/shirin-glander/</a></li>
<li><a href="www.youtube.com/codecentricAI" class="uri">www.youtube.com/codecentricAI</a></li>
</ul>
<blockquote>
<p>Links mentioned:</p>
</blockquote>
<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits/" class="uri">https://shirinsplayground.netlify.com/2018/06/keras_fruits/</a></li>
<li><a href="https://blog.codecentric.de/2018/01/%20vertrauen-und-vorurteile-maschinellem-lernen/">https://blog.codecentric.de/2018/01/ vertrauen-und-vorurteile-maschinellem-lernen/</a></li>
<li><a href="https://shirinsplayground.netlify.com/2018/07/%20explaining_ml_models_code_text_lime/">https://shirinsplayground.netlify.com/2018/07/ explaining_ml_models_code_text_lime/</a></li>
<li><a href="www.codecentric.ai" class="uri">www.codecentric.ai</a></li>
<li><a href="https://www.youtube.com/codecentricAI" class="uri">https://www.youtube.com/codecentricAI</a></li>
</ul>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/M9htTiB6ObhMqI" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen>
</iframe>
<div style="margin-bottom:5px">
<strong> <a href="//www.slideshare.net/ShirinGlander/sap-webinar-explaining-keras-image-classification-models-with-lime" title="SAP webinar: Explaining Keras Image Classification Models with LIME" target="_blank">SAP webinar: Explaining Keras Image Classification Models with LIME</a> </strong> from <strong><a href="//www.slideshare.net/ShirinGlander" target="_blank">Shirin Glander</a></strong>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[MünsteR Meetup on Blog Mining: Deriving the success of blog posts from metadata and text data.]]></title>
    <link href="/2018/08/meetup_august18/"/>
    <id>/2018/08/meetup_august18/</id>
    <published>2018-08-01T00:00:00+00:00</published>
    <updated>2018-08-01T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://shiring.github.io/r_users_group/2017/05/20/map.png" />

</div>
<p>In our <a href="http://meetu.ps/e/FDZ1D/w54bW/f">next MünsteR R-user group meetup</a> on <strong>Tuesday, August 28th, 2018</strong> Jenny Saatkamp will give a talk titled <strong>Blog Mining: Deriving the success of blog posts from metadata and text data</strong>. You can RSVP here: <a href="http://meetu.ps/e/FDZ1D/w54bW/f">http://meetu.ps/e/F7zDN/w54bW/f</a></p>
<blockquote>
<p>In our next MünsteR Meetup, Jenny Saatkamp will present her Blog Mining analysis, which is based on 1.500 blog posts from the codecentric company blog (<a href="https://blog.codecentric.de/" class="uri">https://blog.codecentric.de/</a>) and makes use of different mining techniques for metadata and text data. The presentation is divided into two parts: First, new insights into the blog posts and their success is gained using metadata. Therefore, diverse methods and R-packages are used such as H2O automatic machine learning, lime, clustering and basic explorative statistics. The sources of metadata are the blog itself, Google Analytics and the SEO-Tool Ahrefs. Second, a text mining analysis is performed using the texts and titles of each blog post. You will learn about common preprocessing steps like stemming, removal of stop words and bag of words and how to visualize text data. As the text mining analysis is still in progress, Jenny might also be able to show you results of a machine learning algorithm that is able to derive the success of blog posts based on text data.</p>
</blockquote>
<blockquote>
<p>About the speaker: Jenny Saatkamp is currently studying business informatics at the University of Applied Science in Münster. She is a junior Data Scientist, but enthusiastic about it and has already done several Data Science projects in her study and internships. One of her projects is the presented analysis of metadata, which is the topic of her bachelor thesis. The text analysis is also a project as a part of her study.</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Explaining Black-Box Machine Learning Models - Code Part 2: Text classification with LIME]]></title>
    <link href="/2018/07/explaining_ml_models_code_text_lime/"/>
    <id>/2018/07/explaining_ml_models_code_text_lime/</id>
    <published>2018-07-26T00:00:00+00:00</published>
    <updated>2018-07-26T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>This is code that will encompany an article that will appear in a special edition of a German IT magazine. The article is about explaining black-box machine learning models. In that article I’m showcasing three practical examples:</p>
<p><br></p>
<ol style="list-style-type: decimal">
<li>Explaining supervised classification models built on tabular data using <code>caret</code> and the <code>iml</code> package</li>
<li>Explaining image classification models with <code>keras</code> and <code>lime</code></li>
<li>Explaining text classification models with <code>xgboost</code> and <code>lime</code></li>
</ol>
<p><br></p>
<ul>
<li>The first part has been published <a href="https://shirinsplayground.netlify.com/2018/07/explaining_ml_models_code_caret_iml/">here</a>.</li>
<li>The second part has been published <a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits_lime/">here</a>.</li>
</ul>
<p>Below, you will find the code for the third part: Text classification with <a href="https://cran.r-project.org/web/packages/lime/index.html">lime</a>.</p>
<pre class="r"><code># data wrangling
library(tidyverse)
library(readr)

# plotting
library(ggthemes)
theme_set(theme_minimal())

# text prep
library(text2vec)

# ml
library(caret)
library(xgboost)

# explanation
library(lime)</code></pre>
<div id="text-classification-models" class="section level2">
<h2>Text classification models</h2>
<p>Here I am using another Kaggle dataset: <a href="https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews">Women’s e-commerce cloting reviews</a>. The data contains a text review of different items of clothing, as well as some additional information, like rating, division, etc.</p>
<p>In this example, I will use the review title and text in order to classify whether or not the item was liked. I am creating the response variable from the rating: every item rates with 5 stars is considered “liked” (1), the rest as “not liked” (0). I am also combining review title and text.</p>
<pre class="r"><code>clothing_reviews &lt;- read_csv(&quot;/Users/shiringlander/Documents/Github/ix_lime_etc/Womens Clothing E-Commerce Reviews.csv&quot;) %&gt;%
  mutate(Liked = as.factor(ifelse(Rating == 5, 1, 0)),
         text = paste(Title, `Review Text`),
         text = gsub(&quot;NA&quot;, &quot;&quot;, text))</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   X1 = col_integer(),
##   `Clothing ID` = col_integer(),
##   Age = col_integer(),
##   Title = col_character(),
##   `Review Text` = col_character(),
##   Rating = col_integer(),
##   `Recommended IND` = col_integer(),
##   `Positive Feedback Count` = col_integer(),
##   `Division Name` = col_character(),
##   `Department Name` = col_character(),
##   `Class Name` = col_character()
## )</code></pre>
<pre class="r"><code>glimpse(clothing_reviews)</code></pre>
<pre><code>## Observations: 23,486
## Variables: 13
## $ X1                        &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...
## $ `Clothing ID`             &lt;int&gt; 767, 1080, 1077, 1049, 847, 1080, 85...
## $ Age                       &lt;int&gt; 33, 34, 60, 50, 47, 49, 39, 39, 24, ...
## $ Title                     &lt;chr&gt; NA, NA, &quot;Some major design flaws&quot;, &quot;...
## $ `Review Text`             &lt;chr&gt; &quot;Absolutely wonderful - silky and se...
## $ Rating                    &lt;int&gt; 4, 5, 3, 5, 5, 2, 5, 4, 5, 5, 3, 5, ...
## $ `Recommended IND`         &lt;int&gt; 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...
## $ `Positive Feedback Count` &lt;int&gt; 0, 4, 0, 0, 6, 4, 1, 4, 0, 0, 14, 2,...
## $ `Division Name`           &lt;chr&gt; &quot;Initmates&quot;, &quot;General&quot;, &quot;General&quot;, &quot;...
## $ `Department Name`         &lt;chr&gt; &quot;Intimate&quot;, &quot;Dresses&quot;, &quot;Dresses&quot;, &quot;B...
## $ `Class Name`              &lt;chr&gt; &quot;Intimates&quot;, &quot;Dresses&quot;, &quot;Dresses&quot;, &quot;...
## $ Liked                     &lt;fct&gt; 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, ...
## $ text                      &lt;chr&gt; &quot; Absolutely wonderful - silky and s...</code></pre>
<p>Whether an item was liked or not will thus be my response variable or label for classification.</p>
<pre class="r"><code>clothing_reviews %&gt;%
  ggplot(aes(x = Liked, fill = Liked)) +
    geom_bar(alpha = 0.8) +
    scale_fill_tableau(palette = &quot;tableau20&quot;) +
    guides(fill = FALSE)</code></pre>
<p><img src="/post/2018-07-26_explaining_ml_models_code_text_lime_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Let’s split the data into train and test sets:</p>
<pre class="r"><code>set.seed(42)
idx &lt;- createDataPartition(clothing_reviews$Liked, 
                           p = 0.8, 
                           list = FALSE, 
                           times = 1)

clothing_reviews_train &lt;- clothing_reviews[ idx,]
clothing_reviews_test  &lt;- clothing_reviews[-idx,]</code></pre>
</div>
<div id="lets-start-simple" class="section level2">
<h2>Let’s start simple</h2>
<p>The first text model I’m looking at has been built similarly to the example model in the help for <code>lime::interactive_text_explanations()</code>.</p>
<p>First, we need to prepare the data for modeling: we will need to convert the text to a document term matrix (dtm). There are different ways to do this. One is be with the <code>text2vec</code> package.</p>
<blockquote>
<p>“Because of R’s copy-on-modify semantics, it is not easy to iteratively grow a DTM. Thus constructing a DTM, even for a small collections of documents, can be a serious bottleneck for analysts and researchers. It involves reading the whole collection of text documents into RAM and processing it as single vector, which can easily increase memory use by a factor of 2 to 4. The text2vec package solves this problem by providing a better way of constructing a document-term matrix.” <a href="https://cran.r-project.org/web/packages/text2vec/vignettes/text-vectorization.html" class="uri">https://cran.r-project.org/web/packages/text2vec/vignettes/text-vectorization.html</a></p>
</blockquote>
<p>Alternatives to <code>text2vec</code> would be <code>tm</code> + <code>SnowballC</code> or you could work with the <code>tidytext</code> package.</p>
<p>The <code>itoken()</code> function creates vocabularies (here stemmed words), from which we can create the dtm with the <code>create_dtm()</code> function.</p>
<p>All preprocessing steps, starting from the raw text, need to be wrapped in a function that can then be pasted into the <code>lime::lime()</code> function; this is only necessary if you want to use your model with <code>lime</code>.</p>
<pre class="r"><code>get_matrix &lt;- function(text) {
  it &lt;- itoken(text, progressbar = FALSE)
  create_dtm(it, vectorizer = hash_vectorizer())
}</code></pre>
<p>Now, this preprocessing function can be applied to both training and test data.</p>
<pre class="r"><code>dtm_train &lt;- get_matrix(clothing_reviews_train$text)
str(dtm_train)</code></pre>
<pre><code>## Formal class &#39;dgCMatrix&#39; [package &quot;Matrix&quot;] with 6 slots
##   ..@ i       : int [1:889012] 304 764 786 788 793 794 1228 2799 2819 3041 ...
##   ..@ p       : int [1:262145] 0 0 0 0 0 0 0 0 0 0 ...
##   ..@ Dim     : int [1:2] 18789 262144
##   ..@ Dimnames:List of 2
##   .. ..$ : chr [1:18789] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..$ : NULL
##   ..@ x       : num [1:889012] 1 1 2 1 2 1 1 1 1 1 ...
##   ..@ factors : list()</code></pre>
<pre class="r"><code>dtm_test &lt;- get_matrix(clothing_reviews_test$text)
str(dtm_test)</code></pre>
<pre><code>## Formal class &#39;dgCMatrix&#39; [package &quot;Matrix&quot;] with 6 slots
##   ..@ i       : int [1:222314] 2793 400 477 622 2818 2997 3000 4500 3524 2496 ...
##   ..@ p       : int [1:262145] 0 0 0 0 0 0 0 0 0 0 ...
##   ..@ Dim     : int [1:2] 4697 262144
##   ..@ Dimnames:List of 2
##   .. ..$ : chr [1:4697] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..$ : NULL
##   ..@ x       : num [1:222314] 1 1 1 1 1 1 1 1 1 1 ...
##   ..@ factors : list()</code></pre>
<p>And we use it to train a model with the <code>xgboost</code> package (just as in the example of the <code>lime</code> package).</p>
<pre class="r"><code>xgb_model &lt;- xgb.train(list(max_depth = 7, 
                            eta = 0.1, 
                            objective = &quot;binary:logistic&quot;,
                            eval_metric = &quot;error&quot;, nthread = 1),
                       xgb.DMatrix(dtm_train, 
                                   label = clothing_reviews_train$Liked == &quot;1&quot;),
                       nrounds = 50)</code></pre>
<p>Let’s try it on the test data and see how it performs:</p>
<pre class="r"><code>pred &lt;- predict(xgb_model, dtm_test)

confusionMatrix(clothing_reviews_test$Liked,
                as.factor(round(pred, digits = 0)))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 1370  701
##          1  421 2205
##                                           
##                Accuracy : 0.7611          
##                  95% CI : (0.7487, 0.7733)
##     No Information Rate : 0.6187          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.5085          
##  Mcnemar&#39;s Test P-Value : &lt; 2.2e-16       
##                                           
##             Sensitivity : 0.7649          
##             Specificity : 0.7588          
##          Pos Pred Value : 0.6615          
##          Neg Pred Value : 0.8397          
##              Prevalence : 0.3813          
##          Detection Rate : 0.2917          
##    Detection Prevalence : 0.4409          
##       Balanced Accuracy : 0.7619          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p>Okay, not a perfect score but good enough for me - right now, I’m more interested in the explanations of the model’s predictions. For this, we need to run the <code>lime()</code> function and give it</p>
<ul>
<li>the text input that was used to construct the model</li>
<li>the trained model</li>
<li>the preprocessing function</li>
</ul>
<pre class="r"><code>explainer &lt;- lime(clothing_reviews_train$text, 
                  xgb_model, 
                  preprocess = get_matrix)</code></pre>
<p>With this, we could right away call the interactive explainer Shiny app, where we can type any text we want into the field on the left and see the explanation on the right: words that are underlined green support the classification, red words contradict them.</p>
<pre class="r"><code>interactive_text_explanations(explainer)</code></pre>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/text_explanation_2.png" />

</div>
<p>What happens in the background in the app, we can do explicitly by calling the <code>explain()</code> function and give it</p>
<ul>
<li>the test data (here the first four reviews of the test set)</li>
<li>the explainer defined with the <code>lime()</code> function</li>
<li>the number of labels we want to have explanations for (alternatively, you set the label by name)</li>
<li>and the number of features (in this case words) that should be included in the explanations</li>
</ul>
<p>We can plot them either with the <code>plot_text_explanations()</code> function, which gives an output like in the Shiny app or we use the regular <code>plot_features()</code> function.</p>
<pre class="r"><code>explanations &lt;- lime::explain(clothing_reviews_test$text[1:4], explainer, n_labels = 1, n_features = 5)</code></pre>
<pre class="r"><code>plot_text_explanations(explanations)</code></pre>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/text_explanation_0.png" />

</div>
<pre class="r"><code>plot_features(explanations)</code></pre>
<p><img src="/post/2018-07-26_explaining_ml_models_code_text_lime_files/figure-html/lime_text_plot-1.png" width="576" /></p>
<p>As we can see, our explanations contain a lot of stop-words that don’t really make much sense as features in our model. So…</p>
</div>
<div id="lets-try-a-more-complex-example" class="section level2">
<h2>… let’s try a more complex example</h2>
<p>Okay, our model above works but there are still common words and stop words in our model that LIME picks up on. Ideally, we would want to remove them before modeling and keep only relevant words. This we can accomplish by using additional steps and options in our preprocessing function.</p>
<p>Important to know is that whatever preprocessing we do with our text corpus, train and test data has to have the same features (i.e. words)! If we were to incorporate all the steps shown below into one function and call it separately on train and test data, we would end up with different words in our dtm and the <code>predict()</code> function won’t work any more. In the simple example above, it works <a href="https://cran.r-project.org/web/packages/text2vec/vignettes/text-vectorization.html">because we have been using the <code>hash_vectorizer()</code></a>.</p>
<p>Nevertheless, the <code>lime::explain()</code> function expects a preprocessing function that takes a character vector as input.</p>
<p>How do we go about this? First, we will need to create the vocabulary just from the training data. To reduce the number of words to only the most relevant I am performing the following steps:</p>
<ul>
<li>stem all words</li>
<li>remove step-words</li>
<li>prune vocabulary</li>
<li>transform into vector space</li>
</ul>
<pre class="r"><code>stem_tokenizer &lt;- function(x) {
  lapply(word_tokenizer(x), 
         SnowballC::wordStem, 
         language = &quot;en&quot;)
}

stop_words = tm::stopwords(kind = &quot;en&quot;)

# create prunded vocabulary
vocab_train &lt;- itoken(clothing_reviews_train$text, 
                     preprocess_function = tolower, 
                     tokenizer = stem_tokenizer,
                     progressbar = FALSE)
  
v &lt;- create_vocabulary(vocab_train, 
                       stopwords = stop_words)
  
pruned_vocab &lt;- prune_vocabulary(v, 
                                  doc_proportion_max = 0.99, 
                                  doc_proportion_min = 0.01)
  
vectorizer_train &lt;- vocab_vectorizer(pruned_vocab)</code></pre>
<p>This vector space can now be added to the preprocessing function, which we can then apply to both train and test data. Here, I am also transforming the word counts to <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">tfidf</a> values.</p>
<pre class="r"><code># preprocessing function
create_dtm_mat &lt;- function(text, vectorizer = vectorizer_train) {
  
  vocab &lt;- itoken(text, 
               preprocess_function = tolower, 
               tokenizer = stem_tokenizer,
               progressbar = FALSE)
  
  dtm &lt;- create_dtm(vocab, 
             vectorizer = vectorizer)
  
  tfidf = TfIdf$new()
  fit_transform(dtm, tfidf)
}</code></pre>
<pre class="r"><code>dtm_train2 &lt;- create_dtm_mat(clothing_reviews_train$text)
str(dtm_train2)</code></pre>
<pre><code>## Formal class &#39;dgCMatrix&#39; [package &quot;Matrix&quot;] with 6 slots
##   ..@ i       : int [1:415770] 26 74 169 294 588 693 703 708 727 759 ...
##   ..@ p       : int [1:506] 0 189 380 574 765 955 1151 1348 1547 1740 ...
##   ..@ Dim     : int [1:2] 18789 505
##   ..@ Dimnames:List of 2
##   .. ..$ : chr [1:18789] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..$ : chr [1:505] &quot;ad&quot; &quot;sandal&quot; &quot;depend&quot; &quot;often&quot; ...
##   ..@ x       : num [1:415770] 0.177 0.135 0.121 0.17 0.131 ...
##   ..@ factors : list()</code></pre>
<pre class="r"><code>dtm_test2 &lt;- create_dtm_mat(clothing_reviews_test$text)
str(dtm_test2)</code></pre>
<pre><code>## Formal class &#39;dgCMatrix&#39; [package &quot;Matrix&quot;] with 6 slots
##   ..@ i       : int [1:103487] 228 304 360 406 472 518 522 624 732 784 ...
##   ..@ p       : int [1:506] 0 53 113 151 186 216 252 290 323 360 ...
##   ..@ Dim     : int [1:2] 4697 505
##   ..@ Dimnames:List of 2
##   .. ..$ : chr [1:4697] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..$ : chr [1:505] &quot;ad&quot; &quot;sandal&quot; &quot;depend&quot; &quot;often&quot; ...
##   ..@ x       : num [1:103487] 0.263 0.131 0.135 0.109 0.179 ...
##   ..@ factors : list()</code></pre>
<p>And we will train another gradient boosting model:</p>
<pre class="r"><code>xgb_model2 &lt;- xgb.train(params = list(max_depth = 10, 
                            eta = 0.2, 
                            objective = &quot;binary:logistic&quot;,
                            eval_metric = &quot;error&quot;, nthread = 1),
                       data = xgb.DMatrix(dtm_train2, 
                                   label = clothing_reviews_train$Liked == &quot;1&quot;),
                       nrounds = 500)</code></pre>
<pre class="r"><code>pred2 &lt;- predict(xgb_model2, dtm_test2)

confusionMatrix(clothing_reviews_test$Liked,
                as.factor(round(pred2, digits = 0)))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 1441  630
##          1  426 2200
##                                         
##                Accuracy : 0.7752        
##                  95% CI : (0.763, 0.787)
##     No Information Rate : 0.6025        
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16     
##                                         
##                   Kappa : 0.5392        
##  Mcnemar&#39;s Test P-Value : 4.187e-10     
##                                         
##             Sensitivity : 0.7718        
##             Specificity : 0.7774        
##          Pos Pred Value : 0.6958        
##          Neg Pred Value : 0.8378        
##              Prevalence : 0.3975        
##          Detection Rate : 0.3068        
##    Detection Prevalence : 0.4409        
##       Balanced Accuracy : 0.7746        
##                                         
##        &#39;Positive&#39; Class : 0             
## </code></pre>
<p>Unfortunately, this didn’t really improve the classification accuracy but let’s look at the explanations again:</p>
<pre class="r"><code>explainer2 &lt;- lime(clothing_reviews_train$text, 
                  xgb_model2, 
                  preprocess = create_dtm_mat)</code></pre>
<pre class="r"><code>explanations2 &lt;- lime::explain(clothing_reviews_test$text[1:4], explainer2, n_labels = 1, n_features = 4)
plot_text_explanations(explanations2)</code></pre>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/text_explanation_1.png" />

</div>
<p>The words that get picked up now make much more sense! So, even though making my model more complex didn’t improve “the numbers”, this second model is likely to be much better able to generalize to new reviews because it seems to pick up on words that make intuitive sense.</p>
<p>That’s why I’m sold on the benefits of adding explainer functions to most machine learning workflows - and why I love the <code>lime</code> package in R!</p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] bindrcpp_0.2.2  lime_0.4.0      xgboost_0.71.2  caret_6.0-80   
##  [5] lattice_0.20-35 text2vec_0.5.1  ggthemes_3.5.0  forcats_0.3.0  
##  [9] stringr_1.3.1   dplyr_0.7.6     purrr_0.2.5     readr_1.1.1    
## [13] tidyr_0.8.1     tibble_1.4.2    ggplot2_3.0.0   tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##   [1] colorspace_1.3-2     class_7.3-14         rprojroot_1.3-2     
##   [4] futile.logger_1.4.3  pls_2.6-0            rstudioapi_0.7      
##   [7] DRR_0.0.3            SnowballC_0.5.1      prodlim_2018.04.18  
##  [10] lubridate_1.7.4      xml2_1.2.0           codetools_0.2-15    
##  [13] splines_3.5.1        mnormt_1.5-5         robustbase_0.93-1   
##  [16] knitr_1.20           shinythemes_1.1.1    RcppRoll_0.3.0      
##  [19] mlapi_0.1.0          jsonlite_1.5         broom_0.4.5         
##  [22] ddalpha_1.3.4        kernlab_0.9-26       sfsmisc_1.1-2       
##  [25] shiny_1.1.0          compiler_3.5.1       httr_1.3.1          
##  [28] backports_1.1.2      assertthat_0.2.0     Matrix_1.2-14       
##  [31] lazyeval_0.2.1       cli_1.0.0            later_0.7.3         
##  [34] formatR_1.5          htmltools_0.3.6      tools_3.5.1         
##  [37] NLP_0.1-11           gtable_0.2.0         glue_1.2.0          
##  [40] reshape2_1.4.3       Rcpp_0.12.17         slam_0.1-43         
##  [43] cellranger_1.1.0     nlme_3.1-137         blogdown_0.6        
##  [46] iterators_1.0.9      psych_1.8.4          timeDate_3043.102   
##  [49] gower_0.1.2          xfun_0.3             rvest_0.3.2         
##  [52] mime_0.5             stringdist_0.9.5.1   DEoptimR_1.0-8      
##  [55] MASS_7.3-50          scales_0.5.0         ipred_0.9-6         
##  [58] hms_0.4.2            promises_1.0.1       parallel_3.5.1      
##  [61] lambda.r_1.2.3       yaml_2.1.19          rpart_4.1-13        
##  [64] stringi_1.2.3        foreach_1.4.4        e1071_1.6-8         
##  [67] lava_1.6.2           geometry_0.3-6       rlang_0.2.1         
##  [70] pkgconfig_2.0.1      evaluate_0.10.1      bindr_0.1.1         
##  [73] labeling_0.3         recipes_0.1.3        htmlwidgets_1.2     
##  [76] CVST_0.2-2           tidyselect_0.2.4     plyr_1.8.4          
##  [79] magrittr_1.5         bookdown_0.7         R6_2.2.2            
##  [82] magick_1.9           dimRed_0.1.0         pillar_1.2.3        
##  [85] haven_1.1.2          foreign_0.8-70       withr_2.1.2         
##  [88] survival_2.42-3      abind_1.4-5          nnet_7.3-12         
##  [91] modelr_0.1.2         crayon_1.3.4         futile.options_1.0.1
##  [94] rmarkdown_1.10       grid_3.5.1           readxl_1.1.0        
##  [97] data.table_1.11.4    ModelMetrics_1.1.0   digest_0.6.15       
## [100] tm_0.7-4             xtable_1.8-2         httpuv_1.4.4.2      
## [103] RcppParallel_4.4.0   stats4_3.5.1         munsell_0.5.0       
## [106] glmnet_2.0-16        magic_1.5-8</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Explaining Black-Box Machine Learning Models - Code Part 1: tabular data &#43; caret &#43; iml]]></title>
    <link href="/2018/07/explaining_ml_models_code_caret_iml/"/>
    <id>/2018/07/explaining_ml_models_code_caret_iml/</id>
    <published>2018-07-20T00:00:00+00:00</published>
    <updated>2018-07-20T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>This is code that will encompany an article that will appear in a special edition of a German IT magazine. The article is about explaining black-box machine learning models. In that article I’m showcasing three practical examples:</p>
<p><br></p>
<ol style="list-style-type: decimal">
<li>Explaining supervised classification models built on tabular data using <code>caret</code> and the <code>iml</code> package</li>
<li>Explaining image classification models with <code>keras</code> and <code>lime</code></li>
<li>Explaining text classification models with <code>xgboost</code> and <code>lime</code></li>
</ol>
<p><br></p>
<ul>
<li>The second part has been published <a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits_lime/">here</a>.</li>
<li>The third part has been published <a href="https://shirinsplayground.netlify.com/2018/07/explaining_ml_models_code_text_lime/">here</a>.</li>
</ul>
<p>Below, you will find the code for the first part:</p>
<pre class="r"><code># data wrangling
library(tidyverse)
library(readr)

# ml
library(caret)

# plotting
library(gridExtra)
library(grid)
library(ggridges)
library(ggthemes)
theme_set(theme_minimal())

# explaining models
# https://github.com/christophM/iml
library(iml)

# https://pbiecek.github.io/breakDown/
library(breakDown)

# https://pbiecek.github.io/DALEX/
library(DALEX)</code></pre>
<div id="supervised-classfication-models-with-tabular-data" class="section level2">
<h2>Supervised classfication models with tabular data</h2>
<div id="the-data" class="section level3">
<h3>The data</h3>
<p>The example dataset I am using in this part is the <a href="https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009">wine quality data</a>. Let’s read it in and do some minor housekeeping, like</p>
<ul>
<li>converting the response variable <code>quality</code> into two categories with roughly equal sizes and</li>
<li>replacing the spaces in the column names with “_&quot; to make it easier to handle in the tidyverse</li>
</ul>
<pre class="r"><code>wine_data &lt;- read_csv(&quot;/Users/shiringlander/Documents/Github/ix_lime_etc/winequality-red.csv&quot;) %&gt;%
  mutate(quality = as.factor(ifelse(quality &lt; 6, &quot;qual_low&quot;, &quot;qual_high&quot;)))</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   `fixed acidity` = col_double(),
##   `volatile acidity` = col_double(),
##   `citric acid` = col_double(),
##   `residual sugar` = col_double(),
##   chlorides = col_double(),
##   `free sulfur dioxide` = col_double(),
##   `total sulfur dioxide` = col_double(),
##   density = col_double(),
##   pH = col_double(),
##   sulphates = col_double(),
##   alcohol = col_double(),
##   quality = col_integer()
## )</code></pre>
<pre class="r"><code>colnames(wine_data) &lt;- gsub(&quot; &quot;, &quot;_&quot;, colnames(wine_data))
glimpse(wine_data)</code></pre>
<pre><code>## Observations: 1,599
## Variables: 12
## $ fixed_acidity        &lt;dbl&gt; 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, ...
## $ volatile_acidity     &lt;dbl&gt; 0.700, 0.880, 0.760, 0.280, 0.700, 0.660,...
## $ citric_acid          &lt;dbl&gt; 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06,...
## $ residual_sugar       &lt;dbl&gt; 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2...
## $ chlorides            &lt;dbl&gt; 0.076, 0.098, 0.092, 0.075, 0.076, 0.075,...
## $ free_sulfur_dioxide  &lt;dbl&gt; 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15...
## $ total_sulfur_dioxide &lt;dbl&gt; 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, ...
## $ density              &lt;dbl&gt; 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0...
## $ pH                   &lt;dbl&gt; 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30,...
## $ sulphates            &lt;dbl&gt; 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46,...
## $ alcohol              &lt;dbl&gt; 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, ...
## $ quality              &lt;fct&gt; qual_low, qual_low, qual_low, qual_high, ...</code></pre>
</div>
<div id="eda" class="section level3">
<h3>EDA</h3>
<p>The first step in my machine learning workflows in exploratory data analysis (EDA). This can get pretty extensive, but here I am only looking at the distributions of my features and the class counts of my response variable.</p>
<pre class="r"><code>p1 &lt;- wine_data %&gt;%
  ggplot(aes(x = quality, fill = quality)) +
    geom_bar(alpha = 0.8) +
    scale_fill_tableau() +
    guides(fill = FALSE)</code></pre>
<pre class="r"><code>p2 &lt;- wine_data %&gt;%
  gather(x, y, fixed_acidity:alcohol) %&gt;%
  ggplot(aes(x = y, y = quality, color = quality, fill = quality)) +
    facet_wrap( ~ x, scale = &quot;free&quot;, ncol = 3) +
    scale_fill_tableau() +
    scale_color_tableau() +
    geom_density_ridges(alpha = 0.8) +
    guides(fill = FALSE, color = FALSE)</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, ncol = 2, widths = c(0.3, 0.7))</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/eda_plots-1.png" width="960" /></p>
</div>
<div id="model" class="section level3">
<h3>Model</h3>
<p>For modeling, I am splitting the data into 80% for training and 20% for testing.</p>
<pre class="r"><code>set.seed(42)
idx &lt;- createDataPartition(wine_data$quality, 
                           p = 0.8, 
                           list = FALSE, 
                           times = 1)

wine_train &lt;- wine_data[ idx,]
wine_test  &lt;- wine_data[-idx,]</code></pre>
<p>I am using 5-fold cross-validation, repeated 3x and scale and center the data. The example model I am using here is a Random Forest model.</p>
<pre class="r"><code>fit_control &lt;- trainControl(method = &quot;repeatedcv&quot;,
                           number = 5,
                           repeats = 3)

set.seed(42)
rf_model &lt;- train(quality ~ ., 
                  data = wine_train, 
                  method = &quot;rf&quot;, 
                  preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                  trControl = fit_control,
                  verbose = FALSE)</code></pre>
<pre class="r"><code>rf_model</code></pre>
<pre><code>## Random Forest 
## 
## 1280 samples
##   11 predictor
##    2 classes: &#39;qual_high&#39;, &#39;qual_low&#39; 
## 
## Pre-processing: scaled (11), centered (11) 
## Resampling: Cross-Validated (5 fold, repeated 3 times) 
## Summary of sample sizes: 1023, 1024, 1025, 1024, 1024, 1024, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    2    0.7958240  0.5898607
##    6    0.7893104  0.5766700
##   11    0.7882738  0.5745067
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<p>Now let’s see how good our model is:</p>
<pre class="r"><code>test_predict &lt;- predict(rf_model, wine_test)
confusionMatrix(test_predict, as.factor(wine_test$quality))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  qual_high qual_low
##   qual_high       140       26
##   qual_low         31      122
##                                           
##                Accuracy : 0.8213          
##                  95% CI : (0.7748, 0.8618)
##     No Information Rate : 0.5361          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.6416          
##  Mcnemar&#39;s Test P-Value : 0.5962          
##                                           
##             Sensitivity : 0.8187          
##             Specificity : 0.8243          
##          Pos Pred Value : 0.8434          
##          Neg Pred Value : 0.7974          
##              Prevalence : 0.5361          
##          Detection Rate : 0.4389          
##    Detection Prevalence : 0.5204          
##       Balanced Accuracy : 0.8215          
##                                           
##        &#39;Positive&#39; Class : qual_high       
## </code></pre>
<p>Okay, this model isn’t too accurate but since my focus here is supposed to be on explaining the model, that’s good enough for me at this point.</p>
</div>
<div id="explaininginterpreting-the-model" class="section level3">
<h3>Explaining/interpreting the model</h3>
<p>There are several methods and tools that can be used to explain or interpret machine learning models. You can read more about them in <a href="https://christophm.github.io/interpretable-ml-book/">this ebook</a>. Here, I am going to show a few of them.</p>
<div id="feature-importance" class="section level4">
<h4>Feature importance</h4>
<p>The first metric to look at for Random Forest models (and many other algorithms) is feature importance:</p>
<blockquote>
<p>“Variable importance evaluation functions can be separated into two groups: those that use the model information and those that do not. The advantage of using a model-based approach is that is more closely tied to the model performance and that it may be able to incorporate the correlation structure between the predictors into the importance calculation.” <a href="https://topepo.github.io/caret/variable-importance.html" class="uri">https://topepo.github.io/caret/variable-importance.html</a></p>
</blockquote>
<p>The <code>varImp()</code> function from the <code>caret</code> package can be used to calculate feature importance measures for most methods. For Random Forest classification models such as ours, the prediction error rate is calculated for</p>
<ol style="list-style-type: decimal">
<li>permuted out-of-bag data of each tree and</li>
<li>permutations of every feature</li>
</ol>
<p>These two measures are averaged and normalized as described here:</p>
<blockquote>
<p>“Here are the definitions of the variable importance measures. The first measure is computed from permuting OOB data: For each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression). Then the same is done after permuting each predictor variable. The difference between the two are then averaged over all trees, and normalized by the standard deviation of the differences. If the standard deviation of the differences is equal to 0 for a variable, the division is not done (but the average is almost always equal to 0 in that case). The second measure is the total decrease in node impurities from splitting on the variable, averaged over all trees. For classification, the node impurity is measured by the Gini index. For regression, it is measured by residual sum of squares.” randomForest help function for <code>importance()</code></p>
</blockquote>
<pre class="r"><code>rf_model_imp &lt;- varImp(rf_model, scale = TRUE)
p1 &lt;- rf_model_imp$importance %&gt;%
  as.data.frame() %&gt;%
  rownames_to_column() %&gt;%
  ggplot(aes(x = reorder(rowname, Overall), y = Overall)) +
    geom_bar(stat = &quot;identity&quot;, fill = &quot;#1F77B4&quot;, alpha = 0.8) +
    coord_flip()</code></pre>
<p>We can also use a ROC curve for evaluating feature importance. For this, we have the <code>caret::filterVarImp()</code> function:</p>
<blockquote>
<p>“The importance of each predictor is evaluated individually using a “filter” approach. For classification, ROC curve analysis is conducted on each predictor. For two class problems, a series of cutoffs is applied to the predictor data to predict the class. The sensitivity and specificity are computed for each cutoff and the ROC curve is computed. The trapezoidal rule is used to compute the area under the ROC curve. This area is used as the measure of variable importance. For multi–class outcomes, the problem is decomposed into all pair-wise problems and the area under the curve is calculated for each class pair (i.e class 1 vs. class 2, class 2 vs. class 3 etc.). For a specific class, the maximum area under the curve across the relevant pair–wise AUC’s is used as the variable importance measure. For regression, the relationship between each predictor and the outcome is evaluated. An argument, nonpara, is used to pick the model fitting technique. When nonpara = FALSE, a linear model is fit and the absolute value of the <span class="math inline">\(t\)</span>–value for the slope of the predictor is used. Otherwise, a loess smoother is fit between the outcome and the predictor. The <span class="math inline">\(R^2\)</span> statistic is calculated for this model against the intercept only null model.” caret help for <code>filterVarImp()</code></p>
</blockquote>
<pre class="r"><code>roc_imp &lt;- filterVarImp(x = wine_train[, -ncol(wine_train)], y = wine_train$quality)
p2 &lt;- roc_imp %&gt;%
  as.data.frame() %&gt;%
  rownames_to_column() %&gt;%
  ggplot(aes(x = reorder(rowname, qual_high), y = qual_high)) +
    geom_bar(stat = &quot;identity&quot;, fill = &quot;#1F77B4&quot;, alpha = 0.8) +
    coord_flip()</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, ncol = 2, widths = c(0.5, 0.5))</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/feature_imp_plots-1.png" width="960" /></p>
</div>
<div id="iml" class="section level4">
<h4>iml</h4>
<p>The <code>iml</code> package combines a number of methods for explaining/interpreting machine learning model, like</p>
<ul>
<li>Feature importance</li>
<li>Partial dependence plots</li>
<li>Individual conditional expectation plots (ICE)</li>
<li>Tree surrogate</li>
<li>LocalModel: Local Interpretable Model-agnostic Explanations (similar to <a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits_lime/">lime</a>)</li>
<li>Shapley value for explaining single predictions</li>
</ul>
<p>In order to work with <code>iml</code>, we need to adapt our data a bit by removing the response variable and the creating a new predictor object that holds the model, the data and the class labels.</p>
<blockquote>
<p>“The iml package uses R6 classes: New objects can be created by calling Predictor$new().” <a href="https://github.com/christophM/iml/blob/master/vignettes/intro.Rmd" class="uri">https://github.com/christophM/iml/blob/master/vignettes/intro.Rmd</a></p>
</blockquote>
<pre class="r"><code>X &lt;- wine_train %&gt;%
  select(-quality) %&gt;%
  as.data.frame()

predictor &lt;- Predictor$new(rf_model, data = X, y = wine_train$quality)
str(predictor)</code></pre>
<pre><code>## Classes &#39;Predictor&#39;, &#39;R6&#39; &lt;Predictor&gt;
##   Public:
##     class: NULL
##     clone: function (deep = FALSE) 
##     data: Data, R6
##     initialize: function (model = NULL, data, predict.fun = NULL, y = NULL, class = NULL) 
##     model: train, train.formula
##     predict: function (newdata) 
##     prediction.colnames: NULL
##     prediction.function: function (newdata) 
##     print: function () 
##     task: classification
##   Private:
##     predictionChecked: FALSE</code></pre>
<div id="partial-dependence-individual-conditional-expectation-plots-ice" class="section level5">
<h5>Partial Dependence &amp; Individual Conditional Expectation plots (ICE)</h5>
<p>Now we can explore some of the different methods. Let’s start with <a href="https://christophm.github.io/interpretable-ml-book/pdp.html">partial dependence plots</a> as we had already looked into feature importance.</p>
<blockquote>
<p>“Besides knowing which features were important, we are interested in how the features influence the predicted outcome. The Partial class implements partial dependence plots and individual conditional expectation curves. Each individual line represents the predictions (y-axis) for one data point when we change one of the features (e.g. ‘lstat’ on the x-axis). The highlighted line is the point-wise average of the individual lines and equals the partial dependence plot. The marks on the x-axis indicates the distribution of the ‘lstat’ feature, showing how relevant a region is for interpretation (little or no points mean that we should not over-interpret this region).” <a href="https://github.com/christophM/iml/blob/master/vignettes/intro.Rmd#partial-dependence" class="uri">https://github.com/christophM/iml/blob/master/vignettes/intro.Rmd#partial-dependence</a></p>
</blockquote>
<p>We can look at individual features, like the alcohol or pH and plot the curves:</p>
<pre class="r"><code>pdp_obj &lt;- Partial$new(predictor, feature = &quot;alcohol&quot;)
pdp_obj$center(min(wine_train$alcohol))
glimpse(pdp_obj$results)</code></pre>
<pre><code>## Observations: 51,240
## Variables: 5
## $ alcohol &lt;dbl&gt; 8.400000, 8.742105, 9.084211, 9.426316, 9.768421, 10.1...
## $ .class  &lt;fct&gt; qual_high, qual_high, qual_high, qual_high, qual_high,...
## $ .y.hat  &lt;dbl&gt; 0.00000000, 0.00259375, -0.02496406, -0.03126250, 0.02...
## $ .type   &lt;chr&gt; &quot;pdp&quot;, &quot;pdp&quot;, &quot;pdp&quot;, &quot;pdp&quot;, &quot;pdp&quot;, &quot;pdp&quot;, &quot;pdp&quot;, &quot;pdp&quot;...
## $ .id     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...</code></pre>
<blockquote>
<p>“The partial dependence plot calculates and plots the dependence of f(X) on a single or two features. It’s the aggregate of all individual conditional expectation curves, that describe how, for a single observation, the prediction changes when the feature changes.” iml help for <code>Partial</code></p>
</blockquote>
<pre class="r"><code>pdp_obj$plot()</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/pd_plots-1.png" width="960" /></p>
<pre class="r"><code>pdp_obj2 &lt;- Partial$new(predictor, feature = c(&quot;sulphates&quot;, &quot;pH&quot;))
pdp_obj2$plot()</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/pd_plots_2-1.png" width="960" /></p>
</div>
<div id="feature-interaction" class="section level5">
<h5>Feature interaction</h5>
<blockquote>
<p>“Interactions between features are measured via the decomposition of the prediction function: If a feature j has no interaction with any other feature, the prediction function can be expressed as the sum of the partial function that depends only on j and the partial function that only depends on features other than j. If the variance of the full function is completely explained by the sum of the partial functions, there is no interaction between feature j and the other features. Any variance that is not explained can be attributed to the interaction and is used as a measure of interaction strength. The interaction strength between two features is the proportion of the variance of the 2-dimensional partial dependence function that is not explained by the sum of the two 1-dimensional partial dependence functions. The interaction measure takes on values between 0 (no interaction) to 1.” iml help for <code>Interaction</code></p>
</blockquote>
<pre class="r"><code>interact &lt;- Interaction$new(predictor, feature = &quot;alcohol&quot;)</code></pre>
<p>All of these methods have a plot argument. However, since I am writing this for an article, I want all my plots to have the same look. That’s why I’m customizing the plots I want to use in my article as shown below.</p>
<pre class="r"><code>#plot(interact)
interact$results %&gt;%
  ggplot(aes(x = reorder(.feature, .interaction), y = .interaction, fill = .class)) +
    facet_wrap(~ .class, ncol = 2) +
    geom_bar(stat = &quot;identity&quot;, alpha = 0.8) +
    scale_fill_tableau() +
    coord_flip() +
    guides(fill = FALSE)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/interaction_plot-1.png" width="672" /></p>
</div>
<div id="tree-surrogate" class="section level5">
<h5>Tree Surrogate</h5>
<p>The tree surrogate method uses decision trees on the predictions.</p>
<blockquote>
<p>“A conditional inference tree is fitted on the predicted  from the machine learning model and the data. The partykit package and function are used to fit the tree. By default a tree of maximum depth of 2 is fitted to improve interpretability.” iml help for <code>TreeSurrogate</code></p>
</blockquote>
<pre class="r"><code>tree &lt;- TreeSurrogate$new(predictor, maxdepth = 5)</code></pre>
<p>The R^2 value gives an estimate of the goodness of fit or how well the decision tree approximates the model.</p>
<pre class="r"><code>tree$r.squared</code></pre>
<pre><code>## [1] 0.4571756 0.4571756</code></pre>
<pre class="r"><code>#plot(tree)
tree$results %&gt;%
  mutate(prediction = colnames(select(., .y.hat.qual_high, .y.hat.qual_low))[max.col(select(., .y.hat.qual_high, .y.hat.qual_low),
                                                                                     ties.method = &quot;first&quot;)],
         prediction = ifelse(prediction == &quot;.y.hat.qual_low&quot;, &quot;qual_low&quot;, &quot;qual_high&quot;)) %&gt;%
  ggplot(aes(x = prediction, fill = prediction)) +
    facet_wrap(~ .path, ncol = 5) +
    geom_bar(alpha = 0.8) +
    scale_fill_tableau() +
    guides(fill = FALSE)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/tree_surr_plot-1.png" width="960" /></p>
</div>
<div id="localmodel-local-interpretable-model-agnostic-explanations" class="section level5">
<h5>LocalModel: Local Interpretable Model-agnostic Explanations</h5>
<p>LocalModel is a implementation of the LIME algorithm from <a href="https://arxiv.org/abs/1602.04938">Ribeiro et al. 2016</a>, similar to <a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits_lime/">lime</a>.</p>
<p>According to the LIME principle, we can look at individual predictions. Here, for example on the first row of the test set:</p>
<pre class="r"><code>X2 &lt;- wine_test[, -12]
i = 1
lime_explain &lt;- LocalModel$new(predictor, x.interest = X2[i, ])
lime_explain$results</code></pre>
<pre><code>##            beta x.recoded      effect x.original              feature
## 1 -0.7653408409       0.7 -0.53573859        0.7     volatile_acidity
## 2 -0.0006292149      34.0 -0.02139331         34 total_sulfur_dioxide
## 3  0.2624431667       9.4  2.46696577        9.4              alcohol
## 4  0.7653408409       0.7  0.53573859        0.7     volatile_acidity
## 5  0.0006292149      34.0  0.02139331         34 total_sulfur_dioxide
## 6 -0.2624431667       9.4 -2.46696577        9.4              alcohol
##             feature.value    .class
## 1    volatile_acidity=0.7 qual_high
## 2 total_sulfur_dioxide=34 qual_high
## 3             alcohol=9.4 qual_high
## 4    volatile_acidity=0.7  qual_low
## 5 total_sulfur_dioxide=34  qual_low
## 6             alcohol=9.4  qual_low</code></pre>
<pre class="r"><code>#plot(lime_explain)
p1 &lt;- lime_explain$results %&gt;%
  ggplot(aes(x = reorder(feature.value, -effect), y = effect, fill = .class)) +
    facet_wrap(~ .class, ncol = 2) +
    geom_bar(stat = &quot;identity&quot;, alpha = 0.8) +
    scale_fill_tableau() +
    coord_flip() +
    labs(title = paste0(&quot;Test case #&quot;, i)) +
    guides(fill = FALSE)</code></pre>
<p>… or for the sixth row:</p>
<pre class="r"><code>i = 6
lime_explain$explain(X2[i, ])
p2 &lt;- lime_explain$results %&gt;%
  ggplot(aes(x = reorder(feature.value, -effect), y = effect, fill = .class)) +
    facet_wrap(~ .class, ncol = 2) +
    geom_bar(stat = &quot;identity&quot;, alpha = 0.8) +
    scale_fill_tableau() +
    coord_flip() +
    labs(title = paste0(&quot;Test case #&quot;, i)) +
    guides(fill = FALSE)</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, ncol = 2)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/lime_plot_1-1.png" width="960" /></p>
</div>
<div id="shapley-value-for-explaining-single-predictions" class="section level5">
<h5>Shapley value for explaining single predictions</h5>
<p>Another way to interpret individual predictions is whit Shapley values.</p>
<blockquote>
<p>“Shapley computes feature contributions for single predictions with the Shapley value, an approach from cooperative game theory. The features values of an instance cooperate to achieve the prediction. The Shapley value fairly distributes the difference of the instance’s prediction and the datasets average prediction among the features.” iml help for <code>Shapley</code></p>
</blockquote>
<p>More information about Shapley values can be found <a href="https://christophm.github.io/interpretable-ml-book/shapley.html">here</a>.</p>
<pre class="r"><code>shapley &lt;- Shapley$new(predictor, x.interest = X2[1, ])</code></pre>
<pre class="r"><code>head(shapley$results)</code></pre>
<pre><code>##               feature     class      phi     phi.var
## 1       fixed_acidity qual_high -0.01100 0.003828485
## 2    volatile_acidity qual_high -0.16356 0.019123037
## 3         citric_acid qual_high -0.02318 0.005886472
## 4      residual_sugar qual_high -0.00950 0.001554939
## 5           chlorides qual_high -0.01580 0.002868889
## 6 free_sulfur_dioxide qual_high  0.00458 0.001250044
##            feature.value
## 1      fixed_acidity=7.4
## 2   volatile_acidity=0.7
## 3          citric_acid=0
## 4     residual_sugar=1.9
## 5        chlorides=0.076
## 6 free_sulfur_dioxide=11</code></pre>
<pre class="r"><code>#shapley$plot()
shapley$results %&gt;%
  ggplot(aes(x = reorder(feature.value, -phi), y = phi, fill = class)) +
    facet_wrap(~ class, ncol = 2) +
    geom_bar(stat = &quot;identity&quot;, alpha = 0.8) +
    scale_fill_tableau() +
    coord_flip() +
    guides(fill = FALSE)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/shapley_plot-1.png" width="672" /></p>
</div>
</div>
<div id="breakdown" class="section level4">
<h4>breakDown</h4>
<p>Another package worth mentioning is <a href="https://cran.r-project.org/web/packages/breakDown/index.html">breakDown</a>. It provides</p>
<blockquote>
<p>“Model agnostic tool for decomposition of predictions from black boxes. Break Down Table shows contributions of every variable to a final prediction. Break Down Plot presents variable contributions in a concise graphical way. This package work for binary classifiers and general regression models.” <a href="https://cran.r-project.org/web/packages/breakDown/index.html" class="uri">https://cran.r-project.org/web/packages/breakDown/index.html</a></p>
</blockquote>
<p>The <code>broken()</code> function decomposes model predictions and outputs the contributions of each feature to the final prediction.</p>
<pre class="r"><code>predict.function &lt;- function(model, new_observation) {
  predict(model, new_observation, type=&quot;prob&quot;)[,2]
}
predict.function(rf_model, X2[1, ])</code></pre>
<pre><code>## [1] 0.966</code></pre>
<pre class="r"><code>br &lt;- broken(model = rf_model, 
             new_observation = X2[1, ], 
             data = X, 
             baseline = &quot;Intercept&quot;, 
             predict.function = predict.function, 
             keep_distributions = TRUE)
br</code></pre>
<pre><code>##                             contribution
## (Intercept)                        0.000
## + alcohol = 9.4                    0.138
## + volatile_acidity = 0.7           0.097
## + sulphates = 0.56                 0.060
## + density = 0.9978                 0.038
## + pH = 3.51                        0.012
## + chlorides = 0.076                0.017
## + citric_acid = 0                  0.026
## + fixed_acidity = 7.4              0.048
## + residual_sugar = 1.9             0.014
## + free_sulfur_dioxide = 11         0.016
## + total_sulfur_dioxide = 34        0.034
## final_prognosis                    0.501
## baseline:  0.4654328</code></pre>
<p>The plot function shows the average predictions and the final prognosis:</p>
<pre class="r"><code>#plot(br)
data.frame(y = br$contribution,
           x = br$variable) %&gt;%
  ggplot(aes(x = reorder(x, y), y = y)) +
    geom_bar(stat = &quot;identity&quot;, fill = &quot;#1F77B4&quot;, alpha = 0.8) +
    coord_flip()</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/breakdown_plot_1-1.png" width="672" /></p>
<p>If we set <code>keep_distributions = TRUE</code>, we can plot these distributions of partial predictions, as well as the average.</p>
<pre class="r"><code>plot(br, plot_distributions = TRUE)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/breakdown_plot_2-1.png" width="672" /></p>
</div>
<div id="dalex-descriptive-machine-learning-explanations" class="section level4">
<h4>DALEX: Descriptive mAchine Learning EXplanations</h4>
<p>The third package I want to showcase is <a href="https://cran.r-project.org/web/packages/DALEX/index.html">DALEX</a>, which stands for Descriptive mAchine Learning EXplanations and contains a collection of functions that help with interpreting/explaining black-box models.</p>
<blockquote>
<p>“Machine Learning (ML) models are widely used and have various applications in classification or regression. Models created with boosting, bagging, stacking or similar techniques are often used due to their high performance, but such black-box models usually lack of interpretability. DALEX package contains various explainers that help to understand the link between input variables and model output. The single_variable() explainer extracts conditional response of a model as a function of a single selected variable. It is a wrapper over packages ‘pdp’ and ‘ALEPlot’. The single_prediction() explainer attributes parts of a model prediction to particular variables used in the model. It is a wrapper over ‘breakDown’ package. The variable_dropout() explainer calculates variable importance scores based on variable shuffling. All these explainers can be plotted with generic plot() function and compared across different models.” <a href="https://cran.r-project.org/web/packages/DALEX/index.html" class="uri">https://cran.r-project.org/web/packages/DALEX/index.html</a></p>
</blockquote>
<p>We first create an explain object, that has the correct structure for use with the <code>DALEX</code> package.</p>
<pre class="r"><code>p_fun &lt;- function(object, newdata){predict(object, newdata = newdata, type = &quot;prob&quot;)[, 2]}
yTest &lt;- as.numeric(wine_test$quality)

explainer_classif_rf &lt;- DALEX::explain(rf_model, label = &quot;rf&quot;,
                                       data = wine_test, y = yTest,
                                       predict_function = p_fun)</code></pre>
<div id="model-performance" class="section level5">
<h5>Model performance</h5>
<p>With DALEX we can do several things, for example analyze model performance as the distribution of residuals.</p>
<pre class="r"><code>mp_classif_rf &lt;- model_performance(explainer_classif_rf)</code></pre>
<pre class="r"><code>plot(mp_classif_rf)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/dalex_perf_plot_1-1.png" width="672" /></p>
<pre class="r"><code>plot(mp_classif_rf, geom = &quot;boxplot&quot;)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/dalex_perf_plot_2-1.png" width="672" /></p>
</div>
<div id="feature-importance-1" class="section level5">
<h5>Feature importance</h5>
<p>Feature importance can be measured with <code>variable_importance()</code> function, which gives the loss from variable dropout.</p>
<pre class="r"><code>vi_classif_rf &lt;- variable_importance(explainer_classif_rf, loss_function = loss_root_mean_square)</code></pre>
<pre class="r"><code>plot(vi_classif_rf)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/dalex_var_imp_plot-1.png" width="672" /></p>
</div>
<div id="variable-response" class="section level5">
<h5>Variable response</h5>
<p>And we can calculate the marginal response for a single variable with the <code>variable_response()</code> function.</p>
<blockquote>
<p>“Calculates the average model response as a function of a single selected variable. Use the ‘type’ parameter to select the type of marginal response to be calculated. Currently for numeric variables we have Partial Dependency and Accumulated Local Effects implemented. Current implementation uses the ‘pdp’ package (Brandon M. Greenwell (2017). pdp: An R Package for Constructing Partial Dependence Plots. The R Journal, 9(1), 421–436.) and ‘ALEPlot’ (Dan Apley (2017). ALEPlot: Accumulated Local Effects Plots and Partial Dependence Plots.)” DALEX help for <code>variable_response</code></p>
</blockquote>
<p>As <code>type</code> we can choose between ‘pdp’ for Partial Dependence Plots and ‘ale’ for Accumulated Local Effects.</p>
<pre class="r"><code>pdp_classif_rf  &lt;- variable_response(explainer_classif_rf, variable = &quot;alcohol&quot;, type = &quot;pdp&quot;)</code></pre>
<pre class="r"><code>plot(pdp_classif_rf)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/dalex_var_resp_plot_1-1.png" width="672" /></p>
<pre class="r"><code>ale_classif_rf  &lt;- variable_response(explainer_classif_rf, variable = &quot;alcohol&quot;, type = &quot;ale&quot;)
plot(ale_classif_rf)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/dalex_var_resp_plot_2-1.png" width="672" /></p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] grid      stats     graphics  grDevices utils     datasets  methods  
## [8] base     
## 
## other attached packages:
##  [1] gower_0.1.2     glmnet_2.0-16   foreach_1.4.4   Matrix_1.2-14  
##  [5] bindrcpp_0.2.2  DALEX_0.2.3     breakDown_0.1.6 iml_0.5.1      
##  [9] ggthemes_3.5.0  ggridges_0.5.0  gridExtra_2.3   caret_6.0-80   
## [13] lattice_0.20-35 forcats_0.3.0   stringr_1.3.1   dplyr_0.7.6    
## [17] purrr_0.2.5     readr_1.1.1     tidyr_0.8.1     tibble_1.4.2   
## [21] ggplot2_3.0.0   tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.1.0        backports_1.1.2     plyr_1.8.4         
##   [4] lazyeval_0.2.1      sp_1.3-1            splines_3.5.1      
##   [7] AlgDesign_1.1-7.3   digest_0.6.15       htmltools_0.3.6    
##  [10] gdata_2.18.0        magrittr_1.5        checkmate_1.8.5    
##  [13] cluster_2.0.7-1     sfsmisc_1.1-2       Metrics_0.1.4      
##  [16] recipes_0.1.3       modelr_0.1.2        dimRed_0.1.0       
##  [19] gmodels_2.18.1      colorspace_1.3-2    rvest_0.3.2        
##  [22] haven_1.1.2         xfun_0.3            crayon_1.3.4       
##  [25] jsonlite_1.5        libcoin_1.0-1       ALEPlot_1.1        
##  [28] bindr_0.1.1         survival_2.42-3     iterators_1.0.9    
##  [31] glue_1.2.0          DRR_0.0.3           gtable_0.2.0       
##  [34] ipred_0.9-6         questionr_0.6.2     kernlab_0.9-26     
##  [37] ddalpha_1.3.4       DEoptimR_1.0-8      abind_1.4-5        
##  [40] scales_0.5.0        mvtnorm_1.0-8       miniUI_0.1.1.1     
##  [43] Rcpp_0.12.17        xtable_1.8-2        spData_0.2.9.0     
##  [46] magic_1.5-8         proxy_0.4-22        foreign_0.8-70     
##  [49] spdep_0.7-7         Formula_1.2-3       stats4_3.5.1       
##  [52] lava_1.6.2          prodlim_2018.04.18  httr_1.3.1         
##  [55] yaImpute_1.0-29     RColorBrewer_1.1-2  pkgconfig_2.0.1    
##  [58] nnet_7.3-12         deldir_0.1-15       labeling_0.3       
##  [61] tidyselect_0.2.4    rlang_0.2.1         reshape2_1.4.3     
##  [64] later_0.7.3         munsell_0.5.0       cellranger_1.1.0   
##  [67] tools_3.5.1         cli_1.0.0           factorMerger_0.3.6 
##  [70] pls_2.6-0           broom_0.4.5         evaluate_0.10.1    
##  [73] geometry_0.3-6      yaml_2.1.19         ModelMetrics_1.1.0 
##  [76] knitr_1.20          robustbase_0.93-1   pdp_0.6.0          
##  [79] randomForest_4.6-14 nlme_3.1-137        mime_0.5           
##  [82] RcppRoll_0.3.0      xml2_1.2.0          compiler_3.5.1     
##  [85] rstudioapi_0.7      e1071_1.6-8         klaR_0.6-14        
##  [88] stringi_1.2.3       highr_0.7           blogdown_0.6       
##  [91] psych_1.8.4         pillar_1.2.3        LearnBayes_2.15.1  
##  [94] combinat_0.0-8      data.table_1.11.4   httpuv_1.4.4.2     
##  [97] agricolae_1.2-8     R6_2.2.2            bookdown_0.7       
## [100] promises_1.0.1      codetools_0.2-15    boot_1.3-20        
## [103] MASS_7.3-50         gtools_3.8.1        assertthat_0.2.0   
## [106] CVST_0.2-2          rprojroot_1.3-2     withr_2.1.2        
## [109] mnormt_1.5-5        expm_0.999-2        parallel_3.5.1     
## [112] hms_0.4.2           rpart_4.1-13        timeDate_3043.102  
## [115] coda_0.19-1         class_7.3-14        rmarkdown_1.10     
## [118] inum_1.0-0          ggpubr_0.1.7        partykit_1.2-2     
## [121] shiny_1.1.0         lubridate_1.7.4</code></pre>
</div>
</div>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[My upcoming conference talks &amp; workshops: M-cubed, ML Summit &amp; data2day]]></title>
    <link href="/2018/07/mcubed_mlsummit_data2day/"/>
    <id>/2018/07/mcubed_mlsummit_data2day/</id>
    <published>2018-07-12T00:00:00+00:00</published>
    <updated>2018-07-12T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I’ll be giving talks and workshops at the following three upcoming conferences; hope to meet some of you there!</p>
<p><br></p>
<ul>
<li>From 15th to 17th October 2018, I’ll be in London for the <a href="https://www.mcubed.london/">M-cubed conference</a>. My talk about <a href="https://www.mcubed.london/sessions/explaining-complex-machine-learning-models-lime/">Explaining complex machine learning models with LIME</a> will take place on October 16</li>
</ul>
<blockquote>
<p>Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex machine learning models are essentially black boxes and too complicated to understand, we need to use approximations.</p>
</blockquote>
<blockquote>
<p>Required audience experience: Basic knowledge of machine learning</p>
</blockquote>
<blockquote>
<p>Objective of the talk: Listeners will get an overview of why understanding machine learning models is important, how it can help us improve models and help gain trust in their decisions. I will explain in detail how one popular approach to explaining complex models – LIME – works and show an example analysis.</p>
</blockquote>
<div class="figure">
<img src="https://www.mcubed.london/wp-content/uploads/2017/12/M3_Image_v1_b-1024x626.jpg" alt="M-cubed banner" />
<p class="caption">M-cubed banner</p>
</div>
<hr />
<p><br></p>
<ul>
<li>At the <a href="www.ml-summit.de">ML Summit</a> held on October 1st and 2nd in Berlin, Germany, I’ll be giving a workshop about <a href="https://ml-summit.de/specialized-topics/bildklassifikation-leicht-gemacht-mit-keras-und-tensorflow/">image classification with Keras</a> (<strong>German language</strong>).</li>
</ul>
<blockquote>
<p>Bildklassifikation leicht gemacht – mit Keras und TensorFlow</p>
</blockquote>
<blockquote>
<p>Tuesday, 2. October 2018 | 10:00 - 13:00 Lange Zeit galt die automatische Erkennung von Objekten, Menschen und Szenen auf Bildern durch Computer als unmöglich. Die Komplexität schien schlicht zu groß, um sie einem Algorithmus programmatisch beibringen zu können. Doch Neuronale Netze haben dies drastisch verändert! Inzwischen ist Bilderkennung ist ein weit verbreitetes Anwendungsgebiet von Maschinellem Lernen. Häufig werden dafür sogenannte “Convolutional Neuronal Networks”, oder “ConvNets” verwendet. In diesem Workshop werde ich zeigen, wie einfach es ist, solch ein Neuronales Netz selber zu bauen. Dafür werden wir Keras und TensorFlow verwenden. Wir werden zunächst ein komplettes Netz selber trainieren: vom Einlesen der Bilder, über das Definieren des Netzes, hin zum Evaluieren auf Testbildern. Anschließend gucken wir uns an, wie man mit Transfer Learning und vortrainierten Netzen auch mit wenigen eigenen Bildern schnell Erfolge sehen kann. Im letzten Teil des Workshops soll es dann darum gehen, wie wir diese Bilderkennungsmodelle besser verstehen können – zum Beispiel indem wir die Knoten in Zwischenschichten visualisieren; so können wir Muster und für die Klassifikation wichtige Bildbereiche finden und die Klassifikation durch das Modell nachvollziehen. Installationshinweise: Wir werden mit Python3 in Google Collaboratory arbeiten.</p>
</blockquote>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/MLS_18_Webbanner_300x250_46396_v1.jpg" alt="ML Summit banner" />
<p class="caption">ML Summit banner</p>
</div>
<hr />
<p><br></p>
<ul>
<li>Together with my colleague Mark, I’ll be giving a workshop about <a href="https://www.data2day.de/veranstaltung-6953-end-2-end-vom-keras-tensorflow-modell-zur-produktion.html?id=6953">“END-2-END VOM KERAS TENSORFLOW-MODELL ZUR PRODUKTION”</a> at the data2day conference, which is being held from September 25th - 27th 2018 in Heidelberg, Germany (<strong>German language</strong>).</li>
</ul>
<blockquote>
<p>Durch das stark wachsende Datenvolumen hat sich das Rollenverständnis von Data Scientists erweitert. Statt Machine-Learning-Modelle für einmalige Analysen zu erstellen, wird häufiger in konkreten Entwicklungsprojekten gearbeitet, in denen Prototypen in produktive Anwendungen überführt werden. Keras ist eine High-Level-Schnittstelle, die ein schnelles, einfaches und flexibles Prototypisieren von Neuronalen Netzwerken mit TensorFlow ermöglicht. Zusammen mit Luigi lassen sich beliebig komplexe Datenverarbeitungs-Workflows in Python erstellen. Das führt dazu, dass auch Nicht-Entwickler den End-2-End-Workflow des Keras-TensorFlow-Modells zur Produktionsreife leicht implementieren können.</p>
</blockquote>
<div class="figure">
<img src="https://www.data2day.de/common/images/konferenzen/data2day2018.svg" alt="data2day banner" />
<p class="caption">data2day banner</p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Addendum: Text-to-Speech with the googleLanguageR package]]></title>
    <link href="/2018/06/googlelanguager/"/>
    <id>/2018/06/googlelanguager/</id>
    <published>2018-06-29T00:00:00+00:00</published>
    <updated>2018-06-29T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>After posting my short blog post about <a href="https://shirinsplayground.netlify.com/2018/06/text_to_speech_r/">Text-to-speech with R</a>, I got two very useful tips. One was to use the <code>googleLanguageR</code> package, which uses the Google Cloud Text-to-Speech API.</p>
<p>And indeed, it was very easy to use and the resulting audio sounded much better than what I tried before!</p>
<p>Here’s a short example of how to use the package for TTS:</p>
<div id="set-up-google-cloud-and-authentification" class="section level2">
<h2>Set up Google Cloud and authentification</h2>
<p>You first need to set up a Google Cloud Account and provide credit card information (the first year is free to use, though). If you haven’t used Google Cloud before, you will need to wait until you activated your account; in my case, I had to wait two days until they sent a small amount of money to my bank account which I then needed to enter in order to verify the information.</p>
<p>Then, you create a project, activate the API(s) you want to use and create the authentication information as a JSON file. More information on how to do this is described <a href="http://code.markedmondson.me/googleLanguageR/index.html">here</a>.</p>
<p>Install and load the library and give the path to the saved authentication JSON file.</p>
<pre class="r"><code>library(googleLanguageR)
gl_auth(&quot;path_to_authentication.json&quot;)</code></pre>
<div id="text-to-speech-with-googlelanguager" class="section level3">
<h3>Text-to-Speech with googleLanguageR</h3>
<p>Now, we can use the <a href="http://code.markedmondson.me/googleLanguageR/articles/text-to-speech.html">Google Cloud Text-to-Speech API</a> from R.</p>
<blockquote>
<p>“Google Cloud Text-to-Speech enables developers to synthesize natural-sounding speech with 30 voices, available in multiple languages and variants. It applies DeepMind’s groundbreaking research in WaveNet and Google’s powerful neural networks to deliver the highest fidelity possible. With this easy-to-use API, you can create lifelike interactions with your users, across many applications and devices.” <a href="http://code.markedmondson.me/googleLanguageR/articles/text-to-speech.html" class="uri">http://code.markedmondson.me/googleLanguageR/articles/text-to-speech.html</a></p>
</blockquote>
<pre class="r"><code>content &lt;- &quot;A common mistake that people make when trying to design something completely foolproof is to underestimate the ingenuity of complete fools.&quot;</code></pre>
<p>We can ask for a list of languages with the <code>gl_talk_languages()</code> function; here, I am looking at all English language options:</p>
<pre class="r"><code>gl_talk_languages(languageCode = &quot;en&quot;)</code></pre>
<pre><code>## # A tibble: 18 x 4
##    languageCodes name             ssmlGender naturalSampleRateHertz
##    &lt;chr&gt;         &lt;chr&gt;            &lt;chr&gt;                       &lt;int&gt;
##  1 en-US         en-US-Wavenet-D  MALE                        24000
##  2 en-US         en-US-Wavenet-A  MALE                        24000
##  3 en-US         en-US-Wavenet-B  MALE                        24000
##  4 en-US         en-US-Wavenet-C  FEMALE                      24000
##  5 en-US         en-US-Wavenet-E  FEMALE                      24000
##  6 en-US         en-US-Wavenet-F  FEMALE                      24000
##  7 en-GB         en-GB-Standard-A FEMALE                      24000
##  8 en-GB         en-GB-Standard-B MALE                        24000
##  9 en-GB         en-GB-Standard-C FEMALE                      24000
## 10 en-GB         en-GB-Standard-D MALE                        24000
## 11 en-US         en-US-Standard-B MALE                        24000
## 12 en-US         en-US-Standard-C FEMALE                      24000
## 13 en-US         en-US-Standard-D MALE                        24000
## 14 en-US         en-US-Standard-E FEMALE                      24000
## 15 en-AU         en-AU-Standard-A FEMALE                      24000
## 16 en-AU         en-AU-Standard-B MALE                        24000
## 17 en-AU         en-AU-Standard-C FEMALE                      24000
## 18 en-AU         en-AU-Standard-D MALE                        24000</code></pre>
<p>Let’s try with three:</p>
<pre class="r"><code>names &lt;- c(&quot;en-US-Wavenet-D&quot;, &quot;en-GB-Standard-C&quot;, &quot;en-AU-Standard-A&quot;)</code></pre>
<pre class="r"><code>for (name in names) {
  gl_talk(content, 
        output = paste0(&quot;/Users/shiringlander/Documents/Github/output_&quot;, name, &quot;.wav&quot;),
        name = name,
        speakingRate = 0.9)
}</code></pre>
<p>The audio files are again <a href="https://soundcloud.com/shirin-glander-729692416/sets/addendum-text-to-speech-with-the-googlelanguager-package/s-oyaAe">saved on SoundCloud</a>.</p>
<p>My verdict: Great package! Great API! :-)</p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.0 (2018-04-23)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.5
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] googleLanguageR_0.2.0
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_0.12.17      knitr_1.20        magrittr_1.5     
##  [4] R6_2.2.2          rlang_0.2.1       stringr_1.3.1    
##  [7] httr_1.3.1        tools_3.5.0       xfun_0.2         
## [10] utf8_1.1.4        cli_1.0.0         googleAuthR_0.6.3
## [13] htmltools_0.3.6   openssl_1.0.1     yaml_2.1.19      
## [16] rprojroot_1.3-2   digest_0.6.15     assertthat_0.2.0 
## [19] tibble_1.4.2      crayon_1.3.4      bookdown_0.7     
## [22] purrr_0.2.5       base64enc_0.1-3   curl_3.2         
## [25] memoise_1.1.0     evaluate_0.10.1   rmarkdown_1.10   
## [28] blogdown_0.6      stringi_1.2.3     pillar_1.2.3     
## [31] compiler_3.5.0    backports_1.1.2   jsonlite_1.5</code></pre>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Code for Workshop: Introduction to Machine Learning with R]]></title>
    <link href="/2018/06/intro_to_ml_workshop_heidelberg/"/>
    <id>/2018/06/intro_to_ml_workshop_heidelberg/</id>
    <published>2018-06-29T00:00:00+00:00</published>
    <updated>2018-06-29T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are the slides from my workshop: Introduction to Machine Learning with R which I gave at the University of Heidelberg, Germany on June 28th 2018. The entire code accompanying the workshop can be found below the video.</p>
<p>The workshop covered the basics of machine learning. With an example dataset I went through a standard machine learning workflow in R with the packages caret and h2o:</p>
<ul>
<li>reading in data</li>
<li>exploratory data analysis</li>
<li>missingness</li>
<li>feature engineering</li>
<li>training and test split</li>
<li>model training with Random Forests, Gradient Boosting, Neural Nets, etc.</li>
<li>hyperparameter tuning</li>
</ul>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/lRX4QJ5TvxgWSv" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen>
</iframe>
<div style="margin-bottom:5px">
<strong> <a href="//www.slideshare.net/ShirinGlander/workshop-introduction-to-machine-learning-with-r" title="Workshop - Introduction to Machine Learning with R" target="_blank">Workshop - Introduction to Machine Learning with R</a> </strong> from <strong><a href="https://www.slideshare.net/ShirinGlander" target="_blank">Shirin Glander</a></strong>
</div>
<p><br></p>
<hr />
<div id="setup" class="section level2">
<h2>Setup</h2>
<p>All analyses are done in R using RStudio. For detailed session information including R version, operating system and package versions, see the <code>sessionInfo()</code> output at the end of this document.</p>
<p>All figures are produced with ggplot2.</p>
<ul>
<li>libraries</li>
</ul>
<pre class="r"><code>library(tidyverse) # for tidy data analysis
library(readr)     # for fast reading of input files
library(mice)      # mice package for Multivariate Imputation by Chained Equations (MICE)</code></pre>
<p><br></p>
</div>
<div id="data-preparation" class="section level2 tabset tabset-fade tabset-pills">
<h2>Data preparation</h2>
<div id="the-dataset" class="section level3">
<h3>The dataset</h3>
<p>The dataset I am using in these example analyses, is the <strong>Breast Cancer Wisconsin (Diagnostic) Dataset</strong>. The data was downloaded from the <a href="http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29">UC Irvine Machine Learning Repository</a>.</p>
<p>The first dataset looks at the predictor classes:</p>
<ul>
<li>malignant or</li>
<li>benign breast mass.</li>
</ul>
<p>The features characterise cell nucleus properties and were generated from image analysis of <a href="https://en.wikipedia.org/wiki/Fine-needle_aspiration">fine needle aspirates (FNA)</a> of breast masses:</p>
<ul>
<li>Sample ID (code number)</li>
<li>Clump thickness</li>
<li>Uniformity of cell size</li>
<li>Uniformity of cell shape</li>
<li>Marginal adhesion</li>
<li>Single epithelial cell size</li>
<li>Number of bare nuclei</li>
<li>Bland chromatin</li>
<li>Number of normal nuclei</li>
<li>Mitosis</li>
<li>Classes, i.e. diagnosis</li>
</ul>
<pre class="r"><code>bc_data &lt;- read_delim(&quot;datasets/breast-cancer-wisconsin.data.txt&quot;,
                      delim = &quot;,&quot;,
                      col_names = c(&quot;sample_code_number&quot;, 
                       &quot;clump_thickness&quot;, 
                       &quot;uniformity_of_cell_size&quot;, 
                       &quot;uniformity_of_cell_shape&quot;, 
                       &quot;marginal_adhesion&quot;, 
                       &quot;single_epithelial_cell_size&quot;, 
                       &quot;bare_nuclei&quot;, 
                       &quot;bland_chromatin&quot;, 
                       &quot;normal_nucleoli&quot;, 
                       &quot;mitosis&quot;, 
                       &quot;classes&quot;)) %&gt;%
  mutate(bare_nuclei = as.numeric(bare_nuclei),
         classes = ifelse(classes == &quot;2&quot;, &quot;benign&quot;,
                          ifelse(classes == &quot;4&quot;, &quot;malignant&quot;, NA)))</code></pre>
<pre class="r"><code>summary(bc_data)</code></pre>
<pre><code>##  sample_code_number clump_thickness  uniformity_of_cell_size
##  Min.   :   61634   Min.   : 1.000   Min.   : 1.000         
##  1st Qu.:  870688   1st Qu.: 2.000   1st Qu.: 1.000         
##  Median : 1171710   Median : 4.000   Median : 1.000         
##  Mean   : 1071704   Mean   : 4.418   Mean   : 3.134         
##  3rd Qu.: 1238298   3rd Qu.: 6.000   3rd Qu.: 5.000         
##  Max.   :13454352   Max.   :10.000   Max.   :10.000         
##                                                             
##  uniformity_of_cell_shape marginal_adhesion single_epithelial_cell_size
##  Min.   : 1.000           Min.   : 1.000    Min.   : 1.000             
##  1st Qu.: 1.000           1st Qu.: 1.000    1st Qu.: 2.000             
##  Median : 1.000           Median : 1.000    Median : 2.000             
##  Mean   : 3.207           Mean   : 2.807    Mean   : 3.216             
##  3rd Qu.: 5.000           3rd Qu.: 4.000    3rd Qu.: 4.000             
##  Max.   :10.000           Max.   :10.000    Max.   :10.000             
##                                                                        
##   bare_nuclei     bland_chromatin  normal_nucleoli     mitosis      
##  Min.   : 1.000   Min.   : 1.000   Min.   : 1.000   Min.   : 1.000  
##  1st Qu.: 1.000   1st Qu.: 2.000   1st Qu.: 1.000   1st Qu.: 1.000  
##  Median : 1.000   Median : 3.000   Median : 1.000   Median : 1.000  
##  Mean   : 3.545   Mean   : 3.438   Mean   : 2.867   Mean   : 1.589  
##  3rd Qu.: 6.000   3rd Qu.: 5.000   3rd Qu.: 4.000   3rd Qu.: 1.000  
##  Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :10.000  
##  NA&#39;s   :16                                                         
##    classes         
##  Length:699        
##  Class :character  
##  Mode  :character  
##                    
##                    
##                    
## </code></pre>
<p><br></p>
</div>
<div id="missing-data" class="section level3">
<h3>Missing data</h3>
<pre class="r"><code># how many NAs are in the data
md.pattern(bc_data, plot = FALSE)</code></pre>
<pre><code>##     sample_code_number clump_thickness uniformity_of_cell_size
## 683                  1               1                       1
## 16                   1               1                       1
##                      0               0                       0
##     uniformity_of_cell_shape marginal_adhesion single_epithelial_cell_size
## 683                        1                 1                           1
## 16                         1                 1                           1
##                            0                 0                           0
##     bland_chromatin normal_nucleoli mitosis classes bare_nuclei   
## 683               1               1       1       1           1  0
## 16                1               1       1       1           0  1
##                   0               0       0       0          16 16</code></pre>
<pre class="r"><code>bc_data &lt;- bc_data %&gt;%
  drop_na() %&gt;%
  select(classes, everything(), -sample_code_number)
head(bc_data)</code></pre>
<pre><code>## # A tibble: 6 x 10
##   classes   clump_thickness uniformity_of_cell_si… uniformity_of_cell_sha…
##   &lt;chr&gt;               &lt;int&gt;                  &lt;int&gt;                   &lt;int&gt;
## 1 benign                  5                      1                       1
## 2 benign                  5                      4                       4
## 3 benign                  3                      1                       1
## 4 benign                  6                      8                       8
## 5 benign                  4                      1                       1
## 6 malignant               8                     10                      10
## # ... with 6 more variables: marginal_adhesion &lt;int&gt;,
## #   single_epithelial_cell_size &lt;int&gt;, bare_nuclei &lt;dbl&gt;,
## #   bland_chromatin &lt;int&gt;, normal_nucleoli &lt;int&gt;, mitosis &lt;int&gt;</code></pre>
<p>Missing values can be imputed with the <em>mice</em> package.</p>
<p>More info and tutorial with code: <a href="https://shirinsplayground.netlify.com/2018/04/flu_prediction/" class="uri">https://shirinsplayground.netlify.com/2018/04/flu_prediction/</a></p>
<p><br></p>
</div>
<div id="data-exploration" class="section level3">
<h3>Data exploration</h3>
<ul>
<li>Response variable for classification</li>
</ul>
<pre class="r"><code>ggplot(bc_data, aes(x = classes, fill = classes)) +
  geom_bar()</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/response_classification-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>More info on dealing with unbalanced classes: <a href="https://shiring.github.io/machine_learning/2017/04/02/unbalanced" class="uri">https://shiring.github.io/machine_learning/2017/04/02/unbalanced</a></p>
<p><br></p>
<ul>
<li>Response variable for regression</li>
</ul>
<pre class="r"><code>ggplot(bc_data, aes(x = clump_thickness)) +
  geom_histogram(bins = 10)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/response_regression-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><br></p>
<ul>
<li>Features</li>
</ul>
<pre class="r"><code>gather(bc_data, x, y, clump_thickness:mitosis) %&gt;%
  ggplot(aes(x = y, color = classes, fill = classes)) +
    geom_density(alpha = 0.3) +
    facet_wrap( ~ x, scales = &quot;free&quot;, ncol = 3)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/features-1.png" width="768" style="display: block; margin: auto;" /></p>
<p><br></p>
<ul>
<li>Correlation graphs</li>
</ul>
<pre class="r"><code>co_mat_benign &lt;- filter(bc_data, classes == &quot;benign&quot;) %&gt;%
  select(-1) %&gt;%
  cor()

co_mat_malignant &lt;- filter(bc_data, classes == &quot;malignant&quot;) %&gt;%
  select(-1) %&gt;%
  cor()

library(igraph)
g_benign &lt;- graph.adjacency(co_mat_benign,
                         weighted = TRUE,
                         diag = FALSE,
                         mode = &quot;upper&quot;)

g_malignant &lt;- graph.adjacency(co_mat_malignant,
                         weighted = TRUE,
                         diag = FALSE,
                         mode = &quot;upper&quot;)


# http://kateto.net/networks-r-igraph

cut.off_b &lt;- mean(E(g_benign)$weight)
cut.off_m &lt;- mean(E(g_malignant)$weight)

g_benign_2 &lt;- delete_edges(g_benign, E(g_benign)[weight &lt; cut.off_b])
g_malignant_2 &lt;- delete_edges(g_malignant, E(g_malignant)[weight &lt; cut.off_m])

c_g_benign_2 &lt;- cluster_fast_greedy(g_benign_2) 
c_g_malignant_2 &lt;- cluster_fast_greedy(g_malignant_2) </code></pre>
<pre class="r"><code>par(mfrow = c(1,2))

plot(c_g_benign_2, g_benign_2,
     vertex.size = colSums(co_mat_benign) * 10,
     vertex.frame.color = NA, 
     vertex.label.color = &quot;black&quot;, 
     vertex.label.cex = 0.8,
     edge.width = E(g_benign_2)$weight * 15,
     layout = layout_with_fr(g_benign_2),
     main = &quot;Benign tumors&quot;)

plot(c_g_malignant_2, g_malignant_2,
     vertex.size = colSums(co_mat_malignant) * 10,
     vertex.frame.color = NA, 
     vertex.label.color = &quot;black&quot;, 
     vertex.label.cex = 0.8,
     edge.width = E(g_malignant_2)$weight * 15,
     layout = layout_with_fr(g_malignant_2),
     main = &quot;Malignant tumors&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/cor_graph-1.png" width="1152" /></p>
<p><br></p>
</div>
<div id="principal-component-analysis" class="section level3">
<h3>Principal Component Analysis</h3>
<pre class="r"><code>library(ellipse)

# perform pca and extract scores
pcaOutput &lt;- prcomp(as.matrix(bc_data[, -1]), scale = TRUE, center = TRUE)
pcaOutput2 &lt;- as.data.frame(pcaOutput$x)
  
# define groups for plotting
pcaOutput2$groups &lt;- bc_data$classes
  
centroids &lt;- aggregate(cbind(PC1, PC2) ~ groups, pcaOutput2, mean)

conf.rgn  &lt;- do.call(rbind, lapply(unique(pcaOutput2$groups), function(t)
  data.frame(groups = as.character(t),
             ellipse(cov(pcaOutput2[pcaOutput2$groups == t, 1:2]),
                   centre = as.matrix(centroids[centroids$groups == t, 2:3]),
                   level = 0.95),
             stringsAsFactors = FALSE)))
    
ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, group = groups, color = groups)) + 
    geom_polygon(data = conf.rgn, aes(fill = groups), alpha = 0.2) +
    geom_point(size = 2, alpha = 0.6) + 
    labs(color = &quot;&quot;,
         fill = &quot;&quot;) </code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/pca-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="multidimensional-scaling" class="section level3">
<h3>Multidimensional Scaling</h3>
<pre class="r"><code>select(bc_data, -1) %&gt;%
  dist() %&gt;%
  cmdscale %&gt;%
  as.data.frame() %&gt;%
  mutate(group = bc_data$classes) %&gt;%
  ggplot(aes(x = V1, y = V2, color = group)) +
    geom_point()</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/mds_plot-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="t-sne-dimensionality-reduction" class="section level3">
<h3>t-SNE dimensionality reduction</h3>
<pre class="r"><code>library(tsne)

select(bc_data, -1) %&gt;%
  dist() %&gt;%
  tsne() %&gt;%
  as.data.frame() %&gt;%
  mutate(group = bc_data$classes) %&gt;%
  ggplot(aes(x = V1, y = V2, color = group)) +
    geom_point()</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/tsne_plot-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
</div>
<div id="machine-learning-packages-for-r" class="section level2 tabset tabset-fade tabset-pills">
<h2>Machine Learning packages for R</h2>
<div id="caret" class="section level3">
<h3><a href="http://topepo.github.io/caret/index.html">caret</a></h3>
<pre class="r"><code># configure multicore
library(doParallel)
cl &lt;- makeCluster(detectCores())
registerDoParallel(cl)

library(caret)</code></pre>
<p><br></p>
<div id="training-validation-and-test-data" class="section level4">
<h4>Training, validation and test data</h4>
<pre class="r"><code>set.seed(42)
index &lt;- createDataPartition(bc_data$classes, p = 0.7, list = FALSE)
train_data &lt;- bc_data[index, ]
test_data  &lt;- bc_data[-index, ]</code></pre>
<pre class="r"><code>bind_rows(data.frame(group = &quot;train&quot;, train_data),
      data.frame(group = &quot;test&quot;, test_data)) %&gt;%
  gather(x, y, clump_thickness:mitosis) %&gt;%
  ggplot(aes(x = y, color = group, fill = group)) +
    geom_density(alpha = 0.3) +
    facet_wrap( ~ x, scales = &quot;free&quot;, ncol = 3)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/distribution-1.png" width="768" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="regression" class="section level4">
<h4>Regression</h4>
<pre class="r"><code>set.seed(42)
model_glm &lt;- caret::train(clump_thickness ~ .,
                          data = train_data,
                          method = &quot;glm&quot;,
                          preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                          trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))</code></pre>
<pre class="r"><code>model_glm</code></pre>
<pre><code>## Generalized Linear Model 
## 
## 479 samples
##   9 predictor
## 
## Pre-processing: scaled (9), centered (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 432, 431, 432, 431, 431, 431, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   1.972314  0.5254215  1.648832</code></pre>
<pre class="r"><code>predictions &lt;- predict(model_glm, test_data)</code></pre>
<pre class="r"><code># model_glm$finalModel$linear.predictors == model_glm$finalModel$fitted.values
data.frame(residuals = resid(model_glm),
           predictors = model_glm$finalModel$linear.predictors) %&gt;%
  ggplot(aes(x = predictors, y = residuals)) +
    geom_jitter() +
    geom_smooth(method = &quot;lm&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/residuals-1.png" width="480" style="display: block; margin: auto;" /></p>
<pre class="r"><code># y == train_data$clump_thickness
data.frame(residuals = resid(model_glm),
           y = model_glm$finalModel$y) %&gt;%
  ggplot(aes(x = y, y = residuals)) +
    geom_jitter() +
    geom_smooth(method = &quot;lm&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/residuals-2.png" width="480" style="display: block; margin: auto;" /></p>
<pre class="r"><code>data.frame(actual = test_data$clump_thickness,
           predicted = predictions) %&gt;%
  ggplot(aes(x = actual, y = predicted)) +
    geom_jitter() +
    geom_smooth(method = &quot;lm&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/regression_result-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="classification" class="section level4">
<h4>Classification</h4>
<div id="decision-trees" class="section level5">
<h5>Decision trees</h5>
<p><a href="https://cran.r-project.org/web/packages/rpart/rpart.pdf">rpart</a></p>
<pre class="r"><code>library(rpart)
library(rpart.plot)

set.seed(42)
fit &lt;- rpart(classes ~ .,
            data = train_data,
            method = &quot;class&quot;,
            control = rpart.control(xval = 10, 
                                    minbucket = 2, 
                                    cp = 0), 
             parms = list(split = &quot;information&quot;))

rpart.plot(fit, extra = 100)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/decision_tree-1.png" width="960" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
</div>
<div id="random-forests" class="section level4">
<h4>Random Forests</h4>
<p><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">Random Forests</a> predictions are based on the generation of multiple classification trees. They can be used for both, classification and regression tasks. Here, I show a classification task.</p>
<pre class="r"><code>set.seed(42)
model_rf &lt;- caret::train(classes ~ .,
                         data = train_data,
                         method = &quot;rf&quot;,
                         preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                         trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 5, 
                                                  repeats = 3, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))</code></pre>
<p>When you specify <code>savePredictions = TRUE</code>, you can access the cross-validation resuls with <code>model_rf$pred</code>.</p>
<pre class="r"><code>model_rf</code></pre>
<pre><code>## Random Forest 
## 
## 479 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: scaled (9), centered (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 432, 431, 431, 431, 431, 431, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##   2     0.9776753  0.9513499
##   5     0.9757957  0.9469999
##   9     0.9714200  0.9370285
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<pre class="r"><code>model_rf$finalModel$confusion</code></pre>
<pre><code>##           benign malignant class.error
## benign       304         7  0.02250804
## malignant      5       163  0.02976190</code></pre>
</div>
<div id="dealing-with-unbalanced-data" class="section level4">
<h4>Dealing with unbalanced data</h4>
<p>Luckily, caret makes it very easy to incorporate over- and under-sampling techniques with cross-validation resampling. We can simply add the sampling option to our trainControl and choose down for under- (also called down-) sampling. The rest stays the same as with our original model.</p>
<pre class="r"><code>set.seed(42)
model_rf_down &lt;- caret::train(classes ~ .,
                         data = train_data,
                         method = &quot;rf&quot;,
                         preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                         trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE,
                                                  sampling = &quot;down&quot;))</code></pre>
<pre class="r"><code>model_rf_down</code></pre>
<pre><code>## Random Forest 
## 
## 479 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: scaled (9), centered (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 432, 431, 431, 431, 431, 431, ... 
## Addtional sampling using down-sampling prior to pre-processing
## 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##   2     0.9797503  0.9563138
##   5     0.9741198  0.9438326
##   9     0.9699578  0.9346310
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<p><br></p>
</div>
<div id="feature-importance" class="section level4">
<h4>Feature Importance</h4>
<pre class="r"><code>imp &lt;- model_rf$finalModel$importance
imp[order(imp, decreasing = TRUE), ]</code></pre>
<pre><code>##     uniformity_of_cell_size    uniformity_of_cell_shape 
##                   43.936945                   39.840595 
##                 bare_nuclei             bland_chromatin 
##                   33.820345                   31.984813 
##             normal_nucleoli single_epithelial_cell_size 
##                   21.686039                   17.761202 
##             clump_thickness           marginal_adhesion 
##                   16.318817                    9.518437 
##                     mitosis 
##                    2.220633</code></pre>
<pre class="r"><code># estimate variable importance
importance &lt;- varImp(model_rf, scale = TRUE)
plot(importance)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/importance_rf-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><br></p>
<ul>
<li>predicting test data</li>
</ul>
<pre class="r"><code>confusionMatrix(predict(model_rf, test_data), as.factor(test_data$classes))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  benign malignant
##   benign       128         4
##   malignant      5        67
##                                           
##                Accuracy : 0.9559          
##                  95% CI : (0.9179, 0.9796)
##     No Information Rate : 0.652           
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.9031          
##  Mcnemar&#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.9624          
##             Specificity : 0.9437          
##          Pos Pred Value : 0.9697          
##          Neg Pred Value : 0.9306          
##              Prevalence : 0.6520          
##          Detection Rate : 0.6275          
##    Detection Prevalence : 0.6471          
##       Balanced Accuracy : 0.9530          
##                                           
##        &#39;Positive&#39; Class : benign          
## </code></pre>
<pre class="r"><code>results &lt;- data.frame(actual = test_data$classes,
                      predict(model_rf, test_data, type = &quot;prob&quot;))

results$prediction &lt;- ifelse(results$benign &gt; 0.5, &quot;benign&quot;,
                             ifelse(results$malignant &gt; 0.5, &quot;malignant&quot;, NA))

results$correct &lt;- ifelse(results$actual == results$prediction, TRUE, FALSE)

ggplot(results, aes(x = prediction, fill = correct)) +
  geom_bar(position = &quot;dodge&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/results_bar_rf-1.png" width="480" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(results, aes(x = prediction, y = benign, color = correct, shape = correct)) +
  geom_jitter(size = 3, alpha = 0.6)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/results_jitter_rf-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="extreme-gradient-boosting-trees" class="section level4">
<h4>Extreme gradient boosting trees</h4>
<p><a href="http://xgboost.readthedocs.io/en/latest/model.html">Extreme gradient boosting (XGBoost)</a> is a faster and improved implementation of <a href="https://en.wikipedia.org/wiki/Gradient_boosting">gradient boosting</a> for supervised learning.</p>
<blockquote>
<p>“XGBoost uses a more regularized model formalization to control over-fitting, which gives it better performance.” Tianqi Chen, developer of xgboost</p>
</blockquote>
<p>XGBoost is a tree ensemble model, which means the sum of predictions from a set of classification and regression trees (CART). In that, XGBoost is similar to Random Forests but it uses a different approach to model training. Can be used for classification and regression tasks. Here, I show a classification task.</p>
<pre class="r"><code>set.seed(42)
model_xgb &lt;- caret::train(classes ~ .,
                          data = train_data,
                          method = &quot;xgbTree&quot;,
                          preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                          trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 5, 
                                                  repeats = 3, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))</code></pre>
<pre class="r"><code>model_xgb</code></pre>
<pre><code>## eXtreme Gradient Boosting 
## 
## 479 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: scaled (9), centered (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 432, 431, 431, 431, 431, 431, ... 
## Resampling results across tuning parameters:
## 
##   eta  max_depth  colsample_bytree  subsample  nrounds  Accuracy 
##   0.3  1          0.6               0.50        50      0.9567788
##   0.3  1          0.6               0.50       100      0.9544912
##   0.3  1          0.6               0.50       150      0.9513572
##   0.3  1          0.6               0.75        50      0.9576164
##   0.3  1          0.6               0.75       100      0.9536448
##   0.3  1          0.6               0.75       150      0.9525987
##   0.3  1          0.6               1.00        50      0.9559409
##   0.3  1          0.6               1.00       100      0.9555242
##   0.3  1          0.6               1.00       150      0.9551031
##   0.3  1          0.8               0.50        50      0.9718588
##   0.3  1          0.8               0.50       100      0.9720583
##   0.3  1          0.8               0.50       150      0.9699879
##   0.3  1          0.8               0.75        50      0.9726964
##   0.3  1          0.8               0.75       100      0.9724664
##   0.3  1          0.8               0.75       150      0.9705868
##   0.3  1          0.8               1.00        50      0.9714202
##   0.3  1          0.8               1.00       100      0.9710035
##   0.3  1          0.8               1.00       150      0.9705866
##   0.3  2          0.6               0.50        50      0.9559448
##   0.3  2          0.6               0.50       100      0.9565397
##   0.3  2          0.6               0.50       150      0.9555063
##   0.3  2          0.6               0.75        50      0.9530150
##   0.3  2          0.6               0.75       100      0.9550985
##   0.3  2          0.6               0.75       150      0.9551070
##   0.3  2          0.6               1.00        50      0.9532320
##   0.3  2          0.6               1.00       100      0.9551072
##   0.3  2          0.6               1.00       150      0.9557237
##   0.3  2          0.8               0.50        50      0.9720583
##   0.3  2          0.8               0.50       100      0.9735166
##   0.3  2          0.8               0.50       150      0.9720540
##   0.3  2          0.8               0.75        50      0.9722494
##   0.3  2          0.8               0.75       100      0.9726703
##   0.3  2          0.8               0.75       150      0.9716374
##   0.3  2          0.8               1.00        50      0.9716327
##   0.3  2          0.8               1.00       100      0.9724622
##   0.3  2          0.8               1.00       150      0.9718416
##   0.3  3          0.6               0.50        50      0.9548905
##   0.3  3          0.6               0.50       100      0.9557237
##   0.3  3          0.6               0.50       150      0.9555198
##   0.3  3          0.6               0.75        50      0.9561404
##   0.3  3          0.6               0.75       100      0.9546820
##   0.3  3          0.6               0.75       150      0.9552982
##   0.3  3          0.6               1.00        50      0.9577983
##   0.3  3          0.6               1.00       100      0.9573819
##   0.3  3          0.6               1.00       150      0.9567655
##   0.3  3          0.8               0.50        50      0.9733131
##   0.3  3          0.8               0.50       100      0.9728829
##   0.3  3          0.8               0.50       150      0.9718499
##   0.3  3          0.8               0.75        50      0.9751879
##   0.3  3          0.8               0.75       100      0.9743546
##   0.3  3          0.8               0.75       150      0.9735212
##   0.3  3          0.8               1.00        50      0.9743372
##   0.3  3          0.8               1.00       100      0.9737122
##   0.3  3          0.8               1.00       150      0.9743461
##   0.4  1          0.6               0.50        50      0.9548861
##   0.4  1          0.6               0.50       100      0.9528290
##   0.4  1          0.6               0.50       150      0.9498772
##   0.4  1          0.6               0.75        50      0.9557239
##   0.4  1          0.6               0.75       100      0.9513529
##   0.4  1          0.6               0.75       150      0.9492779
##   0.4  1          0.6               1.00        50      0.9559365
##   0.4  1          0.6               1.00       100      0.9551031
##   0.4  1          0.6               1.00       150      0.9536361
##   0.4  1          0.8               0.50        50      0.9710164
##   0.4  1          0.8               0.50       100      0.9697577
##   0.4  1          0.8               0.50       150      0.9687074
##   0.4  1          0.8               0.75        50      0.9710122
##   0.4  1          0.8               0.75       100      0.9707996
##   0.4  1          0.8               0.75       150      0.9691455
##   0.4  1          0.8               1.00        50      0.9705911
##   0.4  1          0.8               1.00       100      0.9697446
##   0.4  1          0.8               1.00       150      0.9697576
##   0.4  2          0.6               0.50        50      0.9544866
##   0.4  2          0.6               0.50       100      0.9542694
##   0.4  2          0.6               0.50       150      0.9536357
##   0.4  2          0.6               0.75        50      0.9540611
##   0.4  2          0.6               0.75       100      0.9542694
##   0.4  2          0.6               0.75       150      0.9549033
##   0.4  2          0.6               1.00        50      0.9540653
##   0.4  2          0.6               1.00       100      0.9555239
##   0.4  2          0.6               1.00       150      0.9546818
##   0.4  2          0.8               0.50        50      0.9720670
##   0.4  2          0.8               0.50       100      0.9695629
##   0.4  2          0.8               0.50       150      0.9702006
##   0.4  2          0.8               0.75        50      0.9722627
##   0.4  2          0.8               0.75       100      0.9720500
##   0.4  2          0.8               0.75       150      0.9716289
##   0.4  2          0.8               1.00        50      0.9726705
##   0.4  2          0.8               1.00       100      0.9708042
##   0.4  2          0.8               1.00       150      0.9708129
##   0.4  3          0.6               0.50        50      0.9555150
##   0.4  3          0.6               0.50       100      0.9553021
##   0.4  3          0.6               0.50       150      0.9548943
##   0.4  3          0.6               0.75        50      0.9555281
##   0.4  3          0.6               0.75       100      0.9563662
##   0.4  3          0.6               0.75       150      0.9555324
##   0.4  3          0.6               1.00        50      0.9575900
##   0.4  3          0.6               1.00       100      0.9571735
##   0.4  3          0.6               1.00       150      0.9559104
##   0.4  3          0.8               0.50        50      0.9737255
##   0.4  3          0.8               0.50       100      0.9745501
##   0.4  3          0.8               0.50       150      0.9730874
##   0.4  3          0.8               0.75        50      0.9747539
##   0.4  3          0.8               0.75       100      0.9724664
##   0.4  3          0.8               0.75       150      0.9720498
##   0.4  3          0.8               1.00        50      0.9747539
##   0.4  3          0.8               1.00       100      0.9749624
##   0.4  3          0.8               1.00       150      0.9734996
##   Kappa    
##   0.9050828
##   0.8999999
##   0.8930637
##   0.9067208
##   0.8982284
##   0.8959903
##   0.9028825
##   0.9022543
##   0.9014018
##   0.9382467
##   0.9386326
##   0.9340573
##   0.9400323
##   0.9395968
##   0.9353783
##   0.9372262
##   0.9362148
##   0.9353247
##   0.9032270
##   0.9047203
##   0.9024465
##   0.8968511
##   0.9015282
##   0.9016169
##   0.8971329
##   0.9015111
##   0.9028614
##   0.9387022
##   0.9419143
##   0.9387792
##   0.9391933
##   0.9401872
##   0.9379714
##   0.9377309
##   0.9397601
##   0.9384827
##   0.9008861
##   0.9029797
##   0.9024531
##   0.9037859
##   0.9004226
##   0.9019909
##   0.9074584
##   0.9064701
##   0.9051441
##   0.9414031
##   0.9405025
##   0.9380734
##   0.9456856
##   0.9438986
##   0.9419994
##   0.9438642
##   0.9426000
##   0.9439780
##   0.9007223
##   0.8964381
##   0.8897615
##   0.9027951
##   0.8931520
##   0.8886910
##   0.9030461
##   0.9014362
##   0.8982364
##   0.9363059
##   0.9334254
##   0.9311383
##   0.9361883
##   0.9357131
##   0.9320657
##   0.9353688
##   0.9333607
##   0.9334467
##   0.8999756
##   0.8997888
##   0.8983861
##   0.8991356
##   0.8998960
##   0.9013529
##   0.8990428
##   0.9023340
##   0.9004889
##   0.9387165
##   0.9332663
##   0.9345567
##   0.9393855
##   0.9389455
##   0.9380863
##   0.9401366
##   0.9361847
##   0.9361724
##   0.9021263
##   0.9017938
##   0.9010613
##   0.9025263
##   0.9043436
##   0.9024744
##   0.9069828
##   0.9059579
##   0.9031829
##   0.9424523
##   0.9442537
##   0.9410193
##   0.9447486
##   0.9397683
##   0.9388701
##   0.9449064
##   0.9454375
##   0.9422358
## 
## Tuning parameter &#39;gamma&#39; was held constant at a value of 0
## 
## Tuning parameter &#39;min_child_weight&#39; was held constant at a value of 1
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were nrounds = 50, max_depth = 3,
##  eta = 0.3, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1
##  and subsample = 0.75.</code></pre>
<p><br></p>
<ul>
<li>Feature Importance</li>
</ul>
<pre class="r"><code>importance &lt;- varImp(model_xgb, scale = TRUE)
plot(importance)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/importance_xgb-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><br></p>
<ul>
<li>predicting test data</li>
</ul>
<pre class="r"><code>confusionMatrix(predict(model_xgb, test_data), as.factor(test_data$classes))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  benign malignant
##   benign       128         3
##   malignant      5        68
##                                           
##                Accuracy : 0.9608          
##                  95% CI : (0.9242, 0.9829)
##     No Information Rate : 0.652           
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.9142          
##  Mcnemar&#39;s Test P-Value : 0.7237          
##                                           
##             Sensitivity : 0.9624          
##             Specificity : 0.9577          
##          Pos Pred Value : 0.9771          
##          Neg Pred Value : 0.9315          
##              Prevalence : 0.6520          
##          Detection Rate : 0.6275          
##    Detection Prevalence : 0.6422          
##       Balanced Accuracy : 0.9601          
##                                           
##        &#39;Positive&#39; Class : benign          
## </code></pre>
<pre class="r"><code>results &lt;- data.frame(actual = test_data$classes,
                      predict(model_xgb, test_data, type = &quot;prob&quot;))

results$prediction &lt;- ifelse(results$benign &gt; 0.5, &quot;benign&quot;,
                             ifelse(results$malignant &gt; 0.5, &quot;malignant&quot;, NA))

results$correct &lt;- ifelse(results$actual == results$prediction, TRUE, FALSE)

ggplot(results, aes(x = prediction, fill = correct)) +
  geom_bar(position = &quot;dodge&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/results_bar_xgb-1.png" width="480" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(results, aes(x = prediction, y = benign, color = correct, shape = correct)) +
  geom_jitter(size = 3, alpha = 0.6)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/results_jitter_xgb-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="available-models-in-caret" class="section level2">
<h2>Available models in caret</h2>
<p><a href="https://topepo.github.io/caret/available-models.html" class="uri">https://topepo.github.io/caret/available-models.html</a></p>
<p><br></p>
<div id="feature-selection" class="section level4">
<h4>Feature Selection</h4>
<p>Performing feature selection on the whole dataset would lead to prediction bias, we therefore need to run the whole modeling process on the training data alone!</p>
<ul>
<li>Correlation</li>
</ul>
<p>Correlations between all features are calculated and visualised with the <em>corrplot</em> package. I am then removing all features with a correlation higher than 0.7, keeping the feature with the lower mean.</p>
<pre class="r"><code>library(corrplot)

# calculate correlation matrix
corMatMy &lt;- cor(train_data[, -1])
corrplot(corMatMy, order = &quot;hclust&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/corplot-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Apply correlation filter at 0.70,
highlyCor &lt;- colnames(train_data[, -1])[findCorrelation(corMatMy, cutoff = 0.7, verbose = TRUE)]</code></pre>
<pre><code>## Compare row 2  and column  3 with corr  0.908 
##   Means:  0.709 vs 0.594 so flagging column 2 
## Compare row 3  and column  7 with corr  0.749 
##   Means:  0.67 vs 0.569 so flagging column 3 
## All correlations &lt;= 0.7</code></pre>
<pre class="r"><code># which variables are flagged for removal?
highlyCor</code></pre>
<pre><code>## [1] &quot;uniformity_of_cell_size&quot;  &quot;uniformity_of_cell_shape&quot;</code></pre>
<pre class="r"><code>#then we remove these variables
train_data_cor &lt;- train_data[, which(!colnames(train_data) %in% highlyCor)]</code></pre>
<p><br></p>
<ul>
<li>Recursive Feature Elimination (RFE)</li>
</ul>
<p>Another way to choose features is with Recursive Feature Elimination. RFE uses a Random Forest algorithm to test combinations of features and rate each with an accuracy score. The combination with the highest score is usually preferential.</p>
<pre class="r"><code>set.seed(7)
results_rfe &lt;- rfe(x = train_data[, -1], 
                   y = as.factor(train_data$classes), 
                   sizes = c(1:9), 
                   rfeControl = rfeControl(functions = rfFuncs, method = &quot;cv&quot;, number = 10))</code></pre>
<pre class="r"><code># chosen features
predictors(results_rfe)</code></pre>
<pre><code>## [1] &quot;bare_nuclei&quot;                 &quot;clump_thickness&quot;            
## [3] &quot;uniformity_of_cell_size&quot;     &quot;uniformity_of_cell_shape&quot;   
## [5] &quot;bland_chromatin&quot;             &quot;normal_nucleoli&quot;            
## [7] &quot;marginal_adhesion&quot;           &quot;single_epithelial_cell_size&quot;</code></pre>
<pre class="r"><code>train_data_rfe &lt;- train_data[, c(1, which(colnames(train_data) %in% predictors(results_rfe)))]</code></pre>
<p><br></p>
<ul>
<li>Genetic Algorithm (GA)</li>
</ul>
<p>The Genetic Algorithm (GA) has been developed based on evolutionary principles of natural selection: It aims to optimize a population of individuals with a given set of genotypes by modeling selection over time. In each generation (i.e. iteration), each individual’s fitness is calculated based on their genotypes. Then, the fittest individuals are chosen to produce the next generation. This subsequent generation of individuals will have genotypes resulting from (re-) combinations of the parental alleles. These new genotypes will again determine each individual’s fitness. This selection process is iterated for a specified number of generations and (ideally) leads to fixation of the fittest alleles in the gene pool.</p>
<p>This concept of optimization can be applied to non-evolutionary models as well, like feature selection processes in machine learning.</p>
<pre class="r"><code>set.seed(27)
model_ga &lt;- gafs(x = train_data[, -1], 
                 y = as.factor(train_data$classes),
                 iters = 10, # generations of algorithm
                 popSize = 10, # population size for each generation
                 levels = c(&quot;malignant&quot;, &quot;benign&quot;),
                 gafsControl = gafsControl(functions = rfGA, # Assess fitness with RF
                                           method = &quot;cv&quot;,    # 10 fold cross validation
                                           genParallel = TRUE, # Use parallel programming
                                           allowParallel = TRUE))</code></pre>
<pre class="r"><code>plot(model_ga) # Plot mean fitness (AUC) by generation</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-38-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>train_data_ga &lt;- train_data[, c(1, which(colnames(train_data) %in% model_ga$ga$final))]</code></pre>
<p><br></p>
</div>
<div id="hyperparameter-tuning-with-caret" class="section level3">
<h3>Hyperparameter tuning with caret</h3>
<ul>
<li><p>Cartesian Grid</p></li>
<li><p>mtry: Number of variables randomly sampled as candidates at each split.</p></li>
</ul>
<pre class="r"><code>set.seed(42)
grid &lt;- expand.grid(mtry = c(1:10))

model_rf_tune_man &lt;- caret::train(classes ~ .,
                         data = train_data,
                         method = &quot;rf&quot;,
                         preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                         trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE),
                         tuneGrid = grid)</code></pre>
<pre class="r"><code>model_rf_tune_man</code></pre>
<pre><code>## Random Forest 
## 
## 479 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: scaled (9), centered (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 432, 431, 431, 431, 431, 431, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    1    0.9785044  0.9532161
##    2    0.9772586  0.9504377
##    3    0.9774625  0.9508246
##    4    0.9766333  0.9488778
##    5    0.9753789  0.9460274
##    6    0.9737078  0.9422613
##    7    0.9730957  0.9408547
##    8    0.9714155  0.9371611
##    9    0.9718280  0.9380578
##   10    0.9718280  0.9380135
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 1.</code></pre>
<pre class="r"><code>plot(model_rf_tune_man)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-42-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
<ul>
<li>Random Search</li>
</ul>
<pre class="r"><code>set.seed(42)
model_rf_tune_auto &lt;- caret::train(classes ~ .,
                         data = train_data,
                         method = &quot;rf&quot;,
                         preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                         trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE,
                                                  search = &quot;random&quot;),
                         tuneGrid = grid,
                         tuneLength = 15)</code></pre>
<pre class="r"><code>model_rf_tune_auto</code></pre>
<pre><code>## Random Forest 
## 
## 479 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: scaled (9), centered (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 432, 431, 431, 431, 431, 431, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    1    0.9785044  0.9532161
##    2    0.9772586  0.9504377
##    3    0.9774625  0.9508246
##    4    0.9766333  0.9488778
##    5    0.9753789  0.9460274
##    6    0.9737078  0.9422613
##    7    0.9730957  0.9408547
##    8    0.9714155  0.9371611
##    9    0.9718280  0.9380578
##   10    0.9718280  0.9380135
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 1.</code></pre>
<pre class="r"><code>plot(model_rf_tune_auto)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-46-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="grid-search-with-h2o" class="section level3">
<h3>Grid search with h2o</h3>
<p>The R package h2o provides a convenient interface to <a href="http://www.h2o.ai/h2o/">H2O</a>, which is an open-source machine learning and deep learning platform. H2O distributes a wide range of common machine learning algorithms for classification, regression and deep learning.</p>
<pre class="r"><code>library(h2o)
h2o.init(nthreads = -1)</code></pre>
<pre><code>##  Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         3 days 4 minutes 
##     H2O cluster timezone:       Europe/Berlin 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.20.0.2 
##     H2O cluster version age:    16 days  
##     H2O cluster name:           H2O_started_from_R_shiringlander_jrj894 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   3.27 GB 
##     H2O cluster total cores:    8 
##     H2O cluster allowed cores:  8 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.0 (2018-04-23)</code></pre>
<pre class="r"><code>h2o.no_progress()

bc_data_hf &lt;- as.h2o(bc_data)</code></pre>
<pre class="r"><code>h2o.describe(bc_data_hf) %&gt;%
  gather(x, y, Zeros:Sigma) %&gt;%
  mutate(group = ifelse(x %in% c(&quot;Min&quot;, &quot;Max&quot;, &quot;Mean&quot;), &quot;min, mean, max&quot;, 
                        ifelse(x %in% c(&quot;NegInf&quot;, &quot;PosInf&quot;), &quot;Inf&quot;, &quot;sigma, zeros&quot;))) %&gt;% 
  ggplot(aes(x = Label, y = as.numeric(y), color = x)) +
    geom_point(size = 4, alpha = 0.6) +
    scale_color_brewer(palette = &quot;Set1&quot;) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    facet_grid(group ~ ., scales = &quot;free&quot;) +
    labs(x = &quot;Feature&quot;,
         y = &quot;Value&quot;,
         color = &quot;&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/h2o_describe-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>library(reshape2) # for melting

bc_data_hf[, 1] &lt;- h2o.asfactor(bc_data_hf[, 1])

cor &lt;- h2o.cor(bc_data_hf)
rownames(cor) &lt;- colnames(cor)

melt(cor) %&gt;%
  mutate(Var2 = rep(rownames(cor), nrow(cor))) %&gt;%
  mutate(Var2 = factor(Var2, levels = colnames(cor))) %&gt;%
  mutate(variable = factor(variable, levels = colnames(cor))) %&gt;%
  ggplot(aes(x = variable, y = Var2, fill = value)) + 
    geom_tile(width = 0.9, height = 0.9) +
    scale_fill_gradient2(low = &quot;white&quot;, high = &quot;red&quot;, name = &quot;Cor.&quot;) +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
    labs(x = &quot;&quot;, 
         y = &quot;&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/corr_plot-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
<div id="training-validation-and-test-data-1" class="section level4">
<h4>Training, validation and test data</h4>
<pre class="r"><code>splits &lt;- h2o.splitFrame(bc_data_hf, 
                         ratios = c(0.7, 0.15), 
                         seed = 1)

train &lt;- splits[[1]]
valid &lt;- splits[[2]]
test &lt;- splits[[3]]

response &lt;- &quot;classes&quot;
features &lt;- setdiff(colnames(train), response)</code></pre>
<pre class="r"><code>summary(as.factor(train$classes), exact_quantiles = TRUE)</code></pre>
<pre><code>##  classes       
##  benign   :313 
##  malignant:167</code></pre>
<pre class="r"><code>summary(as.factor(valid$classes), exact_quantiles = TRUE)</code></pre>
<pre><code>##  classes      
##  benign   :64 
##  malignant:38</code></pre>
<pre class="r"><code>summary(as.factor(test$classes), exact_quantiles = TRUE)</code></pre>
<pre><code>##  classes      
##  benign   :67 
##  malignant:34</code></pre>
<pre class="r"><code>pca &lt;- h2o.prcomp(training_frame = train,
           x = features,
           validation_frame = valid,
           transform = &quot;NORMALIZE&quot;,
           impute_missing = TRUE,
           k = 3,
           seed = 42)

eigenvec &lt;- as.data.frame(pca@model$eigenvectors)
eigenvec$label &lt;- features

library(ggrepel)
ggplot(eigenvec, aes(x = pc1, y = pc2, label = label)) +
  geom_point(color = &quot;navy&quot;, alpha = 0.7) +
  geom_text_repel()</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/pca_features-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="classification-1" class="section level4">
<h4>Classification</h4>
<div id="random-forest" class="section level5">
<h5>Random Forest</h5>
<pre class="r"><code>hyper_params &lt;- list(
                     ntrees = c(25, 50, 75, 100),
                     max_depth = c(10, 20, 30),
                     min_rows = c(1, 3, 5)
                     )

search_criteria &lt;- list(
                        strategy = &quot;RandomDiscrete&quot;, 
                        max_models = 50,
                        max_runtime_secs = 360,
                        stopping_rounds = 5,          
                        stopping_metric = &quot;AUC&quot;,      
                        stopping_tolerance = 0.0005,
                        seed = 42
                        )</code></pre>
<pre class="r"><code>rf_grid &lt;- h2o.grid(algorithm = &quot;randomForest&quot;, # h2o.randomForest, 
                                                # alternatively h2o.gbm 
                                                # for Gradient boosting trees
                    x = features,
                    y = response,
                    grid_id = &quot;rf_grid&quot;,
                    training_frame = train,
                    validation_frame = valid,
                    nfolds = 25,                           
                    fold_assignment = &quot;Stratified&quot;,
                    hyper_params = hyper_params,
                    search_criteria = search_criteria,
                    seed = 42
                    )</code></pre>
<pre class="r"><code># performance metrics where smaller is better -&gt; order with decreasing = FALSE
sort_options_1 &lt;- c(&quot;mean_per_class_error&quot;, &quot;mse&quot;, &quot;err&quot;, &quot;logloss&quot;)

for (sort_by_1 in sort_options_1) {
  
  grid &lt;- h2o.getGrid(&quot;rf_grid&quot;, sort_by = sort_by_1, decreasing = FALSE)
  
  model_ids &lt;- grid@model_ids
  best_model &lt;- h2o.getModel(model_ids[[1]])
  
  h2o.saveModel(best_model, path=&quot;models&quot;, force = TRUE)
  
}


# performance metrics where bigger is better -&gt; order with decreasing = TRUE
sort_options_2 &lt;- c(&quot;auc&quot;, &quot;precision&quot;, &quot;accuracy&quot;, &quot;recall&quot;, &quot;specificity&quot;)

for (sort_by_2 in sort_options_2) {
  
  grid &lt;- h2o.getGrid(&quot;rf_grid&quot;, sort_by = sort_by_2, decreasing = TRUE)
  
  model_ids &lt;- grid@model_ids
  best_model &lt;- h2o.getModel(model_ids[[1]])
  
  h2o.saveModel(best_model, path = &quot;models&quot;, force = TRUE)
  
}</code></pre>
<pre class="r"><code>files &lt;- list.files(path = &quot;models&quot;)</code></pre>
<pre class="r"><code>rf_models &lt;- files[grep(&quot;rf_grid_model&quot;, files)]

for (model_id in rf_models) {
  
  path &lt;- paste0(getwd(), &quot;/models/&quot;, model_id)
  best_model &lt;- h2o.loadModel(path)
  mse_auc_test &lt;- data.frame(model_id = model_id, 
                             mse = h2o.mse(h2o.performance(best_model, test)),
                             auc = h2o.auc(h2o.performance(best_model, test)))
  
  if (model_id == rf_models[[1]]) {
    
    mse_auc_test_comb &lt;- mse_auc_test
    
  } else {
    
    mse_auc_test_comb &lt;- rbind(mse_auc_test_comb, mse_auc_test)
    
  }
}</code></pre>
<pre class="r"><code>mse_auc_test_comb %&gt;%
  gather(x, y, mse:auc) %&gt;%
  ggplot(aes(x = model_id, y = y, fill = model_id)) +
    facet_grid(x ~ ., scales = &quot;free&quot;) +
    geom_bar(stat = &quot;identity&quot;, alpha = 0.8, position = &quot;dodge&quot;) +
    scale_fill_brewer(palette = &quot;Set1&quot;) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
          plot.margin = unit(c(0.5, 0, 0, 1.5), &quot;cm&quot;)) +
    labs(x = &quot;&quot;, y = &quot;value&quot;, fill = &quot;&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/auc_mse-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>for (model_id in rf_models) {
  
  best_model &lt;- h2o.getModel(model_id)
  
  finalRf_predictions &lt;- data.frame(model_id = rep(best_model@model_id, 
                                                   nrow(test)),
                                    actual = as.vector(test$classes), 
                                    as.data.frame(h2o.predict(object = best_model, 
                                                              newdata = test)))
  
  finalRf_predictions$accurate &lt;- ifelse(finalRf_predictions$actual == 
                                           finalRf_predictions$predict, 
                                         &quot;yes&quot;, &quot;no&quot;)
  
  finalRf_predictions$predict_stringent &lt;- ifelse(finalRf_predictions$benign &gt; 0.8, 
                                                  &quot;benign&quot;, 
                                                  ifelse(finalRf_predictions$malignant 
                                                         &gt; 0.8, &quot;malignant&quot;, &quot;uncertain&quot;))
  
  finalRf_predictions$accurate_stringent &lt;- ifelse(finalRf_predictions$actual == 
                                                     finalRf_predictions$predict_stringent, &quot;yes&quot;, 
                                         ifelse(finalRf_predictions$predict_stringent == 
                                                  &quot;uncertain&quot;, &quot;na&quot;, &quot;no&quot;))
  
  if (model_id == rf_models[[1]]) {
    
    finalRf_predictions_comb &lt;- finalRf_predictions
    
  } else {
    
    finalRf_predictions_comb &lt;- rbind(finalRf_predictions_comb, finalRf_predictions)
    
  }
}</code></pre>
<pre class="r"><code>finalRf_predictions_comb %&gt;%
  ggplot(aes(x = actual, fill = accurate)) +
    geom_bar(position = &quot;dodge&quot;) +
    scale_fill_brewer(palette = &quot;Set1&quot;) +
    facet_wrap(~ model_id, ncol = 2) +
    labs(fill = &quot;Were\npredictions\naccurate?&quot;,
         title = &quot;Default predictions&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/final_predictions_rf-1.png" width="864" style="display: block; margin: auto;" /></p>
<pre class="r"><code>finalRf_predictions_comb %&gt;%
  subset(accurate_stringent != &quot;na&quot;) %&gt;%
  ggplot(aes(x = actual, fill = accurate_stringent)) +
    geom_bar(position = &quot;dodge&quot;) +
    scale_fill_brewer(palette = &quot;Set1&quot;) +
    facet_wrap(~ model_id, ncol = 2) +
    labs(fill = &quot;Were\npredictions\naccurate?&quot;,
         title = &quot;Stringent predictions&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/final_predictions_rf-2.png" width="864" style="display: block; margin: auto;" /></p>
<pre class="r"><code>rf_model &lt;- h2o.loadModel(&quot;models/rf_grid_model_0&quot;)</code></pre>
<pre class="r"><code>h2o.varimp_plot(rf_model)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-60-1.png" width="672" /></p>
<pre class="r"><code>#h2o.varimp(rf_model)</code></pre>
<pre class="r"><code>h2o.mean_per_class_error(rf_model, train = TRUE, valid = TRUE, xval = TRUE)</code></pre>
<pre><code>##      train      valid       xval 
## 0.02196246 0.02343750 0.02515735</code></pre>
<pre class="r"><code>h2o.confusionMatrix(rf_model, valid = TRUE)</code></pre>
<pre><code>## Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.533333333333333:
##           benign malignant    Error    Rate
## benign        61         3 0.046875   =3/64
## malignant      0        38 0.000000   =0/38
## Totals        61        41 0.029412  =3/102</code></pre>
<pre class="r"><code>plot(rf_model,
     timestep = &quot;number_of_trees&quot;,
     metric = &quot;classification_error&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-63-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plot(rf_model,
     timestep = &quot;number_of_trees&quot;,
     metric = &quot;logloss&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-64-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plot(rf_model,
     timestep = &quot;number_of_trees&quot;,
     metric = &quot;AUC&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-65-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plot(rf_model,
     timestep = &quot;number_of_trees&quot;,
     metric = &quot;rmse&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-66-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>h2o.auc(rf_model, train = TRUE)</code></pre>
<pre><code>## [1] 0.9907214</code></pre>
<pre class="r"><code>h2o.auc(rf_model, valid = TRUE)</code></pre>
<pre><code>## [1] 0.9829359</code></pre>
<pre class="r"><code>h2o.auc(rf_model, xval = TRUE)</code></pre>
<pre><code>## [1] 0.9903005</code></pre>
<pre class="r"><code>perf &lt;- h2o.performance(rf_model, test)
perf</code></pre>
<pre><code>## H2OBinomialMetrics: drf
## 
## MSE:  0.03258482
## RMSE:  0.1805127
## LogLoss:  0.1072519
## Mean Per-Class Error:  0.02985075
## AUC:  0.9916594
## Gini:  0.9833187
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##           benign malignant    Error    Rate
## benign        63         4 0.059701   =4/67
## malignant      0        34 0.000000   =0/34
## Totals        63        38 0.039604  =4/101
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.306667 0.944444  18
## 2                       max f2  0.306667 0.977011  18
## 3                 max f0point5  0.720000 0.933735  13
## 4                 max accuracy  0.533333 0.960396  16
## 5                max precision  1.000000 1.000000   0
## 6                   max recall  0.306667 1.000000  18
## 7              max specificity  1.000000 1.000000   0
## 8             max absolute_mcc  0.306667 0.917235  18
## 9   max min_per_class_accuracy  0.533333 0.955224  16
## 10 max mean_per_class_accuracy  0.306667 0.970149  18
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`</code></pre>
<pre class="r"><code>plot(perf)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/auc_curve-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>perf@metrics$thresholds_and_metric_scores %&gt;%
  ggplot(aes(x = fpr, y = tpr)) +
    geom_point() +
    geom_line() +
    geom_abline(slope = 1, intercept = 0) +
    labs(x = &quot;False Positive Rate&quot;,
         y = &quot;True Positive Rate&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-69-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>h2o.logloss(perf)</code></pre>
<pre><code>## [1] 0.1072519</code></pre>
<pre class="r"><code>h2o.mse(perf)</code></pre>
<pre><code>## [1] 0.03258482</code></pre>
<pre class="r"><code>h2o.auc(perf)</code></pre>
<pre><code>## [1] 0.9916594</code></pre>
<pre class="r"><code>head(h2o.metric(perf))</code></pre>
<pre><code>## Metrics for Thresholds: Binomial metrics as a function of classification thresholds
##   threshold       f1       f2 f0point5 accuracy precision   recall
## 1  1.000000 0.583333 0.466667 0.777778 0.801980  1.000000 0.411765
## 2  0.986667 0.666667 0.555556 0.833333 0.831683  1.000000 0.500000
## 3  0.973333 0.716981 0.612903 0.863636 0.851485  1.000000 0.558824
## 4  0.960000 0.740741 0.641026 0.877193 0.861386  1.000000 0.588235
## 5  0.946667 0.763636 0.668790 0.889831 0.871287  1.000000 0.617647
## 6  0.920000 0.807018 0.723270 0.912698 0.891089  1.000000 0.676471
##   specificity absolute_mcc min_per_class_accuracy mean_per_class_accuracy
## 1    1.000000     0.563122               0.411765                0.705882
## 2    1.000000     0.631514               0.500000                0.750000
## 3    1.000000     0.675722               0.558824                0.779412
## 4    1.000000     0.697542               0.588235                0.794118
## 5    1.000000     0.719221               0.617647                0.808824
## 6    1.000000     0.762280               0.676471                0.838235
##   tns fns fps tps      tnr      fnr      fpr      tpr idx
## 1  67  20   0  14 1.000000 0.588235 0.000000 0.411765   0
## 2  67  17   0  17 1.000000 0.500000 0.000000 0.500000   1
## 3  67  15   0  19 1.000000 0.441176 0.000000 0.558824   2
## 4  67  14   0  20 1.000000 0.411765 0.000000 0.588235   3
## 5  67  13   0  21 1.000000 0.382353 0.000000 0.617647   4
## 6  67  11   0  23 1.000000 0.323529 0.000000 0.676471   5</code></pre>
<pre class="r"><code>finalRf_predictions &lt;- data.frame(actual = as.vector(test$classes), 
                                  as.data.frame(h2o.predict(object = rf_model, 
                                                            newdata = test)))

finalRf_predictions$accurate &lt;- ifelse(finalRf_predictions$actual == 
                                         finalRf_predictions$predict, &quot;yes&quot;, &quot;no&quot;)

finalRf_predictions$predict_stringent &lt;- ifelse(finalRf_predictions$benign &gt; 0.8, &quot;benign&quot;, 
                                                ifelse(finalRf_predictions$malignant 
                                                       &gt; 0.8, &quot;malignant&quot;, &quot;uncertain&quot;))
finalRf_predictions$accurate_stringent &lt;- ifelse(finalRf_predictions$actual == 
                                                   finalRf_predictions$predict_stringent, &quot;yes&quot;, 
                                       ifelse(finalRf_predictions$predict_stringent == 
                                                &quot;uncertain&quot;, &quot;na&quot;, &quot;no&quot;))

finalRf_predictions %&gt;%
  group_by(actual, predict) %&gt;%
  dplyr::summarise(n = n())</code></pre>
<pre><code>## # A tibble: 4 x 3
## # Groups:   actual [?]
##   actual    predict       n
##   &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;
## 1 benign    benign       64
## 2 benign    malignant     3
## 3 malignant benign        1
## 4 malignant malignant    33</code></pre>
<pre class="r"><code>finalRf_predictions %&gt;%
  group_by(actual, predict_stringent) %&gt;%
  dplyr::summarise(n = n())</code></pre>
<pre><code>## # A tibble: 5 x 3
## # Groups:   actual [?]
##   actual    predict_stringent     n
##   &lt;fct&gt;     &lt;chr&gt;             &lt;int&gt;
## 1 benign    benign               62
## 2 benign    malignant             2
## 3 benign    uncertain             3
## 4 malignant malignant            29
## 5 malignant uncertain             5</code></pre>
<pre class="r"><code>finalRf_predictions %&gt;%
  ggplot(aes(x = actual, fill = accurate)) +
    geom_bar(position = &quot;dodge&quot;) +
    scale_fill_brewer(palette = &quot;Set1&quot;) +
    labs(fill = &quot;Were\npredictions\naccurate?&quot;,
         title = &quot;Default predictions&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/default_vs_stringent-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>finalRf_predictions %&gt;%
  subset(accurate_stringent != &quot;na&quot;) %&gt;%
  ggplot(aes(x = actual, fill = accurate_stringent)) +
    geom_bar(position = &quot;dodge&quot;) +
    scale_fill_brewer(palette = &quot;Set1&quot;) +
    labs(fill = &quot;Were\npredictions\naccurate?&quot;,
         title = &quot;Stringent predictions&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/default_vs_stringent-2.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>df &lt;- finalRf_predictions[, c(1, 3, 4)]

thresholds &lt;- seq(from = 0, to = 1, by = 0.1)

prop_table &lt;- data.frame(threshold = thresholds, prop_true_b = NA, prop_true_m = NA)

for (threshold in thresholds) {
  pred &lt;- ifelse(df$benign &gt; threshold, &quot;benign&quot;, &quot;malignant&quot;)
  pred_t &lt;- ifelse(pred == df$actual, TRUE, FALSE)
  
  group &lt;- data.frame(df, &quot;pred&quot; = pred_t) %&gt;%
  group_by(actual, pred) %&gt;%
  dplyr::summarise(n = n())
  
  group_b &lt;- filter(group, actual == &quot;benign&quot;)
  
  prop_b &lt;- sum(filter(group_b, pred == TRUE)$n) / sum(group_b$n)
  prop_table[prop_table$threshold == threshold, &quot;prop_true_b&quot;] &lt;- prop_b
  
  group_m &lt;- filter(group, actual == &quot;malignant&quot;)
  
  prop_m &lt;- sum(filter(group_m, pred == TRUE)$n) / sum(group_m$n)
  prop_table[prop_table$threshold == threshold, &quot;prop_true_m&quot;] &lt;- prop_m
}

prop_table %&gt;%
  gather(x, y, prop_true_b:prop_true_m) %&gt;%
  ggplot(aes(x = threshold, y = y, color = x)) +
    geom_point() +
    geom_line() +
    scale_color_brewer(palette = &quot;Set1&quot;) +
    labs(y = &quot;proportion of true predictions&quot;,
         color = &quot;b: benign cases\nm: malignant cases&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/prop_table-1.png" width="576" style="display: block; margin: auto;" /></p>
<hr />
<p>If you are interested in more machine learning posts, check out the category listing for <strong>machine_learning</strong> on my blog - <a href="https://shirinsplayground.netlify.com/categories/#posts-list-machine-learning" class="uri">https://shirinsplayground.netlify.com/categories/#posts-list-machine-learning</a> - <a href="https://shiring.github.io/categories.html#machine_learning-ref" class="uri">https://shiring.github.io/categories.html#machine_learning-ref</a></p>
<hr />
<p><br></p>
<pre class="r"><code>stopCluster(cl)
h2o.shutdown()</code></pre>
<pre><code>## Are you sure you want to shutdown the H2O instance running at http://localhost:54321/ (Y/N)?</code></pre>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.0 (2018-04-23)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.5
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods  
## [8] base     
## 
## other attached packages:
##  [1] ggrepel_0.8.0     reshape2_1.4.3    h2o_3.20.0.2     
##  [4] corrplot_0.84     caret_6.0-80      doParallel_1.0.11
##  [7] iterators_1.0.9   foreach_1.4.4     ellipse_0.4.1    
## [10] igraph_1.2.1      bindrcpp_0.2.2    mice_3.1.0       
## [13] lattice_0.20-35   forcats_0.3.0     stringr_1.3.1    
## [16] dplyr_0.7.5       purrr_0.2.5       readr_1.1.1      
## [19] tidyr_0.8.1       tibble_1.4.2      ggplot2_2.2.1    
## [22] tidyverse_1.2.1  
## 
## loaded via a namespace (and not attached):
##  [1] minqa_1.2.4         colorspace_1.3-2    class_7.3-14       
##  [4] rprojroot_1.3-2     pls_2.6-0           rstudioapi_0.7     
##  [7] DRR_0.0.3           prodlim_2018.04.18  lubridate_1.7.4    
## [10] xml2_1.2.0          codetools_0.2-15    splines_3.5.0      
## [13] mnormt_1.5-5        robustbase_0.93-1   knitr_1.20         
## [16] RcppRoll_0.3.0      jsonlite_1.5        nloptr_1.0.4       
## [19] broom_0.4.4         ddalpha_1.3.4       kernlab_0.9-26     
## [22] sfsmisc_1.1-2       compiler_3.5.0      httr_1.3.1         
## [25] backports_1.1.2     assertthat_0.2.0    Matrix_1.2-14      
## [28] lazyeval_0.2.1      cli_1.0.0           htmltools_0.3.6    
## [31] tools_3.5.0         gtable_0.2.0        glue_1.2.0         
## [34] Rcpp_0.12.17        cellranger_1.1.0    nlme_3.1-137       
## [37] blogdown_0.6        psych_1.8.4         timeDate_3043.102  
## [40] xfun_0.2            gower_0.1.2         lme4_1.1-17        
## [43] rvest_0.3.2         pan_1.4             DEoptimR_1.0-8     
## [46] MASS_7.3-50         scales_0.5.0        ipred_0.9-6        
## [49] hms_0.4.2           RColorBrewer_1.1-2  yaml_2.1.19        
## [52] rpart_4.1-13        stringi_1.2.3       randomForest_4.6-14
## [55] e1071_1.6-8         lava_1.6.1          geometry_0.3-6     
## [58] bitops_1.0-6        rlang_0.2.1         pkgconfig_2.0.1    
## [61] evaluate_0.10.1     bindr_0.1.1         recipes_0.1.3      
## [64] labeling_0.3        CVST_0.2-2          tidyselect_0.2.4   
## [67] plyr_1.8.4          magrittr_1.5        bookdown_0.7       
## [70] R6_2.2.2            mitml_0.3-5         dimRed_0.1.0       
## [73] pillar_1.2.3        haven_1.1.1         foreign_0.8-70     
## [76] withr_2.1.2         RCurl_1.95-4.10     survival_2.42-3    
## [79] abind_1.4-5         nnet_7.3-12         modelr_0.1.2       
## [82] crayon_1.3.4        jomo_2.6-2          xgboost_0.71.2     
## [85] utf8_1.1.4          rmarkdown_1.10      grid_3.5.0         
## [88] readxl_1.1.0        data.table_1.11.4   ModelMetrics_1.1.0 
## [91] digest_0.6.15       stats4_3.5.0        munsell_0.5.0      
## [94] magic_1.5-8</code></pre>
</div>
</div>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Text-to-speech with R]]></title>
    <link href="/2018/06/text_to_speech_r/"/>
    <id>/2018/06/text_to_speech_r/</id>
    <published>2018-06-27T00:00:00+00:00</published>
    <updated>2018-06-27T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Computers started talking to us! They do this with so called Text-to-Speech (TTS) systems. With neural nets, deep learning and lots of training data, these systems have gotten a whole lot better in recent years. In some cases, they are so good that you can’t distinguish between human and machine voice.</p>
<p>In one of our recent <a href="https://blog.codecentric.de/2018/04/kuenstliche-intelligenz-codecentric_ai/">codecentric.AI</a> <a href="https://youtu.be/2EEMSsVBE8w">videos</a>, we compared <a href="https://youtu.be/2EEMSsVBE8w">different Text-to-Speech systems</a> (the video is in German, though - but the text snippets and their voice recordings we show in the video are a mix of German and English). In this video, we had a small contest between Polly, Alexa, Siri And Co to find out who best speaks different tongue twisters.</p>
<p><br> <iframe width="560" height="315" src="https://www.youtube.com/embed/2EEMSsVBE8w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
<p>Here, I want to find out what’s possible with R and Text-to-Speech packages.</p>
<p><br></p>
<p>PS: In a second post I also tried the <a href="https://shirinsplayground.netlify.com/2018/06/googlelanguager">googleLanguageR</a> package - with much better results!</p>
<div id="how-does-tts-work" class="section level2">
<h2>How does TTS work?</h2>
<p>Challenges for good TTS systems are the complexity of the human language: we intone words differently, depending on where they are in a sentence, what we want to convey with that sentence, how our mood is, and so on. AI-based TTS systems can take phonemes and intonation into account.</p>
<p>There are different ways to artificially produce speech. A very important method is Unit Selection synthesis. With this method, text is first normalized and divided into smaller entities that represent sentences, syllables, words, phonemes, etc. The structure (e.g. the pronunciation) of these entities is then learned in context. We call this part Natural Language Processing (NLP). Usually, these learned segments are stored in a database (either as human voice recordings or synthetically generated) that can be searched to find suitable speech parts (Unit Selection). This search is often done with decision trees, neural nets or Hidden-Markov-Models.</p>
<p>If the speech has been generated by a computer, this is called formant synthesis. It offers more flexibility because the collection of words isn’t limited to what has been pre-recorded by a human. Even imaginary or new words can easily be produced and the voices can be readily exchanged. Until recently, this synthetic voice did not sound anything like a human recorded voice; you could definitely hear that it was “fake”. Most of the TTS systems today still suffer from this, but this is in the process of changing: there are already a few artificial TTS systems that do sound very human.</p>
</div>
<div id="what-tts-systems-are-there" class="section level2">
<h2>What TTS systems are there?</h2>
<p>We already find TTS systems in many digital devices, like computers, smart phones, etc. Most of the “big players” offer TTS-as-a-service, but there are also many “smaller” and free programs for TTS. Many can be downloaded as software or used from a web browser or as an API. Here is an incomplete list:</p>
<ul>
<li>Microsoft/Windows: includes <a href="https://support.microsoft.com/de-de/help/22798/windows-10-narrator-get-started"><strong>Narrator</strong></a> and <a href="https://www.microsoft.com/en-us/download/details.aspx?id=10121"><strong>Microsoft Speech API</strong></a></li>
<li>Mac: <a href="https://www.apple.com/de/accessibility/mac/vision/"><strong>VoiceOver</strong></a></li>
<li>Linux: different software can be installed, e.g. <a href="https://sourceforge.net/projects/espeak/files/espeak/"><strong>eSpeak</strong></a></li>
<li><a href="https://www.ibm.com/watson/services/text-to-speech/">IBM Watson</a></li>
<li><a href="https://cloud.google.com/text-to-speech/">Google Cloud</a></li>
<li><a href="https://azure.microsoft.com/de-de/services/cognitive-services/speech/">Microsoft Azure</a></li>
<li><a href="https://developer.amazon.com/de/alexa">Amazon Alexa</a></li>
<li><a href="https://9to5mac.com/2017/03/10/how-to-get-siri-to-read-articles-to-you-on-ios-macos/">Siri on iPhone</a></li>
<li><a href="https://aws.amazon.com/de/polly/">Polly on Amazon AWS</a></li>
<li><a href="https://support.microsoft.com/de-de/help/17214/windows-10-what-is">Microsoft Cortana</a></li>
<li><a href="https://freetts.sourceforge.io/docs/">FreeTTS</a></li>
<li><a href="https://www.ispeech.org/text.to.speech">iSpeech</a></li>
<li><a href="https://www.naturalreaders.com/online/">Natural Readers</a></li>
<li><a href="http://www.cross-plus-a.com/balabolka.htm">Balabolka</a></li>
<li><a href="https://www.panopreter.com/en/products/pb/download.php">Panopreter</a></li>
<li><a href="https://www.text2speech.org/">text2speech.org</a></li>
<li><a href="http://text-to-speech-translator.paralink.com/">text-to-speech-translator.paralink.com/</a></li>
</ul>
</div>
<div id="text-to-speech-in-r" class="section level2">
<h2>Text-to-Speech in R</h2>
<p>The only package for TTS I found was <code>Rtts</code>. It doesn’t seem very comprehensive but it does the job of converting text to speech. The only API that works right now is **ITRI (<a href="http://tts.itri.org.tw)**" class="uri">http://tts.itri.org.tw)**</a>. And it only supports English and Chinese.</p>
<p>Let’s try it out!</p>
<pre class="r"><code>library(Rtts)</code></pre>
<pre><code>## Lade nötiges Paket: RCurl</code></pre>
<pre><code>## Lade nötiges Paket: bitops</code></pre>
<p>Here, I’ll be using a quote from <strong>DOUGLAS ADAMS’ THE HITCHHIKER’S GUIDE TO THE GALAXY</strong>:</p>
<pre class="r"><code>content &lt;- &quot;A common mistake that people make when trying to design something completely foolproof is to underestimate the ingenuity of complete fools.&quot;</code></pre>
<p>The main TTS function is <code>tts_ITRI()</code> and I’m going to loop over the different voice options.</p>
<pre class="r"><code>speakers = c(&quot;Bruce&quot;, &quot;Theresa&quot;, &quot;Angela&quot;, &quot;MCHEN_Bruce&quot;, &quot;MCHEN_Joddess&quot;, &quot;ENG_Bob&quot;, &quot;ENG_Alice&quot;, &quot;ENG_Tracy&quot;)
lapply(speakers, function(x) tts_ITRI(content, speaker = x,
         destfile = paste0(&quot;audio_tts_&quot;, x, &quot;.mp3&quot;)))</code></pre>
<p>I uploaded the results to Soundcloud for you to hear: - <a href="https://soundcloud.com/shirin-glander-729692416/audio-tts-bruce/s-iZC6u?in=shirin-glander-729692416/sets/text-to-speech-blogpost/s-lXYea">audio-tts-bruce</a> - <a href="https://soundcloud.com/shirin-glander-729692416/audio-tts-theresa/s-lYt0R?in=shirin-glander-729692416/sets/text-to-speech-blogpost/s-lXYea">audio-tts-theresa</a> - <a href="https://soundcloud.com/shirin-glander-729692416/audio-tts-angela/s-KVUMS?in=shirin-glander-729692416/sets/text-to-speech-blogpost/s-lXYea">audio-tts-angela</a> - <a href="https://soundcloud.com/shirin-glander-729692416/audio-tts-mchen-bruce/s-KVDeb?in=shirin-glander-729692416/sets/text-to-speech-blogpost/s-lXYea">audio-tts-mchen-bruce</a> - <a href="https://soundcloud.com/shirin-glander-729692416/audio-tts-mchen-joddess/s-mDdik?in=shirin-glander-729692416/sets/text-to-speech-blogpost/s-lXYea">audio-tts-mchen-joddess</a> - <a href="https://soundcloud.com/shirin-glander-729692416/audio-tts-eng-bob/s-520Y2?in=shirin-glander-729692416/sets/text-to-speech-blogpost/s-lXYea">audio-tts-eng-bob</a> - <a href="https://soundcloud.com/shirin-glander-729692416/audio-tts-eng-alice/s-BKTpj?in=shirin-glander-729692416/sets/text-to-speech-blogpost/s-lXYea">audio-tts-eng-alice</a> - <a href="https://soundcloud.com/shirin-glander-729692416/audio-tts-eng-tracy/s-SKVDm?in=shirin-glander-729692416/sets/text-to-speech-blogpost/s-lXYea">audio-tts-eng-tracy</a></p>
<p>As you can hear, it sounds quite wonky. There are many better alternatives out there, but most of them aren’t free and/or can’t be used (as easily) from R. Noam Ross tried <a href="https://rpubs.com/noamross/153216">IBM Watson’s TTS API in this post</a>, which would be a very good solution. Or you could access the <a href="https://cloud.google.com/text-to-speech/">Google Cloud</a> API from within R.</p>
<p>The most convenient solution for me was to use <a href="https://sourceforge.net/projects/espeak/files/espeak/"><strong>eSpeak</strong></a> from the command line. The output sounds relatively good, it is free and offers many languages and voices with lots of parameters to tweak. This is how you would produce audio from text with eSpeak:</p>
<ul>
<li>English US</li>
</ul>
<pre><code>espeak -v english-us -s 150 -w &#39;/Users/shiringlander/Documents/Github/audio_tts_espeak_en_us.wav&#39; &quot;A common mistake that people make when trying to design something completely foolproof is to underestimate the ingenuity of complete fools.&quot;</code></pre>
<ul>
<li>just for fun: English Scottish</li>
</ul>
<pre><code>espeak -v en-scottish -s 150 -w &#39;/Users/shiringlander/Documents/Github/audio_tts_espeak_en-scottish.wav&#39; &quot;A common mistake that people make when trying to design something completely foolproof is to underestimate the ingenuity of complete fools.&quot;</code></pre>
<ul>
<li>even funnier: German</li>
</ul>
<pre><code>espeak -v german -s 150 -w &#39;/Users/shiringlander/Documents/Github/audio_tts_espeak_german.wav&#39; &quot;A common mistake that people make when trying to design something completely foolproof is to underestimate the ingenuity of complete fools.&quot;</code></pre>
<p>The <a href="https://soundcloud.com/shirin-glander-729692416/sets/text-to-speech-blogpost/s-lXYea">playlist</a> contains all audio files I generated in this post.</p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.0 (2018-04-23)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.5
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] Rtts_0.3.3      RCurl_1.95-4.10 bitops_1.0-6   
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_0.12.17    bookdown_0.7    digest_0.6.15   rprojroot_1.3-2
##  [5] backports_1.1.2 magrittr_1.5    evaluate_0.10.1 blogdown_0.6   
##  [9] stringi_1.2.3   rmarkdown_1.10  tools_3.5.0     stringr_1.3.1  
## [13] xfun_0.2        yaml_2.1.19     compiler_3.5.0  htmltools_0.3.6
## [17] knitr_1.20</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Explaining Keras image classification models with lime]]></title>
    <link href="/2018/06/keras_fruits_lime/"/>
    <id>/2018/06/keras_fruits_lime/</id>
    <published>2018-06-21T00:00:00+00:00</published>
    <updated>2018-06-21T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits/">Last week I published a blog post about how easy it is to train image classification models with Keras</a>.</p>
<p>What I did not show in that post was how to use the model for making predictions. This, I will do here. But predictions alone are boring, so I’m adding explanations for the predictions using the <code>lime</code> package.</p>
<p>I have already written a few blog posts (<a href="https://shirinsplayground.netlify.com/2018/01/looking_beyond_accuracy_to_improve_trust_in_ml/">here</a>, <a href="https://shiring.github.io/machine_learning/2017/04/23/lime">here</a> and <a href="https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/">here</a>) about LIME and have given talks (<a href="https://shirinsplayground.netlify.com/2018/02/m3_2018/">here</a> and <a href="https://shirinsplayground.netlify.com/2018/04/hh_datascience_meetup_2018_slides/">here</a>) about it, too.</p>
<p>Neither of them applies LIME to image classification models, though. And with the new(ish) release from March of <a href="https://cran.r-project.org/web/packages/lime/index.html">Thomas Lin Pedersen’s <code>lime</code> package</a>, <code>lime</code> is now not only on CRAN but it natively supports Keras and image classification models.</p>
<p>Thomas wrote a very nice <a href="https://www.data-imaginist.com/2018/lime-v0-4-the-kitten-picture-edition/">article about how to use <code>keras</code> and <code>lime</code> in R</a>! Here, I am following this article to use Imagenet (VGG16) to make and explain predictions of fruit images and then I am extending the analysis to <a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits/">last week’s model</a> and compare it with the pretrained net.</p>
<div id="loading-libraries-and-models" class="section level2">
<h2>Loading libraries and models</h2>
<pre class="r"><code>library(keras)   # for working with neural nets
library(lime)    # for explaining models
library(magick)  # for preprocessing images
library(ggplot2) # for additional plotting</code></pre>
<ul>
<li>Loading the pretrained Imagenet model</li>
</ul>
<pre class="r"><code>model &lt;- application_vgg16(weights = &quot;imagenet&quot;, include_top = TRUE)
model</code></pre>
<pre><code>## Model
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## input_1 (InputLayer)             (None, 224, 224, 3)           0           
## ___________________________________________________________________________
## block1_conv1 (Conv2D)            (None, 224, 224, 64)          1792        
## ___________________________________________________________________________
## block1_conv2 (Conv2D)            (None, 224, 224, 64)          36928       
## ___________________________________________________________________________
## block1_pool (MaxPooling2D)       (None, 112, 112, 64)          0           
## ___________________________________________________________________________
## block2_conv1 (Conv2D)            (None, 112, 112, 128)         73856       
## ___________________________________________________________________________
## block2_conv2 (Conv2D)            (None, 112, 112, 128)         147584      
## ___________________________________________________________________________
## block2_pool (MaxPooling2D)       (None, 56, 56, 128)           0           
## ___________________________________________________________________________
## block3_conv1 (Conv2D)            (None, 56, 56, 256)           295168      
## ___________________________________________________________________________
## block3_conv2 (Conv2D)            (None, 56, 56, 256)           590080      
## ___________________________________________________________________________
## block3_conv3 (Conv2D)            (None, 56, 56, 256)           590080      
## ___________________________________________________________________________
## block3_pool (MaxPooling2D)       (None, 28, 28, 256)           0           
## ___________________________________________________________________________
## block4_conv1 (Conv2D)            (None, 28, 28, 512)           1180160     
## ___________________________________________________________________________
## block4_conv2 (Conv2D)            (None, 28, 28, 512)           2359808     
## ___________________________________________________________________________
## block4_conv3 (Conv2D)            (None, 28, 28, 512)           2359808     
## ___________________________________________________________________________
## block4_pool (MaxPooling2D)       (None, 14, 14, 512)           0           
## ___________________________________________________________________________
## block5_conv1 (Conv2D)            (None, 14, 14, 512)           2359808     
## ___________________________________________________________________________
## block5_conv2 (Conv2D)            (None, 14, 14, 512)           2359808     
## ___________________________________________________________________________
## block5_conv3 (Conv2D)            (None, 14, 14, 512)           2359808     
## ___________________________________________________________________________
## block5_pool (MaxPooling2D)       (None, 7, 7, 512)             0           
## ___________________________________________________________________________
## flatten (Flatten)                (None, 25088)                 0           
## ___________________________________________________________________________
## fc1 (Dense)                      (None, 4096)                  102764544   
## ___________________________________________________________________________
## fc2 (Dense)                      (None, 4096)                  16781312    
## ___________________________________________________________________________
## predictions (Dense)              (None, 1000)                  4097000     
## ===========================================================================
## Total params: 138,357,544
## Trainable params: 138,357,544
## Non-trainable params: 0
## ___________________________________________________________________________</code></pre>
<ul>
<li>loading my own model from <a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits/">last week’s post</a></li>
</ul>
<pre class="r"><code>model2 &lt;- load_model_hdf5(filepath = &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/keras/fruits_checkpoints.h5&quot;)
model2</code></pre>
<pre><code>## Model
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## conv2d_1 (Conv2D)                (None, 20, 20, 32)            896         
## ___________________________________________________________________________
## activation_1 (Activation)        (None, 20, 20, 32)            0           
## ___________________________________________________________________________
## conv2d_2 (Conv2D)                (None, 20, 20, 16)            4624        
## ___________________________________________________________________________
## leaky_re_lu_1 (LeakyReLU)        (None, 20, 20, 16)            0           
## ___________________________________________________________________________
## batch_normalization_1 (BatchNorm (None, 20, 20, 16)            64          
## ___________________________________________________________________________
## max_pooling2d_1 (MaxPooling2D)   (None, 10, 10, 16)            0           
## ___________________________________________________________________________
## dropout_1 (Dropout)              (None, 10, 10, 16)            0           
## ___________________________________________________________________________
## flatten_1 (Flatten)              (None, 1600)                  0           
## ___________________________________________________________________________
## dense_1 (Dense)                  (None, 100)                   160100      
## ___________________________________________________________________________
## activation_2 (Activation)        (None, 100)                   0           
## ___________________________________________________________________________
## dropout_2 (Dropout)              (None, 100)                   0           
## ___________________________________________________________________________
## dense_2 (Dense)                  (None, 16)                    1616        
## ___________________________________________________________________________
## activation_3 (Activation)        (None, 16)                    0           
## ===========================================================================
## Total params: 167,300
## Trainable params: 167,268
## Non-trainable params: 32
## ___________________________________________________________________________</code></pre>
</div>
<div id="load-and-prepare-images" class="section level2">
<h2>Load and prepare images</h2>
<p>Here, I am loading and preprocessing two images of fruits (and yes, I am cheating a bit because I am choosing images where I expect my model to work as they are similar to the training images…).</p>
<ul>
<li>Banana</li>
</ul>
<pre class="r"><code>test_image_files_path &lt;- &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Test&quot;

img &lt;- image_read(&#39;https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Banana-Single.jpg/272px-Banana-Single.jpg&#39;)
img_path &lt;- file.path(test_image_files_path, &quot;Banana&quot;, &#39;banana.jpg&#39;)
image_write(img, img_path)
#plot(as.raster(img))</code></pre>
<ul>
<li>Clementine</li>
</ul>
<pre class="r"><code>img2 &lt;- image_read(&#39;https://cdn.pixabay.com/photo/2010/12/13/09/51/clementine-1792_1280.jpg&#39;)
img_path2 &lt;- file.path(test_image_files_path, &quot;Clementine&quot;, &#39;clementine.jpg&#39;)
image_write(img2, img_path2)
#plot(as.raster(img2))</code></pre>
<div id="superpixels" class="section level3">
<h3>Superpixels</h3>
<blockquote>
<p>The segmentation of an image into superpixels are an important step in generating explanations for image models. It is both important that the segmentation is correct and follows meaningful patterns in the picture, but also that the size/number of superpixels are appropriate. If the important features in the image are chopped into too many segments the permutations will probably damage the picture beyond recognition in almost all cases leading to a poor or failing explanation model. As the size of the object of interest is varying it is impossible to set up hard rules for the number of superpixels to segment into - the larger the object is relative to the size of the image, the fewer superpixels should be generated. Using plot_superpixels it is possible to evaluate the superpixel parameters before starting the time consuming explanation function. (help(plot_superpixels))</p>
</blockquote>
<pre class="r"><code>plot_superpixels(img_path, n_superpixels = 35, weight = 10)</code></pre>
<p><img src="/post/2018-06-21_keras_fruits_lime_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>plot_superpixels(img_path2, n_superpixels = 50, weight = 20)</code></pre>
<p><img src="/post/2018-06-21_keras_fruits_lime_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>From the superpixel plots we can see that the clementine image has a higher resolution than the banana image.</p>
</div>
</div>
<div id="prepare-images-for-imagenet" class="section level2">
<h2>Prepare images for Imagenet</h2>
<pre class="r"><code>image_prep &lt;- function(x) {
  arrays &lt;- lapply(x, function(path) {
    img &lt;- image_load(path, target_size = c(224,224))
    x &lt;- image_to_array(img)
    x &lt;- array_reshape(x, c(1, dim(x)))
    x &lt;- imagenet_preprocess_input(x)
  })
  do.call(abind::abind, c(arrays, list(along = 1)))
}</code></pre>
<ul>
<li>test predictions</li>
</ul>
<pre class="r"><code>res &lt;- predict(model, image_prep(c(img_path, img_path2)))
imagenet_decode_predictions(res)</code></pre>
<pre><code>## [[1]]
##   class_name class_description        score
## 1  n07753592            banana 0.9929747581
## 2  n03532672              hook 0.0013420776
## 3  n07747607            orange 0.0010816186
## 4  n07749582             lemon 0.0010625814
## 5  n07716906  spaghetti_squash 0.0009176208
## 
## [[2]]
##   class_name class_description      score
## 1  n07747607            orange 0.78233224
## 2  n07753592            banana 0.04653566
## 3  n07749582             lemon 0.03868873
## 4  n03134739      croquet_ball 0.03350329
## 5  n07745940        strawberry 0.01862431</code></pre>
<ul>
<li>load labels and train explainer</li>
</ul>
<pre class="r"><code>model_labels &lt;- readRDS(system.file(&#39;extdata&#39;, &#39;imagenet_labels.rds&#39;, package = &#39;lime&#39;))
explainer &lt;- lime(c(img_path, img_path2), as_classifier(model, model_labels), image_prep)</code></pre>
<p>Training the explainer (<code>explain()</code> function) can take pretty long. It will be much faster with the smaller images in my own model but with the bigger Imagenet it takes a few minutes to run.</p>
<pre class="r"><code>explanation &lt;- explain(c(img_path, img_path2), explainer, 
                       n_labels = 2, n_features = 35,
                       n_superpixels = 35, weight = 10,
                       background = &quot;white&quot;)</code></pre>
<ul>
<li><code>plot_image_explanation()</code> only supports showing one case at a time</li>
</ul>
<pre class="r"><code>plot_image_explanation(explanation)</code></pre>
<p><img src="/post/2018-06-21_keras_fruits_lime_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code>clementine &lt;- explanation[explanation$case == &quot;clementine.jpg&quot;,]
plot_image_explanation(clementine)</code></pre>
<p><img src="/post/2018-06-21_keras_fruits_lime_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="prepare-images-for-my-own-model" class="section level2">
<h2>Prepare images for my own model</h2>
<ul>
<li>test predictions (analogous to training and validation images)</li>
</ul>
<pre class="r"><code>test_datagen &lt;- image_data_generator(rescale = 1/255)

test_generator &lt;- flow_images_from_directory(
        test_image_files_path,
        test_datagen,
        target_size = c(20, 20),
        class_mode = &#39;categorical&#39;)

predictions &lt;- as.data.frame(predict_generator(model2, test_generator, steps = 1))

load(&quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/fruits_classes_indices.RData&quot;)
fruits_classes_indices_df &lt;- data.frame(indices = unlist(fruits_classes_indices))
fruits_classes_indices_df &lt;- fruits_classes_indices_df[order(fruits_classes_indices_df$indices), , drop = FALSE]
colnames(predictions) &lt;- rownames(fruits_classes_indices_df)

t(round(predictions, digits = 2))</code></pre>
<pre><code>##             [,1] [,2]
## Kiwi           0    0
## Banana         0    1
## Apricot        0    0
## Avocado        0    0
## Cocos          0    0
## Clementine     1    0
## Mandarine      0    0
## Orange         0    0
## Limes          0    0
## Lemon          0    0
## Peach          0    0
## Plum           0    0
## Raspberry      0    0
## Strawberry     0    0
## Pineapple      0    0
## Pomegranate    0    0</code></pre>
<pre class="r"><code>for (i in 1:nrow(predictions)) {
  cat(i, &quot;:&quot;)
  print(unlist(which.max(predictions[i, ])))
}</code></pre>
<pre><code>## 1 :Clementine 
##          6 
## 2 :Banana 
##      2</code></pre>
<p>This seems to be incompatible with lime, though (or if someone knows how it works, please let me know) - so I prepared the images similarly to the Imagenet images.</p>
<pre class="r"><code>image_prep2 &lt;- function(x) {
  arrays &lt;- lapply(x, function(path) {
    img &lt;- image_load(path, target_size = c(20, 20))
    x &lt;- image_to_array(img)
    x &lt;- reticulate::array_reshape(x, c(1, dim(x)))
    x &lt;- x / 255
  })
  do.call(abind::abind, c(arrays, list(along = 1)))
}</code></pre>
<ul>
<li>prepare labels</li>
</ul>
<pre class="r"><code>fruits_classes_indices_l &lt;- rownames(fruits_classes_indices_df)
names(fruits_classes_indices_l) &lt;- unlist(fruits_classes_indices)
fruits_classes_indices_l</code></pre>
<pre><code>##             9            10             8             2            11 
##        &quot;Kiwi&quot;      &quot;Banana&quot;     &quot;Apricot&quot;     &quot;Avocado&quot;       &quot;Cocos&quot; 
##             3            13            14             7             6 
##  &quot;Clementine&quot;   &quot;Mandarine&quot;      &quot;Orange&quot;       &quot;Limes&quot;       &quot;Lemon&quot; 
##             1             5             0             4            15 
##       &quot;Peach&quot;        &quot;Plum&quot;   &quot;Raspberry&quot;  &quot;Strawberry&quot;   &quot;Pineapple&quot; 
##            12 
## &quot;Pomegranate&quot;</code></pre>
<ul>
<li>train explainer</li>
</ul>
<pre class="r"><code>explainer2 &lt;- lime(c(img_path, img_path2), as_classifier(model2, fruits_classes_indices_l), image_prep2)
explanation2 &lt;- explain(c(img_path, img_path2), explainer2, 
                        n_labels = 1, n_features = 20,
                        n_superpixels = 35, weight = 10,
                        background = &quot;white&quot;)</code></pre>
<ul>
<li>plot feature weights to find a good threshold for plotting <code>block</code> (see below)</li>
</ul>
<pre class="r"><code>explanation2 %&gt;%
  ggplot(aes(x = feature_weight)) +
    facet_wrap(~ case, scales = &quot;free&quot;) +
    geom_density()</code></pre>
<p><img src="/post/2018-06-21_keras_fruits_lime_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<ul>
<li>plot predictions</li>
</ul>
<pre class="r"><code>plot_image_explanation(explanation2, display = &#39;block&#39;, threshold = 5e-07)</code></pre>
<p><img src="/post/2018-06-21_keras_fruits_lime_files/figure-html/banana_explanation-1.png" width="672" /></p>
<pre class="r"><code>clementine2 &lt;- explanation2[explanation2$case == &quot;clementine.jpg&quot;,]
plot_image_explanation(clementine2, display = &#39;block&#39;, threshold = 0.16)</code></pre>
<p><img src="/post/2018-06-21_keras_fruits_lime_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] ggplot2_3.0.0 magick_1.9    lime_0.4.0    keras_2.1.6  
## 
## loaded via a namespace (and not attached):
##  [1] stringdist_0.9.5.1    reticulate_1.8.0.9003 tidyselect_0.2.4     
##  [4] xfun_0.3              purrr_0.2.5           lattice_0.20-35      
##  [7] colorspace_1.3-2      htmltools_0.3.6       yaml_2.1.19          
## [10] base64enc_0.1-3       rlang_0.2.1           pillar_1.2.3         
## [13] later_0.7.3           withr_2.1.2           glue_1.2.0           
## [16] bindrcpp_0.2.2        foreach_1.4.4         plyr_1.8.4           
## [19] bindr_0.1.1           tensorflow_1.8        stringr_1.3.1        
## [22] munsell_0.5.0         blogdown_0.6          gtable_0.2.0         
## [25] htmlwidgets_1.2       codetools_0.2-15      evaluate_0.10.1      
## [28] labeling_0.3          knitr_1.20            httpuv_1.4.4.2       
## [31] tfruns_1.3            curl_3.2              parallel_3.5.1       
## [34] Rcpp_0.12.17          xtable_1.8-2          scales_0.5.0         
## [37] backports_1.1.2       promises_1.0.1        jsonlite_1.5         
## [40] abind_1.4-5           mime_0.5              digest_0.6.15        
## [43] stringi_1.2.3         bookdown_0.7          dplyr_0.7.6          
## [46] shiny_1.1.0           grid_3.5.1            rprojroot_1.3-2      
## [49] tools_3.5.1           magrittr_1.5          shinythemes_1.1.1    
## [52] lazyeval_0.2.1        glmnet_2.0-16         tibble_1.4.2         
## [55] whisker_0.3-2         pkgconfig_2.0.1       zeallot_0.1.0        
## [58] Matrix_1.2-14         gower_0.1.2           assertthat_0.2.0     
## [61] rmarkdown_1.10        iterators_1.0.9       R6_2.2.2             
## [64] compiler_3.5.1</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI: Practical Deep Learning with Rachel Thomas]]></title>
    <link href="/2018/06/twimlai138/"/>
    <id>/2018/06/twimlai138/</id>
    <published>2018-06-18T00:00:00+00:00</published>
    <updated>2018-06-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Practical Deep Learning with Rachel Thomas</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai138.jpg" alt="Sketchnotes from TWiMLAI talk: Practical Deep Learning with Rachel Thomas" />
<p class="caption">Sketchnotes from TWiMLAI talk: Practical Deep Learning with Rachel Thomas</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-138-practical-deep-learning-with-rachel-thomas/">here</a>.</p>
<blockquote>
<p>In this episode, i’m joined by Rachel Thomas, founder and researcher at Fast AI. If you’re not familiar with Fast AI, the company offers a series of courses including Practical Deep Learning for Coders, Cutting Edge Deep Learning for Coders and Rachel’s Computational Linear Algebra course. The courses are designed to make deep learning more accessible to those without the extensive math backgrounds some other courses assume. Rachel and I cover a lot of ground in this conversation, starting with the philosophy and goals behind the Fast AI courses. We also cover Fast AI’s recent decision to switch to their courses from Tensorflow to Pytorch, the reasons for this, and the lessons they’ve learned in the process. We discuss the role of the Fast AI deep learning library as well, and how it was recently used to held their team achieve top results on a popular industry benchmark of training time and training cost by a factor of more than ten. <a href="https://twimlai.com/twiml-talk-138-practical-deep-learning-with-rachel-thomas/" class="uri">https://twimlai.com/twiml-talk-138-practical-deep-learning-with-rachel-thomas/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[It&#39;s that easy! Image classification with keras in roughly 100 lines of code.]]></title>
    <link href="/2018/06/keras_fruits/"/>
    <id>/2018/06/keras_fruits/</id>
    <published>2018-06-15T00:00:00+00:00</published>
    <updated>2018-06-15T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I’ve been using keras and TensorFlow for a while now - and love its simplicity and straight-forward way to modeling. As part of the latest update to my <a href="https://shirinsplayground.netlify.com/2018/05/deep_learning_keras_tensorflow_18_07/">Workshop about deep learning with R and keras</a> I’ve added a new example analysis:</p>
<p><strong>Building an image classifier to differentiate different types of fruits</strong></p>
<p>And I was (again) suprised how fast and easy it was to build the model; it took not even half an hour and only around 100 lines of code (counting only the main code; for this post I added comments and line breaks to make it easier to read)!</p>
<iframe src="https://giphy.com/embed/5p2wQFyu8GsFO" width="480" height="271" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>
<a href="https://giphy.com/gifs/5p2wQFyu8GsFO">via GIPHY</a>
</p>
<p>That’s why I wanted to share it here and spread the <code>keras</code> love. &lt;3</p>
<div id="the-code" class="section level2">
<h2>The code</h2>
<p>If you haven’t installed keras before, follow the instructions of <a href="https://keras.rstudio.com/">RStudio’s keras site</a></p>
<pre class="r"><code>library(keras)</code></pre>
<p>The dataset is the <a href="https://www.kaggle.com/moltean/fruits/data">fruit images dataset from Kaggle</a>. I downloaded it to my computer and unpacked it. Because I don’t want to build a model for all the different fruits, I define a list of fruits (corresponding to the folder names) that I want to include in the model.</p>
<p>I also define a few other parameters in the beginning to make adapting as easy as possible.</p>
<pre class="r"><code># list of fruits to modle
fruit_list &lt;- c(&quot;Kiwi&quot;, &quot;Banana&quot;, &quot;Apricot&quot;, &quot;Avocado&quot;, &quot;Cocos&quot;, &quot;Clementine&quot;, &quot;Mandarine&quot;, &quot;Orange&quot;,
                &quot;Limes&quot;, &quot;Lemon&quot;, &quot;Peach&quot;, &quot;Plum&quot;, &quot;Raspberry&quot;, &quot;Strawberry&quot;, &quot;Pineapple&quot;, &quot;Pomegranate&quot;)

# number of output classes (i.e. fruits)
output_n &lt;- length(fruit_list)

# image size to scale down to (original images are 100 x 100 px)
img_width &lt;- 20
img_height &lt;- 20
target_size &lt;- c(img_width, img_height)

# RGB = 3 channels
channels &lt;- 3

# path to image folders
train_image_files_path &lt;- &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Training/&quot;
valid_image_files_path &lt;- &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Validation/&quot;</code></pre>
<div id="loading-images" class="section level3">
<h3>Loading images</h3>
<p>The handy <code>image_data_generator()</code> and <code>flow_images_from_directory()</code> functions can be used to load images from a directory. If you want to use data augmentation, you can directly define how and in what way you want to augment your images with <code>image_data_generator</code>. Here I am not augmenting the data, I only scale the pixel values to fall between 0 and 1.</p>
<pre class="r"><code># optional data augmentation
train_data_gen = image_data_generator(
  rescale = 1/255 #,
  #rotation_range = 40,
  #width_shift_range = 0.2,
  #height_shift_range = 0.2,
  #shear_range = 0.2,
  #zoom_range = 0.2,
  #horizontal_flip = TRUE,
  #fill_mode = &quot;nearest&quot;
)

# Validation data shouldn&#39;t be augmented! But it should also be scaled.
valid_data_gen &lt;- image_data_generator(
  rescale = 1/255
  )  </code></pre>
<p>Now we load the images into memory and resize them.</p>
<pre class="r"><code># training images
train_image_array_gen &lt;- flow_images_from_directory(train_image_files_path, 
                                          train_data_gen,
                                          target_size = target_size,
                                          class_mode = &quot;categorical&quot;,
                                          classes = fruit_list,
                                          seed = 42)

# validation images
valid_image_array_gen &lt;- flow_images_from_directory(valid_image_files_path, 
                                          valid_data_gen,
                                          target_size = target_size,
                                          class_mode = &quot;categorical&quot;,
                                          classes = fruit_list,
                                          seed = 42)</code></pre>
<pre class="r"><code>cat(&quot;Number of images per class:&quot;)</code></pre>
<pre><code>## Number of images per class:</code></pre>
<pre class="r"><code>table(factor(train_image_array_gen$classes))</code></pre>
<pre><code>## 
##   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15 
## 466 490 492 427 490 490 490 479 490 492 492 447 490 492 490 492</code></pre>
<pre class="r"><code>cat(&quot;\nClass label vs index mapping:\n&quot;)</code></pre>
<pre><code>## 
## Class label vs index mapping:</code></pre>
<pre class="r"><code>train_image_array_gen$class_indices</code></pre>
<pre><code>## $Lemon
## [1] 9
## 
## $Peach
## [1] 10
## 
## $Limes
## [1] 8
## 
## $Apricot
## [1] 2
## 
## $Plum
## [1] 11
## 
## $Avocado
## [1] 3
## 
## $Strawberry
## [1] 13
## 
## $Pineapple
## [1] 14
## 
## $Orange
## [1] 7
## 
## $Mandarine
## [1] 6
## 
## $Banana
## [1] 1
## 
## $Clementine
## [1] 5
## 
## $Kiwi
## [1] 0
## 
## $Cocos
## [1] 4
## 
## $Pomegranate
## [1] 15
## 
## $Raspberry
## [1] 12</code></pre>
<pre class="r"><code>fruits_classes_indices &lt;- train_image_array_gen$class_indices
save(fruits_classes_indices, file = &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/fruits_classes_indices.RData&quot;)</code></pre>
</div>
<div id="define-model" class="section level3">
<h3>Define model</h3>
<p>Next, we define the <code>keras</code> model.</p>
<pre class="r"><code># number of training samples
train_samples &lt;- train_image_array_gen$n
# number of validation samples
valid_samples &lt;- valid_image_array_gen$n

# define batch size and number of epochs
batch_size &lt;- 32
epochs &lt;- 10</code></pre>
<p>The model I am using here is a very simple sequential convolutional neural net with the following hidden layers: 2 convolutional layers, one pooling layer and one dense layer.</p>
<pre class="r"><code># initialise model
model &lt;- keras_model_sequential()

# add layers
model %&gt;%
  layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = &quot;same&quot;, input_shape = c(img_width, img_height, channels)) %&gt;%
  layer_activation(&quot;relu&quot;) %&gt;%
  
  # Second hidden layer
  layer_conv_2d(filter = 16, kernel_size = c(3,3), padding = &quot;same&quot;) %&gt;%
  layer_activation_leaky_relu(0.5) %&gt;%
  layer_batch_normalization() %&gt;%

  # Use max pooling
  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;%
  layer_dropout(0.25) %&gt;%
  
  # Flatten max filtered output into feature vector 
  # and feed into dense layer
  layer_flatten() %&gt;%
  layer_dense(100) %&gt;%
  layer_activation(&quot;relu&quot;) %&gt;%
  layer_dropout(0.5) %&gt;%

  # Outputs from dense layer are projected onto output layer
  layer_dense(output_n) %&gt;% 
  layer_activation(&quot;softmax&quot;)

# compile
model %&gt;% compile(
  loss = &quot;categorical_crossentropy&quot;,
  optimizer = optimizer_rmsprop(lr = 0.0001, decay = 1e-6),
  metrics = &quot;accuracy&quot;
)</code></pre>
<p>Fit the model; because I used <code>image_data_generator()</code> and <code>flow_images_from_directory()</code> I am now also using the <code>fit_generator()</code> to run the training.</p>
<pre class="r"><code># fit
hist &lt;- model %&gt;% fit_generator(
  # training data
  train_image_array_gen,
  
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = epochs, 
  
  # validation data
  validation_data = valid_image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size),
  
  # print progress
  verbose = 2,
  callbacks = list(
    # save best model after every epoch
    callback_model_checkpoint(&quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/keras/fruits_checkpoints.h5&quot;, save_best_only = TRUE),
    # only needed for visualising with TensorBoard
    callback_tensorboard(log_dir = &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/keras/logs&quot;)
  )
)</code></pre>
<p>In RStudio we are seeing the output as an interactive plot in the “Viewer” pane but we can also plot it:</p>
<pre class="r"><code>plot(hist)</code></pre>
<p><img src="/post/2018-06-15_keras_fruits_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>As we can see, the model is quite accurate on the validation data. However, we need to keep in mind that our images are very uniform, they all have the same white background and show the fruits centered and without anything else in the images. Thus, our model will not work with images that don’t look similar as the ones we trained on (that’s also why we can achieve such good results with such a small neural net).</p>
<p>Finally, I want to have a look at the TensorFlow graph with TensorBoard.</p>
<pre class="r"><code>tensorboard(&quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/keras/logs&quot;)</code></pre>
<div class="figure">
<img src="/img/tensorboard.png" />

</div>
<p>That’s all there is to it!</p>
<p>Of course, you could now save your model and/or the weights, visualize the hidden layers, run predictions on test data, etc. For now, I’ll leave it at that, though. :-)</p>
<hr />
<p>There now is a <a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits_lime/">second part: Explaining Keras image classification models with lime</a></p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.0 (2018-04-23)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.5
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] keras_2.1.6
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_0.12.17     compiler_3.5.0   pillar_1.2.3     plyr_1.8.4      
##  [5] base64enc_0.1-3  tools_3.5.0      zeallot_0.1.0    digest_0.6.15   
##  [9] jsonlite_1.5     evaluate_0.10.1  tibble_1.4.2     gtable_0.2.0    
## [13] lattice_0.20-35  rlang_0.2.1      Matrix_1.2-14    yaml_2.1.19     
## [17] blogdown_0.6     xfun_0.2         stringr_1.3.1    knitr_1.20      
## [21] rprojroot_1.3-2  grid_3.5.0       reticulate_1.8   R6_2.2.2        
## [25] rmarkdown_1.10   bookdown_0.7     ggplot2_2.2.1    reshape2_1.4.3  
## [29] magrittr_1.5     whisker_0.3-2    backports_1.1.2  scales_0.5.0    
## [33] tfruns_1.3       htmltools_0.3.6  colorspace_1.3-2 labeling_0.3    
## [37] tensorflow_1.8   stringi_1.2.3    lazyeval_0.2.1   munsell_0.5.0</code></pre>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[rOpenSci unconference 2018 &#43; introduction to TensorFlow Probability &amp; the &#39;greta&#39; package]]></title>
    <link href="/2018/05/ropensci_unconf18/"/>
    <id>/2018/05/ropensci_unconf18/</id>
    <published>2018-05-30T00:00:00+00:00</published>
    <updated>2018-05-30T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/viz/viz.js"></script>
<link href="/rmarkdown-libs/DiagrammeR-styles/styles.css" rel="stylesheet" />
<script src="/rmarkdown-libs/grViz-binding/grViz.js"></script>


<p>On May 21st and 22nd, I had the honor of having been chosen to attend the rOpenSci unconference 2018 in Seattle. It was a great event and I got to meet many amazing people!</p>
<div id="ropensci" class="section level2">
<h2>rOpenSci</h2>
<p><a href="https://ropensci.org/">rOpenSci</a> is a non-profit organisation that maintains a number of widely used R <a href="https://ropensci.org/packages/">packages</a> and is very active in promoting a <a href="https://ropensci.org/community/">community</a> spirit around the R-world. Their core values are to have open and reproducible research, shared data and easy-to-use tools and to make all this accessible to a large number of people.</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/rOpenSci/IMG_3637.jpg" />

</div>
</div>
<div id="ropensci-unconference" class="section level2">
<h2>rOpenSci unconference</h2>
<p>Part of creating a welcoming community infrastructure is their yearly unconference. At the unconference, about 60 invited R users from around the world get together to work on small projects that are relevant to the R community at the time. Project ideas are collected and discussed in Github issues during the weeks before the unconference but the final decision which projects will be worked on is made by the participants on the first morning of the unconference.</p>
<p><a href="http://unconf18.ropensci.org/">This year’s rOpenSci unconference</a> was held at the Microsoft Reactor in Seattle.</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/rOpenSci/IMG_3716.jpg" />

</div>
<p>The whole organizing team - most and foremost Stefanie Butland - did a wonderful job hosting this event. Everybody made sure that the spirit of the unconference was inclusive and very welcoming to everybody, from long-established fixtures in the R-world to newbies and anyone in between.</p>
<p>We were a pretty diverse group of social scientists, bioinformaticians, ecologists, historians, data scientists, developers, people working with Google, Microsoft or RStudio and R enthusiasts from many other fields. Some people already knew a few others, many knew each other from Twitter, R-Ladies, or other online communities but most of us (including me) had never met in person.</p>
<p>Therefore, the official part of the unconference was started on Monday morning with a few “ice breakers”: Stefanie would ask a question or make a statement and we would position ourselves in the room according to our answer and discuss with the people close to us. Starting with “Are you a dog or a cat person?” and finishing with “I know my place in the R community”, we all quickly had a lot to talk about! It was a great way to meet many of the people we would spend the next two days with.</p>
<p>It was a great experience working with so many talented and motivated people who share my passion for the R language - particularly because in my line of work as a data scientist R is often considered inferior to Python and the majority of the active R community is situated in the Pacific Northwest and California. It was a whole new experience to work together with other people on an R project and I absolutely loved it!</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/rOpenSci/IMG_3717.jpg" />

</div>
</div>
<div id="working-on-greta" class="section level2">
<h2>Working on <code>greta</code></h2>
<p>During the 2 days of the unconference, people worked on many interesting, useful and cool projects <a href="https://github.com/ropenscilabs/runconf18-projects/blob/master/index.Rmd">(click here for a complete list with links to the Github repos for every project!)</a>!</p>
<p>The group I joined originally wanted to bring <strong>TensorFlow Probability</strong> to R.</p>
<blockquote>
<p>TensorFlow Probability is a library for probabilistic reasoning and statistical analysis in TensorFlow. As part of the TensorFlow ecosystem, TensorFlow Probability provides integration of probabilistic methods with deep networks, gradient-based inference via automatic differentiation, and scalability to large datasets and models via hardware acceleration (e.g., GPUs) and distributed computation. <a href="https://github.com/tensorflow/probability" class="uri">https://github.com/tensorflow/probability</a></p>
</blockquote>
<p>In the end, we - that is <a href="https://github.com/michaelquinn32">Michael Quinn</a>, <a href="https://github.com/revodavid">David Smith</a>, <a href="https://github.com/TiphaineCMartin">Tiphaine Martin</a>, <a href="https://github.com/mmulvahill">Matt Mulvahill</a> and I - ended up working with the R package <code>greta</code>, which has similar functionalities as TensorFlow Probability. We recreated some of the examples from the TensorFlow Probability package tutorials in <code>greta</code> and we also added a few additional examples that show how you can use <code>greta</code>.</p>
<p>Check out <a href="https://github.com/ropenscilabs/greta/tree/unconf/README.md">the README repo for an overview and links to everything we’ve contributed</a>; it is a forked repo from the <a href="https://github.com/greta-dev/greta">original package repo of <code>greta</code></a> and the vignettes will hopefully get included in the main repo at some point in the near future.</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/rOpenSci/IMG_3715.jpg" />

</div>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/rOpenSci/IMG_3724.jpg" />

</div>
<div id="what-is-greta" class="section level3">
<h3>What is <code>greta</code>?</h3>
<p><a href="https://greta-dev.github.io/greta"><code>greta</code></a> is an R package that has been created by <a href="https://github.com/goldingn">Nick Golding</a> for implementing Markov-Chain Monte-Carlo (MCMC) models, e.g. a Hamiltonian Monte Carlo (HMC) method. It offers a number of functions that make it easy to define these models, particularly for Bayesian statistics (similar to Stan).</p>
<blockquote>
<p>greta lets us build statistical models interactively in R, and then sample from them by MCMC. <a href="https://greta-dev.github.io/greta/get_started.html#how_greta_works" class="uri">https://greta-dev.github.io/greta/get_started.html#how_greta_works</a></p>
</blockquote>
<p>Google’s TensorFlow is used as a backend to compute the defined models. Because TensorFlow has been optimized for large-scale computing, multi-core and GPU calculations are supported as well, <code>greta</code> is particularly efficient and useful for working with complex models. As TensorFlow is not natively an R package, <code>greta</code> makes use of RStudio’s <a href="https://rstudio.github.io/reticulate/articles/python_packages.html">reticulate</a> and <a href="https://tensorflow.rstudio.com/">tensorflow</a> packages to connect with the TensorFlow backend. This way, we can work with all the TensorFlow functions directly from within R.</p>
</div>
<div id="how-does-greta-work" class="section level3">
<h3>How does <code>greta</code> work?</h3>
<blockquote>
<p>There are three layers to how greta defines a model: users manipulate greta arrays, these define nodes, and nodes then define Tensors. <a href="https://greta-dev.github.io/greta/technical_details.html" class="uri">https://greta-dev.github.io/greta/technical_details.html</a></p>
</blockquote>
<p>This is the minimum working example of the linear mixed model that we developed in <code>greta</code> based on an example from a TensorFlow Probability Jupyter notebook. The full example with explanations can be found <a href="https://github.com/ropenscilabs/greta/blob/unconf/vignettes/8_schools_example_model.Rmd">here</a>.</p>
<pre class="r"><code>library(greta)</code></pre>
<pre class="text"><code># data
N &lt;- letters[1:8]
treatment_effects &lt;- c(28.39, 7.94, -2.75 , 6.82, -0.64, 0.63, 18.01, 12.16)
treatment_stddevs &lt;- c(14.9, 10.2, 16.3, 11.0, 9.4, 11.4, 10.4, 17.6)</code></pre>
<pre class="r"><code># variables and priors
avg_effect &lt;- normal(mean = 0, sd = 10)
avg_stddev &lt;- normal(5, 1)
school_effects_standard &lt;- normal(0, 1, dim = length(N))
school_effects &lt;- avg_effect + exp(avg_stddev) * school_effects_standard

# likelihood
distribution(treatment_effects) &lt;- normal(school_effects, treatment_stddevs)

# defining the hierarchical model
m &lt;- model(avg_effect, avg_stddev, school_effects_standard)
m</code></pre>
<pre><code>## greta model</code></pre>
<pre class="r"><code>plot(m)</code></pre>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"diagram":"digraph {\n\ngraph [layout = \"dot\",\n       outputorder = \"edgesfirst\",\n       bgcolor = \"white\",\n       rankdir = \"LR\"]\n\nnode [fontname = \"Helvetica\",\n      fontsize = \"10\",\n      shape = \"circle\",\n      fixedsize = \"true\",\n      width = \"0.5\",\n      style = \"filled\",\n      fillcolor = \"aliceblue\",\n      color = \"gray70\",\n      fontcolor = \"gray50\"]\n\nedge [fontname = \"Helvetica\",\n     fontsize = \"8\",\n     len = \"1.5\",\n     color = \"gray80\",\n     arrowsize = \"0.5\"]\n\n  \"1\" [label = \"avg_effect\n\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"circle\", color = \"#E0D2EE\", width = \"0.6\", height = \"0.48\", fillcolor = \"#F4F0F9\"] \n  \"2\" [label = \"normal\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"diamond\", color = \"#B797D7\", width = \"1\", height = \"0.8\", fillcolor = \"#E0D2EE\"] \n  \"3\" [label = \"0\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"square\", color = \"#E0D2EE\", width = \"0.5\", height = \"0.4\", fillcolor = \"#FFFFFF\"] \n  \"4\" [label = \"10\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"square\", color = \"#E0D2EE\", width = \"0.5\", height = \"0.4\", fillcolor = \"#FFFFFF\"] \n  \"5\" [label = \"school_effects\n\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"circle\", color = \"lightgray\", width = \"0.2\", height = \"0.16\", fillcolor = \"#D3D3D3\"] \n  \"6\" [label = \"normal\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"diamond\", color = \"#B797D7\", width = \"1\", height = \"0.8\", fillcolor = \"#E0D2EE\"] \n  \"7\" [label = \"\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"square\", color = \"#E0D2EE\", width = \"0.5\", height = \"0.4\", fillcolor = \"#FFFFFF\"] \n  \"8\" [label = \"\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"square\", color = \"#E0D2EE\", width = \"0.5\", height = \"0.4\", fillcolor = \"#FFFFFF\"] \n  \"9\" [label = \"\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"circle\", color = \"lightgray\", width = \"0.2\", height = \"0.16\", fillcolor = \"#D3D3D3\"] \n  \"10\" [label = \"\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"circle\", color = \"lightgray\", width = \"0.2\", height = \"0.16\", fillcolor = \"#D3D3D3\"] \n  \"11\" [label = \"avg_stddev\n\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"circle\", color = \"#E0D2EE\", width = \"0.6\", height = \"0.48\", fillcolor = \"#F4F0F9\"] \n  \"12\" [label = \"normal\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"diamond\", color = \"#B797D7\", width = \"1\", height = \"0.8\", fillcolor = \"#E0D2EE\"] \n  \"13\" [label = \"5\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"square\", color = \"#E0D2EE\", width = \"0.5\", height = \"0.4\", fillcolor = \"#FFFFFF\"] \n  \"14\" [label = \"1\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"square\", color = \"#E0D2EE\", width = \"0.5\", height = \"0.4\", fillcolor = \"#FFFFFF\"] \n  \"15\" [label = \"school_effects_standard\n\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"circle\", color = \"#E0D2EE\", width = \"0.6\", height = \"0.48\", fillcolor = \"#F4F0F9\"] \n  \"16\" [label = \"normal\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"diamond\", color = \"#B797D7\", width = \"1\", height = \"0.8\", fillcolor = \"#E0D2EE\"] \n  \"17\" [label = \"0\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"square\", color = \"#E0D2EE\", width = \"0.5\", height = \"0.4\", fillcolor = \"#FFFFFF\"] \n  \"18\" [label = \"1\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"square\", color = \"#E0D2EE\", width = \"0.5\", height = \"0.4\", fillcolor = \"#FFFFFF\"] \n\"1\"->\"5\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"add\", style = \"solid\"] \n\"2\"->\"1\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", penwidth = \"3\", style = \"dashed\"] \n\"3\"->\"2\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"mean\", style = \"solid\"] \n\"4\"->\"2\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"sd\", style = \"solid\"] \n\"5\"->\"6\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"mean\", style = \"solid\"] \n\"6\"->\"8\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", penwidth = \"3\", style = \"dashed\"] \n\"7\"->\"6\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"sd\", style = \"solid\"] \n\"9\"->\"5\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"add\", style = \"solid\"] \n\"10\"->\"9\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"multiply\", style = \"solid\"] \n\"11\"->\"10\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"exp\", style = \"solid\"] \n\"12\"->\"11\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", penwidth = \"3\", style = \"dashed\"] \n\"13\"->\"12\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"mean\", style = \"solid\"] \n\"14\"->\"12\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"sd\", style = \"solid\"] \n\"15\"->\"9\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"multiply\", style = \"solid\"] \n\"16\"->\"15\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", penwidth = \"3\", style = \"dashed\"] \n\"17\"->\"16\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"mean\", style = \"solid\"] \n\"18\"->\"16\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"sd\", style = \"solid\"] \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<pre class="r"><code># sampling
draws &lt;- greta::mcmc(m)
plot(draws)</code></pre>
<p>The main type of object you’ll be using in <code>greta</code> is the <strong>greta array</strong>. You can <a href="https://greta-dev.github.io/greta/structures.html">create <code>greta</code> arrays</a> or <a href="https://greta-dev.github.io/greta/as_data.html">convert R objects, like data frames into <code>greta</code> arrays</a>. <code>greta</code> arrays are basically a list with one element: an R6 class object with <code>node</code> + <code>data</code>, <code>operation</code> or <code>variable</code> property. This way, <code>greta</code> makes use of the graph-based organisation of modeling. Every node in our model graph is from a <code>greta</code> array node and thus connects variables, data and operations to create a directed acyclic graph (DAG) that defines the model when the <code>model()</code> function is called.</p>
</div>
<div id="tensorflow-probability" class="section level3">
<h3>TensorFlow Probability</h3>
<p>While <code>greta</code> makes it super easy to build similar models as with TensorFlow Probability, I also tried migrating the example code directly into R using the <code>reticulate</code> package. It’s still a work in progress but for everyone who might want to try as well (and achieve what I couldn’t up until now), here is how I started out.</p>
<p>TensorFlow Probability isn’t part of the core TensorFlow package, so we won’t have it loaded with <code>library(tensorflow)</code>. But we can use the <code>reticulate</code> package instead to import any Python module (aka library) into R and use it there. This way, we could use the original functions from the <code>tensorflow_probability</code> Python package in R.</p>
<p>We could, for example, work with the Edward2 functionalities from TensorFlow probabilities.</p>
<blockquote>
<p>Edward is a Python library for probabilistic modeling, inference, and criticism. It is a testbed for fast experimentation and research with probabilistic models, ranging from classical hierarchical models on small data sets to complex deep probabilistic models on large data sets. Edward fuses three fields: Bayesian statistics and machine learning, deep learning, and probabilistic programming. […] Edward is built on TensorFlow. It enables features such as computational graphs, distributed training, CPU/GPU integration, automatic differentiation, and visualization with TensorBoard. <a href="http://edwardlib.org/" class="uri">http://edwardlib.org/</a></p>
</blockquote>
<pre class="r"><code>library(reticulate)
tf &lt;- import(&quot;tensorflow&quot;)
tfp &lt;- import(&quot;tensorflow_probability&quot;)
ed &lt;- tfp$edward2</code></pre>
</div>
<div id="note-on-installing-a-working-version-of-tensorflow-probability-for-r" class="section level3">
<h3>Note on installing a working version of TensorFlow Probability for R</h3>
<p>As TensorFlow Probability isn’t part of the core TensorFlow package, we need to install the nightly bleeding edge version. However, we had a few problems installing a working version of TensorFlow Probability that had all the necessary submodules we wanted to use (like <code>edward2</code>). So, this is the version that worked in the end (as of today):</p>
<ul>
<li>TensorFlow Probability version 0.0.1.dev20180515</li>
<li>TensorFlow version 1.9.0.dev20180515</li>
</ul>
<p>For full disclosure: I worked from within the R virtualenv <strong>r-tensorflow</strong> that was created when I ran <code>install_tensorflow()</code> from within R. In this environment I installed:</p>
<pre><code>pip install tfp-nightly==0.0.1.dev20180515
pip install tf-nightly==1.9.0.dev20180515</code></pre>
<p>I used Python 3.6 on a Mac OS High Sierra version 10.13.4.</p>
</div>
</div>
<div id="thanks" class="section level2">
<h2>Thanks</h2>
<p>Huge thanks go out to my amazing <code>greta</code> team and to rOpenSci - particularly Stefanie Butland - for organizing such a wonderful event!</p>
<p>Thank you also to all <a href="http://unconf18.ropensci.org/#sponsors">sponsors</a>, who made it possible for me to fly all the way over to the Pacific Northwest and attend the unconf!</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="en" dir="ltr">
A sincere thank you to all participants in <a href="https://twitter.com/hashtag/runconf18?src=hash&amp;ref_src=twsrc%5Etfw">#runconf18</a> <br><br>This thread👇includes links to all project repos: <a href="https://t.co/2PhAz4zSuK">https://t.co/2PhAz4zSuK</a><a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> <a href="https://t.co/8SICcWkQ0v">pic.twitter.com/8SICcWkQ0v</a>
</p>
— rOpenSci (<span class="citation">@rOpenSci</span>) <a href="https://twitter.com/rOpenSci/status/1000024996876468224?ref_src=twsrc%5Etfw">May 25, 2018</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
<div id="session-information" class="section level2">
<h2>Session Information</h2>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.0 (2018-04-23)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.4
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] bindrcpp_0.2.2 greta_0.2.4   
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_0.12.17       lattice_0.20-35    tidyr_0.8.1       
##  [4] visNetwork_2.0.3   prettyunits_1.0.2  assertthat_0.2.0  
##  [7] rprojroot_1.3-2    digest_0.6.15      R6_2.2.2          
## [10] plyr_1.8.4         backports_1.1.2    evaluate_0.10.1   
## [13] coda_0.19-1        ggplot2_2.2.1      blogdown_0.6      
## [16] pillar_1.2.3       tfruns_1.3         rlang_0.2.1       
## [19] progress_1.1.2     lazyeval_0.2.1     rstudioapi_0.7    
## [22] whisker_0.3-2      Matrix_1.2-14      reticulate_1.7    
## [25] rmarkdown_1.9      DiagrammeR_1.0.0   downloader_0.4    
## [28] readr_1.1.1        stringr_1.3.1      htmlwidgets_1.2   
## [31] igraph_1.2.1       munsell_0.4.3      compiler_3.5.0    
## [34] influenceR_0.1.0   rgexf_0.15.3       xfun_0.1          
## [37] pkgconfig_2.0.1    base64enc_0.1-3    tensorflow_1.5    
## [40] htmltools_0.3.6    tidyselect_0.2.4   tibble_1.4.2      
## [43] gridExtra_2.3      bookdown_0.7       XML_3.98-1.11     
## [46] viridisLite_0.3.0  dplyr_0.7.5        grid_3.5.0        
## [49] jsonlite_1.5       gtable_0.2.0       magrittr_1.5      
## [52] scales_0.5.0       stringi_1.2.2      viridis_0.5.1     
## [55] brew_1.0-6         RColorBrewer_1.1-2 tools_3.5.0       
## [58] glue_1.2.0         purrr_0.2.5        hms_0.4.2         
## [61] Rook_1.1-1         parallel_3.5.0     yaml_2.1.19       
## [64] colorspace_1.3-2   knitr_1.20         bindr_0.1.1</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[July 5th &amp; 6th in Münster: Workshop on Deep Learning with Keras and TensorFlow in R]]></title>
    <link href="/2018/05/deep_learning_keras_tensorflow_18_07/"/>
    <id>/2018/05/deep_learning_keras_tensorflow_18_07/</id>
    <published>2018-05-22T00:00:00+00:00</published>
    <updated>2018-05-22T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Registration is now open for my 1.5-day <a href="https://www.eventbrite.de/e/workshop-deep-learning-mit-keras-und-tensorflow-tickets-42710095044?utm-medium=discovery&amp;utm-campaign=social&amp;utm-content=attendeeshare&amp;aff=escb&amp;utm-source=cp&amp;utm-term=listing">workshop on deep learning with Keras and TensorFlow using R</a>.</p>
<p>It will take place on <strong>July 5th &amp; 6th</strong> in <strong>Münster, Germany</strong>.</p>
<p><br></p>
<div class="figure">
<img src="https://blog.keras.io/img/keras-tensorflow-logo.jpg" />

</div>
<p><br></p>
<p><a href="https://blog.codecentric.de/en/2018/02/deep-learning-workshop-bei-der-codecentric-ag-solingen/">You can read about one participant’s experience in my last workshop:</a></p>
<blockquote>
<p>Big Data – a buzz word you can find everywhere these days, from nerdy blogs to scientific research papers and even in the news. But how does Big Data Analysis work, exactly? In order to find that out, I attended the workshop on “Deep Learning with Keras and TensorFlow”. On a stormy Thursday afternoon, we arrived at the modern and light-flooded codecentric AG headquarters. There, we met performance expert Dieter Dirkes and Data Scientist Dr. Shirin Glander. In the following two days, Shirin gave us a hands-on introduction into the secrets of Deep Learning and helped us to program our first Neural Net. After a short round of introduction of the participants, it became clear that many different areas and domains are interested in Deep Learning: geologists want to classify (satellite) images, energy providers want to analyse time-series, insurers want to predict numbers and I – a humanities major – want to classify text. And codecentric employees were also interested in getting to know the possibilities of Deep Learning, so that a third of the participants were employees from the company itself.</p>
</blockquote>
<p><a href="https://blog.codecentric.de/en/2018/02/deep-learning-workshop-bei-der-codecentric-ag-solingen/">Continue reading…</a></p>
<p>In my workshop, you will learn</p>
<ul>
<li>the basics of deep learning</li>
<li>what cross-entropy and loss is</li>
<li>about activation functions</li>
<li>how to optimize weights and biases with backpropagation and gradient descent</li>
<li>how to build (deep) neural networks with Keras and TensorFlow</li>
<li>how to save and load models and model weights</li>
<li>how to visualize models with TensorBoard</li>
<li>how to make predictions on test data</li>
</ul>
<p>The workshop will be held in German but my slides and all material is in English. :-)</p>
<p><a href="https://www.eventbrite.de/e/workshop-deep-learning-mit-keras-und-tensorflow-tickets-46223151691">Tickets can be booked via eventbrite</a>.</p>
<p><br></p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/keras_workshop_april18.png" />

</div>
<p>Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. Keras is very convenient for fast and easy prototyping of neural networks. It is highly modular and very flexible, so that you can build basically any type of neural network you want. It supports convolutional neural networks and recurrent neural networks, as well as combinations of both. Due to its layer structure, it is highly extensible and can run on CPU or GPU.</p>
<p>The <code>keras</code> R package provides an interface to the Python library of Keras, just as the tensorflow package provides an interface to TensorFlow. Basically, R creates a conda instance and runs Keras it it, while you can still use all the functionalities of R for plotting, etc. Almost all function names are the same, so models can easily be recreated in Python for deployment.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI: Adversarial Attacks Against Reinforcement Learning Agents with Ian Goodfellow &amp; Sandy Huang]]></title>
    <link href="/2018/05/twimlai_adversarial_attacks/"/>
    <id>/2018/05/twimlai_adversarial_attacks/</id>
    <published>2018-05-14T00:00:00+00:00</published>
    <updated>2018-05-14T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Adversarial Attacks Against Reinforcement Learning Agents with Ian Goodfellow &amp; Sandy Huang</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai_adversarial_attacks.jpg" alt="Sketchnotes from TWiMLAI talk: Adversarial Attacks Against Reinforcement Learning Agents with Ian Goodfellow &amp; Sandy Huang" />
<p class="caption">Sketchnotes from TWiMLAI talk: Adversarial Attacks Against Reinforcement Learning Agents with Ian Goodfellow &amp; Sandy Huang</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-119-adversarial-attacks-reinforcement-learning-agents-ian-goodfellow-sandy-huang/">here</a>.</p>
<blockquote>
<p>In this episode, I’m joined by Ian Goodfellow, Staff Research Scientist at Google Brain and Sandy Huang, Phd Student in the EECS department at UC Berkeley, to discuss their work on the paper Adversarial Attacks on Neural Network Policies. If you’re a regular listener here you’ve probably heard of adversarial attacks, and have seen examples of deep learning based object detectors that can be fooled into thinking that, for example, a giraffe is actually a school bus, by injecting some imperceptible noise into the image. Well, Sandy and Ian’s paper sits at the intersection of adversarial attacks and reinforcement learning, another area we’ve discussed quite a bit on the podcast. In their paper, they describe how adversarial attacks can also be effective at targeting neural network policies in reinforcement learning. Sandy gives us an overview of the paper, including how changing a single pixel value can throw off performance of a model trained to play Atari games. We also cover a lot of interesting topics relating to adversarial attacks and RL individually, and some related areas such as hierarchical reward functions and transfer learning. This was a great conversation that I’m really excited to bring to you! <a href="https://twimlai.com/twiml-talk-119-adversarial-attacks-reinforcement-learning-agents-ian-goodfellow-sandy-huang/" class="uri">https://twimlai.com/twiml-talk-119-adversarial-attacks-reinforcement-learning-agents-ian-goodfellow-sandy-huang/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Comparing dependencies of popular machine learning packages with `pkgnet`]]></title>
    <link href="/2018/04/pkgnet/"/>
    <id>/2018/04/pkgnet/</id>
    <published>2018-04-30T00:00:00+00:00</published>
    <updated>2018-04-30T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>When looking through the CRAN list of packages, I stumbled upon <a href="https://cran.r-project.org/web/packages/pkgnet/vignettes/pkgnet-intro.html">this little gem</a>:</p>
<blockquote>
<p>pkgnet is an R library designed for the analysis of R libraries! The goal of the package is to build a graph representation of a package and its dependencies.</p>
</blockquote>
<p>And I thought it would be fun to play around with it. The little analysis I ended up doing was to compare dependencies of popular machine learning packages.</p>
<div id="update-an-alternative-package-to-use-would-be-cranly." class="section level2">
<h2><strong>Update:</strong> An alternative package to use would be <a href="https://cran.r-project.org/web/packages/cranly/vignettes/cranly.html"><code>cranly</code></a>.</h2>
<ul>
<li>I first loaded the packages:</li>
</ul>
<pre class="r"><code>library(pkgnet)
library(tidygraph)</code></pre>
<pre><code>## 
## Attache Paket: &#39;tidygraph&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     filter</code></pre>
<pre class="r"><code>library(ggraph)</code></pre>
<pre><code>## Lade nötiges Paket: ggplot2</code></pre>
<ul>
<li>I then created a function that will</li>
</ul>
<ol style="list-style-type: decimal">
<li>create the package report with <code>pkgnet::CreatePackageReport</code></li>
<li>convert the edge (<code>report$DependencyReporter$edges</code>) and node (<code>report$DependencyReporter$nodes</code>) data into a graph object with <code>tidygraph::as_tbl_graph</code></li>
</ol>
<pre class="r"><code>create_pkg_graph &lt;- function(package_name, DependencyReporter = TRUE) {
  
  report &lt;- CreatePackageReport(pkg_name = package_name)
  
  if (DependencyReporter) {
    graph &lt;- as_tbl_graph(report$DependencyReporter$edges,
                      directed = TRUE,
                      nodes = as.data.frame(report$DependencyReporter$nodes))
  } else {
    graph &lt;- as_tbl_graph(report$FunctionReporter$edges,
                      directed = TRUE,
                      nodes = as.data.frame(report$FunctionReporter$nodes))
  }
  
  return(graph)
}</code></pre>
<ul>
<li><p>To create a vector of machine learning packages from R I looked at <a href="https://cran.r-project.org/web/views/MachineLearning.html">CRAN’s machine learning task view</a></p></li>
<li><p>These are the packages I ended up including:</p></li>
</ul>
<pre class="r"><code>pkg_list &lt;- c(&quot;caret&quot;, &quot;h2o&quot;, &quot;e1071&quot;, &quot;mlr&quot;)</code></pre>
<p><em>Note</em>: I wanted to include other packages, like <code>tensorflow</code>, <code>randomFores</code>, <code>gbm</code>, etc. but for those, <code>pkgnet</code> threw an error:</p>
<blockquote>
<p>Error in data.table::data.table(node = names(igraph::V(self$pkg_graph)), : column or argument 1 is NULL</p>
</blockquote>
<ul>
<li>Next, I ran them through my function from before and assigned them each a unique name.</li>
</ul>
<pre class="r"><code>for (pkg in pkg_list) {
  graph &lt;- create_pkg_graph(pkg)
  assign(paste0(&quot;graph_&quot;, pkg), graph)
}</code></pre>
<ul>
<li>These individual objects I combined with <a href="https://cran.r-project.org/web/packages/tidygraph/index.html"><code>tidygraph</code></a> and calculated node centrality as the number of outgoing edges.</li>
</ul>
<pre class="r"><code>graph &lt;- graph_caret %&gt;% 
  graph_join(graph_h2o, by = &quot;name&quot;) %&gt;%
  graph_join(graph_e1071, by = &quot;name&quot;) %&gt;%
  graph_join(graph_mlr, by = &quot;name&quot;) %&gt;%
  mutate(color = ifelse(name %in% pkg_list, &quot;a&quot;, &quot;b&quot;),
         centrality = centrality_degree(mode = &quot;out&quot;))</code></pre>
<ul>
<li>Finally, I plotted the dependency network with <a href="https://github.com/thomasp85/ggraph"><code>ggraph</code></a>:</li>
</ul>
<p>The bigger the node labels (package names), the higher their centrality. Seems like the more basic utilitarian packages have the highest centrality (not really a surprise…).</p>
<pre class="r"><code>graph %&gt;%
  ggraph(layout = &#39;nicely&#39;) + 
    geom_edge_link(arrow = arrow()) + 
    geom_node_point() +
    geom_node_label(aes(label = name, fill = color, size = centrality), show.legend = FALSE, repel = TRUE) +
    theme_graph() +
    scale_fill_brewer(palette = &quot;Set1&quot;)</code></pre>
<p><img src="/post/2018-04-30_pkgnet_files/figure-html/graph-1.png" width="960" /></p>
<ul>
<li>Because the complete network is a bit hard to make sense of, I plotted it again with only the packages I wanted to analyze plus dependencies that had at least 1 outgoing edge; now it is easier to see shared dependencies.</li>
</ul>
<p>For example, <code>methods</code> and <code>stats</code> are dependencies of <code>caret</code>, <code>mlr</code> and <code>e1071</code> but not <code>h2o</code>, while <code>utils</code> is a dependency of all four.</p>
<pre class="r"><code>graph %&gt;%
  filter(centrality &gt; 1 | color == &quot;a&quot;) %&gt;%
  ggraph(layout = &#39;nicely&#39;) + 
    geom_edge_link(arrow = arrow()) + 
    geom_node_point() +
    geom_node_label(aes(label = name, fill = color, size = centrality), show.legend = FALSE, repel = TRUE) +
    theme_graph() +
    scale_fill_brewer(palette = &quot;Set1&quot;)</code></pre>
<p><img src="/post/2018-04-30_pkgnet_files/figure-html/graph_subset-1.png" width="576" /></p>
<p>It would of course be interesting to analyse a bigger network with more packages. Maybe someone knows how to get these other packages to work with <code>pkgnet</code>?</p>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.0 (2018-04-23)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.4
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] bindrcpp_0.2.2  ggraph_1.0.1    ggplot2_2.2.1   tidygraph_1.1.0
## [5] pkgnet_0.2.0   
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_0.12.16         RColorBrewer_1.1-2   plyr_1.8.4          
##  [4] compiler_3.5.0       pillar_1.2.2         formatR_1.5         
##  [7] futile.logger_1.4.3  bindr_0.1.1          viridis_0.5.1       
## [10] futile.options_1.0.1 tools_3.5.0          digest_0.6.15       
## [13] viridisLite_0.3.0    gtable_0.2.0         jsonlite_1.5        
## [16] evaluate_0.10.1      tibble_1.4.2         pkgconfig_2.0.1     
## [19] rlang_0.2.0          igraph_1.2.1         ggrepel_0.7.0       
## [22] yaml_2.1.18          blogdown_0.6         xfun_0.1            
## [25] gridExtra_2.3        stringr_1.3.0        dplyr_0.7.4         
## [28] knitr_1.20           htmlwidgets_1.2      grid_3.5.0          
## [31] rprojroot_1.3-2      glue_1.2.0           data.table_1.10.4-3 
## [34] R6_2.2.2             rmarkdown_1.9        bookdown_0.7        
## [37] udunits2_0.13        tweenr_0.1.5         tidyr_0.8.0         
## [40] purrr_0.2.4          lambda.r_1.2.2       magrittr_1.5        
## [43] units_0.5-1          MASS_7.3-49          scales_0.5.0        
## [46] backports_1.1.2      mvbutils_2.7.4.1     htmltools_0.3.6     
## [49] assertthat_0.2.0     ggforce_0.1.1        colorspace_1.3-2    
## [52] labeling_0.3         stringi_1.1.7        visNetwork_2.0.3    
## [55] lazyeval_0.2.1       munsell_0.4.3</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Slides from my JAX 2018 talk: Deep Learning - a Primer]]></title>
    <link href="/2018/04/jax2018_slides/"/>
    <id>/2018/04/jax2018_slides/</id>
    <published>2018-04-27T00:00:00+00:00</published>
    <updated>2018-04-27T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Here I am sharing the slides for a talk that my colleague Uwe Friedrichsen and I gave about <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/">Deep Learning - a Primer</a> at the JAX conference on Tuesday, April 24th 2018 in Mainz, Germany.</p>
<p>Slides can be found here: <a href="https://www.slideshare.net/ShirinGlander/deep-learning-a-primer-95197733" class="uri">https://www.slideshare.net/ShirinGlander/deep-learning-a-primer-95197733</a></p>
<blockquote>
<p>Deep Learning is one of the “hot” topics in the AI area – a lot of hype, a lot of inflated expectation, but also quite some impressive success stories. As some AI experts already predict that Deep Learning will become “Software 2.0”, it might be a good time to have a closer look at the topic. In this session I will try to give a comprehensive overview of Deep Learning. We will start with a bit of history and some theoretical foundations that we will use to create a little Deep Learning taxonomy. Then we will have a look at current and upcoming application areas: Where can we apply Deep Learning successfully and what does it differentiate from other approaches? Afterwards we will examine the ecosystem: Which tools and libraries are available? What are their strengths and weaknesses? And to complete the session, we will look into some practical code examples and the typical pitfalls of Deep Learning. After this session you will have a much better idea of the why, what and how of Deep Learning, including if and how you might want to apply it to your own work. <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/" class="uri">https://jax.de/big-data-machine-learning/deep-learning-a-primer/</a></p>
</blockquote>
<div class="figure">
<img src="https://pbs.twimg.com/media/DUt3SXyUQAE3TOv.jpg" alt="https://twitter.com/jaxcon/status/957990506331557890" />
<p class="caption"><a href="https://twitter.com/jaxcon/status/957990506331557890" class="uri">https://twitter.com/jaxcon/status/957990506331557890</a></p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #121: Reproducibility and the Philosophy of Data with Clare Gollnick]]></title>
    <link href="/2018/04/twimlai121/"/>
    <id>/2018/04/twimlai121/</id>
    <published>2018-04-22T00:00:00+00:00</published>
    <updated>2018-04-22T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Reproducibility and the Philosophy of Data with Clare Gollnick</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai121.jpg" alt="Sketchnotes from TWiMLAI talk #121: Reproducibility and the Philosophy of Data with Clare Gollnick" />
<p class="caption">Sketchnotes from TWiMLAI talk #121: Reproducibility and the Philosophy of Data with Clare Gollnick</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-121-reproducibility-philosophy-data-clare-gollnick/">here</a>.</p>
<blockquote>
<p>In this episode, i’m joined by Clare Gollnick, CTO of Terbium Labs, to discuss her thoughts on the “reproducibility crisis” currently haunting the scientific landscape. For a little background, a “Nature” survey in 2016 showed that more than 70% of researchers have tried and failed to reproduce another scientist’s experiments, and more than half have failed to reproduce their own experiments. Clare gives us her take on the situation, and how it applies to data science, along with some great nuggets about the philosophy of data and a few interesting use cases as well. We also cover her thoughts on Bayesian vs Frequentist techniques and while we’re at it, the Vim vs Emacs debate. No, actually I’m just kidding on that last one. But this was indeed a very fun conversation that I think you’ll enjoy! <a href="https://twimlai.com/twiml-talk-121-reproducibility-philosophy-data-clare-gollnick/" class="uri">https://twimlai.com/twiml-talk-121-reproducibility-philosophy-data-clare-gollnick/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Update: Can we predict flu outcome with Machine Learning in R?]]></title>
    <link href="/2018/04/flu_prediction/"/>
    <id>/2018/04/flu_prediction/</id>
    <published>2018-04-22T00:00:00+00:00</published>
    <updated>2018-04-22T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Since I migrated my blog from <a href="shiring.github.io">Github Pages</a> to <a href="www.shirin-glander.de">blogdown and Netlify</a>, I wanted to start migrating (most of) my old posts too - and use that opportunity to update them and make sure the code still works.</p>
<p>Here I am updating my very first machine learning post from 27 Nov 2016: <a href="https://shiring.github.io/machine_learning/2016/11/27/flu_outcome_ML_post">Can we predict flu deaths with Machine Learning and R?</a>. Changes are marked as <strong>bold</strong> comments.</p>
<p>The main changes I made are:</p>
<ul>
<li>using the <code>tidyverse</code> more consistently throughout the analysis</li>
<li>focusing on comparing multiple imputations from the <code>mice</code> package, rather than comparing different algorithms</li>
<li>using <code>purrr</code>, <code>map()</code>, <code>nest()</code> and <code>unnest()</code> to model and predict the machine learning algorithm over the different imputed datasets</li>
</ul>
<hr />
<p>Among the many nice R packages containing data collections is the <a href="https://mran.microsoft.com/web/packages/outbreaks/outbreaks.pdf">outbreaks</a> package. It contains a dataset on epidemics and among them is data from the 2013 outbreak of <a href="http://www.who.int/influenza/human_animal_interface/faq_H7N9/en/">influenza A H7N9</a> in <a href="http://www.who.int/influenza/human_animal_interface/influenza_h7n9/ChinaH7N9JointMissionReport2013u.pdf?ua=1">China</a> as analysed by Kucharski et al. (2014):</p>
<blockquote>
<p>A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. PLOS Currents Outbreaks. Mar 7, edition 1. doi: 10.1371/currents.outbreaks.e1473d9bfc99d080ca242139a06c455f.</p>
</blockquote>
<blockquote>
<p>A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Data from: Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. Dryad Digital Repository. <a href="http://dx.doi.org/10.5061/dryad.2g43n" class="uri">http://dx.doi.org/10.5061/dryad.2g43n</a>.</p>
</blockquote>
<p>I will be using their data as an example to show how to use Machine Learning algorithms for predicting disease outcome.</p>
<p><br></p>
<pre class="r"><code>library(outbreaks)
library(tidyverse)
library(plyr)
library(mice)
library(caret)
library(purrr)</code></pre>
<div id="the-data" class="section level1">
<h1>The data</h1>
<p>The dataset contains case ID, date of onset, date of hospitalization, date of outcome, gender, age, province and of course outcome: Death or Recovery.</p>
<div id="pre-processing" class="section level2">
<h2>Pre-processing</h2>
<p><strong>Change: variable names (i.e. column names) have been renamed, dots have been replaced with underscores, letters are all lower case now.</strong></p>
<p><strong>Change: I am using the tidyverse notation more consistently.</strong></p>
<p>First, I’m doing some preprocessing, including:</p>
<ul>
<li>renaming missing data as NA</li>
<li>adding an ID column</li>
<li>setting column types</li>
<li>gathering date columns</li>
<li>changing factor names of dates (to make them look nicer in plots) and of <code>province</code> (to combine provinces with few cases)</li>
</ul>
<pre class="r"><code>fluH7N9_china_2013$age[which(fluH7N9_china_2013$age == &quot;?&quot;)] &lt;- NA
fluH7N9_china_2013_gather &lt;- fluH7N9_china_2013 %&gt;%
  mutate(case_id = paste(&quot;case&quot;, case_id, sep = &quot;_&quot;),
         age = as.numeric(age)) %&gt;%
  gather(Group, Date, date_of_onset:date_of_outcome) %&gt;%
  mutate(Group = as.factor(mapvalues(Group, from = c(&quot;date_of_onset&quot;, &quot;date_of_hospitalisation&quot;, &quot;date_of_outcome&quot;), 
          to = c(&quot;date of onset&quot;, &quot;date of hospitalisation&quot;, &quot;date of outcome&quot;))),
         province = mapvalues(province, from = c(&quot;Anhui&quot;, &quot;Beijing&quot;, &quot;Fujian&quot;, &quot;Guangdong&quot;, &quot;Hebei&quot;, &quot;Henan&quot;, &quot;Hunan&quot;, &quot;Jiangxi&quot;, &quot;Shandong&quot;, &quot;Taiwan&quot;), to = rep(&quot;Other&quot;, 10)))</code></pre>
<p>I’m also</p>
<ul>
<li>adding a third gender level for unknown gender</li>
</ul>
<pre class="r"><code>levels(fluH7N9_china_2013_gather$gender) &lt;- c(levels(fluH7N9_china_2013_gather$gender), &quot;unknown&quot;)
fluH7N9_china_2013_gather$gender[is.na(fluH7N9_china_2013_gather$gender)] &lt;- &quot;unknown&quot;
head(fluH7N9_china_2013_gather)</code></pre>
<pre><code>##   case_id outcome gender age province         Group       Date
## 1  case_1   Death      m  58 Shanghai date of onset 2013-02-19
## 2  case_2   Death      m   7 Shanghai date of onset 2013-02-27
## 3  case_3   Death      f  11    Other date of onset 2013-03-09
## 4  case_4    &lt;NA&gt;      f  18  Jiangsu date of onset 2013-03-19
## 5  case_5 Recover      f  20  Jiangsu date of onset 2013-03-19
## 6  case_6   Death      f   9  Jiangsu date of onset 2013-03-21</code></pre>
<p>For plotting, I am defining a custom <code>ggplot2</code> theme:</p>
<pre class="r"><code>my_theme &lt;- function(base_size = 12, base_family = &quot;sans&quot;){
  theme_minimal(base_size = base_size, base_family = base_family) +
  theme(
    axis.text = element_text(size = 12),
    axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5),
    axis.title = element_text(size = 14),
    panel.grid.major = element_line(color = &quot;grey&quot;),
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = &quot;aliceblue&quot;),
    strip.background = element_rect(fill = &quot;lightgrey&quot;, color = &quot;grey&quot;, size = 1),
    strip.text = element_text(face = &quot;bold&quot;, size = 12, color = &quot;black&quot;),
    legend.position = &quot;bottom&quot;,
    legend.justification = &quot;top&quot;, 
    legend.box = &quot;horizontal&quot;,
    legend.box.background = element_rect(colour = &quot;grey50&quot;),
    legend.background = element_blank(),
    panel.border = element_rect(color = &quot;grey&quot;, fill = NA, size = 0.5)
  )
}</code></pre>
<p>And use that theme to visualize the data:</p>
<pre class="r"><code>ggplot(data = fluH7N9_china_2013_gather, aes(x = Date, y = age, fill = outcome)) +
  stat_density2d(aes(alpha = ..level..), geom = &quot;polygon&quot;) +
  geom_jitter(aes(color = outcome, shape = gender), size = 1.5) +
  geom_rug(aes(color = outcome)) +
  scale_y_continuous(limits = c(0, 90)) +
  labs(
    fill = &quot;Outcome&quot;,
    color = &quot;Outcome&quot;,
    alpha = &quot;Level&quot;,
    shape = &quot;Gender&quot;,
    x = &quot;Date in 2013&quot;,
    y = &quot;Age&quot;,
    title = &quot;2013 Influenza A H7N9 cases in China&quot;,
    subtitle = &quot;Dataset from &#39;outbreaks&#39; package (Kucharski et al. 2014)&quot;,
    caption = &quot;&quot;
  ) +
  facet_grid(Group ~ province) +
  my_theme() +
  scale_shape_manual(values = c(15, 16, 17)) +
  scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) +
  scale_fill_brewer(palette=&quot;Set1&quot;)</code></pre>
<p><img src="/post/2018-04-22_flu_outcome_ML_post_files/figure-html/main-1.png" width="1152" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(data = fluH7N9_china_2013_gather, aes(x = Date, y = age, color = outcome)) +
  geom_point(aes(color = outcome, shape = gender), size = 1.5, alpha = 0.6) +
  geom_path(aes(group = case_id)) +
  facet_wrap( ~ province, ncol = 2) +
  my_theme() +
  scale_shape_manual(values = c(15, 16, 17)) +
  scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) +
  scale_fill_brewer(palette=&quot;Set1&quot;) +
  labs(
    color = &quot;Outcome&quot;,
    shape = &quot;Gender&quot;,
    x = &quot;Date in 2013&quot;,
    y = &quot;Age&quot;,
    title = &quot;2013 Influenza A H7N9 cases in China&quot;,
    subtitle = &quot;Dataset from &#39;outbreaks&#39; package (Kucharski et al. 2014)&quot;,
    caption = &quot;\nTime from onset of flu to outcome.&quot;
  )</code></pre>
<p><img src="/post/2018-04-22_flu_outcome_ML_post_files/figure-html/unnamed-chunk-5-1.png" width="768" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
</div>
<div id="features" class="section level1">
<h1>Features</h1>
<p>In machine learning-speak features are what we call the variables used for model training. Using the right features dramatically influences the accuracy and success of your model.</p>
<p>For this example, I am keeping age, but I am also generating new features from the date information and converting gender and province into numerical values.</p>
<pre class="r"><code>dataset &lt;- fluH7N9_china_2013 %&gt;%
  mutate(hospital = as.factor(ifelse(is.na(date_of_hospitalisation), 0, 1)),
         gender_f = as.factor(ifelse(gender == &quot;f&quot;, 1, 0)),
         province_Jiangsu = as.factor(ifelse(province == &quot;Jiangsu&quot;, 1, 0)),
         province_Shanghai = as.factor(ifelse(province == &quot;Shanghai&quot;, 1, 0)),
         province_Zhejiang = as.factor(ifelse(province == &quot;Zhejiang&quot;, 1, 0)),
         province_other = as.factor(ifelse(province == &quot;Zhejiang&quot; | province == &quot;Jiangsu&quot; | province == &quot;Shanghai&quot;, 0, 1)),
         days_onset_to_outcome = as.numeric(as.character(gsub(&quot; days&quot;, &quot;&quot;,
                                      as.Date(as.character(date_of_outcome), format = &quot;%Y-%m-%d&quot;) - 
                                        as.Date(as.character(date_of_onset), format = &quot;%Y-%m-%d&quot;)))),
         days_onset_to_hospital = as.numeric(as.character(gsub(&quot; days&quot;, &quot;&quot;,
                                      as.Date(as.character(date_of_hospitalisation), format = &quot;%Y-%m-%d&quot;) - 
                                        as.Date(as.character(date_of_onset), format = &quot;%Y-%m-%d&quot;)))),
         age = age,
         early_onset = as.factor(ifelse(date_of_onset &lt; summary(fluH7N9_china_2013$date_of_onset)[[3]], 1, 0)),
         early_outcome = as.factor(ifelse(date_of_outcome &lt; summary(fluH7N9_china_2013$date_of_outcome)[[3]], 1, 0))) %&gt;%
  subset(select = -c(2:4, 6, 8))
rownames(dataset) &lt;- dataset$case_id
dataset[, -2] &lt;- as.numeric(as.matrix(dataset[, -2]))
head(dataset)</code></pre>
<pre><code>##   case_id outcome age hospital gender_f province_Jiangsu province_Shanghai
## 1       1   Death  87        0        0                0                 1
## 2       2   Death  27        1        0                0                 1
## 3       3   Death  35        1        1                0                 0
## 4       4    &lt;NA&gt;  45        1        1                1                 0
## 5       5 Recover  48        1        1                1                 0
## 6       6   Death  32        1        1                1                 0
##   province_Zhejiang province_other days_onset_to_outcome
## 1                 0              0                    13
## 2                 0              0                    11
## 3                 0              1                    31
## 4                 0              0                    NA
## 5                 0              0                    57
## 6                 0              0                    36
##   days_onset_to_hospital early_onset early_outcome
## 1                     NA           1             1
## 2                      4           1             1
## 3                     10           1             1
## 4                      8           1            NA
## 5                     11           1             0
## 6                      7           1             1</code></pre>
<pre class="r"><code>summary(dataset$outcome)</code></pre>
<pre><code>##   Death Recover    NA&#39;s 
##      32      47      57</code></pre>
<p><br></p>
<div id="imputing-missing-values" class="section level2">
<h2>Imputing missing values</h2>
<p>I am using the <a href="https://gerkovink.github.io/miceVignettes/Ad_hoc_and_mice/Ad_hoc_methods.html"><code>mice</code> package for imputing missing values</a></p>
<p><strong>Note: Since publishing this blogpost I learned that the idea behind using <code>mice</code> is to compare different imputations to see how stable they are, instead of picking one imputed set as fixed for the remainder of the analysis. Therefore, I changed the focus of this post a little bit: in the old post I compared many different algorithms and their outcome; in this updated version I am only showing the Random Forest algorithm and focus on comparing the different imputed datasets. I am ignoring feature importance and feature plots because nothing changed compared to the old post.</strong></p>
<p><br></p>
<ul>
<li><code>md.pattern()</code> shows the pattern of missingness in the data:</li>
</ul>
<pre class="r"><code>md.pattern(dataset)</code></pre>
<pre><code>##    case_id hospital province_Jiangsu province_Shanghai province_Zhejiang
## 42       1        1                1                 1                 1
## 27       1        1                1                 1                 1
##  2       1        1                1                 1                 1
##  2       1        1                1                 1                 1
## 18       1        1                1                 1                 1
##  1       1        1                1                 1                 1
## 36       1        1                1                 1                 1
##  3       1        1                1                 1                 1
##  3       1        1                1                 1                 1
##  2       1        1                1                 1                 1
##          0        0                0                 0                 0
##    province_other age gender_f early_onset outcome early_outcome
## 42              1   1        1           1       1             1
## 27              1   1        1           1       1             1
##  2              1   1        1           1       1             0
##  2              1   1        1           0       1             1
## 18              1   1        1           1       0             0
##  1              1   1        1           1       1             0
## 36              1   1        1           1       0             0
##  3              1   1        1           0       1             0
##  3              1   1        1           0       0             0
##  2              1   0        0           0       1             0
##                 0   2        2          10      57            65
##    days_onset_to_outcome days_onset_to_hospital    
## 42                     1                      1   0
## 27                     1                      0   1
##  2                     0                      1   2
##  2                     0                      0   3
## 18                     0                      1   3
##  1                     0                      0   3
## 36                     0                      0   4
##  3                     0                      0   4
##  3                     0                      0   5
##  2                     0                      0   6
##                       67                     74 277</code></pre>
<ul>
<li><code>mice()</code> generates the imputations</li>
</ul>
<pre class="r"><code>dataset_impute &lt;- mice(data = dataset[, -2],  print = FALSE)</code></pre>
<ul>
<li>by default, <code>mice()</code> calculates five (m = 5) imputed data sets</li>
<li>we can combine them all in one output with the <code>complete(&quot;long&quot;)</code> function</li>
<li>I did not want to impute missing values in the <code>outcome</code> column, so I have to merge it back in with the imputed data</li>
</ul>
<pre class="r"><code>datasets_complete &lt;- right_join(dataset[, c(1, 2)], 
                           complete(dataset_impute, &quot;long&quot;),
                           by = &quot;case_id&quot;) %&gt;%
  select(-.id)</code></pre>
<pre class="r"><code>head(datasets_complete)</code></pre>
<pre><code>##   case_id outcome .imp age hospital gender_f province_Jiangsu
## 1       1   Death    1  87        0        0                0
## 2       2   Death    1  27        1        0                0
## 3       3   Death    1  35        1        1                0
## 4       4    &lt;NA&gt;    1  45        1        1                1
## 5       5 Recover    1  48        1        1                1
## 6       6   Death    1  32        1        1                1
##   province_Shanghai province_Zhejiang province_other days_onset_to_outcome
## 1                 1                 0              0                    13
## 2                 1                 0              0                    11
## 3                 0                 0              1                    31
## 4                 0                 0              0                    28
## 5                 0                 0              0                    57
## 6                 0                 0              0                    36
##   days_onset_to_hospital early_onset early_outcome
## 1                      5           1             1
## 2                      4           1             1
## 3                     10           1             1
## 4                      8           1             1
## 5                     11           1             0
## 6                      7           1             1</code></pre>
<p>Let’s compare the distributions of the five different imputed datasets:</p>
<pre class="r"><code>datasets_complete %&gt;%
  gather(x, y, age:early_outcome) %&gt;%
  ggplot(aes(x = y, fill = .imp, color = .imp)) +
    facet_wrap(~ x, ncol = 3, scales = &quot;free&quot;) +
    geom_density(alpha = 0.4) +
    scale_fill_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) +
    scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) +
    my_theme()</code></pre>
<p><img src="/post/2018-04-22_flu_outcome_ML_post_files/figure-html/unnamed-chunk-12-1.png" width="768" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
</div>
<div id="test-train-and-validation-data-sets" class="section level1">
<h1>Test, train and validation data sets</h1>
<p>Now, we can go ahead with machine learning!</p>
<p>The dataset contains a few missing values in the <code>outcome</code> column; those will be the test set used for final predictions (see the old blog post for this).</p>
<pre class="r"><code>train_index &lt;- which(is.na(datasets_complete$outcome))
train_data &lt;- datasets_complete[-train_index, ]
test_data  &lt;- datasets_complete[train_index, -2]</code></pre>
<p>The remainder of the data will be used for modeling. Here, I am splitting the data into 70% training and 30% test data.</p>
<p>Because I want to model each imputed dataset separately, I am using the <code>nest()</code> and <code>map()</code> functions.</p>
<pre class="r"><code>set.seed(42)
val_data &lt;- train_data %&gt;%
  group_by(.imp) %&gt;%
  nest() %&gt;%
  mutate(val_index = map(data, ~ createDataPartition(.$outcome, p = 0.7, list = FALSE)),
         val_train_data = map2(data, val_index, ~ .x[.y, ]),
         val_test_data = map2(data, val_index, ~ .x[-.y, ]))</code></pre>
<p><br></p>
</div>
<div id="machine-learning-algorithms" class="section level1">
<h1>Machine Learning algorithms</h1>
<div id="random-forest" class="section level2">
<h2>Random Forest</h2>
<p>To make the code tidier, I am first defining the modeling function with the parameters I want.</p>
<pre class="r"><code>model_function &lt;- function(df) {
  caret::train(outcome ~ .,
               data = df,
               method = &quot;rf&quot;,
               preProcess = c(&quot;scale&quot;, &quot;center&quot;),
               trControl = trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 3, verboseIter = FALSE))
}</code></pre>
<p>Next, I am using the nested tibble from before to <code>map()</code> the model function, predict the outcome and calculate confusion matrices.</p>
<pre class="r"><code>set.seed(42)
val_data_model &lt;- val_data %&gt;%
  mutate(model = map(val_train_data, ~ model_function(.x)),
         predict = map2(model, val_test_data, ~ data.frame(prediction = predict(.x, .y[, -2]))),
         predict_prob = map2(model, val_test_data, ~ data.frame(outcome = .y[, 2],
                                                                prediction = predict(.x, .y[, -2], type = &quot;prob&quot;))),
         confusion_matrix = map2(val_test_data, predict, ~ confusionMatrix(.x$outcome, .y$prediction)),
         confusion_matrix_tbl = map(confusion_matrix, ~ as.tibble(.x$table)))</code></pre>
<p><br></p>
</div>
<div id="comparing-accuracy-of-models" class="section level2">
<h2>Comparing accuracy of models</h2>
<p>To compare how the different imputations did, I am plotting</p>
<ul>
<li>the confusion matrices:</li>
</ul>
<pre class="r"><code>val_data_model %&gt;%
  unnest(confusion_matrix_tbl) %&gt;%
  ggplot(aes(x = Prediction, y = Reference, fill = n)) +
    facet_wrap(~ .imp, ncol = 5, scales = &quot;free&quot;) +
    geom_tile() +
    my_theme()</code></pre>
<p><img src="/post/2018-04-22_flu_outcome_ML_post_files/figure-html/unnamed-chunk-17-1.png" width="1440" style="display: block; margin: auto;" /></p>
<ul>
<li>and the prediction probabilities for correct and wrong predictions:</li>
</ul>
<pre class="r"><code>val_data_model %&gt;%
  unnest(predict_prob) %&gt;%
  gather(x, y, prediction.Death:prediction.Recover) %&gt;%
  ggplot(aes(x = x, y = y, fill = outcome)) +
    facet_wrap(~ .imp, ncol = 5, scales = &quot;free&quot;) +
    geom_boxplot() +
    scale_fill_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) +
    my_theme()</code></pre>
<p><img src="/post/2018-04-22_flu_outcome_ML_post_files/figure-html/unnamed-chunk-18-1.png" width="1440" style="display: block; margin: auto;" /></p>
<p>Hope, you found that example interesting and helpful!</p>
<p><br></p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.0 (2018-04-23)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.4
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] bindrcpp_0.2.2  caret_6.0-79    mice_2.46.0     lattice_0.20-35
##  [5] plyr_1.8.4      forcats_0.3.0   stringr_1.3.1   dplyr_0.7.4    
##  [9] purrr_0.2.4     readr_1.1.1     tidyr_0.8.0     tibble_1.4.2   
## [13] ggplot2_2.2.1   tidyverse_1.2.1 outbreaks_1.3.0
## 
## loaded via a namespace (and not attached):
##  [1] nlme_3.1-137        lubridate_1.7.4     dimRed_0.1.0       
##  [4] RColorBrewer_1.1-2  httr_1.3.1          rprojroot_1.3-2    
##  [7] tools_3.5.0         backports_1.1.2     R6_2.2.2           
## [10] rpart_4.1-13        lazyeval_0.2.1      colorspace_1.3-2   
## [13] nnet_7.3-12         withr_2.1.2         tidyselect_0.2.4   
## [16] mnormt_1.5-5        compiler_3.5.0      cli_1.0.0          
## [19] rvest_0.3.2         xml2_1.2.0          labeling_0.3       
## [22] bookdown_0.7        scales_0.5.0        sfsmisc_1.1-2      
## [25] DEoptimR_1.0-8      psych_1.8.4         robustbase_0.93-0  
## [28] randomForest_4.6-14 digest_0.6.15       foreign_0.8-70     
## [31] rmarkdown_1.9       pkgconfig_2.0.1     htmltools_0.3.6    
## [34] rlang_0.2.0         readxl_1.1.0        ddalpha_1.3.3      
## [37] rstudioapi_0.7      bindr_0.1.1         jsonlite_1.5       
## [40] ModelMetrics_1.1.0  magrittr_1.5        Matrix_1.2-14      
## [43] Rcpp_0.12.17        munsell_0.4.3       abind_1.4-5        
## [46] stringi_1.2.2       yaml_2.1.19         MASS_7.3-50        
## [49] recipes_0.1.2       grid_3.5.0          parallel_3.5.0     
## [52] crayon_1.3.4        haven_1.1.1         splines_3.5.0      
## [55] hms_0.4.2           knitr_1.20          pillar_1.2.2       
## [58] reshape2_1.4.3      codetools_0.2-15    stats4_3.5.0       
## [61] CVST_0.2-1          magic_1.5-8         glue_1.2.0         
## [64] evaluate_0.10.1     blogdown_0.6        modelr_0.1.2       
## [67] foreach_1.4.4       cellranger_1.1.0    gtable_0.2.0       
## [70] kernlab_0.9-26      assertthat_0.2.0    DRR_0.0.3          
## [73] xfun_0.1            gower_0.1.2         prodlim_2018.04.18 
## [76] broom_0.4.4         e1071_1.6-8         class_7.3-14       
## [79] survival_2.42-3     geometry_0.3-6      timeDate_3043.102  
## [82] RcppRoll_0.2.2      iterators_1.0.9     lava_1.6.1         
## [85] ipred_0.9-6</code></pre>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Look, something shiny: How to use R Shiny to make Münster traffic data accessible. Join MünsteR for our next meetup!]]></title>
    <link href="/2018/04/meetup_june18/"/>
    <id>/2018/04/meetup_june18/</id>
    <published>2018-04-19T00:00:00+00:00</published>
    <updated>2018-04-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://shiring.github.io/r_users_group/2017/05/20/map.png" />

</div>
<p>In our <a href="http://meetu.ps/e/DVFrp/w54bW/f">next MünsteR R-user group meetup</a> on <strong>Monday, June 11th, 2018</strong> Thomas Kluth and Thorben Jensen will give a talk titled <strong>Look, something shiny: How to use R Shiny to make Münster traffic data accessible</strong>. You can RSVP here: <a href="http://meetu.ps/e/F7zDN/w54bW/f" class="uri">http://meetu.ps/e/F7zDN/w54bW/f</a></p>
<blockquote>
<p>About a year ago, we stumbled upon rich datasets on <em>traffic dynamics</em> of Münster: count data of bikes, cars, and bus passengers of high resolution. Since that day we have been crunching, modeling, and visualizing it. To involve local stakeholders and NGOs (e.g., the <a href="http://fahrradstadt.ms">IG Fahrradstadt Münster</a>), we found the R Shiny framework to be very useful.</p>
</blockquote>
<blockquote>
<p>Shiny is probably the fastest way to take your R projects online. According to <a href="https://shiny.rstudio.com/">RStudio</a>, it “is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage or embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions.”</p>
</blockquote>
<blockquote>
<p>We would like to introduce Shiny to you using the following topics:</p>
</blockquote>
<ol style="list-style-type: decimal">
<li>Why we love Shiny, and why you should, too</li>
<li>How Shiny works, from code to browser</li>
<li>How to deploy your R Shiny project with Docker</li>
<li>Our <a href="https://traffics.codeformuenster.org">Shiny project traffic dynamics</a></li>
<li>Plotting results of Bayesian and frequentist models within Shiny</li>
<li>Group discussion: what else to present with Shiny?</li>
</ol>
<blockquote>
<p>You will not have to bring much previous knowledge to our talk. A basic understanding of how R code works will take you far. The part about statistical modeling will be as intuitive as possible. Overall, we will try to <em>keep it simple and shiny</em>.</p>
</blockquote>
<blockquote>
<p>All parts of our talk will be connected to traffic data for Münster. We look forward to your feedback and ideas for more analyses. You find our <em>traffic dynamics</em> projects on <a href="https://github.com/codeformuenster">Code for Münster’s github page</a>.</p>
</blockquote>
<div id="about-the-speakers" class="section level2">
<h2>About the speakers</h2>
<p>The speakers Thomas Kluth and Thorben Jensen are members of <a href="http://codeformuenster.org">Code for Münster</a>. We meet each Tuesday at 18:30 at the Dreiklang bar to make our city a better place by coding. New coders are always welcome!</p>
<p>Thomas Kluth has studied Computer Science in Münster and Bremen. During his currently ongoing (close-to-be-finished) Linguistics PhD in Bielefeld, he models human cognitive behavior. Using computational cognitive models, he aims to link spatial language use with perceptual mechanisms such as visual attention. The statistical analysis of empirical data convinced him to use R and Bayesian modeling to explain almost everything. He is looking forward to applying his skill set to real-world domains for creating a sustainable future.</p>
<p>Thorben Jensen has studied, designed, and implemented predictive models since more than 10 years. After studying modeling and computer science in 5 countries, he graduated from the PhD program at Delft University of Technology. His PhD thesis and other publications propose increased use of optimization methods and automation when building simulations with software agents. When consulting clients on Data Science, he enjoys making predictions intuitive with R Shiny.</p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[HH Data Science Meetup slides: Explaining complex machine learning models with LIME]]></title>
    <link href="/2018/04/hh_datascience_meetup_2018_slides/"/>
    <id>/2018/04/hh_datascience_meetup_2018_slides/</id>
    <published>2018-04-18T00:00:00+00:00</published>
    <updated>2018-04-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>On April 12th, 2018 I gave a talk about <strong>Explaining complex machine learning models with LIME</strong> at the <a href="https://www.meetup.com/Hamburg-Data-Science-Meetup/events/krrqhpyxfbkc/">Hamburg Data Science Meetup</a> - so if you’re intersted: the slides can be found here: <a href="https://www.slideshare.net/ShirinGlander/hh-data-science-meetup-explaining-complex-machine-learning-models-with-lime-94218890" class="uri">https://www.slideshare.net/ShirinGlander/hh-data-science-meetup-explaining-complex-machine-learning-models-with-lime-94218890</a></p>
<blockquote>
<p>Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex machine learning models are essentially black boxes and too complicated to understand, we need to use approximations to get a better sense of how they work. One such approach is LIME, which stands for Local Interpretable Model-agnostic Explanations and is a tool that helps understand and explain the decisions made by complex machine learning models.</p>
</blockquote>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/3CI6SYAltdniw" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; display: block; margin: auto; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen>
</iframe>
<div style="margin-bottom:5px">
<strong> <a href="//www.slideshare.net/ShirinGlander/hh-data-science-meetup-explaining-complex-machine-learning-models-with-lime-94218890" title="HH Data Science Meetup: Explaining complex machine learning models with LIME" target="_blank">HH Data Science Meetup: Explaining complex machine learning models with LIME</a> </strong> from <strong><a href="https://www.slideshare.net/ShirinGlander" target="_blank">Shirin Glander</a></strong>
</div>
<p>– slide deck was produced with <a href="www.beautiful.ai">beautiful.ai</a> –</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #124: Systems and Software for Machine Learning at Scale with Jeff Dean]]></title>
    <link href="/2018/04/twimlai124/"/>
    <id>/2018/04/twimlai124/</id>
    <published>2018-04-18T00:00:00+00:00</published>
    <updated>2018-04-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Systems and Software for Machine Learning at Scale with Jeff Dean</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai124.jpg" alt="Sketchnotes from TWiMLAI talk #124: Systems and Software for Machine Learning at Scale with Jeff Dean" />
<p class="caption">Sketchnotes from TWiMLAI talk #124: Systems and Software for Machine Learning at Scale with Jeff Dean</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-124-systems-software-machine-learning-scale-jeff-dean/">here</a>.</p>
<blockquote>
<p>In this episode I’m joined by Jeff Dean, Google Senior Fellow and head of the company’s deep learning research team Google Brain, who I had a chance to sit down with last week at the Googleplex in Mountain View. As you’ll hear, I was very excited for this interview, because so many of Jeff’s contributions since he started at Google in ‘99 have touched my life and work. In our conversation, Jeff and I dig into a bunch of the core machine learning innovations we’ve seen from Google. Of course we discuss TensorFlow, and its origins and evolution at Google. We also explore AI acceleration hardware, including TPU v1, v2 and future directions from Google and the broader market in this area. We talk through the machine learning toolchain, including some things that Googlers might take for granted, and where the recently announced Cloud AutoML fits in. We also discuss Google’s process for mapping problems across a variety of domains to deep learning, and much, much more. This was definitely one of my favorite conversations, and I’m pumped to be able to share it with you. <a href="https://twimlai.com/twiml-talk-124-systems-software-machine-learning-scale-jeff-dean/" class="uri">https://twimlai.com/twiml-talk-124-systems-software-machine-learning-scale-jeff-dean/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Meetup slides: Introducing Deep Learning with Keras]]></title>
    <link href="/2018/04/ruhrpy_meetup_2018_slides/"/>
    <id>/2018/04/ruhrpy_meetup_2018_slides/</id>
    <published>2018-04-11T00:00:00+00:00</published>
    <updated>2018-04-11T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>On April 4th, 2018 I gave a talk about <strong>Deep Learning with Keras</strong> at the Ruhr.Py Meetup in Essen, Germany. The talk was not specific to Python, though - so if you’re intersted: the slides can be found here: <a href="https://www.slideshare.net/ShirinGlander/ruhrpy-introducing-deep-learning-with-keras-and-python" class="uri">https://www.slideshare.net/ShirinGlander/ruhrpy-introducing-deep-learning-with-keras-and-python</a></p>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/cZz1j6qtlwm62l" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; display: block; margin: auto; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen>
</iframe>
<div style="margin-bottom:5px">
<strong> <a href="//www.slideshare.net/ShirinGlander/ruhrpy-introducing-deep-learning-with-keras-and-python" title="Ruhr.PY - Introducing Deep Learning with Keras and Python" target="_blank">Ruhr.PY - Introducing Deep Learning with Keras and Python</a> </strong> von <strong><a href="//www.slideshare.net/ShirinGlander" target="_blank">Shirin Glander</a></strong>
</div>
<p>There is also a video recording of my talk, which you can see here: <a href="https://youtu.be/Q8hVXnpEPmc" class="uri">https://youtu.be/Q8hVXnpEPmc</a></p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Q8hVXnpEPmc?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Join MünsteR for our next meetup on deep learning with Keras and R]]></title>
    <link href="/2018/03/meetup_april18/"/>
    <id>/2018/03/meetup_april18/</id>
    <published>2018-03-28T00:00:00+00:00</published>
    <updated>2018-03-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://shiring.github.io/r_users_group/2017/05/20/map.png" />

</div>
<p>In our <a href="http://meetu.ps/e/DVFrp/w54bW/f">next MünsteR R-user group meetup</a> on <strong>Tuesday, April 17th, 2018</strong> Kai Lichtenberg will talk about deep learning with Keras. You can RSVP here: <a href="http://meetu.ps/e/DDY1B/w54bW/f" class="uri">http://meetu.ps/e/DDY1B/w54bW/f</a></p>
<blockquote>
<p>Although neural networks have been around for quite a while now, deep learning really just took of a few years ago. It pretty much all started when Alex Krizhevsky and Geoffrey Hinton utterly crushed classic image recognition in the 2012 ImageNet Large Scale Visual Recognition Challenge by implementing a deep neural network with CUDA on graphics cards. A lot has changed since that time: The toolchain to do deep learning has rapidly evolved into API’s with a very high level of abstraction. Nowadays everyone can train complex neural networks with billions of free parameters. Just last year RStudio announced the Keras for R package. Keras is a high level neural network API that makes it really easy to define the architecture of a neural network. In this talk we will rush through an explanation of convolutional neural networks for image recognition, learn how easy it has become to do production ready deep learning with the use of docker and why R’s syntax is even better suited to define a neural network than python’s. (Hint: Probably this is not a pipe ;-)</p>
</blockquote>
<p>Kai Lichtenberg is a PhD student in the Bosch PhD Program working on models to predict the reliability of components in drive trains by leveraging the ever more available high dimensional data in the era of the Internet of Things. Coming from the field of mechanical engineering and technical reliability (which has a lot to do with stochastic processes) he found his destination in data science. As a long time computer enthusiast he is always keen to use the newest technologies. To bear the pain of getting a toolchain up and running is probably his super power.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[My upcoming meetup talks about Deep Learning with Keras and explaining complex Machine Learning Models with LIME]]></title>
    <link href="/2018/03/meetup_talk_ruhrpy_april_18/"/>
    <id>/2018/03/meetup_talk_ruhrpy_april_18/</id>
    <published>2018-03-28T00:00:00+00:00</published>
    <updated>2018-03-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I’ll be talking about Deep Learning with Keras in R and Python at the following upcoming meetup:</p>
<ul>
<li><a href="https://www.meetup.com/Ruhr-py/events/248093628/">Ruhr.Py 2018</a> on Wednesday, April 4th</li>
</ul>
<blockquote>
<p>Introducing Deep Learning with Keras and Python Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. In this talk we build, train and visualize a Model using Python and Keras - all interactive with Jupyter Notebooks!</p>
</blockquote>
<hr />
<p>And I’ll be talking about explaining complex Machine Learning Models with LIME at this upcoming meetup:</p>
<ul>
<li><a href="https://www.meetup.com/Hamburg-Data-Science-Meetup/events/244145443/">Data Science Meetup Hamburg</a> on Thursday, April 12th</li>
</ul>
<blockquote>
<p>Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex machine learning models are essentially black boxes and too complicated to understand, we need to use approximations to get a better sense of how they work. One such approach is LIME, which stands for Local Interpretable Model-agnostic Explanations and is a tool that helps understand and explain the decisions made by complex machine learning models.</p>
</blockquote>
<blockquote>
<p>Dr. Shirin Glander is Data Scientist at codecentric AG. She has received a PhD in Bioinformatics and applies methods of analysis and visualization from different areas - for instance, machine learning, classical statistics, text mining, etc. -to extract and leverage information from data.</p>
</blockquote>
<div class="figure">
<img src="https://secure.meetupstatic.com/s/img/5455565085016210254/logo/svg/logo--script.svg" />

</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #115: Scaling Machine Learning at Uber with Mike Del Balso]]></title>
    <link href="/2018/03/twimlai115/"/>
    <id>/2018/03/twimlai115/</id>
    <published>2018-03-07T00:00:00+00:00</published>
    <updated>2018-03-07T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Scaling Machine Learning at Uber with Mike Del Balso</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai115.jpg" alt="Sketchnotes from TWiMLAI talk #115: Scaling Machine Learning at Uber with Mike Del Balso" />
<p class="caption">Sketchnotes from TWiMLAI talk #115: Scaling Machine Learning at Uber with Mike Del Balso</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-115-scaling-machine-learning-uber-mike-del-balso/">here</a>.</p>
<blockquote>
<p>In this episode, I speak with Mike Del Balso, Product Manager for Machine Learning Platforms at Uber. Mike and I sat down last fall at the Georgian Partners Portfolio conference to discuss his presentation “Finding success with machine learning in your company.” In our discussion, Mike shares some great advice for organizations looking to get value out of machine learning. He also details some of the pitfalls companies run into, such as not have proper infrastructure in place for maintenance and monitoring, not managing their expectations, and not putting the right tools in place for data science and development teams. On this last point, we touch on the Michelangelo platform, which Uber uses internally to build, deploy and maintain ML systems at scale, and the open source distributed TensorFlow system they’ve created, Horovod. This was a very insightful interview, so get your notepad ready! <a href="https://twimlai.com/twiml-talk-115-scaling-machine-learning-uber-mike-del-balso/" class="uri">https://twimlai.com/twiml-talk-115-scaling-machine-learning-uber-mike-del-balso/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Another Game of Thrones network analysis - this time with tidygraph and ggraph]]></title>
    <link href="/2018/03/got_network/"/>
    <id>/2018/03/got_network/</id>
    <published>2018-03-04T00:00:00+00:00</published>
    <updated>2018-03-04T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>A while back, I did <a href="https://shiring.github.io/networks/2017/05/15/got_final">an analysis of the family network of major characters from the A Song of Ice and Fire books and the Game of Thrones TV show</a>. In that analysis I found out that House Stark (specifically Ned and Sansa) and House Lannister (especially Tyrion) are the most important family connections in Game of Thrones; they also connect many of the story lines and are central parts of the narrative.</p>
<p>In that old post, I used <code>igraph</code> for plotting and calculating network metrics.</p>
<p>But there are two packages that integrate network analysis much more nicely with the <code>tidyverse</code>: <code>tidygraph</code> and <code>ggraph</code>. These, I am going to show how to use for analyzing yet another network of characters from <strong>A Song of Ice and Fire</strong> / <strong>Game of Thrones</strong> (to be correct, this new network here is strictly based on the <strong>A Song of Ice and Fire</strong> books, NOT on the TV show).</p>
<div id="what-can-network-analysis-tell-us" class="section level2">
<h2>What can network analysis tell us?</h2>
<p>Network analysis can e.g. be used to explore relationships in social or professional networks. In such cases, we would typically ask questions like:</p>
<ul>
<li>How many connections does each person have?</li>
<li>Who is the most connected (i.e. influential or “important”) person?</li>
<li>Are there clusters of tightly connected people?</li>
<li>Are there a few key players that connect clusters of people?</li>
<li>etc.</li>
</ul>
<p>These answers can give us a lot of information about the patterns of how people interact.</p>
<p>So, how do we find out who the most important characters are in this network? We consider a character “important” if he has connections to many other characters. There are a few network properties, that tell us more about this, like node centrality and which characters are key-players in the books.</p>
<p><strong>A word of caution before you read on: BEWARE of SPOILERS for all books!</strong></p>
<div class="figure">
<img src="https://shirinsplayground.netlify.com/post/2018-03-03_got_network_files/figure-html/unnamed-chunk-18-1.png" alt="A Song of Ice and Fire character network across all five books; find out how I made it by following the code below…" />
<p class="caption">A Song of Ice and Fire character network across all five books; find out how I made it by following the code below…</p>
</div>
<pre class="r"><code>library(readr)     # fast reading of csv files
library(tidyverse) # tidy data analysis
library(tidygraph) # tidy graph analysis
library(ggraph)    # for plotting</code></pre>
</div>
<div id="the-data" class="section level2">
<h2>The Data</h2>
<p>I obtained the following data by cloning <a href="https://github.com/mathbeveridge/asoiaf">this Github repository</a> from Andrew Beveridge:</p>
<blockquote>
<p>Character Interaction Networks for George R. R. Martin’s “A Song of Ice and Fire” saga These networks were created by connecting two characters whenever their names (or nicknames) appeared within 15 words of one another in one of the books in “A Song of Ice and Fire.” The edge weight corresponds to the number of interactions. You can use this data to explore the dynamics of the Seven Kingdoms using network science techniques. For example, community detection finds coherent plotlines. Centrality measures uncover the multiple ways in which characters play important roles in the saga.</p>
</blockquote>
<p>Andrew already did a great job analyzing these character networks and you can read all his conclusions on his site <a href="https://networkofthrones.wordpress.com" class="uri">https://networkofthrones.wordpress.com</a>. Here, I don’t aim to replicate his analyses but I want to show how you could do this or similar analyses with <code>tidygraph</code> and <code>ggraph</code>. Thus, I am also not going to use all of his node and edge files.</p>
<pre class="r"><code>path &lt;- &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data/&quot;
files &lt;- list.files(path = path, full.names = TRUE)
files</code></pre>
<pre><code>##  [1] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-all-edges.csv&quot;   
##  [2] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-all-nodes.csv&quot;   
##  [3] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book1-edges.csv&quot; 
##  [4] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book1-nodes.csv&quot; 
##  [5] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book2-edges.csv&quot; 
##  [6] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book2-nodes.csv&quot; 
##  [7] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book3-edges.csv&quot; 
##  [8] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book3-nodes.csv&quot; 
##  [9] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book4-edges.csv&quot; 
## [10] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book4-nodes.csv&quot; 
## [11] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book45-edges.csv&quot;
## [12] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book45-nodes.csv&quot;
## [13] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book5-edges.csv&quot; 
## [14] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book5-nodes.csv&quot;</code></pre>
</div>
<div id="characters-across-all-books" class="section level2">
<h2>Characters across all books</h2>
<p>The first data set I am going to use are the character interactions in all five books. I am not using the node files here, because I find the edge names sufficient for this demonstration. If you wanted to have nice name labels, you could use the node files.</p>
<pre class="r"><code>cooc_all_edges &lt;- read_csv(files[1])</code></pre>
<p>Because there are so many characters in the books, many of them minor, I am subsetting the data to the 100 characters with the most interactions across all books. The edges are undirected, therefore there are no redundant Source-Target combinations; because of this, I gathered Source and Target data before summing up the weights.</p>
<pre class="r"><code>main_ch &lt;- cooc_all_edges %&gt;%
  select(-Type) %&gt;%
  gather(x, name, Source:Target) %&gt;%
  group_by(name) %&gt;%
  summarise(sum_weight = sum(weight)) %&gt;%
  ungroup()

main_ch_l &lt;- main_ch %&gt;%
  arrange(desc(sum_weight)) %&gt;%
  top_n(100, sum_weight)
main_ch_l</code></pre>
<pre><code>## # A tibble: 100 x 2
##    name               sum_weight
##    &lt;chr&gt;                   &lt;int&gt;
##  1 Tyrion-Lannister         2873
##  2 Jon-Snow                 2757
##  3 Cersei-Lannister         2232
##  4 Joffrey-Baratheon        1762
##  5 Eddard-Stark             1649
##  6 Daenerys-Targaryen       1608
##  7 Jaime-Lannister          1569
##  8 Sansa-Stark              1547
##  9 Bran-Stark               1508
## 10 Robert-Baratheon         1488
## # ... with 90 more rows</code></pre>
<pre class="r"><code>cooc_all_f &lt;- cooc_all_edges %&gt;%
  filter(Source %in% main_ch_l$name &amp; Target %in% main_ch_l$name)</code></pre>
</div>
<div id="tidygraph-and-ggraph" class="section level2">
<h2>tidygraph and ggraph</h2>
<p>Both <code>tidygraph</code> and <code>ggraph</code> have been developed by <a href="https://www.data-imaginist.com">Thomas Lin Pedersen</a>:</p>
<blockquote>
<p>With tidygraph I set out to make it easier to get your data into a graph and perform common transformations on it, but the aim has expanded since its inception. The goal of tidygraph is to empower the user to formulate complex questions regarding relational data as simple steps, thus enabling them to retrieve insights directly from the data itself. The central idea this all boils down to is this: you don’t have to plot a network to understand it. While I absolutely love the field of network visualisation, it is in many ways overused in data science — especially when it comes to extracting knowledge from a network. Just as you don’t need a plot to tell you which car in a dataset is the fastest, you don’t need a plot to tell you which pair of friends are the closest. What you do need, instead of a plot, is a tool that allow you to formulate your question into a logic sequence of operations. For many people in the world of rectangular data, this tool is increasingly dplyr (and friends), and I do hope that tidygraph can take on the same role in the world of relational data. <a href="https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/" class="uri">https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/</a></p>
</blockquote>
<p>The first step is to convert our edge table into a <code>tbl_graph</code> object structure. Here, we use the <code>as_tbl_graph()</code> function from <code>tidygraph</code>; it can take many different types of input data, like <code>data.frame</code>, <code>matrix</code>, <code>dendrogram</code>, <code>igraph</code>, etc.</p>
<blockquote>
<p>Underneath the hood of tidygraph lies the well-oiled machinery of igraph, ensuring efficient graph manipulation. Rather than keeping the node and edge data in a list and creating igraph objects on the fly when needed, tidygraph subclasses igraph with the tbl_graph class and simply exposes it in a tidy manner. This ensures that all your beloved algorithms that expects igraph objects still works with tbl_graph objects. Further, tidygraph is very careful not to override any of igraphs exports so the two packages can coexist quite happily. <a href="https://www.data-imaginist.com/2017/introducing-tidygraph/" class="uri">https://www.data-imaginist.com/2017/introducing-tidygraph/</a></p>
</blockquote>
<p>A central aspect of <code>tidygraph</code> is that you can directly manipulate node and edge data from this <code>tbl_graph</code> object by <strong>activating</strong> nodes or edges. When we first create a <code>tbl_graph</code> object, the nodes will be activated. We can then directly calculate node or edge metrics, like centrality, using <code>tidyverse</code> functions.</p>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE)</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Node Data: 100 x 1 (active)
##   name                           
##   &lt;chr&gt;                          
## 1 Aemon-Targaryen-(Maester-Aemon)
## 2 Aeron-Greyjoy                  
## 3 Aerys-II-Targaryen             
## 4 Alliser-Thorne                 
## 5 Arianne-Martell                
## 6 Arya-Stark                     
## # ... with 94 more rows
## #
## # Edge Data: 798 x 5
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1     1     4 Undirected    43      7
## 2     1    13 Undirected    44      4
## 3     1    28 Undirected    52      3
## # ... with 795 more rows</code></pre>
<p>We can change that with the <code>activate()</code> function. We can now, for example, remove multiple edges. When you are using RStudio, start typing <code>?edge_is_</code> and wait for the autocomplete function to show you what else is possible (or go to the <code>tidygraph</code> manual).</p>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(edges) %&gt;%
  filter(!edge_is_multiple())</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Edge Data: 798 x 5 (active)
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1     1     4 Undirected    43      7
## 2     1    13 Undirected    44      4
## 3     1    28 Undirected    52      3
## 4     1    32 Undirected    53     20
## 5     1    34 Undirected    54      5
## 6     1    41 Undirected    56      5
## # ... with 792 more rows
## #
## # Node Data: 100 x 1
##   name                           
##   &lt;chr&gt;                          
## 1 Aemon-Targaryen-(Maester-Aemon)
## 2 Aeron-Greyjoy                  
## 3 Aerys-II-Targaryen             
## # ... with 97 more rows</code></pre>
<div id="node-ranking" class="section level3">
<h3>Node ranking</h3>
<blockquote>
<p>Often, especially when visualising networks with certain layouts, the order in which the nodes appear will have a huge influence on the insight you can get out (e.g. matrix plots and arc diagrams). The node_rank_*() family of algorithms have been introduced to provide different ways of sorting nodes so that closely related nodes are positionally close. As there is often not a single correct answer to this endeavor, there’s a lot of different algorithms that may provide different insights into your network. Many of them are based on the seriation package, and the vignette provided therein serves as a nice introduction to the different algorithms. <a href="https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/" class="uri">https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/</a></p>
</blockquote>
<p>There are many options for node ranking (go to <code>?node_rank</code> for a full list); let’s try out <strong>Minimize hamiltonian path length using a travelling salesperson solver</strong>.</p>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(nodes) %&gt;% 
  mutate(n_rank_trv = node_rank_traveller()) %&gt;%
  arrange(n_rank_trv)</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Node Data: 100 x 2 (active)
##   name             n_rank_trv
##   &lt;chr&gt;                 &lt;int&gt;
## 1 Gendry                    1
## 2 Hot-Pie                   2
## 3 Lem                       3
## 4 Beric-Dondarrion          4
## 5 Eddard-Stark              5
## 6 Ramsay-Snow               6
## # ... with 94 more rows
## #
## # Edge Data: 798 x 5
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1    44    46 Undirected    43      7
## 2    44    45 Undirected    44      4
## 3    44    92 Undirected    52      3
## # ... with 795 more rows</code></pre>
</div>
<div id="centrality" class="section level3">
<h3>Centrality</h3>
<p>Centrality describes the number of edges that are in- or outgoing to/from nodes. High centrality networks have few nodes with many connections, low centrality networks have many nodes with similar numbers of edges. The centrality of a node measures the importance of it in the network.</p>
<blockquote>
<p>This version adds 19(!) new ways to define the notion of centrality along with a manual version where you can mix and match different distance measures and summation strategies opening up the world to even more centrality scores. All of this wealth of centrality comes from the netrankr package that provides a framework for defining and calculating centrality scores. If you use centrality measures somewhere in your analysis I cannot recommend the vignettes provided by netrankr enough as they provide a fundamental intuition about the nature of such measures and how they can/should be used. <a href="https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/" class="uri">https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/</a></p>
</blockquote>
<p>Again, type <code>?centrality</code> for an overview about all possible centrality measures you can use. Let’s try out <code>centrality_degree()</code>.</p>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(nodes) %&gt;% 
  mutate(neighbors = centrality_degree()) %&gt;%
  arrange(-neighbors)</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Node Data: 100 x 2 (active)
##   name              neighbors
##   &lt;chr&gt;                 &lt;dbl&gt;
## 1 Tyrion-Lannister        54.
## 2 Cersei-Lannister        49.
## 3 Joffrey-Baratheon       49.
## 4 Robert-Baratheon        47.
## 5 Jaime-Lannister         45.
## 6 Sansa-Stark             44.
## # ... with 94 more rows
## #
## # Edge Data: 798 x 5
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1    41    42 Undirected    43      7
## 2    41    60 Undirected    44      4
## 3    41    63 Undirected    52      3
## # ... with 795 more rows</code></pre>
</div>
<div id="grouping-and-clustering" class="section level3">
<h3>Grouping and clustering</h3>
<blockquote>
<p>Another common operation is to group nodes based on the graph topology, sometimes referred to as community detection based on its commonality in social network analysis. All clustering algorithms from igraph is available in tidygraph using the group_* prefix. All of these functions return an integer vector with nodes (or edges) sharing the same integer being grouped together. <a href="https://www.data-imaginist.com/2017/introducing-tidygraph/" class="uri">https://www.data-imaginist.com/2017/introducing-tidygraph/</a></p>
</blockquote>
<p>We can use <code>?group_graph</code> for an overview about all possible ways to cluster and group nodes. Here I am using <code>group_infomap()</code>: <strong>Group nodes by minimizing description length using</strong>.</p>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(nodes) %&gt;% 
  mutate(group = group_infomap()) %&gt;%
  arrange(-group)</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Node Data: 100 x 2 (active)
##   name              group
##   &lt;chr&gt;             &lt;int&gt;
## 1 Arianne-Martell       7
## 2 Doran-Martell         7
## 3 Davos-Seaworth        6
## 4 Melisandre            6
## 5 Selyse-Florent        6
## 6 Stannis-Baratheon     6
## # ... with 94 more rows
## #
## # Edge Data: 798 x 5
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1    32    33 Undirected    43      7
## 2    32    34 Undirected    44      4
## 3    32    36 Undirected    52      3
## # ... with 795 more rows</code></pre>
</div>
<div id="querying-node-types" class="section level3">
<h3>Querying node types</h3>
<p>We can also query different node types (<code>?node_types</code> gives us a list of options):</p>
<blockquote>
<p>These functions all lets the user query whether each node is of a certain type. All of the functions returns a logical vector indicating whether the node is of the type in question. Do note that the types are not mutually exclusive and that nodes can thus be of multiple types.</p>
</blockquote>
<p>Here, I am trying out <code>node_is_center()</code> (does the node have the minimal eccentricity in the graph) and <code>node_is_keyplayer()</code> to identify the top 10 key-players in the network. You can read more about the <code>node_is_keyplayer()</code> function in the manual for the <code>influenceR</code> package:</p>
<blockquote>
<p>The “Key Player” family of node importance algorithms (Borgatti 2006) involves the selection of a metric of node importance and a combinatorial optimization strategy to choose the set S of vertices of size k that maximize that metric. This function implements KPP-Pos, a metric intended to identify k nodes which optimize resource diffusion through the net … <a href="https://cran.r-project.org/web/packages/influenceR/" class="uri">https://cran.r-project.org/web/packages/influenceR/</a></p>
</blockquote>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(nodes) %&gt;% 
  mutate(center = node_is_center(),
         keyplayer = node_is_keyplayer(k = 10))</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Node Data: 100 x 3 (active)
##   name                            center keyplayer
##   &lt;chr&gt;                           &lt;lgl&gt;  &lt;lgl&gt;    
## 1 Aemon-Targaryen-(Maester-Aemon) FALSE  FALSE    
## 2 Aeron-Greyjoy                   FALSE  FALSE    
## 3 Aerys-II-Targaryen              FALSE  FALSE    
## 4 Alliser-Thorne                  FALSE  FALSE    
## 5 Arianne-Martell                 FALSE  FALSE    
## 6 Arya-Stark                      FALSE  FALSE    
## # ... with 94 more rows
## #
## # Edge Data: 798 x 5
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1     1     4 Undirected    43      7
## 2     1    13 Undirected    44      4
## 3     1    28 Undirected    52      3
## # ... with 795 more rows</code></pre>
</div>
</div>
<div id="node-pairs" class="section level2">
<h2>Node pairs</h2>
<blockquote>
<p>Some statistics are a measure between two nodes, such as distance or similarity between nodes. In a tidy context one of the ends must always be the node defined by the row, while the other can be any other node. All of the node pair functions are prefixed with node_* and ends with _from/_to if the measure is not symmetric and _with if it is; e.g. there’s both a node_max_flow_to() and node_max_flow_from() function while only a single node_cocitation_with() function. The other part of the node pair can be specified as an integer vector that will get recycled if needed, or a logical vector which will get recycled and converted to indexes with which(). This means that output from node type functions can be used directly in the calls. <a href="https://www.data-imaginist.com/2017/introducing-tidygraph/" class="uri">https://www.data-imaginist.com/2017/introducing-tidygraph/</a></p>
</blockquote>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(nodes) %&gt;% 
  mutate(dist_to_center = node_distance_to(node_is_center()))</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Node Data: 100 x 2 (active)
##   name                            dist_to_center
##   &lt;chr&gt;                                    &lt;dbl&gt;
## 1 Aemon-Targaryen-(Maester-Aemon)             1.
## 2 Aeron-Greyjoy                               2.
## 3 Aerys-II-Targaryen                          1.
## 4 Alliser-Thorne                              1.
## 5 Arianne-Martell                             2.
## 6 Arya-Stark                                  1.
## # ... with 94 more rows
## #
## # Edge Data: 798 x 5
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1     1     4 Undirected    43      7
## 2     1    13 Undirected    44      4
## 3     1    28 Undirected    52      3
## # ... with 795 more rows</code></pre>
<div id="edge-betweenness" class="section level3">
<h3>Edge betweenness</h3>
<p>Similarly to node metrics, we can calculate all kinds of edge metrics. Betweenness, for example, describes the shortest paths between nodes. More about what you can do with edges can be found with <code>?edge_types</code> and in the <a href="https://cran.r-project.org/web/packages/tidygraph/tidygraph.pdf">tidygraph manual</a>.</p>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(edges) %&gt;% 
  mutate(centrality_e = centrality_edge_betweenness())</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Edge Data: 798 x 6 (active)
##    from    to Type          id weight centrality_e
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;        &lt;dbl&gt;
## 1     1     4 Undirected    43      7         1.00
## 2     1    13 Undirected    44      4        30.2 
## 3     1    28 Undirected    52      3        42.1 
## 4     1    32 Undirected    53     20         0.  
## 5     1    34 Undirected    54      5        35.2 
## 6     1    41 Undirected    56      5        18.9 
## # ... with 792 more rows
## #
## # Node Data: 100 x 1
##   name                           
##   &lt;chr&gt;                          
## 1 Aemon-Targaryen-(Maester-Aemon)
## 2 Aeron-Greyjoy                  
## 3 Aerys-II-Targaryen             
## # ... with 97 more rows</code></pre>
</div>
</div>
<div id="the-complete-code" class="section level2">
<h2>The complete code</h2>
<p>Now let’s combine what we’ve done above in true tidyverse fashion:</p>
<pre class="r"><code>cooc_all_f_graph &lt;- as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  mutate(n_rank_trv = node_rank_traveller(),
         neighbors = centrality_degree(),
         group = group_infomap(),
         center = node_is_center(),
         dist_to_center = node_distance_to(node_is_center()),
         keyplayer = node_is_keyplayer(k = 10)) %&gt;%
  activate(edges) %&gt;% 
  filter(!edge_is_multiple()) %&gt;%
  mutate(centrality_e = centrality_edge_betweenness())</code></pre>
<p>We can also convert our active node or edge table back to a <code>tibble</code>:</p>
<pre class="r"><code>cooc_all_f_graph %&gt;%
  activate(nodes) %&gt;% # %N&gt;%
  as.tibble()</code></pre>
<pre><code>## # A tibble: 100 x 7
##    name         n_rank_trv neighbors group center dist_to_center keyplayer
##    &lt;chr&gt;             &lt;int&gt;     &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt;           &lt;dbl&gt; &lt;lgl&gt;    
##  1 Aemon-Targa…          7       13.     2 FALSE              1. FALSE    
##  2 Aeron-Greyj…         85        5.     5 FALSE              2. FALSE    
##  3 Aerys-II-Ta…         59       12.     1 FALSE              1. FALSE    
##  4 Alliser-Tho…          6       13.     2 FALSE              1. FALSE    
##  5 Arianne-Mar…         95        4.     7 FALSE              2. FALSE    
##  6 Arya-Stark           34       37.     1 FALSE              1. FALSE    
##  7 Asha-Greyjoy         87        7.     5 FALSE              1. FALSE    
##  8 Balon-Greyj…         86       11.     5 FALSE              2. FALSE    
##  9 Barristan-S…         91       23.     3 FALSE              1. FALSE    
## 10 Belwas               42        6.     3 FALSE              2. FALSE    
## # ... with 90 more rows</code></pre>
<pre class="r"><code>cooc_all_f_graph %&gt;%
  activate(edges) %&gt;% # %E&gt;%
  as.tibble()</code></pre>
<pre><code>## # A tibble: 798 x 6
##     from    to Type          id weight centrality_e
##    &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;        &lt;dbl&gt;
##  1     1     4 Undirected    43      7         1.00
##  2     1    13 Undirected    44      4        30.2 
##  3     1    28 Undirected    52      3        42.1 
##  4     1    32 Undirected    53     20         0.  
##  5     1    34 Undirected    54      5        35.2 
##  6     1    41 Undirected    56      5        18.9 
##  7     1    42 Undirected    57     25         0.  
##  8     1    48 Undirected    58    110         0.  
##  9     1    58 Undirected    60      5        24.5 
## 10     1    71 Undirected    62      5        17.0 
## # ... with 788 more rows</code></pre>
<div id="plotting-with-ggraph" class="section level3">
<h3>Plotting with ggraph</h3>
<p>For plotting our graph object, we can make good use of the <code>ggraph</code> package:</p>
<blockquote>
<p>ggraph is an extension of ggplot2 aimed at supporting relational data structures such as networks, graphs, and trees. While it builds upon the foundation of ggplot2 and its API it comes with its own self-contained set of geoms, facets, etc., as well as adding the concept of layouts to the grammar. <a href="https://github.com/thomasp85/ggraph" class="uri">https://github.com/thomasp85/ggraph</a></p>
</blockquote>
<p>First, I am going to define a layout. There are lots of <a href="https://www.data-imaginist.com/2017/ggraph-introduction-layouts/">options for layouts</a>, here I am using a <a href="http://igraph.org/r/doc/layout_with_fr.html">Fruchterman-Reingold</a> algorithm.</p>
<pre class="r"><code>layout &lt;- create_layout(cooc_all_f_graph, 
                        layout = &quot;fr&quot;)</code></pre>
<p>The rest works like any <code>ggplot2</code> function call, just that we use special geoms for our network, like <code>geom_edge_density()</code> to draw a shadow where the edge density is higher, <code>geom_edge_link()</code> to connect edges with a straight line, <code>geom_node_point()</code> to draw node points and <code>geom_node_text()</code> to draw the labels. More options can be found <a href="https://github.com/thomasp85/ggraph">here</a>.</p>
<p>Here are three options of plotting the network with the metrics we just calculated:</p>
<pre class="r"><code>ggraph(layout) + 
    geom_edge_density(aes(fill = weight)) +
    geom_edge_link(aes(width = weight), alpha = 0.2) + 
    geom_node_point(aes(color = factor(group)), size = 10) +
    geom_node_text(aes(label = name), size = 8, repel = TRUE) +
    scale_color_brewer(palette = &quot;Set1&quot;) +
    theme_graph() +
    labs(title = &quot;A Song of Ice and Fire character network&quot;,
         subtitle = &quot;Nodes are colored by group&quot;)</code></pre>
<p><img src="/post/2018-03-03_got_network_files/figure-html/unnamed-chunk-18-1.png" width="2880" /></p>
<p>Interestingly, many of the groups reflect the narrative perfectly: the men from the Night’s Watch are grouped together with the Wildlings, Stannis, Davos, Selyse and Melisandre form another group, the Greyjoys, Bran’s group in Winterfell before they left for the North, Dany and her squad and the Martells (except for Quentyn, who “belongs” to Dany - just like in the books ;-)). The big group around the remaining characters is the only one that’s not split up very well.</p>
<p>For the next graphs, I want specific colors form the <code>RColorBrewer</code> palette “Set1”:</p>
<pre class="r"><code>cols &lt;- RColorBrewer::brewer.pal(3, &quot;Set1&quot;)</code></pre>
<pre class="r"><code>ggraph(layout) + 
    geom_edge_density(aes(fill = weight)) +
    geom_edge_link(aes(width = weight), alpha = 0.2) + 
    geom_node_point(aes(color = factor(center), size = dist_to_center)) +
    geom_node_text(aes(label = name), size = 8, repel = TRUE) +
    scale_colour_manual(values = c(cols[2], cols[1])) +
    theme_graph() +
    labs(title = &quot;A Song of Ice and Fire character network&quot;,
         subtitle = &quot;Nodes are colored by centeredness&quot;)</code></pre>
<p><img src="/post/2018-03-03_got_network_files/figure-html/unnamed-chunk-20-1.png" width="2880" /></p>
<p>In the next graph I plotted the center-most characters in red and the distance to center as node size. The two center characters across all books are Robert Baratheon and Tyrion Lannister. I had not expected Robert, since he dies pretty much right away but I guess he and his rebellion following Lyanna’s “abduction” is the main trigger for most of what happens in the books, so why not… And that Tyrion is the best character (and George RR Martin’s favorite) is a given, anyways! ;-)</p>
</div>
</div>
<div id="characters-devided-by-books" class="section level2">
<h2>Characters devided by books</h2>
<p>The second data set I am going to use is a comparison of character interactions in the five books.</p>
<p><strong>A little node on the side:</strong> My original plan was to loop over the separate edge files for each book, concatenate them together with the information from which book they are and then plot them via faceting. This turned out to be a bad solution because I wanted to show the different key-players in each of the five books. So, instead of using one joined graph, I created separate graphs for every book and used the <code>bind_graphs()</code> and <code>facet_nodes()</code> functions to plot them together.</p>
<pre class="r"><code>for (i in 1:5) {
  cooc &lt;- read_csv(paste0(&quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book&quot;, i, &quot;-edges.csv&quot;)) %&gt;%
    mutate(book = paste0(&quot;book_&quot;, i)) %&gt;%
    filter(Source %in% main_ch_l$name &amp; Target %in% main_ch_l$name)
  
  assign(paste0(&quot;coocs_book_&quot;, i), cooc)
}</code></pre>
<p>The concepts are the same as above, here I want to know the key-players in each book:</p>
<pre class="r"><code>cooc_books_1_graph &lt;- as_tbl_graph(coocs_book_1, directed = FALSE) %&gt;%
  mutate(book = &quot;Book 1: A Game of Thrones&quot;,
         keyplayer = node_is_keyplayer(k = 10))

cooc_books_2_graph &lt;- as_tbl_graph(coocs_book_2, directed = FALSE) %&gt;%
  mutate(book = &quot;Book 2: A Clash of Kings&quot;,
         keyplayer = node_is_keyplayer(k = 10))

cooc_books_3_graph &lt;- as_tbl_graph(coocs_book_3, directed = FALSE) %&gt;%
  mutate(book = &quot;Book 3: A Storm of Swords&quot;,
         keyplayer = node_is_keyplayer(k = 10))

cooc_books_4_graph &lt;- as_tbl_graph(coocs_book_4, directed = FALSE) %&gt;%
  mutate(book = &quot;Book 4: A Feast for Crows&quot;,
         keyplayer = node_is_keyplayer(k = 10))

cooc_books_5_graph &lt;- as_tbl_graph(coocs_book_5, directed = FALSE) %&gt;%
  mutate(book = &quot;Book 5: A Dance with Dragons&quot;,
         keyplayer = node_is_keyplayer(k = 10))</code></pre>
<p>And let’s combine and plot the key-players:</p>
<pre class="r"><code>cooc_books_1_graph %&gt;% 
  bind_graphs(cooc_books_2_graph)  %&gt;%
  bind_graphs(cooc_books_3_graph)  %&gt;%
  bind_graphs(cooc_books_4_graph)  %&gt;%
  bind_graphs(cooc_books_5_graph)  %&gt;%
  ggraph(layout = &quot;fr&quot;) + 
    facet_nodes( ~ book, scales = &quot;free&quot;, ncol = 1) +
    geom_edge_density(aes(fill = weight)) +
    geom_edge_link(aes(edge_width = weight), alpha = 0.2) + 
    geom_node_point(aes(color = factor(keyplayer)), size = 3) +
    geom_node_text(aes(label = name), color = &quot;black&quot;, size = 3, repel = TRUE) +
    theme_graph() +
    scale_colour_manual(values = c(cols[2], cols[1]))</code></pre>
<p><img src="/post/2018-03-03_got_network_files/figure-html/unnamed-chunk-23-1.png" width="960" /></p>
<p>The networks and key-players of the five different books also offer a few surprises but also a lot that reflects the narrative quite well. I’m not going to go into details here as that would go a bit too far for an R-related blog - but if you are interested in in-depth discussions about the books, email me… ;-)</p>
</div>
<div id="more-info" class="section level2">
<h2>More info</h2>
<p>You can find more info about</p>
<ul>
<li><code>tidygraph</code> <a href="https://cran.r-project.org/web/packages/tidygraph">here</a></li>
<li><code>ggraph</code> <a href="https://cran.r-project.org/web/packages/ggraph">here</a></li>
<li><code>influenceR</code> <a href="https://cran.r-project.org/web/packages/influenceR">here</a></li>
<li>and DataCamp has a Python project for the same data set <a href="https://www.datacamp.com/projects/76?utm_campaign=broadcast&amp;utm_medium=broadcast_8&amp;utm_source=main">here</a></li>
</ul>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.4
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] methods   stats     graphics  grDevices utils     datasets  base     
## 
## other attached packages:
##  [1] bindrcpp_0.2       ggraph_1.0.1       tidygraph_1.1.0   
##  [4] forcats_0.3.0      stringr_1.3.0      dplyr_0.7.4       
##  [7] purrr_0.2.4        tidyr_0.8.0        tibble_1.4.2      
## [10] ggplot2_2.2.1.9000 tidyverse_1.2.1    readr_1.1.1       
## 
## loaded via a namespace (and not attached):
##  [1] nlme_3.1-131.1     bitops_1.0-6       lubridate_1.7.3   
##  [4] RColorBrewer_1.1-2 httr_1.3.1         prabclus_2.2-6    
##  [7] rprojroot_1.3-2    tools_3.4.3        backports_1.1.2   
## [10] utf8_1.1.3         R6_2.2.2           KernSmooth_2.23-15
## [13] lazyeval_0.2.1     colorspace_1.3-2   trimcluster_0.1-2 
## [16] nnet_7.3-12        withr_2.1.1.9000   tidyselect_0.2.4  
## [19] gridExtra_2.3      mnormt_1.5-5       compiler_3.4.3    
## [22] cli_1.0.0          rvest_0.3.2        TSP_1.1-5         
## [25] influenceR_0.1.0   xml2_1.2.0         labeling_0.3      
## [28] bookdown_0.7       diptest_0.75-7     caTools_1.17.1    
## [31] scales_0.5.0.9000  DEoptimR_1.0-8     robustbase_0.92-8 
## [34] mvtnorm_1.0-7      psych_1.7.8        digest_0.6.15     
## [37] foreign_0.8-69     rmarkdown_1.8      pkgconfig_2.0.1   
## [40] htmltools_0.3.6    rlang_0.2.0.9000   readxl_1.0.0      
## [43] rstudioapi_0.7     bindr_0.1          jsonlite_1.5      
## [46] mclust_5.4         gtools_3.5.0       dendextend_1.7.0  
## [49] magrittr_1.5       modeltools_0.2-21  Rcpp_0.12.15      
## [52] munsell_0.4.3      viridis_0.5.0      stringi_1.1.6     
## [55] whisker_0.3-2      yaml_2.1.17        MASS_7.3-49       
## [58] flexmix_2.3-14     gplots_3.0.1       plyr_1.8.4        
## [61] grid_3.4.3         parallel_3.4.3     gdata_2.18.0      
## [64] ggrepel_0.7.0      crayon_1.3.4       udunits2_0.13     
## [67] lattice_0.20-35    haven_1.1.1        hms_0.4.1         
## [70] knitr_1.20         pillar_1.2.1       igraph_1.1.2      
## [73] fpc_2.1-11         stats4_3.4.3       reshape2_1.4.3    
## [76] codetools_0.2-15   glue_1.2.0         gclus_1.3.1       
## [79] evaluate_0.10.1    blogdown_0.5       modelr_0.1.1      
## [82] tweenr_0.1.5       foreach_1.4.4      cellranger_1.1.0  
## [85] gtable_0.2.0       kernlab_0.9-25     assertthat_0.2.0  
## [88] xfun_0.1           ggforce_0.1.1      broom_0.4.3       
## [91] class_7.3-14       viridisLite_0.3.0  seriation_1.2-3   
## [94] iterators_1.0.9    registry_0.5       units_0.5-1       
## [97] cluster_2.0.6</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #111: Learning “Common Sense” and Physical Concepts with Roland Memisevic]]></title>
    <link href="/2018/02/twimlai111/"/>
    <id>/2018/02/twimlai111/</id>
    <published>2018-02-19T00:00:00+00:00</published>
    <updated>2018-02-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Learning “Common Sense” and Physical Concepts with Roland Memisevic</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai111.jpg" alt="Sketchnotes from TWiMLAI talk #111: Learning Common Sense and Physical Concepts with Roland Memisevic" />
<p class="caption">Sketchnotes from TWiMLAI talk #111: Learning “Common Sense” and Physical Concepts with Roland Memisevic</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-111-learning-common-sense-physical-concepts-roland-memisevic/">here</a>.</p>
<blockquote>
<p>In today’s episode, I’m joined by Roland Memisevic, co-founder, CEO, and chief scientist at Twenty Billion Neurons. Roland joined me at the RE•WORK Deep Learning Summit in Montreal to discuss the work his company is doing to train deep neural networks to understand physical actions. In our conversation, we dig into video analysis and understanding, including how data-rich video can help us develop what Roland calls comparative understanding, or AI “common sense”. We briefly touch on the implications of AI/ML systems having comparative understanding, and how Roland and his team are addressing problems like getting properly labeled training data. <a href="https://twimlai.com/twiml-talk-111-learning-common-sense-physical-concepts-roland-memisevic/" class="uri">https://twimlai.com/twiml-talk-111-learning-common-sense-physical-concepts-roland-memisevic/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[April 12th &amp; 13th in Hamburg: Workshop on Deep Learning with Keras and TensorFlow in R]]></title>
    <link href="/2018/02/deep_learning_keras_tensorflow_18_04/"/>
    <id>/2018/02/deep_learning_keras_tensorflow_18_04/</id>
    <published>2018-02-06T00:00:00+00:00</published>
    <updated>2018-02-06T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Registration is now open for my 1.5-day <a href="https://www.eventbrite.de/e/workshop-deep-learning-mit-keras-und-tensorflow-tickets-42710095044?utm-medium=discovery&amp;utm-campaign=social&amp;utm-content=attendeeshare&amp;aff=escb&amp;utm-source=cp&amp;utm-term=listing">workshop on deep learning with Keras and TensorFlow using R</a>.</p>
<p>It will take place on <strong>April 12th and 13th</strong> in <strong>Hamburg, Germany</strong>.</p>
<p><a href="https://blog.codecentric.de/en/2018/02/deep-learning-workshop-bei-der-codecentric-ag-solingen/">You can read about one participant’s experience in my last workshop:</a></p>
<blockquote>
<p>Big Data – a buzz word you can find everywhere these days, from nerdy blogs to scientific research papers and even in the news. But how does Big Data Analysis work, exactly? In order to find that out, I attended the workshop on “Deep Learning with Keras and TensorFlow”. On a stormy Thursday afternoon, we arrived at the modern and light-flooded codecentric AG headquarters. There, we met performance expert Dieter Dirkes and Data Scientist Dr. Shirin Glander. In the following two days, Shirin gave us a hands-on introduction into the secrets of Deep Learning and helped us to program our first Neural Net. After a short round of introduction of the participants, it became clear that many different areas and domains are interested in Deep Learning: geologists want to classify (satellite) images, energy providers want to analyse time-series, insurers want to predict numbers and I – a humanities major – want to classify text. And codecentric employees were also interested in getting to know the possibilities of Deep Learning, so that a third of the participants were employees from the company itself.</p>
</blockquote>
<p><a href="https://blog.codecentric.de/en/2018/02/deep-learning-workshop-bei-der-codecentric-ag-solingen/">Continue reading…</a></p>
<p>In my workshop, you will learn</p>
<ul>
<li>the basics of deep learning</li>
<li>what cross-entropy and loss is</li>
<li>about activation functions</li>
<li>how to optimize weights and biases with backpropagation and gradient descent</li>
<li>how to build (deep) neural networks with Keras and TensorFlow</li>
<li>how to save and load models and model weights</li>
<li>how to visualize models with TensorBoard</li>
<li>how to make predictions on test data</li>
</ul>
<p>My slides and all material is in English, so I’m flexible with the language. If it’s all German participants it’ll be in German, if some prefer English it’ll be English or a mix of German and English so that everybody understands. :-)</p>
<p><a href="https://www.eventbrite.de/e/workshop-deep-learning-mit-keras-und-tensorflow-tickets-42710095044?utm-medium=discovery&amp;utm-campaign=social&amp;utm-content=attendeeshare&amp;aff=escb&amp;utm-source=cp&amp;utm-term=listing">Tickets can be booked via eventbrite</a>.</p>
<p><br></p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/keras_workshop_april18.png" />

</div>
<p>Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. Keras is very convenient for fast and easy prototyping of neural networks. It is highly modular and very flexible, so that you can build basically any type of neural network you want. It supports convolutional neural networks and recurrent neural networks, as well as combinations of both. Due to its layer structure, it is highly extensible and can run on CPU or GPU.</p>
<p>The <code>keras</code> R package provides an interface to the Python library of Keras, just as the tensorflow package provides an interface to TensorFlow. Basically, R creates a conda instance and runs Keras it it, while you can still use all the functionalities of R for plotting, etc. Almost all function names are the same, so models can easily be recreated in Python for deployment.</p>
<p><br></p>
<div class="figure">
<img src="https://blog.keras.io/img/keras-tensorflow-logo.jpg" />

</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Conferences, webinars, podcasts and the likes]]></title>
    <link href="/page/conferences_podcasts_webinars/"/>
    <id>/page/conferences_podcasts_webinars/</id>
    <published>2018-02-01T16:06:06+02:00</published>
    <updated>2018-02-01T16:06:06+02:00</updated>
    <content type="html"><![CDATA[

<p>Here, you can find a list of all the talks I gave at conferences, webinars, podcasts, workshops, and all the other places you can and could hear me talk. :-)</p>

<p><img src="https://secure.meetupstatic.com/s/img/5455565085016210254/logo/svg/logo--script.svg" alt="" /></p>

<h2 id="workshops-i-am-giving">Workshops I am giving</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2017/11/deep_learning_keras_tensorflow/">Workshop on Deep Learning with Keras and TensorFlow in R</a></li>
</ul>

<blockquote>
<p>I offer a workshop on deep learning with Keras and TensorFlow using R.
Date and place depend on who and how many people are interested, so please contact me either directly or via the workshop page: <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/">https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/</a> (the description is in German but I also offer to give the workshop in English).</p>
</blockquote>

<p><a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/">Official link</a></p>

<p><br></p>

<ul>
<li><a href="https://www.codecentric.de/schulung/end-2-end-vom-keras-tensorflow-modell-zur-produktion/#schulung-detail">Workshop: End-2-End vom Keras TensorFlow-Modell zur Produktion</a></li>
</ul>

<blockquote>
<p>Durch das stark wachsende Datenvolumen hat sich das Rollenverständnis von Data Scientists erweitert. Statt Machine-Learning-Modelle für einmalige Analysen zu erstellen,  wird häufiger in konkreten Entwicklungsprojekten gearbeitet, in denen Prototypen in produktive Anwendungen überführt werden.
Keras ist eine High-Level-Schnittstelle, die ein schnelles, einfaches und flexibles Prototypisieren von Neuronalen Netzwerken mit TensorFlow ermöglicht. Zusammen mit Luigi lassen sich beliebig komplexe Datenverarbeitungs-Workflows in Python erstellen. Das führt dazu, dass auch Nicht-Entwickler den End-2-End-Workflow des Keras-TensorFlow-Modells zur Produktionsreife leicht
implementieren können.</p>
</blockquote>

<p><br></p>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/06/intro_to_ml_workshop_heidelberg/">Machine Learning Basics in R</a></li>
</ul>

<hr />

<p><br></p>

<h2 id="upcoming-talks-webinars-podcasts-etc">Upcoming talks, webinars, podcasts, etc.</h2>

<ul>
<li><p>Together with a colleague from codecentric, I&rsquo;ll be giving a workshop about <a href="https://www.data2day.de/veranstaltung-6953-end-2-end-vom-keras-tensorflow-modell-zur-produktion.html?id=6953">&ldquo;END-2-END VOM KERAS TENSORFLOW-MODELL ZUR PRODUKTION&rdquo;</a> at the data2day conference, which is being held from September 25th - 27th 2018 in Heidelberg, Germany (German language):
<a href="https://www.data2day.de/veranstaltung-6953-end-2-end-vom-keras-tensorflow-modell-zur-produktion.html?id=6953">https://www.data2day.de/veranstaltung-6953-end-2-end-vom-keras-tensorflow-modell-zur-produktion.html?id=6953</a></p></li>

<li><p>At the <a href="www.ml-summit.de">ML Summit</a> held on October 1st and 2nd in Berlin, Germany, I&rsquo;ll be giving a workshop about <a href="https://ml-summit.de/specialized-topics/bildklassifikation-leicht-gemacht-mit-keras-und-tensorflow/">image classification with Keras</a>: <a href="https://ml-summit.de/specialized-topics/bildklassifikation-leicht-gemacht-mit-keras-und-tensorflow/">https://ml-summit.de/specialized-topics/bildklassifikation-leicht-gemacht-mit-keras-und-tensorflow/</a> (German language)</p></li>

<li><p>From 15th to 17th October 2018, I&rsquo;ll be in London for the <a href="https://www.mcubed.london/">M-cubed conference</a>. My talk about <a href="https://www.mcubed.london/sessions/explaining-complex-machine-learning-models-lime/">Explaining complex machine learning models with LIME</a> will be on October 16</p></li>
</ul>

<hr />

<p><br></p>

<h2 id="past-talks-webinars-podcasts-etc">Past talks, webinars, podcasts, etc.</h2>

<ul>
<li>In August 2018 I gave a webinar for SAP about <a href="https://shirinsplayground.netlify.com/2018/08/sap_webinar_slides/">Explaining Keras Image Classification Models with LIME</a>.</li>
</ul>

<p><br></p>

<ul>
<li>In June 2018 I gave a 3-hour workshop about the basics of machine learning with R at the University of Heidelberg, Germany. <a href="https://shirinsplayground.netlify.com/2018/06/intro_to_ml_workshop_heidelberg/">Slides and workshop code can be found here</a>.</li>
</ul>

<p><br></p>

<ul>
<li>In May 2018 I was at the <a href="http://unconf18.ropensci.org/">ROpenSci unconference in Seattle, WA</a></li>
</ul>

<p>You can read about my experience and the project I worked on <a href="https://shirinsplayground.netlify.com/2018/05/ropensci_unconf18/">here</a>.</p>

<p><br></p>

<ul>
<li>At the <a href="https://aws.amazon.com/de/events/web-days-2018/">Amazon AWS AI &amp; Machine Learning Web Day on May 8th</a>, I gave a presentation on how to get started with Amazon SageMaker. The recording can be found on <a href="https://youtu.be/lQmcXAN7GJ4">YouTube</a>; slides are on <a href="https://www.slideshare.net/AWSAktuell/ai-machine-learning-web-day-wie-gelingt-der-einstieg-in-amazon-sagemaker-prsentiert-von-codecentric">Slideshare</a></li>
</ul>

<p><br></p>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/02/m3_2018/">I talked about explaining complex machine learning models at Minds Mastering Machines Conference</a> on Wednesday, April 25th 2018 in Colone</li>
</ul>

<p>The presentation was in German but the slides were similar to those: <a href="https://shirinsplayground.netlify.com/2018/04/hh_datascience_meetup_2018_slides/">https://shirinsplayground.netlify.com/2018/04/hh_datascience_meetup_2018_slides/</a></p>

<p><br></p>

<ul>
<li>My colleague Uwe Friedrichsen and I gave a talk at the <a href="https://shirinsplayground.netlify.com/2018/01/jax2018/">JAX conference 2018: Deep Learning - a Primer</a> on April 24th 2018 in Mainz</li>
</ul>

<p>Slides can be found here: <a href="https://www.slideshare.net/ShirinGlander/deep-learning-a-primer-95197733">https://www.slideshare.net/ShirinGlander/deep-learning-a-primer-95197733</a></p>

<blockquote>
<p>Deep Learning is one of the &ldquo;hot&rdquo; topics in the AI area – a lot of hype, a lot of inflated expectation, but also quite some impressive success stories. As some AI experts already predict that Deep Learning will become &ldquo;Software 2.0&rdquo;, it might be a good time to have a closer look at the topic. In this session I will try to give a comprehensive overview of Deep Learning. We will start with a bit of history and some theoretical foundations that we will use to create a little Deep Learning taxonomy. Then we will have a look at current and upcoming application areas: Where can we apply Deep Learning successfully and what does it differentiate from other approaches? Afterwards we will examine the ecosystem: Which tools and libraries are available? What are their strengths and weaknesses? And to complete the session, we will look into some practical code examples and the typical pitfalls of Deep Learning. After this session you will have a much better idea of the why, what and how of Deep Learning, including if and how you might want to apply it to your own work. <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/">https://jax.de/big-data-machine-learning/deep-learning-a-primer/</a></p>
</blockquote>

<p><br></p>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/03/meetup_talk_ruhrpy_april_18/">I talked about explaining complex Machine Learning Models with LIME</a> at this meetup: <a href="https://www.meetup.com/Hamburg-Data-Science-Meetup/events/244145443/">Data Science Meetup Hamburg</a> on Thursday, April 12th 2018</li>
</ul>

<p>Slides can be found here: <a href="https://shirinsplayground.netlify.com/2018/04/hh_datascience_meetup_2018_slides/">https://shirinsplayground.netlify.com/2018/04/hh_datascience_meetup_2018_slides/</a></p>

<blockquote>
<p>Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex machine learning models are essentially black boxes and too complicated to understand, we need to use approximations to get a better sense of how they work. One such approach is LIME, which stands for Local Interpretable Model-agnostic Explanations and is a tool that helps understand and explain the decisions made by complex machine learning models.
Dr. Shirin Glander is Data Scientist at codecentric AG. She has received a PhD in Bioinformatics and applies methods of analysis and visualization from different areas - for instance, machine learning, classical statistics, text mining, etc. -to extract and leverage information from data.</p>
</blockquote>

<p><br></p>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/03/meetup_talk_ruhrpy_april_18/">I talked about Deep Learning with Keras in R and Python</a> at this meetup: <a href="https://www.meetup.com/Ruhr-py/events/248093628/">Ruhr.Py 2018</a> on Wednesday, April 4th 2018</li>
</ul>

<blockquote>
<p>Introducing Deep Learning with Keras and Python
Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. In this talk we build, train and visualize a Model using Python and Keras - all interactive with Jupyter Notebooks!</p>
</blockquote>

<p>Slides can be found here: <a href="https://shirinsplayground.netlify.com/2018/04/ruhrpy_meetup_2018_slides/">https://shirinsplayground.netlify.com/2018/04/ruhrpy_meetup_2018_slides/</a></p>

<p><br></p>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/02/herr_mies_wills_wissen/">I talked about machine learning with Daniel Mies (Podcast in German, though)</a></li>
</ul>

<blockquote>
<p>In January 2018 I was interviewed for a tech podcast where I talked about machine learning, neural nets, why I love R and Rstudio and how I became a Data Scientist.</p>
</blockquote>

<p><br></p>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/">Explaining Predictions of Machine Learning Models with LIME - Münster Data Science Meetup</a></li>
</ul>

<blockquote>
<p>In December 2017 I talked about Explaining Predictions of Machine Learning Models with LIME at the Münster Data Science Meetup.</p>
</blockquote>

<p><br></p>

<ul>
<li><a href="https://shiring.github.io/blogging/2017/09/20/webinar_biology_to_data_science">From Biology to Industry. A Blogger’s Journey to Data Science</a></li>
</ul>

<blockquote>
<p>In September 2017 I gave a webinar for the Applied Epidemiology Didactic of the University of Wisconsin - Madison titled “From Biology to Industry. A Blogger’s Journey to Data Science.”
I talked about how blogging about R and Data Science helped me become a Data Scientist. I also gave a short introduction to Machine Learning, Big Data and Neural Networks.</p>
</blockquote>

<p><br></p>

<ul>
<li><a href="https://shiring.github.io/machine_learning/2017/03/31/webinar_code">Building meaningful machine learning models for disease prediction</a></li>
</ul>

<blockquote>
<p>In March 2017 I gave a webinar for the ISDS R Group about my work on building machine-learning models to predict the course of different diseases. I went over building a model, evaluating its performance, and answering or addressing different disease related questions using machine learning. My talk covered the theory of machine learning as it is applied using R.</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Announcing my talk about explainability of machine learning models at Minds Mastering Machines conference]]></title>
    <link href="/2018/02/m3_2018/"/>
    <id>/2018/02/m3_2018/</id>
    <published>2018-02-01T00:00:00+00:00</published>
    <updated>2018-02-01T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>On Wednesday, April 25th 2018 I am going to <a href="https://www.m3-konferenz.de/veranstaltung-6274-erkl%C3%A4rbarkeit-von-machine-learning%3A-wie-k%C3%B6nnen-wir-vertrauen-in-komplexe-modelle-schaffen.html?id=6274">talk about explainability of machine learning models at the Minds Mastering Machines conference in Cologne</a>. The conference will be in German, though.</p>
<div class="figure">
<img src="https://www.m3-konferenz.de/common/images/konferenzen/m3.png" />

</div>
<blockquote>
<p>ERKLÄRBARKEIT VON MACHINE LEARNING: WIE KÖNNEN WIR VERTRAUEN IN KOMPLEXE MODELLE SCHAFFEN?</p>
</blockquote>
<blockquote>
<p>Mit Machine-Learning getroffene Entscheidungen sind inhärent schwierig – wenn nicht gar unmöglich – nachzuvollziehen. Die Komplexität einiger der besten Modelle, wie Neuronale Netzwerke, ist genau das, was sie so erfolgreich macht. Aber es macht sie gleichzeitig zu einer Black Box. Das kann problematisch sein, denn Geschäftsführer oder Vorstände werden weniger geneigt sein einer Entscheidung zu vertrauen und nach ihr zu handeln, wenn sie sie nicht verstehen.</p>
</blockquote>
<blockquote>
<p>Local Interpretable Model-Agnostic Explanations (LIME) ist ein Versuch, diese komplexen Modelle zumindest teilweise nachvollziehbar zu machen. In diesem Vortrag erkläre ich das Prinzip und zeige Anwendungsbeispiele von LIME.</p>
</blockquote>
<blockquote>
<p>Vorkenntnisse Grundkenntnisse Machine Learning &amp; Statistik</p>
</blockquote>
<blockquote>
<p>Lernziele * Einblick in Möglichkeit, die komplexe Modelle erklärbar machen * Vertrauen in Entscheidungen durch Machine Learning schaffen</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[I talk about machine learning with Daniel Mies (Podcast in German, though)]]></title>
    <link href="/2018/02/herr_mies_wills_wissen/"/>
    <id>/2018/02/herr_mies_wills_wissen/</id>
    <published>2018-02-01T00:00:00+00:00</published>
    <updated>2018-02-01T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>For those of you out there who speak German:</p>
<p>I was interviewed for a tech podcast where I talked about machine learning, neural nets, why I love R and Rstudio and how I became a Data Scientist.</p>
<p>You can download and listen to the podcast here: <a href="https://mies.me/2018/01/31/hmww17-machine-learning-mit-dr-shirin-glander/" class="uri">https://mies.me/2018/01/31/hmww17-machine-learning-mit-dr-shirin-glander/</a></p>
<div class="figure">
<img src="https://mies.me/wp-content/cache/podlove/09/cad1c2bcc3b506410d277c27cc12fb/herr-mies-wills-wissen_500x500.png" />

</div>
<blockquote>
<p>In der aktuellen Episode gibt Dr. Shirin Glander (Twitter, Homepage) uns ein paar Einblicke in das Thema Machine Learning. Wir klären zunächst, was Machine Learning ist und welche Möglichkeiten es bietet bevor wir etwas mehr in die Tiefe gehen. Wir beginnen mit Neuronalen Netzen und Entscheidungsbäumen und wie sich diese unterschieden. Hier kommen wir natürlich auch nicht an Supervised Learning, Unsupervised Learning und Reinforcement Learning vorbei. Wichtig bei der Arbeit mit Machine Learning sind die verwendeten Daten: Hier beginnt man mit Testdaten und Trainingsdaten, welche man mit Hilfe von Feature Engineering für die jeweilige Aufgabe optimieren kann. Shirin erzählt, wie sie mit Daten arbeitet und wie sie die richtigen Algorithmen findet. Eine wichtige Rolle spielen hier R und R Studio, welches sich besonders für statistische Analysen eignet. Gerade die Visualisierung der Daten ist hier hilfreich um selbige besser zu verstehen. Aber auch die Möglichkeiten Reports zu erzeugen und beispielsweise als PDF zu exportieren überzeugen. Wenn ihr R für Machine Learning einsetzen wollt, solltet ihr Euch auch caret ansehen. Shirin organisiert übrigens auch MünsteR, die R Users group in Münster. Wenn ihr Euch näher mit Machine Learning beschäftigen wollt, solltet ihr Euch Datacamp oder Coursera ansehen. Wenn ihr Euch für R interessiert schaut Euch die R Bloggers an Am Ende sprechen wir auch noch kurz über Deep Dreaming. Den passenden Generator hierfür, findet ihr unter deepdreamgenerator.com.</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[JAX 2018 talk announcement: Deep Learning - a Primer]]></title>
    <link href="/2018/01/jax2018/"/>
    <id>/2018/01/jax2018/</id>
    <published>2018-01-30T00:00:00+00:00</published>
    <updated>2018-01-30T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I am happy to announce that on Tuesday, April 24th 2018 Uwe Friedrichsen and I will give a talk about <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/">Deep Learning - a Primer</a> at the JAX conference in Mainz, Germany.</p>
<blockquote>
<p>Deep Learning is one of the “hot” topics in the AI area – a lot of hype, a lot of inflated expectation, but also quite some impressive success stories. As some AI experts already predict that Deep Learning will become “Software 2.0”, it might be a good time to have a closer look at the topic. In this session I will try to give a comprehensive overview of Deep Learning. We will start with a bit of history and some theoretical foundations that we will use to create a little Deep Learning taxonomy. Then we will have a look at current and upcoming application areas: Where can we apply Deep Learning successfully and what does it differentiate from other approaches? Afterwards we will examine the ecosystem: Which tools and libraries are available? What are their strengths and weaknesses? And to complete the session, we will look into some practical code examples and the typical pitfalls of Deep Learning. After this session you will have a much better idea of the why, what and how of Deep Learning, including if and how you might want to apply it to your own work. <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/" class="uri">https://jax.de/big-data-machine-learning/deep-learning-a-primer/</a></p>
</blockquote>
<div class="figure">
<img src="https://pbs.twimg.com/media/DUt3SXyUQAE3TOv.jpg" alt="https://twitter.com/jaxcon/status/957990506331557890" />
<p class="caption"><a href="https://twitter.com/jaxcon/status/957990506331557890" class="uri">https://twitter.com/jaxcon/status/957990506331557890</a></p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #94: Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley]]></title>
    <link href="/2018/01/twimlai94/"/>
    <id>/2018/01/twimlai94/</id>
    <published>2018-01-28T00:00:00+00:00</published>
    <updated>2018-01-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai94.jpg" alt="Sketchnotes from TWiMLAI talk #94: Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley" />
<p class="caption">Sketchnotes from TWiMLAI talk #94: Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-94-neuroevolution-evolving-novel-neural-network-architectures-kenneth-stanley/">here</a>.</p>
<blockquote>
<p>Kenneth studied under TWiML Talk #47 guest Risto Miikkulainen at UT Austin, and joined Uber AI Labs after Geometric Intelligence , the company he co-founded with Gary Marcus and others, was acquired in late 2016. Kenneth’s research focus is what he calls Neuroevolution, applies the idea of genetic algorithms to the challenge of evolving neural network architectures. In this conversation, we discuss the Neuroevolution of Augmenting Topologies (or NEAT) paper that Kenneth authored along with Risto, which won the 2017 International Society for Artificial Life’s Award for Outstanding Paper of the Decade 2002 – 2012. We also cover some of the extensions to that approach he’s created since, including, HyperNEAT, which can efficiently evolve very large networks with connectivity patterns that look more like those of the human and that are generally much larger than what prior approaches to neural learning could produce, and novelty search, an approach which unlike most evolutionary algorithms has no defined objective, but rather simply searches for novel behaviors. We also cover concepts like “Complexification” and “Deception”, biology vs computation including differences and similarities, and some of his other work including his book, and NERO, a video game complete with Real-time Neuroevolution. This is a meaty “Nerd Alert” interview that I think you’ll really enjoy. <a href="https://twimlai.com/twiml-talk-94-neuroevolution-evolving-novel-neural-network-architectures-kenneth-stanley/" class="uri">https://twimlai.com/twiml-talk-94-neuroevolution-evolving-novel-neural-network-architectures-kenneth-stanley/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Join MünsteR for our next meetup on obtaining functional implications of gene expression data with R]]></title>
    <link href="/2018/01/meetup_march18/"/>
    <id>/2018/01/meetup_march18/</id>
    <published>2018-01-24T00:00:00+00:00</published>
    <updated>2018-01-24T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://shiring.github.io/r_users_group/2017/05/20/map.png" />

</div>
<p>In our <a href="http://meetu.ps/e/DDY1B/w54bW/f">next MünsteR R-user group meetup</a> on <strong>March 5th, 2018</strong> Frank Rühle will talk about bioinformatics and how to analyse genome data.</p>
<p>You can RSVP here: <a href="http://meetu.ps/e/DDY1B/w54bW/f" class="uri">http://meetu.ps/e/DDY1B/w54bW/f</a></p>
<blockquote>
<p>Next-Generation sequencing and array-based technologies provided a plethora of gene expression data in the public genomics databases. But how to get meaningful information and functional implications out of this vast amount of data? Various R-packages have been published by the Bioconductor user community for distinct kinds of analysis strategies. Here, several approaches will be presented for functional gene annotation, gene enrichment analysis and co-expression network analysis. A collection of wrapper functions for streamlined analysis of expression data can be found at: <a href="https://github.com/frankRuehle/systemsbio" class="uri">https://github.com/frankRuehle/systemsbio</a>.</p>
</blockquote>
<p>Dr. Frank Rühle is a post-doctoral research fellow in the group of genetic epidemiology at the Institute of Human Genetics at the University of Münster. As biologist with focus on computational biology he aims at identifying genomic biomarker for complex cardiovascular diseases by analyzing multiomics data with respect to a systems biology view. Further research interests include the functions of long non-coding RNAs and their impact on gene regulation in heart-related phenotypes.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #92: Learning State Representations with Yael Niv]]></title>
    <link href="/2018/01/twimlai92/"/>
    <id>/2018/01/twimlai92/</id>
    <published>2018-01-19T00:00:00+00:00</published>
    <updated>2018-01-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about Learning State Representations with Yael Niv: <a href="https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/" class="uri">https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/</a></p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai92.jpg" alt="Sketchnotes from TWiMLAI talk #92: Learning State Representations with Yael Niv" />
<p class="caption">Sketchnotes from TWiMLAI talk #92: Learning State Representations with Yael Niv</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/">here</a>.</p>
<blockquote>
<p>In this interview Yael and I explore the relationship between neuroscience and machine learning. In particular, we discusses the importance of state representations in human learning, some of her experimental results in this area, and how a better understanding of representation learning can lead to insights into machine learning problems such as reinforcement and transfer learning. Did I mention this was a nerd alert show? I really enjoyed this interview and I know you will too. Be sure to send over any thoughts or feedback via the show notes page. <a href="https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/" class="uri">https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[How to make your machine learning model available as an API with the plumber package]]></title>
    <link href="/2018/01/plumber/"/>
    <id>/2018/01/plumber/</id>
    <published>2018-01-16T00:00:00+00:00</published>
    <updated>2018-01-16T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://d33wubrfki0l68.cloudfront.net/03f3d9156c39a8043be42caeee1507d43d832472/8d968/components/images/plumber.png" />

</div>
<p>The <strong>plumber</strong> package for R makes it easy to expose existing R code as a webservice via an API (<a href="https://www.rplumber.io/" class="uri">https://www.rplumber.io/</a>, Trestle Technology, LLC 2017).</p>
<p>You take an existing R script and make it accessible with <code>plumber</code> by simply adding a few lines of comments. If you have worked with Roxygen before, e.g. when building a package, you will already be familiar with the core concepts. If not, here are the most important things to know:</p>
<ul>
<li>you define the output or endpoint</li>
<li>you can add additional annotation to customize your input, output and other functionalities of your API</li>
<li>you can define every input parameter that will go into your function</li>
<li>every such annotation will begin with either <code>#'</code> or <code>#*</code></li>
</ul>
<p>With this setup, we can take a trained machine learning model and make it available as an API. With this API, other programs can access it and use it to make predictions.</p>
<div id="what-are-apis-and-webservices" class="section level2">
<h2>What are APIs and webservices?</h2>
<p>With <code>plumber</code>, we can build so called <strong>HTTP APIs</strong>. HTTP stands for Hypertext Transfer Protocol and is used to transmit information on the web; API stands for Application Programming Interface and governs the connection between some software and underlying applications. Software can then communicate via HTTP APIs. This way, our R script can be called from other software, even if the other program is not written in R and we have built a tool for machine-to-machine communication, i.e. a webservice.</p>
</div>
<div id="how-to-convert-your-r-script-into-an-api-with-plumber" class="section level2">
<h2>How to convert your R script into an API with plumber</h2>
<div id="training-and-saving-a-model" class="section level3">
<h3>Training and saving a model</h3>
<p>Let’s say we have trained a machine learning model as in <a href="https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/">this post about LIME</a>. I loaded a data set on chronic kidney disease, did some preprocessing (converting categorical features into dummy variables, scaling and centering), split it into training and test data and trained a Random Forest model with <code>caret</code>. We can use this trained model to make predictions for one test case with the following code:</p>
<pre class="r"><code>library(tidyverse)

# load test and train data
load(&quot;../../data/test_data.RData&quot;)
load(&quot;../../data/train_data.RData&quot;)

# load model
load(&quot;../../data/model_rf.RData&quot;)

# take first test case for prediction
input_data &lt;- test_data[1, ] %&gt;%
  select(-class)

# predict test case using model
pred &lt;- predict(model_rf, input_data)
cat(&quot;----------------\nTest case predicted to be&quot;, as.character(pred), &quot;\n----------------&quot;)</code></pre>
<pre><code>## ----------------
## Test case predicted to be ckd 
## ----------------</code></pre>
</div>
<div id="the-input" class="section level3">
<h3>The input</h3>
<p>For our API to work, we need to define the input, in our case the features of the test data. When we look at the model object, we see that it expects the following parameters:</p>
<pre class="r"><code>var_names &lt;- model_rf$finalModel$xNames
var_names</code></pre>
<pre><code>##  [1] &quot;age&quot;            &quot;bp&quot;             &quot;sg_1.005&quot;       &quot;sg_1.010&quot;      
##  [5] &quot;sg_1.015&quot;       &quot;sg_1.020&quot;       &quot;sg_1.025&quot;       &quot;al_0&quot;          
##  [9] &quot;al_1&quot;           &quot;al_2&quot;           &quot;al_3&quot;           &quot;al_4&quot;          
## [13] &quot;al_5&quot;           &quot;su_0&quot;           &quot;su_1&quot;           &quot;su_2&quot;          
## [17] &quot;su_3&quot;           &quot;su_4&quot;           &quot;su_5&quot;           &quot;rbc_normal&quot;    
## [21] &quot;rbc_abnormal&quot;   &quot;pc_normal&quot;      &quot;pc_abnormal&quot;    &quot;pcc_present&quot;   
## [25] &quot;pcc_notpresent&quot; &quot;ba_present&quot;     &quot;ba_notpresent&quot;  &quot;bgr&quot;           
## [29] &quot;bu&quot;             &quot;sc&quot;             &quot;sod&quot;            &quot;pot&quot;           
## [33] &quot;hemo&quot;           &quot;pcv&quot;            &quot;wbcc&quot;           &quot;rbcc&quot;          
## [37] &quot;htn_yes&quot;        &quot;htn_no&quot;         &quot;dm_yes&quot;         &quot;dm_no&quot;         
## [41] &quot;cad_yes&quot;        &quot;cad_no&quot;         &quot;appet_good&quot;     &quot;appet_poor&quot;    
## [45] &quot;pe_yes&quot;         &quot;pe_no&quot;          &quot;ane_yes&quot;        &quot;ane_no&quot;</code></pre>
<p>Good practice is to write the input parameter definition into you <a href="https://swagger.io/swagger-ui/">API Swagger UI</a>, but the code would work without these annotations. We define the parameters by annotating them with name and description in our R-script using <code>@parameter</code>. For this purpose, I want to know the type and min/max values for each of my variables in the training data. Because categorical data has been converted to dummy variables and then scaled and centered, these values will all be numeric and between 0 and 1 in this example. If I would build this script for a real case, I’d use the raw data as input and add a preprocessing function to my script, though!</p>
<pre class="r"><code># show parameter definition for the first three features
for (i in 1:3) {
# if you wanted to see it for all features, use
#for (i in 1:length(var_names)) {
  var &lt;- var_names[i]
  train_data_subs &lt;- train_data[, which(colnames(train_data) == var)]
  type &lt;- class(train_data_subs)
  
  if (type == &quot;numeric&quot;) {
    min &lt;- min(train_data_subs)
    max &lt;- max(train_data_subs)
  }
  
  cat(&quot;Variable:&quot;, var, &quot;is of type:&quot;, type, &quot;\n&quot;,
      &quot;Min value in training data =&quot;, min, &quot;\n&quot;,
      &quot;Max value in training data =&quot;, max, &quot;\n----------\n&quot;)

}</code></pre>
<pre><code>## Variable: age is of type: numeric 
##  Min value in training data = 0 
##  Max value in training data = 0.9777778 
## ----------
## Variable: bp is of type: numeric 
##  Min value in training data = 0 
##  Max value in training data = 0.7222222 
## ----------
## Variable: sg_1.005 is of type: numeric 
##  Min value in training data = 0 
##  Max value in training data = 1 
## ----------</code></pre>
<blockquote>
<p>Unless otherwise instructed, all parameters passed into plumber endpoints from query strings or dynamic paths will be character strings. <a href="https://www.rplumber.io/docs/routing-and-input.html#typed-dynamic-routes" class="uri">https://www.rplumber.io/docs/routing-and-input.html#typed-dynamic-routes</a></p>
</blockquote>
<p>This means that we need to convert numeric values before we process them further. Or we can define the parameter type explicitly, e.g. by writing <code>variable_1:numeric</code> if we want to specifiy that <em>variable_1</em> is supposed to be numeric.</p>
<p>To make sure that the model will perform as expected, it is also advisable to add a few validation functions. Here, I will validate</p>
<ul>
<li>whether every parameter is numeric/integer by checking for NAs (which would have resulted from <code>as.numeric()</code>/<code>as.integer()</code> applied to data of character type)</li>
<li>whether every parameter is between 0 and 1</li>
</ul>
<p>In order for <code>plumber</code> to work with our input, it needs to be part of the HTTP request, which can then be routed to our R function. The <a href="https://www.rplumber.io/docs/routing-and-input.html#query-strings">plumber documentation</a> describes how to use query strings as inputs. But in our case, manually writing query strings is not practical because we have so many parameters. Of course, there are programs that let us generate query strings but the easiest way to format the input from a line of data I found is to use JSON.</p>
<p>The <code>toJSON()</code> function from the <code>rjson</code> package converts our input line to JSON format:</p>
<pre class="r"><code>library(rjson)
test_case_json &lt;- toJSON(input_data)
cat(test_case_json)</code></pre>
<pre><code>## {&quot;age&quot;:0.511111111111111,&quot;bp&quot;:0.111111111111111,&quot;sg_1.005&quot;:1,&quot;sg_1.010&quot;:0,&quot;sg_1.015&quot;:0,&quot;sg_1.020&quot;:0,&quot;sg_1.025&quot;:0,&quot;al_0&quot;:0,&quot;al_1&quot;:0,&quot;al_2&quot;:0,&quot;al_3&quot;:0,&quot;al_4&quot;:1,&quot;al_5&quot;:0,&quot;su_0&quot;:1,&quot;su_1&quot;:0,&quot;su_2&quot;:0,&quot;su_3&quot;:0,&quot;su_4&quot;:0,&quot;su_5&quot;:0,&quot;rbc_normal&quot;:1,&quot;rbc_abnormal&quot;:0,&quot;pc_normal&quot;:0,&quot;pc_abnormal&quot;:1,&quot;pcc_present&quot;:1,&quot;pcc_notpresent&quot;:0,&quot;ba_present&quot;:0,&quot;ba_notpresent&quot;:1,&quot;bgr&quot;:0.193877551020408,&quot;bu&quot;:0.139386189258312,&quot;sc&quot;:0.0447368421052632,&quot;sod&quot;:0.653374233128834,&quot;pot&quot;:0,&quot;hemo&quot;:0.455056179775281,&quot;pcv&quot;:0.425925925925926,&quot;wbcc&quot;:0.170454545454545,&quot;rbcc&quot;:0.225,&quot;htn_yes&quot;:1,&quot;htn_no&quot;:0,&quot;dm_yes&quot;:0,&quot;dm_no&quot;:1,&quot;cad_yes&quot;:0,&quot;cad_no&quot;:1,&quot;appet_good&quot;:0,&quot;appet_poor&quot;:1,&quot;pe_yes&quot;:1,&quot;pe_no&quot;:0,&quot;ane_yes&quot;:1,&quot;ane_no&quot;:0}</code></pre>
</div>
<div id="defining-the-endpoint-and-output" class="section level3">
<h3>Defining the endpoint and output</h3>
<p>In order to convert this very simple script into an API, we need to define the endpoint(s). Endpoints will return an output, in our case it will return the output of the <code>predict()</code> function pasted into a line of text (e.g. “Test case predicted to be ckd”). Here, we want to have the predictions returned, so we annotate the entire function with <code>@get</code>. This endpoint in the API will get a custom name, so that we can call it later; here we call it <code>predict</code> and therefore write <code>#' @get /predict</code>.</p>
<blockquote>
<p>According to the design of the HTTP specification, GET (along with HEAD) requests are used only to read data and not change it. Therefore, when used this way, they are considered safe. That is, they can be called without risk of data modification or corruption — calling it once has the same effect as calling it 10 times, or none at all. Additionally, GET (and HEAD) is idempotent, which means that making multiple identical requests ends up having the same result as a single request. <a href="http://www.restapitutorial.com/lessons/httpmethods.html" class="uri">http://www.restapitutorial.com/lessons/httpmethods.html</a></p>
</blockquote>
<p>In this case, we could also consider using <code>@post</code> to avoid caching issues, but for this example I’ll leave it as <code>@get</code>.</p>
<blockquote>
<p>The POST verb is most-often utilized to <strong>create</strong> new resources. In particular, it’s used to create subordinate resources. That is, subordinate to some other (e.g. parent) resource. In other words, when creating a new resource, POST to the parent and the service takes care of associating the new resource with the parent, assigning an ID (new resource URI), etc. On successful creation, return HTTP status 201, returning a Location header with a link to the newly-created resource with the 201 HTTP status. POST is neither safe nor idempotent. It is therefore recommended for non-idempotent resource requests. Making two identical POST requests will most-likely result in two resources containing the same information. <a href="http://www.restapitutorial.com/lessons/httpmethods.html" class="uri">http://www.restapitutorial.com/lessons/httpmethods.html</a></p>
</blockquote>
<p>We can also customize the output. Keep in mind though, that the output should be <a href="https://www.rplumber.io/docs/rendering-and-output.html#serializers">“serialized”</a>. By default, the output will be in JSON format. Here, I want to have a text output, so I’ll specify <code>@html</code> without html formatting specifications, although I could add them if I wanted to display the text on a website. If we were to <a href="https://www.rplumber.io/docs/runtime.html#external-data-store">store the data in a database</a>, however, this would not be a good idea. In that case, it would be better to output the result as a JSON object.</p>
</div>
<div id="logging-with-filters" class="section level3">
<h3>Logging with filters</h3>
<p>It is also useful to provide some sort of logging for your API. Here, I am using the simple example from the <a href="https://www.rplumber.io/docs/routing-and-input.html#filters">plumber documentation</a> that uses filters and output the logs to the console or your API server logs. You could also <a href="https://www.rplumber.io/docs/runtime.html#file-system">write your logging output to a file</a>. In production, it would be better to use a real logging setup that stores information about each request, e.g. the time stamp, whether any errors or warnings occurred, etc. The <code>forward()</code> part of the logging function passes control on to the next handler in the pipeline, here our predict function.</p>
</div>
<div id="running-the-plumber-script" class="section level3">
<h3>Running the plumber script</h3>
<p>We need to save the entire script with annotations as an <em>.R</em> file as seen below. The regular comments <code>#</code> describe what each section does.</p>
<pre><code># script name:
# plumber.R

# set API title and description to show up in http://localhost:8000/__swagger__/

#&#39; @apiTitle Run predictions for Chronic Kidney Disease with Random Forest Model
#&#39; @apiDescription This API takes as patient data on Chronic Kidney Disease and returns a prediction whether the lab values
#&#39; indicate Chronic Kidney Disease (ckd) or not (notckd).
#&#39; For details on how the model is built, see https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/
#&#39; For further explanations of this plumber function, see https://shirinsplayground.netlify.com/2018/01/plumber/

# load model
# this path would have to be adapted if you would deploy this
load(&quot;/Users/shiringlander/Documents/Github/shirinsplayground/data/model_rf.RData&quot;)

#&#39; Log system time, request method and HTTP user agent of the incoming request
#&#39; @filter logger
function(req){
  cat(&quot;System time:&quot;, as.character(Sys.time()), &quot;\n&quot;,
      &quot;Request method:&quot;, req$REQUEST_METHOD, req$PATH_INFO, &quot;\n&quot;,
      &quot;HTTP user agent:&quot;, req$HTTP_USER_AGENT, &quot;@&quot;, req$REMOTE_ADDR, &quot;\n&quot;)
  plumber::forward()
}

# core function follows below:
# define parameters with type and description
# name endpoint
# return output as html/text
# specify 200 (okay) return

#&#39; predict Chronic Kidney Disease of test case with Random Forest model
#&#39; @param age:numeric The age of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param bp:numeric The blood pressure of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param sg_1.005:int The urinary specific gravity of the patient, integer (1: sg = 1.005, otherwise 0)
#&#39; @param sg_1.010:int The urinary specific gravity of the patient, integer (1: sg = 1.010, otherwise 0)
#&#39; @param sg_1.015:int The urinary specific gravity of the patient, integer (1: sg = 1.015, otherwise 0)
#&#39; @param sg_1.020:int The urinary specific gravity of the patient, integer (1: sg = 1.020, otherwise 0)
#&#39; @param sg_1.025:int The urinary specific gravity of the patient, integer (1: sg = 1.025, otherwise 0)
#&#39; @param al_0:int The urine albumin level of the patient, integer (1: al = 0, otherwise 0)
#&#39; @param al_1:int The urine albumin level of the patient, integer (1: al = 1, otherwise 0)
#&#39; @param al_2:int The urine albumin level of the patient, integer (1: al = 2, otherwise 0)
#&#39; @param al_3:int The urine albumin level of the patient, integer (1: al = 3, otherwise 0)
#&#39; @param al_4:int The urine albumin level of the patient, integer (1: al = 4, otherwise 0)
#&#39; @param al_5:int The urine albumin level of the patient, integer (1: al = 5, otherwise 0)
#&#39; @param su_0:int The sugar level of the patient, integer (1: su = 0, otherwise 0)
#&#39; @param su_1:int The sugar level of the patient, integer (1: su = 1, otherwise 0)
#&#39; @param su_2:int The sugar level of the patient, integer (1: su = 2, otherwise 0)
#&#39; @param su_3:int The sugar level of the patient, integer (1: su = 3, otherwise 0)
#&#39; @param su_4:int The sugar level of the patient, integer (1: su = 4, otherwise 0)
#&#39; @param su_5:int The sugar level of the patient, integer (1: su = 5, otherwise 0)
#&#39; @param rbc_normal:int The red blood cell count of the patient, integer (1: rbc = normal, otherwise 0)
#&#39; @param rbc_abnormal:int The red blood cell count of the patient, integer (1: rbc = abnormal, otherwise 0)
#&#39; @param pc_normal:int The pus cell level of the patient, integer (1: pc = normal, otherwise 0)
#&#39; @param pc_abnormal:int The pus cell level of the patient, integer (1: pc = abnormal, otherwise 0)
#&#39; @param pcc_present:int The puc cell clumps status of the patient, integer (1: pcc = present, otherwise 0)
#&#39; @param pcc_notpresent:int The puc cell clumps status of the patient, integer (1: pcc = notpresent, otherwise 0)
#&#39; @param ba_present:int The bacteria status of the patient, integer (1: ba = present, otherwise 0)
#&#39; @param ba_notpresent:int The bacteria status of the patient, integer (1: ba = notpresent, otherwise 0)
#&#39; @param bgr:numeric The blood glucose random level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param bu:numeric The blood urea level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param sc:numeric The serum creatinine level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param sod:numeric The sodium level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param pot:numeric The potassium level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param hemo:numeric The hemoglobin level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param pcv:numeric The packed cell volume of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param wbcc:numeric The white blood cell count of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param rbcc:numeric The red blood cell count of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param htn_yes:int The hypertension status of the patient, integer (1: htn = yes, otherwise 0)
#&#39; @param htn_no:int The hypertension status of the patient, integer (1: htn = no, otherwise 0)
#&#39; @param dm_yes:int The diabetes mellitus status of the patient, integer (1: dm = yes, otherwise 0)
#&#39; @param dm_no:int The diabetes mellitus status of the patient, integer (1: dm = no, otherwise 0)
#&#39; @param cad_yes:int The coronary artery disease status of the patient, integer (1: cad = yes, otherwise 0)
#&#39; @param cad_no:int The coronary artery disease status of the patient, integer (1: cad = no, otherwise 0)
#&#39; @param appet_good:int The appetite of the patient, integer (1: appet = good, otherwise 0)
#&#39; @param appet_poor:int The appetite of the patient, integer (1: appet = poor, otherwise 0)
#&#39; @param pe_yes:int The pedal edema status of the patient, integer (1: pe = yes, otherwise 0)
#&#39; @param pe_no:int The pedal edema status of the patient, integer (1: pe = no, otherwise 0)
#&#39; @param ane_yes:int The anemia status of the patient, integer (1: ane = yes, otherwise 0)
#&#39; @param ane_no:int The anemia status of the patient, integer (1: ane = no, otherwise 0)
#&#39; @get /predict
#&#39; @html
#&#39; @response 200 Returns the class (ckd or notckd) prediction from the Random Forest model; ckd = Chronic Kidney Disease
calculate_prediction &lt;- function(age, bp, sg_1.005, sg_1.010, sg_1.015, sg_1.020, sg_1.025, al_0, al_1, al_2, 
                                al_3, al_4, al_5, su_0, su_1, su_2, su_3, su_4, su_5, rbc_normal, rbc_abnormal, pc_normal, pc_abnormal,
                                pcc_present, pcc_notpresent, ba_present, ba_notpresent, bgr, bu, sc, sod, pot, hemo, pcv, 
                                wbcc, rbcc, htn_yes, htn_no, dm_yes, dm_no, cad_yes, cad_no, appet_good, appet_poor, pe_yes, pe_no, 
                                ane_yes, ane_no) {
  
  # make data frame from numeric parameters
  input_data_num &lt;&lt;- data.frame(age, bp, bgr, bu, sc, sod, pot, hemo, pcv, wbcc, rbcc,
                     stringsAsFactors = FALSE)
  # and make sure they really are numeric
  input_data_num &lt;&lt;- as.data.frame(t(sapply(input_data_num, as.numeric)))
  
  # make data frame from (binary) integer parameters
  input_data_int &lt;&lt;- data.frame(sg_1.005, sg_1.010, sg_1.015, sg_1.020, sg_1.025, al_0, al_1, al_2, 
                                al_3, al_4, al_5, su_0, su_1, su_2, su_3, su_4, su_5, rbc_normal, rbc_abnormal, pc_normal, pc_abnormal,
                                pcc_present, pcc_notpresent, ba_present, ba_notpresent, htn_yes, htn_no, dm_yes, dm_no, 
                                cad_yes, cad_no, appet_good, appet_poor, pe_yes, pe_no, ane_yes, ane_no,
                                stringsAsFactors = FALSE)
  # and make sure they really are numeric
  input_data_int &lt;&lt;- as.data.frame(t(sapply(input_data_int, as.integer)))
  # combine into one data frame
  input_data &lt;&lt;- as.data.frame(cbind(input_data_num, input_data_int))
  
  # validation for parameter
  if (any(is.na(input_data))) {
    res$status &lt;- 400
    res$body &lt;- &quot;Parameters have to be numeric or integers&quot;
  }
  
  if (any(input_data &lt; 0) || any(input_data &gt; 1)) {
    res$status &lt;- 400
    res$body &lt;- &quot;Parameters have to be between 0 and 1&quot;
  }

  # predict and return result
  pred_rf &lt;&lt;- predict(model_rf, input_data)
  paste(&quot;----------------\nTest case predicted to be&quot;, as.character(pred_rf), &quot;\n----------------\n&quot;)
}
</code></pre>
<p>Note that I am using the “double-assignment” operator <code>&lt;&lt;-</code> in my function, because I want to make sure that objects are overwritten at the top level (i.e. globally). This would have been relevant had I set a global parameter, but to show it the example, I decided to use it here as well.</p>
<p>We can now call our script with the <code>plumb()</code> function, run it with <code>run()</code> and open it on port 800. Calling <code>plumb()</code> creates an environment in which all our functions are evaluated.</p>
<pre class="r"><code>library(plumber)</code></pre>
<pre class="r"><code>r &lt;- plumb(&quot;/Users/shiringlander/Documents/Github/shirinsplayground/static/scripts/plumber.R&quot;)
r$run(port = 8000)</code></pre>
<p>We will now see the following message in our R console:</p>
<pre><code>Starting server to listen on port 8000
Running the swagger UI at http://127.0.0.1:8000/__swagger__/</code></pre>
<p>If you go to *<a href="http://localhost:8000/__swagger__/*" class="uri">http://localhost:8000/__swagger__/*</a>, you could now try out the function by manually choosing values for all the parameters we defined in the script.</p>
<p><img src="https://shiring.github.io/netlify_images/swagger1.png" alt="http://localhost:8000/__swagger__/" /> … <img src="https://shiring.github.io/netlify_images/swagger2.png" alt="http://localhost:8000/__swagger__/ continued" /></p>
<p>Because we annotated the <code>calculate_prediction()</code> function in our script with <code>#' @get /predict</code> we can access it via *<a href="http://localhost:8000/predict*" class="uri">http://localhost:8000/predict*</a>. But because we have no input specified as of yet, we will only see an error on this site. So, we still need to put our JSON formatted input into the function. To do this, we can use <a href="https://en.wikipedia.org/wiki/CURL"><em>curl</em></a> from the terminal and feed in the JSON string from above. If you are using RStudio in the latest version, you have a handy terminal window open in your working directory. You find it right next to the Console.</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/terminal_rstudio.png" alt="Terminal in RStudio" />
<p class="caption">Terminal in RStudio</p>
</div>
<pre><code>curl -H &quot;Content-Type: application/json&quot; -X GET -d &#39;{&quot;age&quot;:0.511111111111111,&quot;bp&quot;:0.111111111111111,&quot;sg_1.005&quot;:1,&quot;sg_1.010&quot;:0,&quot;sg_1.015&quot;:0,&quot;sg_1.020&quot;:0,&quot;sg_1.025&quot;:0,&quot;al_0&quot;:0,&quot;al_1&quot;:0,&quot;al_2&quot;:0,&quot;al_3&quot;:0,&quot;al_4&quot;:1,&quot;al_5&quot;:0,&quot;su_0&quot;:1,&quot;su_1&quot;:0,&quot;su_2&quot;:0,&quot;su_3&quot;:0,&quot;su_4&quot;:0,&quot;su_5&quot;:0,&quot;rbc_normal&quot;:1,&quot;rbc_abnormal&quot;:0,&quot;pc_normal&quot;:0,&quot;pc_abnormal&quot;:1,&quot;pcc_present&quot;:1,&quot;pcc_notpresent&quot;:0,&quot;ba_present&quot;:0,&quot;ba_notpresent&quot;:1,&quot;bgr&quot;:0.193877551020408,&quot;bu&quot;:0.139386189258312,&quot;sc&quot;:0.0447368421052632,&quot;sod&quot;:0.653374233128834,&quot;pot&quot;:0,&quot;hemo&quot;:0.455056179775281,&quot;pcv&quot;:0.425925925925926,&quot;wbcc&quot;:0.170454545454545,&quot;rbcc&quot;:0.225,&quot;htn_yes&quot;:1,&quot;htn_no&quot;:0,&quot;dm_yes&quot;:0,&quot;dm_no&quot;:1,&quot;cad_yes&quot;:0,&quot;cad_no&quot;:1,&quot;appet_good&quot;:0,&quot;appet_poor&quot;:1,&quot;pe_yes&quot;:1,&quot;pe_no&quot;:0,&quot;ane_yes&quot;:1,&quot;ane_no&quot;:0}&#39; &quot;http://localhost:8000/predict&quot;</code></pre>
<blockquote>
<p><strong>-H</strong> defines an extra header to include in the request when sending HTTP to a server (<a href="https://curl.haxx.se/docs/manpage.html#-H" class="uri">https://curl.haxx.se/docs/manpage.html#-H</a>).</p>
</blockquote>
<blockquote>
<p><strong>-X</strong> pecifies a custom request method to use when communicating with the HTTP server (<a href="https://curl.haxx.se/docs/manpage.html#-X" class="uri">https://curl.haxx.se/docs/manpage.html#-X</a>).</p>
</blockquote>
<blockquote>
<p><strong>-d</strong> sends the specified data in a request to the HTTP server, in the same way that a browser does when a user has filled in an HTML form and presses the submit button. This will cause curl to pass the data to the server using the content-type application/x-www-form-urlencoded (<a href="https://curl.haxx.se/docs/manpage.html#-d" class="uri">https://curl.haxx.se/docs/manpage.html#-d</a>).</p>
</blockquote>
<p>This will return the following output:</p>
<ul>
<li><code>cat()</code> outputs to the R console if you use R interactively; if you use R on a server, it will be included in the server logs.</li>
</ul>
<pre><code>System time: 2018-01-15 13:34:32 
 Request method: GET /predict 
 HTTP user agent: curl/7.54.0 @ 127.0.0.1 </code></pre>
<ul>
<li><code>paste</code> outputs to the terminal</li>
</ul>
<pre><code>----------------
Test case predicted to be ckd 
----------------</code></pre>
</div>
<div id="security" class="section level3">
<h3>Security</h3>
<p>This example shows a pretty simply R-script API. But if you plan on deploying your API to production, you should consider the <a href="https://www.rplumber.io/docs/security.html">security section of the plumber documentation</a>. It give additional information about how you can make your code (more) secure.</p>
</div>
<div id="finalize" class="section level3">
<h3>Finalize</h3>
<p>If you wanted to deploy this API you would need to <a href="https://www.rplumber.io/docs/hosting.html">host</a> it, i.e. provide the model and run an R environment with plumber, ideally on a server. A good way to do this, would be to package everything in a <a href="https://www.rplumber.io/docs/hosting.html#docker">Docker</a> container and run this. Docker will ensure that you have a working snapshot of the system settings, R and package versions that won’t change. For more information on dockerizing your API, check out <a href="https://hub.docker.com/r/trestletech/plumber/" class="uri">https://hub.docker.com/r/trestletech/plumber/</a>.</p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] plumber_0.4.6   rjson_0.2.20    forcats_0.3.0   stringr_1.3.1  
##  [5] dplyr_0.7.6     purrr_0.2.5     readr_1.1.1     tidyr_0.8.1    
##  [9] tibble_1.4.2    ggplot2_3.0.0   tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##  [1] nlme_3.1-137        lubridate_1.7.4     dimRed_0.1.0       
##  [4] httr_1.3.1          rprojroot_1.3-2     tools_3.5.1        
##  [7] backports_1.1.2     R6_2.2.2            rpart_4.1-13       
## [10] lazyeval_0.2.1      colorspace_1.3-2    nnet_7.3-12        
## [13] withr_2.1.2         tidyselect_0.2.4    compiler_3.5.1     
## [16] cli_1.0.0           rvest_0.3.2         xml2_1.2.0         
## [19] bookdown_0.7        scales_0.5.0        sfsmisc_1.1-2      
## [22] DEoptimR_1.0-8      robustbase_0.93-2   randomForest_4.6-14
## [25] digest_0.6.15       rmarkdown_1.10      pkgconfig_2.0.1    
## [28] htmltools_0.3.6     rlang_0.2.1         readxl_1.1.0       
## [31] ddalpha_1.3.4       rstudioapi_0.7      bindr_0.1.1        
## [34] jsonlite_1.5        ModelMetrics_1.1.0  magrittr_1.5       
## [37] Matrix_1.2-14       Rcpp_0.12.18        munsell_0.5.0      
## [40] abind_1.4-5         stringi_1.2.4       yaml_2.2.0         
## [43] MASS_7.3-50         plyr_1.8.4          recipes_0.1.3      
## [46] grid_3.5.1          promises_1.0.1      pls_2.6-0          
## [49] crayon_1.3.4        lattice_0.20-35     haven_1.1.2        
## [52] splines_3.5.1       hms_0.4.2           knitr_1.20         
## [55] pillar_1.3.0        reshape2_1.4.3      codetools_0.2-15   
## [58] stats4_3.5.1        CVST_0.2-2          magic_1.5-8        
## [61] glue_1.3.0          evaluate_0.11       blogdown_0.8       
## [64] modelr_0.1.2        httpuv_1.4.5        foreach_1.4.4      
## [67] cellranger_1.1.0    gtable_0.2.0        kernlab_0.9-26     
## [70] assertthat_0.2.0    DRR_0.0.3           xfun_0.3           
## [73] gower_0.1.2         prodlim_2018.04.18  broom_0.5.0        
## [76] later_0.7.3         class_7.3-14        survival_2.42-6    
## [79] geometry_0.3-6      timeDate_3043.102   RcppRoll_0.3.0     
## [82] iterators_1.0.10    bindrcpp_0.2.2      lava_1.6.2         
## [85] caret_6.0-80        ipred_0.9-6</code></pre>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #91: Philosophy of Intelligence with Matthew Crosby]]></title>
    <link href="/2018/01/twimlai91/"/>
    <id>/2018/01/twimlai91/</id>
    <published>2018-01-14T00:00:00+00:00</published>
    <updated>2018-01-14T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about Philosophy of Intelligence with Matthew Crosby: <a href="https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/" class="uri">https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/</a></p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai91.jpg" alt="Sketchnotes from TWiMLAI talk #92: Philosophy of Intelligence with Matthew Crosby" />
<p class="caption">Sketchnotes from TWiMLAI talk #92: Philosophy of Intelligence with Matthew Crosby</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-91-philosophy-intelligence-matthew-crosby/">here</a>.</p>
<blockquote>
<p>This week on the podcast we’re featuring a series of conversations from the NIPs conference in Long Beach, California. I attended a bunch of talks and learned a ton, organized an impromptu roundtable on Building AI Products, and met a bunch of great people, including some former TWiML Talk guests.This time around i’m joined by Matthew Crosby, a researcher at Imperial College London, working on the Kinds of Intelligence Project. Matthew joined me after the NIPS Symposium of the same name, an event that brought researchers from a variety of disciplines together towards three aims: a broader perspective of the possible types of intelligence beyond human intelligence, better measurements of intelligence, and a more purposeful analysis of where progress should be made in AI to best benefit society. Matthew’s research explores intelligence from a philosophical perspective, exploring ideas like predictive processing and controlled hallucination, and how these theories of intelligence impact the way we approach creating artificial intelligence. This was a very interesting conversation, i’m sure you’ll enjoy. <a href="https://twimlai.com/twiml-talk-91-philosophy-intelligence-matthew-crosby/" class="uri">https://twimlai.com/twiml-talk-91-philosophy-intelligence-matthew-crosby/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Looking beyond accuracy to improve trust in machine learning]]></title>
    <link href="/2018/01/looking_beyond_accuracy_to_improve_trust_in_ml/"/>
    <id>/2018/01/looking_beyond_accuracy_to_improve_trust_in_ml/</id>
    <published>2018-01-10T00:00:00+00:00</published>
    <updated>2018-01-10T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I have written another blogpost about <a href="https://blog.codecentric.de/en/2018/01/look-beyond-accuracy-improve-trust-machine-learning/">Looking beyond accuracy to improve trust in machine learning</a> at my company <a href="https://blog.codecentric.de/en/">codecentric</a>&rsquo;s blog:</p>

<blockquote>
<p>Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex machine learning models are essentially black boxes and too complicated to understand, we need to use approximations to get a better sense of how they work. One such approach is LIME, which stands for Local Interpretable Model-agnostic Explanations and is a tool that helps understand and explain the decisions made by complex machine learning models.</p>
</blockquote>

<p>Continue reading at <a href="https://blog.codecentric.de/en/2018/01/look-beyond-accuracy-improve-trust-machine-learning/">https://blog.codecentric.de/en/2018/01/look-beyond-accuracy-improve-trust-machine-learning/</a>&hellip;</p>

<p>Links to the entire example code and more info are given at the end of the blog post.</p>

<p>The blog post is <a href="https://blog.codecentric.de/2018/01/vertrauen-und-vorurteile-maschinellem-lernen/">also available in German</a>.</p>

<p><img src="https://blog.codecentric.de/files/2018/01/lime_output_figure.png" alt="" /></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[TWiMLAI talk 88 sketchnotes: Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru]]></title>
    <link href="/2018/01/twimlai88_sketchnotes/"/>
    <id>/2018/01/twimlai88_sketchnotes/</id>
    <published>2018-01-10T00:00:00+00:00</published>
    <updated>2018-01-10T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes taken from the <a href="https://twimlai.com/twiml-talk-88-using-deep-learning-google-street-view-estimate-demographics-timnit-gebru/">“This week in Machine Learning &amp; AI” podcast number 88 about Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru</a>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai88_sketchnotes_vhjzac.jpg" alt="Sketchnotes from TWiMLAI talk #88: Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru" />
<p class="caption">Sketchnotes from TWiMLAI talk #88: Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru</p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Registration now open for workshop on Deep Learning with Keras and TensorFlow using R]]></title>
    <link href="/2017/12/keras_sketchnotes/"/>
    <id>/2017/12/keras_sketchnotes/</id>
    <published>2017-12-20T00:00:00+00:00</published>
    <updated>2017-12-20T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Recently, I announced <a href="https://shirinsplayground.netlify.com/2017/11/deep_learning_keras_tensorflow/">my workshop on Deep Learning with Keras and TensorFlow</a>.</p>
<p>The next dates for it are <strong>January 18th and 19th</strong> in Solingen, Germany.</p>
<p>You can register now by following this link: <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow" class="uri">https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow</a></p>
<p>If any non-German-speaking people want to attend, I’m happy to give the course in English!</p>
<p><a href="mailto:shirin.glander@codecentric.de">Contact me if you have further questions.</a></p>
<hr />
<p>As a little bonus, I am also sharing my sketch notes from a Podcast I listened to when first getting into Keras:</p>
<ul>
<li><a href="https://softwareengineeringdaily.com/2016/01/29/deep-learning-and-keras-with-francois-chollet/">Software Engineering Daily with Francois Chollet</a></li>
</ul>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/keras_sketchnotes_lgrnvo.jpg" alt="Sketchnotes: Software Engineering Daily - Podcast from Jan 29th 2016" />
<p class="caption">Sketchnotes: Software Engineering Daily - Podcast from Jan 29th 2016</p>
</div>
<p>Links from the notes:</p>
<ul>
<li><a href="https://keras.io/">Keras for Python</a></li>
<li><a href="https://keras.rstudio.com/">Keras for R</a></li>
<li><a href="https://github.com/maxpumperla/elephas">elephas</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Explaining Predictions of Machine Learning Models with LIME - Münster Data Science Meetup]]></title>
    <link href="/2017/12/lime_sketchnotes/"/>
    <id>/2017/12/lime_sketchnotes/</id>
    <published>2017-12-12T00:00:00+00:00</published>
    <updated>2017-12-12T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="slides-from-munster-data-science-meetup" class="section level2">
<h2>Slides from Münster Data Science Meetup</h2>
<p><a href="https://shiring.github.io/netlify_images/lime_meetup_slides_wvsh6s.pdf">These are my slides</a> from the <a href="https://www.meetup.com/Data-Science-Meetup-Muenster/events/244173239/">Münster Data Science Meetup on December 12th, 2017</a>.</p>
<pre class="r"><code>knitr::include_url(&quot;https://shiring.github.io/netlify_images/lime_meetup_slides_wvsh6s.pdf&quot;)</code></pre>
<iframe src="https://shiring.github.io/netlify_images/lime_meetup_slides_wvsh6s.pdf" width="672" height="400px">
</iframe>
<p><br></p>
<p>My sketchnotes were collected from these two podcasts:</p>
<ul>
<li><a href="https://twimlai.com/twiml-talk-7-carlos-guestrin-explaining-predictions-machine-learning-models/" class="uri">https://twimlai.com/twiml-talk-7-carlos-guestrin-explaining-predictions-machine-learning-models/</a></li>
<li><a href="https://dataskeptic.com/blog/episodes/2016/trusting-machine-learning-models-with-lime" class="uri">https://dataskeptic.com/blog/episodes/2016/trusting-machine-learning-models-with-lime</a></li>
</ul>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/lime_sketchnotes_guq6u5.jpg" alt="Sketchnotes: TWiML Talk #7 with Carlos Guestrin – Explaining the Predictions of Machine Learning Models &amp; Data Skeptic Podcast - Trusting Machine Learning Models with Lime" />
<p class="caption">Sketchnotes: TWiML Talk #7 with Carlos Guestrin – Explaining the Predictions of Machine Learning Models &amp; Data Skeptic Podcast - Trusting Machine Learning Models with Lime</p>
</div>
<hr />
</div>
<div id="example-code" class="section level2">
<h2>Example Code</h2>
<ul>
<li>the following libraries were loaded:</li>
</ul>
<pre class="r"><code>library(tidyverse)  # for tidy data analysis
library(farff)      # for reading arff file
library(missForest) # for imputing missing values
library(dummies)    # for creating dummy variables
library(caret)      # for modeling
library(lime)       # for explaining predictions</code></pre>
<div id="data" class="section level3">
<h3>Data</h3>
<p>The Chronic Kidney Disease dataset was downloaded from UC Irvine’s Machine Learning repository: <a href="http://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease" class="uri">http://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease</a></p>
<pre class="r"><code>data_file &lt;- file.path(&quot;path/to/chronic_kidney_disease_full.arff&quot;)</code></pre>
<ul>
<li>load data with the <code>farff</code> package</li>
</ul>
<pre class="r"><code>data &lt;- readARFF(data_file)</code></pre>
<div id="features" class="section level4">
<h4>Features</h4>
<ul>
<li>age - age</li>
<li>bp - blood pressure</li>
<li>sg - specific gravity</li>
<li>al - albumin</li>
<li>su - sugar</li>
<li>rbc - red blood cells</li>
<li>pc - pus cell</li>
<li>pcc - pus cell clumps</li>
<li>ba - bacteria</li>
<li>bgr - blood glucose random</li>
<li>bu - blood urea</li>
<li>sc - serum creatinine</li>
<li>sod - sodium</li>
<li>pot - potassium</li>
<li>hemo - hemoglobin</li>
<li>pcv - packed cell volume</li>
<li>wc - white blood cell count</li>
<li>rc - red blood cell count</li>
<li>htn - hypertension</li>
<li>dm - diabetes mellitus</li>
<li>cad - coronary artery disease</li>
<li>appet - appetite</li>
<li>pe - pedal edema</li>
<li>ane - anemia</li>
<li>class - class</li>
</ul>
</div>
</div>
<div id="missing-data" class="section level3">
<h3>Missing data</h3>
<ul>
<li>impute missing data with Nonparametric Missing Value Imputation using Random Forest (<code>missForest</code> package)</li>
</ul>
<pre class="r"><code>data_imp &lt;- missForest(data)</code></pre>
</div>
<div id="one-hot-encoding" class="section level3">
<h3>One-hot encoding</h3>
<ul>
<li>create dummy variables (<code>caret::dummy.data.frame()</code>)</li>
<li>scale and center</li>
</ul>
<pre class="r"><code>data_imp_final &lt;- data_imp$ximp
data_dummy &lt;- dummy.data.frame(dplyr::select(data_imp_final, -class), sep = &quot;_&quot;)
data &lt;- cbind(dplyr::select(data_imp_final, class), scale(data_dummy, 
                                                   center = apply(data_dummy, 2, min),
                                                   scale = apply(data_dummy, 2, max)))</code></pre>
</div>
<div id="modeling" class="section level3">
<h3>Modeling</h3>
<pre class="r"><code># training and test set
set.seed(42)
index &lt;- createDataPartition(data$class, p = 0.9, list = FALSE)
train_data &lt;- data[index, ]
test_data  &lt;- data[-index, ]

# modeling
model_rf &lt;- caret::train(class ~ .,
  data = train_data,
  method = &quot;rf&quot;, # random forest
  trControl = trainControl(method = &quot;repeatedcv&quot;, 
       number = 10, 
       repeats = 5, 
       verboseIter = FALSE))</code></pre>
<pre class="r"><code>model_rf</code></pre>
<pre><code>## Random Forest 
## 
## 360 samples
##  48 predictor
##   2 classes: &#39;ckd&#39;, &#39;notckd&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 324, 324, 324, 324, 325, 324, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    2    0.9922647  0.9838466
##   25    0.9917392  0.9826070
##   48    0.9872930  0.9729881
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<pre class="r"><code># predictions
pred &lt;- data.frame(sample_id = 1:nrow(test_data), predict(model_rf, test_data, type = &quot;prob&quot;), actual = test_data$class) %&gt;%
  mutate(prediction = colnames(.)[2:3][apply(.[, 2:3], 1, which.max)], correct = ifelse(actual == prediction, &quot;correct&quot;, &quot;wrong&quot;))

confusionMatrix(pred$actual, pred$prediction)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction ckd notckd
##     ckd     23      2
##     notckd   0     15
##                                           
##                Accuracy : 0.95            
##                  95% CI : (0.8308, 0.9939)
##     No Information Rate : 0.575           
##     P-Value [Acc &gt; NIR] : 1.113e-07       
##                                           
##                   Kappa : 0.8961          
##  Mcnemar&#39;s Test P-Value : 0.4795          
##                                           
##             Sensitivity : 1.0000          
##             Specificity : 0.8824          
##          Pos Pred Value : 0.9200          
##          Neg Pred Value : 1.0000          
##              Prevalence : 0.5750          
##          Detection Rate : 0.5750          
##    Detection Prevalence : 0.6250          
##       Balanced Accuracy : 0.9412          
##                                           
##        &#39;Positive&#39; Class : ckd             
## </code></pre>
</div>
<div id="lime" class="section level3">
<h3>LIME</h3>
<ul>
<li>LIME needs data without response variable</li>
</ul>
<pre class="r"><code>train_x &lt;- dplyr::select(train_data, -class)
test_x &lt;- dplyr::select(test_data, -class)

train_y &lt;- dplyr::select(train_data, class)
test_y &lt;- dplyr::select(test_data, class)</code></pre>
<ul>
<li>build explainer</li>
</ul>
<pre class="r"><code>explainer &lt;- lime(train_x, model_rf, n_bins = 5, quantile_bins = TRUE)</code></pre>
<ul>
<li>run <code>explain()</code> function</li>
</ul>
<pre class="r"><code>explanation_df &lt;- lime::explain(test_x, explainer, n_labels = 1, n_features = 8, n_permutations = 1000, feature_select = &quot;forward_selection&quot;)</code></pre>
<ul>
<li>model reliability</li>
</ul>
<pre class="r"><code>explanation_df %&gt;%
  ggplot(aes(x = model_r2, fill = label)) +
    geom_density(alpha = 0.5)</code></pre>
<p><img src="/post/2017-12-12_lime_sketchnotes_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<ul>
<li>plot explanations</li>
</ul>
<pre class="r"><code>plot_features(explanation_df[1:24, ], ncol = 1)</code></pre>
<p><img src="/post/2017-12-12_lime_sketchnotes_files/figure-html/unnamed-chunk-15-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="session-info" class="section level3">
<h3>Session Info</h3>
<pre><code>## Session info -------------------------------------------------------------</code></pre>
<pre><code>##  setting  value                       
##  version  R version 3.4.3 (2017-11-30)
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  de_DE.UTF-8                 
##  tz       Europe/Berlin               
##  date     2018-04-22</code></pre>
<pre><code>## Packages -----------------------------------------------------------------</code></pre>
<pre><code>##  package      * version    date       source                            
##  assertthat     0.2.0      2017-04-11 CRAN (R 3.4.0)                    
##  backports      1.1.2      2017-12-13 CRAN (R 3.4.3)                    
##  base         * 3.4.3      2017-12-07 local                             
##  BBmisc         1.11       2017-03-10 CRAN (R 3.4.0)                    
##  bindr          0.1        2016-11-13 CRAN (R 3.4.0)                    
##  bindrcpp     * 0.2        2017-06-17 CRAN (R 3.4.0)                    
##  blogdown       0.5        2018-01-24 CRAN (R 3.4.3)                    
##  bookdown       0.7        2018-02-18 CRAN (R 3.4.3)                    
##  broom          0.4.3      2017-11-20 CRAN (R 3.4.2)                    
##  caret        * 6.0-78     2017-12-10 CRAN (R 3.4.3)                    
##  cellranger     1.1.0      2016-07-27 CRAN (R 3.4.0)                    
##  checkmate      1.8.5      2017-10-24 CRAN (R 3.4.2)                    
##  class          7.3-14     2015-08-30 CRAN (R 3.4.3)                    
##  cli            1.0.0      2017-11-05 CRAN (R 3.4.2)                    
##  codetools      0.2-15     2016-10-05 CRAN (R 3.4.3)                    
##  colorspace     1.3-2      2016-12-14 CRAN (R 3.4.0)                    
##  compiler       3.4.3      2017-12-07 local                             
##  crayon         1.3.4      2017-09-16 CRAN (R 3.4.1)                    
##  CVST           0.2-1      2013-12-10 CRAN (R 3.4.0)                    
##  datasets     * 3.4.3      2017-12-07 local                             
##  ddalpha        1.3.1.1    2018-02-02 CRAN (R 3.4.3)                    
##  DEoptimR       1.0-8      2016-11-19 CRAN (R 3.4.0)                    
##  devtools       1.13.5     2018-02-18 CRAN (R 3.4.3)                    
##  digest         0.6.15     2018-01-28 CRAN (R 3.4.3)                    
##  dimRed         0.1.0      2017-05-04 CRAN (R 3.4.0)                    
##  dplyr        * 0.7.4      2017-09-28 CRAN (R 3.4.2)                    
##  DRR            0.0.3      2018-01-06 CRAN (R 3.4.3)                    
##  dummies      * 1.5.6      2012-06-14 CRAN (R 3.4.0)                    
##  e1071          1.6-8      2017-02-02 CRAN (R 3.4.0)                    
##  evaluate       0.10.1     2017-06-24 CRAN (R 3.4.1)                    
##  farff        * 1.0        2016-09-11 CRAN (R 3.4.0)                    
##  forcats      * 0.3.0      2018-02-19 CRAN (R 3.4.3)                    
##  foreach      * 1.4.4      2017-12-12 CRAN (R 3.4.3)                    
##  foreign        0.8-69     2017-06-22 CRAN (R 3.4.3)                    
##  ggplot2      * 2.2.1.9000 2018-02-28 Github (thomasp85/ggplot2@7859a29)
##  glmnet         2.0-13     2017-09-22 CRAN (R 3.4.2)                    
##  glue           1.2.0      2017-10-29 CRAN (R 3.4.2)                    
##  gower          0.1.2      2017-02-23 CRAN (R 3.4.0)                    
##  graphics     * 3.4.3      2017-12-07 local                             
##  grDevices    * 3.4.3      2017-12-07 local                             
##  grid           3.4.3      2017-12-07 local                             
##  gtable         0.2.0      2016-02-26 CRAN (R 3.4.0)                    
##  haven          1.1.1      2018-01-18 CRAN (R 3.4.3)                    
##  highr          0.6        2016-05-09 CRAN (R 3.4.0)                    
##  hms            0.4.1      2018-01-24 CRAN (R 3.4.3)                    
##  htmltools      0.3.6      2017-04-28 CRAN (R 3.4.0)                    
##  htmlwidgets    1.0        2018-01-20 CRAN (R 3.4.3)                    
##  httpuv         1.3.6.1    2018-02-28 CRAN (R 3.4.3)                    
##  httr           1.3.1      2017-08-20 CRAN (R 3.4.1)                    
##  ipred          0.9-6      2017-03-01 CRAN (R 3.4.0)                    
##  iterators    * 1.0.9      2017-12-12 CRAN (R 3.4.3)                    
##  itertools    * 0.1-3      2014-03-12 CRAN (R 3.4.0)                    
##  jsonlite       1.5        2017-06-01 CRAN (R 3.4.0)                    
##  kernlab        0.9-25     2016-10-03 CRAN (R 3.4.0)                    
##  knitr          1.20       2018-02-20 CRAN (R 3.4.3)                    
##  labeling       0.3        2014-08-23 CRAN (R 3.4.0)                    
##  lattice      * 0.20-35    2017-03-25 CRAN (R 3.4.3)                    
##  lava           1.6        2018-01-13 CRAN (R 3.4.3)                    
##  lazyeval       0.2.1      2017-10-29 CRAN (R 3.4.2)                    
##  lime         * 0.3.1      2017-11-24 CRAN (R 3.4.3)                    
##  lubridate      1.7.3      2018-02-27 CRAN (R 3.4.3)                    
##  magrittr       1.5        2014-11-22 CRAN (R 3.4.0)                    
##  MASS           7.3-49     2018-02-23 CRAN (R 3.4.3)                    
##  Matrix         1.2-12     2017-11-20 CRAN (R 3.4.3)                    
##  memoise        1.1.0      2017-04-21 CRAN (R 3.4.0)                    
##  methods      * 3.4.3      2017-12-07 local                             
##  mime           0.5        2016-07-07 CRAN (R 3.4.0)                    
##  missForest   * 1.4        2013-12-31 CRAN (R 3.4.0)                    
##  mnormt         1.5-5      2016-10-15 CRAN (R 3.4.0)                    
##  ModelMetrics   1.1.0      2016-08-26 CRAN (R 3.4.0)                    
##  modelr         0.1.1      2017-07-24 CRAN (R 3.4.1)                    
##  munsell        0.4.3      2016-02-13 CRAN (R 3.4.0)                    
##  nlme           3.1-131.1  2018-02-16 CRAN (R 3.4.3)                    
##  nnet           7.3-12     2016-02-02 CRAN (R 3.4.3)                    
##  parallel       3.4.3      2017-12-07 local                             
##  pillar         1.2.1      2018-02-27 CRAN (R 3.4.3)                    
##  pkgconfig      2.0.1      2017-03-21 CRAN (R 3.4.0)                    
##  plyr           1.8.4      2016-06-08 CRAN (R 3.4.0)                    
##  prodlim        1.6.1      2017-03-06 CRAN (R 3.4.0)                    
##  psych          1.7.8      2017-09-09 CRAN (R 3.4.1)                    
##  purrr        * 0.2.4      2017-10-18 CRAN (R 3.4.2)                    
##  R6             2.2.2      2017-06-17 CRAN (R 3.4.0)                    
##  randomForest * 4.6-12     2015-10-07 CRAN (R 3.4.0)                    
##  Rcpp           0.12.15    2018-01-20 CRAN (R 3.4.3)                    
##  RcppRoll       0.2.2      2015-04-05 CRAN (R 3.4.0)                    
##  readr        * 1.1.1      2017-05-16 CRAN (R 3.4.0)                    
##  readxl         1.0.0      2017-04-18 CRAN (R 3.4.0)                    
##  recipes        0.1.2      2018-01-11 CRAN (R 3.4.3)                    
##  reshape2       1.4.3      2017-12-11 CRAN (R 3.4.3)                    
##  rlang          0.2.0.9000 2018-02-28 Github (tidyverse/rlang@9ea33dd)  
##  rmarkdown      1.8        2017-11-17 CRAN (R 3.4.2)                    
##  robustbase     0.92-8     2017-11-01 CRAN (R 3.4.2)                    
##  rpart          4.1-13     2018-02-23 CRAN (R 3.4.3)                    
##  rprojroot      1.3-2      2018-01-03 CRAN (R 3.4.3)                    
##  rstudioapi     0.7        2017-09-07 CRAN (R 3.4.1)                    
##  rvest          0.3.2      2016-06-17 CRAN (R 3.4.0)                    
##  scales         0.5.0.9000 2018-02-28 Github (hadley/scales@d767915)    
##  sfsmisc        1.1-1      2017-06-08 CRAN (R 3.4.0)                    
##  shiny          1.0.5      2017-08-23 CRAN (R 3.4.1)                    
##  shinythemes    1.1.1      2016-10-12 CRAN (R 3.4.0)                    
##  splines        3.4.3      2017-12-07 local                             
##  stats        * 3.4.3      2017-12-07 local                             
##  stats4         3.4.3      2017-12-07 local                             
##  stringdist     0.9.4.6    2017-07-31 CRAN (R 3.4.1)                    
##  stringi        1.1.6      2017-11-17 CRAN (R 3.4.2)                    
##  stringr      * 1.3.0      2018-02-19 CRAN (R 3.4.3)                    
##  survival       2.41-3     2017-04-04 CRAN (R 3.4.3)                    
##  tibble       * 1.4.2      2018-01-22 CRAN (R 3.4.3)                    
##  tidyr        * 0.8.0      2018-01-29 CRAN (R 3.4.3)                    
##  tidyselect     0.2.4      2018-02-26 CRAN (R 3.4.3)                    
##  tidyverse    * 1.2.1      2017-11-14 CRAN (R 3.4.2)                    
##  timeDate       3043.102   2018-02-21 CRAN (R 3.4.3)                    
##  tools          3.4.3      2017-12-07 local                             
##  utils        * 3.4.3      2017-12-07 local                             
##  withr          2.1.1.9000 2018-02-28 Github (jimhester/withr@5d05571)  
##  xfun           0.1        2018-01-22 CRAN (R 3.4.3)                    
##  xml2           1.2.0      2018-01-24 CRAN (R 3.4.3)                    
##  xtable         1.8-2      2016-02-05 CRAN (R 3.4.0)                    
##  yaml           2.1.17     2018-02-27 CRAN (R 3.4.3)</code></pre>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[MICE (Multiple Imputation by Chained Equations) in R - sketchnotes from MünsteR Meetup]]></title>
    <link href="/2017/11/mice_sketchnotes/"/>
    <id>/2017/11/mice_sketchnotes/</id>
    <published>2017-11-28T00:00:00+00:00</published>
    <updated>2017-11-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Last night, the <a href="http://meetu.ps/c/3ffGL/w54bW/f">MünsteR R user-group</a> had <a href="https://www.meetup.com/Munster-R-Users-Group/events/243388360/">another great meetup</a>:</p>
<p>Karin Groothuis-Oudshoorn, Assistant Professor at the University of Twente, presented her R package <code>mice</code> about Multivariate Imputation by Chained Equations.</p>
<p>It was a very interesting talk and here are my sketchnotes that I took during it:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/mice_sketchnote_gxjsgc.jpg" alt="MICE talk sketchnotes" />
<p class="caption">MICE talk sketchnotes</p>
</div>
<p>Here is the link to the paper referenced in my notes: <a href="https://www.jstatsoft.org/article/view/v045i03" class="uri">https://www.jstatsoft.org/article/view/v045i03</a></p>
<blockquote>
<p>“The mice package implements a method to deal with missing data. The package creates multiple imputations (replacement values) for multivariate missing data. The method is based on Fully Conditional Specification, where each incomplete variable is imputed by a separate model. The MICE algorithm can impute mixes of continuous, binary, unordered categorical and ordered categorical data. In addition, MICE can impute continuous two-level data, and maintain consistency between imputations by means of passive imputation. Many diagnostic plots are implemented to inspect the quality of the imputations.”&quot; (<a href="https://cran.r-project.org/web/packages/mice/README.html" class="uri">https://cran.r-project.org/web/packages/mice/README.html</a>)</p>
</blockquote>
<p>For more information on the package go to <a href="http://stefvanbuuren.github.io/mice/" class="uri">http://stefvanbuuren.github.io/mice/</a>.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Workshop on Deep Learning with Keras and TensorFlow in R]]></title>
    <link href="/2017/11/deep_learning_keras_tensorflow/"/>
    <id>/2017/11/deep_learning_keras_tensorflow/</id>
    <published>2017-11-20T00:00:00+00:00</published>
    <updated>2017-11-20T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>You can now book me and my 1-day workshop on <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/">deep learning with Keras and TensorFlow using R</a>.</p>
<p>In my workshop, you will learn</p>
<ul>
<li>the basics of deep learning</li>
<li>what cross-entropy and loss is</li>
<li>about activation functions</li>
<li>how to optimize weights and biases with backpropagation and gradient descent</li>
<li>how to build (deep) neural networks with Keras and TensorFlow</li>
<li>how to save and load models and model weights</li>
<li>how to visualize models with TensorBoard</li>
<li>how to make predictions on test data</li>
</ul>
<p>Date and place depend on who and how many people are interested, so please contact me either directly or via the workshop page: <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/" class="uri">https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/</a> (the description is in German but I also offer to give the workshop in English).</p>
<p><br></p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/mlp_r7pv7z.jpg" alt="Neural Network with three densely connected hidden layers" />
<p class="caption">Neural Network with three densely connected hidden layers</p>
</div>
<p>Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. Keras is very convenient for fast and easy prototyping of neural networks. It is highly modular and very flexible, so that you can build basically any type of neural network you want. It supports convolutional neural networks and recurrent neural networks, as well as combinations of both. Due to its layer structure, it is highly extensible and can run on CPU or GPU.</p>
<p>The <code>keras</code> R package provides an interface to the Python library of Keras, just as the tensorflow package provides an interface to TensorFlow. Basically, R creates a conda instance and runs Keras it it, while you can still use all the functionalities of R for plotting, etc. Almost all function names are the same, so models can easily be recreated in Python for deployment.</p>
<p><br></p>
<div class="figure">
<img src="https://blog.keras.io/img/keras-tensorflow-logo.jpg" />

</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[How to combine point and boxplots in timeline charts with ggplot2 facets]]></title>
    <link href="/2017/11/combine_point_boxplot_ggplot/"/>
    <id>/2017/11/combine_point_boxplot_ggplot/</id>
    <published>2017-11-18T00:00:00+00:00</published>
    <updated>2017-11-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>In a recent project, I was looking to plot data from different variables along the same time axis. The difficulty was, that some of these variables I wanted to have as point plots, while others I wanted as box-plots.</p>
<p>Because I work with the tidyverse, I wanted to produce these plots with ggplot2. Faceting was the obvious first step but it took me quite a while to figure out how to best combine facets with point plots (where I have one value per time point) with and box-plots (where I have multiple values per time point).</p>
<p>The reason why this isn’t trivial is that box plots require groups or factors on the x-axis, while points can be plotted over a continuous range of x-values. If your alarm bells are ringing right now, you are absolutely right: before you try to combine plots with different x-axis properties, you should think long and hard whether this is an accurate representation of the data and if its a good idea to do so! Here, I had multiple values per time point for one variable and I wanted to make the median + variation explicitly clear, while also showing the continuous changes of other variables over the same range of time.</p>
<p>So, I am writing this short tutorial here in hopes that it saves the next person trying to do something similar from spending an entire morning on stackoverflow. ;-)</p>
<p>For this demonstration, I am creating some fake data:</p>
<pre class="r"><code>library(tidyverse)
dates &lt;- seq(as.POSIXct(&quot;2017-10-01 07:00&quot;), as.POSIXct(&quot;2017-10-01 10:30&quot;), by = 180) # 180 seconds == 3 minutes
fake_data &lt;- data.frame(time = dates,
                        var1_1 = runif(length(dates)),
                        var1_2 = runif(length(dates)),
                        var1_3 = runif(length(dates)),
                        var2 = runif(length(dates))) %&gt;%
  sample_frac(size = 0.33)
head(fake_data)</code></pre>
<pre><code>##                   time    var1_1    var1_2     var1_3      var2
## 51 2017-10-01 09:30:00 0.4534363 0.9947001 0.07223936 0.8891859
## 35 2017-10-01 08:42:00 0.4260230 0.5613454 0.77475368 0.5780837
## 3  2017-10-01 07:06:00 0.0871770 0.2824280 0.97726978 0.4705974
## 59 2017-10-01 09:54:00 0.6824320 0.9735636 0.67654248 0.4235517
## 5  2017-10-01 07:12:00 0.7979666 0.5857256 0.03911439 0.6918448
## 52 2017-10-01 09:33:00 0.7537796 0.3054030 0.61354248 0.5045606</code></pre>
<p>Here, variable 1 (<code>var1</code>) has three measurements per time point, while variable 2 (<code>var2</code>) has one.</p>
<p>First, for plotting with ggplot2 we want our data in a tidy long format. I also add another column for faceting that groups the variables from <code>var1</code> together.</p>
<pre class="r"><code>fake_data_long &lt;- fake_data %&gt;%
  gather(x, y, var1_1:var2) %&gt;%
  mutate(facet = ifelse(x %in% c(&quot;var1_1&quot;, &quot;var1_2&quot;, &quot;var1_3&quot;), &quot;var1&quot;, x))
head(fake_data_long)</code></pre>
<pre><code>##                  time      x         y facet
## 1 2017-10-01 09:30:00 var1_1 0.4534363  var1
## 2 2017-10-01 08:42:00 var1_1 0.4260230  var1
## 3 2017-10-01 07:06:00 var1_1 0.0871770  var1
## 4 2017-10-01 09:54:00 var1_1 0.6824320  var1
## 5 2017-10-01 07:12:00 var1_1 0.7979666  var1
## 6 2017-10-01 09:33:00 var1_1 0.7537796  var1</code></pre>
<p>Now, we can plot this the following way:</p>
<ul>
<li>facet by variable</li>
<li>subset data to facets for point plots and give aesthetics in <code>geom_point()</code></li>
<li>subset data to facets for box plots and give aesthetics in <code>geom_boxplot()</code>. Here we also need to set the <code>group</code> aesthetic; if we don’t specifically give that, we will get a plot with one big box, instead of a box for every time point.</li>
</ul>
<pre class="r"><code>fake_data_long %&gt;%
  ggplot() +
    facet_grid(facet ~ ., scales = &quot;free&quot;) +
    geom_point(data = subset(fake_data_long, facet == &quot;var2&quot;), 
               aes(x = time, y = y),
               size = 1) +
    geom_line(data = subset(fake_data_long, facet == &quot;var2&quot;), 
               aes(x = time, y = y)) +
    geom_boxplot(data = subset(fake_data_long, facet == &quot;var1&quot;), 
               aes(x = time, y = y, group = time))</code></pre>
<p><img src="/post/2017-11-18-combine_point_boxplot_ggplot_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.4
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] methods   stats     graphics  grDevices utils     datasets  base     
## 
## other attached packages:
##  [1] bindrcpp_0.2       forcats_0.3.0      stringr_1.3.0     
##  [4] dplyr_0.7.4        purrr_0.2.4        readr_1.1.1       
##  [7] tidyr_0.8.0        tibble_1.4.2       ggplot2_2.2.1.9000
## [10] tidyverse_1.2.1   
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_0.2.4  xfun_0.1          reshape2_1.4.3   
##  [4] haven_1.1.1       lattice_0.20-35   colorspace_1.3-2 
##  [7] htmltools_0.3.6   yaml_2.1.17       rlang_0.2.0.9000 
## [10] pillar_1.2.1      withr_2.1.1.9000  foreign_0.8-69   
## [13] glue_1.2.0        modelr_0.1.1      readxl_1.0.0     
## [16] bindr_0.1         plyr_1.8.4        munsell_0.4.3    
## [19] blogdown_0.5      gtable_0.2.0      cellranger_1.1.0 
## [22] rvest_0.3.2       psych_1.7.8       evaluate_0.10.1  
## [25] labeling_0.3      knitr_1.20        parallel_3.4.3   
## [28] broom_0.4.3       Rcpp_0.12.15      backports_1.1.2  
## [31] scales_0.5.0.9000 jsonlite_1.5      mnormt_1.5-5     
## [34] hms_0.4.1         digest_0.6.15     stringi_1.1.6    
## [37] bookdown_0.7      grid_3.4.3        rprojroot_1.3-2  
## [40] cli_1.0.0         tools_3.4.3       magrittr_1.5     
## [43] lazyeval_0.2.1    crayon_1.3.4      pkgconfig_2.0.1  
## [46] xml2_1.2.0        lubridate_1.7.3   assertthat_0.2.0 
## [49] rmarkdown_1.8     httr_1.3.1        rstudioapi_0.7   
## [52] R6_2.2.2          nlme_3.1-131.1    compiler_3.4.3</code></pre>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Explore Predictive Maintenance with flexdashboard]]></title>
    <link href="/2017/11/predictive_maintenance_dashboard/"/>
    <id>/2017/11/predictive_maintenance_dashboard/</id>
    <published>2017-11-02T00:00:00+00:00</published>
    <updated>2017-11-02T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I have written the following post about <a href="https://blog.codecentric.de/en/2017/11/explore-predictive-maintenance-flexdashboard/">Predictive Maintenance and flexdashboard</a> at my company <a href="https://blog.codecentric.de/en/">codecentric</a>’s blog:</p>
<blockquote>
<p>Predictive Maintenance is an increasingly popular strategy associated with Industry 4.0; it uses advanced analytics and machine learning to optimize machine costs and output (see Google Trends plot below). A common use-case for Predictive Maintenance is to proactively monitor machines, so as to predict when a check-up is needed to reduce failure and maximize performance. In contrast to traditional maintenance, where each machine has to undergo regular routine check-ups, Predictive Maintenance can save costs and reduce downtime. A machine learning approach to such a problem would be to analyze machine failure over time to train a supervised classification model that predicts failure. Data from sensors and weather information is often used as features in modeling.</p>
</blockquote>
<blockquote>
<p>…</p>
</blockquote>
<blockquote>
<p>With flexdashboard RStudio provides a great way to create interactive dashboards with R. It is an easy and very fast way to present analyses or create story maps. Here, I have used it to demonstrate different analysis techniques for Predictive Maintenance. It uses Shiny run-time to create interactive content.</p>
</blockquote>
<blockquote>
<p>…</p>
</blockquote>
<p>Continue reading at <a href="https://blog.codecentric.de/en/2017/11/explore-predictive-maintenance-flexdashboard/" class="uri">https://blog.codecentric.de/en/2017/11/explore-predictive-maintenance-flexdashboard/</a></p>
<div class="figure">
<img src="https://blog.codecentric.de/files/2017/10/dashboard_screenshot.png" />

</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Blockchain &amp; distributed ML - my report from the data2day conference]]></title>
    <link href="/2017/09/data2day/"/>
    <id>/2017/09/data2day/</id>
    <published>2017-09-28T00:00:00+00:00</published>
    <updated>2017-09-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://www.data2day.de/common/images/konferenzen/data2day2017.svg" />

</div>
<p>Yesterday and today I attended the <a href="www.data2day.de">data2day</a>, a conference about Big Data, Machine Learning and Data Science in Heidelberg, Germany. Topics and workshops covered a range of topics surrounding (big) data analysis and Machine Learning, like Deep Learning, Reinforcement Learning, TensorFlow applications, etc. Distributed systems and scalability were a major part of a lot of the talks as well, reflecting the growing desire to build bigger and more complex models that can’t (or would take too long to) run on a single computer. Most of the application examples were built in Python but one talk by Andreas Prawitt was specifically titled “Using R for Predictive Maintenance: an example from the TRUMPF Laser GmbH”. I also saw quite a few graphs that were obviously made with ggplot!</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="de" dir="ltr">
Guten Morgen auf der <a href="https://twitter.com/data2day"><span class="citation">@data2day</span></a> Kommt uns doch mal am Stand besuchen :-) <a href="https://t.co/YK46ACdNj9">pic.twitter.com/YK46ACdNj9</a>
</p>
— codecentric AG (<span class="citation">@codecentric</span>) <a href="https://twitter.com/codecentric/status/912928993279606784">September 27, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><br></p>
<p>The keynote lecture on Wednesday about <strong>Blockchains for AI</strong> was given by Trent McConaghy. <a href="https://www.sitepen.com/blog/2017/09/21/blockchain-basics/">Blockchain technology</a> is based on a decentralized system of storing and validating data and changes in data. It experiences a huge hype at the moment but it is only starting to gain track in Data Science and Machine Learning as well. I therefore found it a very fitting topic for the keynote lecture! Trent and his colleagues at <a href="www.bigchaindb.com">BigchainDB</a> are implementing an “internet-scale blockchain database for the world” - the Interplanetary Database (IPDB).</p>
<blockquote>
<p>“IPDB is a blockchain database that offers decentralized control, immutability and the creation and trading of digital assets. […] As a database for the world, IPDB offers decentralized control, strong governance and universal accessibility. IPDB relies on “caretaker” organizations around the world, who share responsibility for managing the network and governing the IPDB Foundation. Anyone in the world will be able to use IPDB. […]” <a href="https://blog.bigchaindb.com/ipdb-announced-as-public-planetary-scale-blockchain-database-7a363824fc14" class="uri">https://blog.bigchaindb.com/ipdb-announced-as-public-planetary-scale-blockchain-database-7a363824fc14</a></p>
</blockquote>
<p>He presented a number of examples where blockchain technology for decentralized data storage/access can be beneficial to Machine Learning and AI, like exchanging data from self-driving cars, of online market places and for generating art with computers. You can learn more about him <a href="https://blog.oceanprotocol.com/from-ai-to-blockchain-to-data-meet-ocean-f210ff460465">here</a>:</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="en" dir="ltr">
It's always been about the data.<br>Announcing Ocean.<a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a> <a href="https://twitter.com/hashtag/Blockchain?src=hash&amp;ref_src=twsrc%5Etfw">#Blockchain</a> <a href="https://twitter.com/oceanprotocol?ref_src=twsrc%5Etfw"><span class="citation">@OceanProtocol</span></a><a href="https://t.co/Do4XNn3ucN">https://t.co/Do4XNn3ucN</a>
</p>
— Trent McConaghy (<span class="citation">@trentmc0</span>) <a href="https://twitter.com/trentmc0/status/909793166416662528?ref_src=twsrc%5Etfw">September 18, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><br></p>
<p>The other talks were a mix of high- and low-level topics: from introductions to machine learning, Apache Spark and data analysis with Python to (distributed) data streaming with Kappa architecture or Apache Kafka, containerization with Docker and Kubernetes, data archiving with Apache Cassandra, relevance tuning with Solr and much more. While I spent most of the time at my company’s conference stand, I did hear three of the talks. I summarize each of them below…</p>
<p><br></p>
<ol style="list-style-type: decimal">
<li><strong>Scalable Machine Learning with Apache Spark for Fraud Detection</strong></li>
</ol>
<p>In this first talk I heard, Dr. Patrick Baier and Dr. Stanimir Dragiev presented their work at <a href="www.zalando.de/">Zalando</a>. They built a scalable machine learning framework with Apache Spark, Scala and AWS to assess and predict fraud in online transactions. <a href="www.zalando.de/">Zalando</a> is a German online store that sells clothes, shoes and accessories. Normally, they allow registered customers to buy via invoice, i.e. they receive their ordered items before they pay them. This leaves them vulnerable to fraud where item are not paid for. The goal of their data science team is to use customer and basket data to obtain a probability score for how likely a transaction is going to be fraudulent. High-risk payment options, like invoice, can then be disabled in transactions with high fraud probability. To build and run such machine learning models, the Zalando data science team uses a combination of Spark, Scala, R, AWS, SQL, Python, Docker, etc. In their workflow, they use a combination of static and dynamic features, imputing missing values and building a decision model. In order to scale their modeling workflow to process more requests, use more data in training, update models more frequently and/or run more models, they described a workflow that uses Spark, Scala and Amazon Web Services (AWS). Spark’s machine learning library can be used for modeling and scaled horizontally by increasing the number of clusters on which to run the models. Scala provides multi-threading functionality and AWS is used for storing data in S3 and extending computation power depending on changing needs. Finally, they include a model inspector into their workflow to assure comparability of training and test data and check that the model is behaving as expected. Problems that they are dealing with include highly unbalanced data (which is getting even worse the better their models work as they keep reducing the number of fraud cases), delayed labeling due to the long process of the transactions, seasonality in data.</p>
<p><br></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Sparse Data: Don’t Mind the Gap!</strong></li>
</ol>
<p>In this talk, my colleagues from <a href="www.codecentric.de">codecentric</a> Dr. Daniel Pape and Dr. Michael Plümacher showed an example from ad targeting of how to deal with sparse data. Sparse data occurs in many areas, e.g. as rare events over a long period of time or in areas where there are many items and few occurrences per item, like in recommender systems or in natural language processing (NLP). In ad targeting, the measure of success is the rate of the click-through rate (CRT): this is the number of clicks on a given advertisement displayed to a user on a website divided by the total number of advertisements, or impressions. Because financial revenue comes from a high CTR, advertisements should be placed in a way that maximizes their chance of being clicked, i.e. we want to recommend advertisements for specific users that match their interests or are of actual relevance. Sparsity come into play with ad targeting because the number of clicks is very low compared to two metrics: a) from all the potential ads that a user could see, only a small proportion is actually shown to her/him and b) of the ads that a user sees, she/he only clicks on very few. This means that, a CTR matrix of advertisements x targets will have very few combinations that have been clicked (the mean CTR is 0.003) and contain many missing values. The approach they took was to impute the missing values and predict for each target/user the most similar ads from the imputed CTR matrix. This approach worked well for a reasonably large data set but it didn’t perform so well with smaller (and therefore even sparser) data. They then talked about alternative approaches, like grouping users and/or ads into groups in order to reduce the sparsity of the data. Their take-home messages were that 1) there is no one-size-fits-all solution, what works depends on the context and 2) if the underlying data is of bad quality, the results will be sub-optimal - no matter how sophisticated the model.</p>
<p><br></p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Distributed TensorFlow with Kubernetes</strong></li>
</ol>
<p>In the third talk, another colleague of mine from <a href="www.codecentric.de">codecentric</a>, Jakob Karalus, explained in detail how to set up a distributed machine learning modelling set-up with <a href="https://www.tensorflow.org/">TensorFlow</a> and <a href="https://kubernetes.io/">Kubernetes</a>. TensorFlow is used to build neural networks in a graph-based manner. Distributed and parallel machine learning can be necessary when training big neural networks with a lot of training data, very deep neural networks, with complex parameters, grid search for hyper-parameter tuning, etc. A good way to build neural networks in a controlled and stable environment is to use <a href="https://www.docker.com/">Docker</a> containers. Kubernetes is a container orchestration tool that can set up distribution of nodes from our TensorFlow modeling container. Setting up this distributed system is quite complex, though and Jakob recommended to try to stay on one CPU/GPU as long as possible.</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="de" dir="ltr">
Tag 2 auf der <a href="https://twitter.com/data2day?ref_src=twsrc%5Etfw"><span class="citation">@data2day</span></a> kommt am Stand vorbei, wir haben noch ein paar T-Shirts und Softwerker für euch :-) <a href="https://t.co/xyG8Leg3lF">pic.twitter.com/xyG8Leg3lF</a>
</p>
— codecentric AG (<span class="citation">@codecentric</span>) <a href="https://twitter.com/codecentric/status/913301091755941888?ref_src=twsrc%5Etfw">September 28, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="de" dir="ltr">
Verteiltes Deep Learning mit TensorFlow und Kubernetes - <a href="https://twitter.com/krallistic"><span class="citation">@krallistic</span></a> auf der <a href="https://twitter.com/data2day"><span class="citation">@data2day</span></a> <a href="https://t.co/5AGJdhL5U1">pic.twitter.com/5AGJdhL5U1</a>
</p>
— codecentric AG (<span class="citation">@codecentric</span>) <a href="https://twitter.com/codecentric/status/913041395128111105">September 27, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[From Biology to Industry. A Blogger’s Journey to Data Science.]]></title>
    <link href="/2017/09/from-biology-to-industry.-a-bloggers-journey-to-data-science./"/>
    <id>/2017/09/from-biology-to-industry.-a-bloggers-journey-to-data-science./</id>
    <published>2017-09-20T00:00:00+00:00</published>
    <updated>2017-09-20T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Today, I have given a webinar for the Applied Epidemiology Didactic of the University of Wisconsin - Madison titled “From Biology to Industry. A Blogger’s Journey to Data Science.”</p>
<p>I talked about how blogging about R and Data Science helped me become a Data Scientist. I also gave a short introduction to Machine Learning, Big Data and Neural Networks.</p>
<p>My slides can be found here: <a href="https://www.slideshare.net/ShirinGlander/from-biology-to-industry-a-bloggers-journey-to-data-science" class="uri">https://www.slideshare.net/ShirinGlander/from-biology-to-industry-a-bloggers-journey-to-data-science</a></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Why I use R for Data Science - An Ode to R]]></title>
    <link href="/2017/09/ode_to_r/"/>
    <id>/2017/09/ode_to_r/</id>
    <published>2017-09-19T00:00:00+00:00</published>
    <updated>2017-09-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Working in Data Science, I often feel like I have to justify using R over Python. And while I do use Python for running scripts in production, I am much more comfortable with the R environment. Basically, whenever I can, I use R for prototyping, testing, visualizing and teaching. But because personal gut-feeling preference isn’t a very good reason to give to (scientifically minded) people, I’ve thought a lot about the pros and cons of using R. This is what I came up with why I still prefer R…</p>
<p><em>Disclaimer:</em> I have “grown up” with R and I’m much more familiar with it, so I admit that I am quite biased in my assessment. If you think I’m not doing other languages justice, I’ll be happy to hear your pros and cons!</p>
<p><br></p>
<ol style="list-style-type: decimal">
<li><p>First of, <a href="https://www.r-project.org/">R</a> is an <a href="https://cran.r-project.org/">open-source, cross-platform</a> language, so it’s free to use by any- and everybody. This in itself doesn’t make it special, though, because so are other languages, like Python.</p></li>
<li><p>It is an established language, so that there are lots and lots of packages for basically every type of analysis you can think of. You find packages for <a href="https://www.analyticsvidhya.com/blog/2015/08/list-r-packages-data-analysis/">data analysis</a>, <a href="http://www.kdnuggets.com/2017/02/top-r-packages-machine-learning.html">machine learning</a>, <a href="https://support.rstudio.com/hc/en-us/articles/201057987-Quick-list-of-useful-R-packages">visualization</a>, <a href="https://www.computerworld.com/article/2921176/business-intelligence/great-r-packages-for-data-import-wrangling-visualization.html">data wrangling</a>, <a href="https://cran.r-project.org/web/views/Spatial.html">spatial analysis</a>, <a href="https://www.bioconductor.org/">bioinformatics</a> and much more. But, same as with Python, this plethora of packages can sometimes make things a bit confusing: you would often need to test and compare several similar packages in order to find the best one.</p></li>
<li><p>Most of the packages are of very high quality. And when a package is on <a href="https://cran.r-project.org/web/packages/available_packages_by_name.html">CRAN</a> or <a href="https://www.bioconductor.org/">Bioconductor</a> (as most are), you can be sure that it has been checked, that you will get proper documentation and that you won’t have problems with installation, dependencies, etc. In my experience, R package and function documentation generally tends to be better than, say, of Python packages.</p></li>
<li><p>R’s graphics capabilities are superior to any other I know. Especially <a href="http://ggplot2.org/">ggplot2</a> with all its <a href="http://www.ggplot2-exts.org/">extensions</a> provides a structured, yet powerful set of tools for producing <a href="http://www.r-graph-gallery.com/portfolio/ggplot2-package/">high-quality publication-ready graphs and figures</a>. Moreover, ggplot2 is part of the <a href="https://www.tidyverse.org/">tidyverse</a> and works well with <a href="https://cran.r-project.org/web/packages/broom/vignettes/broom.html">broom</a>. This has made data wrangling and analysis much more convenient and structured and structured for me.</p></li>
<li><p>The suite of tools around <a href="https://www.rstudio.com/">R Studio</a> make it perfect for documenting data analysis workflows and for teaching. You can provide easy instructions for installation and <a href="http://rmarkdown.rstudio.com/">R Markdown</a> files for your students to follow along. Everybody is going to use the same system. In Python, you are always dealing with questions like version 2 vs version 3, Spyder vs Jupyter Notebook, pip vs conda, etc. <a href="https://www.rstudio.com/products/rpackages/">Everything around R Studio</a> is very well maintained and comes with extensive documentation and detailed tutorials. You find add-ins for <a href="https://support.rstudio.com/hc/en-us/articles/200532077-Version-Control-with-Git-and-SVN">version control</a>, <a href="http://shiny.rstudio.com/">Shiny</a> apps, writing books or other documents (<a href="https://bookdown.org/yihui/bookdown/">bookdown</a>) and you can write presentations directly in R Markdown, including code + output and everything as <a href="http://rmarkdown.rstudio.com/beamer_presentation_format.html">LaTeX beamer presentations</a>, <a href="http://rmarkdown.rstudio.com/ioslides_presentation_format.html">ioslides</a> or <a href="http://rmarkdown.rstudio.com/revealjs_presentation_format.html">reveal.js</a>. You can also create <a href="http://rmarkdown.rstudio.com/flexdashboard/">Dashboards</a>, include interactive <a href="http://rmarkdown.rstudio.com/developer_html_widgets.html">HTML widgets</a> and you can even build your blog (as this one is) with <a href="https://bookdown.org/yihui/blogdown/">blogdown</a> conveniently from within RStudio!</p></li>
<li><p>If you are looking for advanced functionality, it is very likely that somebody has already written a package for it. There are packages that allow you to access <a href="https://spark.rstudio.com/">Spark</a>, <a href="https://cran.r-project.org/web/packages/h2o/index.html">H2O</a>, <a href="https://ropensci.org/tutorials/elastic_tutorial.html">elasticsearch</a>, <a href="https://tensorflow.rstudio.com/">TensorFlow</a>, <a href="https://tensorflow.rstudio.com/keras/">Keras</a>, <a href="https://ropensci.org/blog/blog/2016/11/16/tesseract">tesseract</a>, and so many more with no hassle at all. And you can even run <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/system2.html">bash</a>, <a href="https://github.com/rstudio/reticulate">Python</a> from within R!</p></li>
<li><p>There is a big - and very active - community! This is one of the things I most enjoy about working with R. You can find many high-quality <a href="https://cran.r-project.org/manuals.html">manuals</a>, <a href="https://cran.r-project.org/other-docs.html">resources</a> and tutorials for all kinds of topics. Most of them provided free of charge by people who often dedicate their spare time to help others. The same goes for asking questions on <a href="https://stackoverflow.com/questions/tagged/r">Stack Overflow</a>, putting up issues on <a href="https://github.com/">Github</a> or <a href="https://groups.google.com/forum/#!forum/r-help-archive">Google groups</a>: usually you will get several answers within a short period of time (from my experience minutes to hours). What other community is so supportive and so helpful!? But for most things, you wouldn’t even need to ask for help because many of the packages come with absolutely amazing vignettes, that describe the functions and workflows in a detailed, yet easy to understand way. If that’s not enough, you will very likely find additional tutorials on <a href="https://www.r-bloggers.com/">R-bloggers</a>, a site maintained by Tal Galili that aggregates hundreds of R-blogs. There are several <a href="https://www.r-project.org/conferences.html">R Conferences</a>, like the <a href="https://user2018.r-project.org/">useR</a>, <a href="https://ropensci.org/community/events.html">rOpenSci Unconference</a> and many <a href="https://jumpingrivers.github.io/meetingsR/r-user-groups.html">R-user groups</a> all around the globe.</p></li>
</ol>
<p>I can’t stress enough how much I appreciate all the people who are involved in the R-community; who write packages, tutorials, blogs, who share information, provide support and who think about how to make data analysis easy, more convenient and - dare I say - fun!</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/circle-159252_1280_mfs0ku.png" alt="Community is everything!" />
<p class="caption">Community is everything!</p>
</div>
<p>The main drawbacks I experience with R are that scripts tends to be harder to deploy than Python (<a href="https://www.microsoft.com/en-us/cloud-platform/r-server">R-server</a> might be a solution, but I don’t know enough about it to really judge). Dealing with memory, space and security issues is often difficult in R. But there has already been a vast improvement over the last months/years, so I’m sure we will see development there in the future…</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Welcome to my page!]]></title>
    <link href="/page/about/"/>
    <id>/page/about/</id>
    <published>2017-09-12T16:06:06+02:00</published>
    <updated>2017-09-12T16:06:06+02:00</updated>
    <content type="html"><![CDATA[

<p>I&rsquo;m Shirin, a biologist turned bioinformatician turned data scientist. <img src="/img/Bewerbungsfoto_klein.jpg" alt="" /></p>

<p>I&rsquo;m especially interested in machine learning and data visualization. While I am using R most every day at work, I wanted to have an incentive to regularly explore other types of analyses and other types of data that I don&rsquo;t normally work with. I have also very often benefited from other people&rsquo;s published code in that it gave me ideas for my own work; and I hope that sharing my own analyses will inspire others as much as I often am by what can be be done with data.  It&rsquo;s amazing to me what can be learned from analyzing and visualizing data!</p>

<p>My tool of choice for data analysis so far has been R. I also organize the <a href="https://shiring.github.io/r_users_group/2017/05/20/muenster_r_user_group">MünsteR R-users group on meetup.com</a>.</p>

<p><img src="https://shiring.github.io/netlify_images/my_story_wml3zm.png" alt="My journey to Data Science" /></p>

<p>I love dancing and used to do competitive ballroom and latin dancing. Even though I don&rsquo;t have time for that anymore, I still enjoy teaching &ldquo;social dances&rdquo; once a week with the Hochschulsport (university sports courses).</p>

<p>I created the R package <a href="https://github.com/ShirinG/exprAnalysis">exprAnalysis</a>, designed to streamline my RNA-seq data analysis pipeline. It is available via Github. Instructions for installation and usage can be found <a href="https://shiring.github.io/rna-seq/microarray/2016/09/28/exprAnalysis">here</a>.</p>

<p>This blog will showcase some of the analyses I have been doing with different data sets (all freely available). I will also host teaching materials for students to access in conjunction with R courses I am giving.</p>

<hr />

<h2 id="contact-me">Contact me:</h2>

<ul>
<li><a href="https://www.codecentric.de/team/shirin-glander/">Codecentric AG</a></li>
<li><a href="mailto:shirin.glander@gmail.com">Email</a></li>
<li><a href="http://www.xing.com/profile/Shirin_Glander">Xing</a></li>
<li><a href="http://de.linkedin.com/in/shirin-glander-01120881">Linkedin</a></li>
<li><a href="http://twitter.com/ShirinGlander">Twitter</a></li>
</ul>

<hr />

<p>Also check out <a href="http://www.R-bloggers.com">R-bloggers</a> for lots of cool R stuff!</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Moving my blog to blogdown]]></title>
    <link href="/2017/09/moving-my-blog-to-blogdown/"/>
    <id>/2017/09/moving-my-blog-to-blogdown/</id>
    <published>2017-09-12T00:00:00+00:00</published>
    <updated>2017-09-12T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>It’s been a long time coming but I finally moved my blog from Jekyll/Bootstrap on Github pages to blogdown, Hugo and <a href="https://www.netlify.com/">Netlify</a>! Moreover, I also now have my own domain name <a href="https://www.shirin-glander.de">www.shirin-glander.de</a>. :-)</p>
<p>I followed the <a href="https://bookdown.org/yihui/blogdown/">blogdown ebook</a> to set up my blog. I chose Thibaud Leprêtre’s <a href="https://themes.gohugo.io/hugo-tranquilpeak-theme/">tranquilpeak theme</a>. It looks much more polished than my old blog.</p>
<p>My old blog will remain where it is, so that all the links that are out there will still work (and I don’t have to go through the hassle of migrating all my posts to my new site). You find a link to my old site in the sidebar.</p>
<p><br></p>
<hr />
<p>Just to test that everything works, I run the example code:</p>
<div id="r-markdown" class="section level1">
<h1>R Markdown</h1>
<p>This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <a href="http://rmarkdown.rstudio.com" class="uri">http://rmarkdown.rstudio.com</a>.</p>
<p>You can embed an R code chunk like this:</p>
<pre class="r"><code>summary(cars)
##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00
fit &lt;- lm(dist ~ speed, data = cars)
fit
## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Coefficients:
## (Intercept)        speed  
##     -17.579        3.932</code></pre>
</div>
<div id="including-plots" class="section level1">
<h1>Including Plots</h1>
<p>You can also embed plots. See Figure <a href="#fig:pie">1</a> for example:</p>
<pre class="r"><code>par(mar = c(0, 1, 0, 1))
pie(
  c(280, 60, 20),
  c(&#39;Sky&#39;, &#39;Sunny side of pyramid&#39;, &#39;Shady side of pyramid&#39;),
  col = c(&#39;#0292D8&#39;, &#39;#F7EA39&#39;, &#39;#C4B632&#39;),
  init.angle = -50, border = NA
)</code></pre>
<div class="figure"><span id="fig:pie"></span>
<img src="/post/2017-09-12-moving-my-blog-to-blogdown_files/figure-html/pie-1.png" alt="A fancy pie chart." width="672" />
<p class="caption">
Figure 1: A fancy pie chart.
</p>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Data Science for Fraud Detection]]></title>
    <link href="/2017/09/data-science-fraud-detection/"/>
    <id>/2017/09/data-science-fraud-detection/</id>
    <published>2017-09-06T00:00:00+00:00</published>
    <updated>2017-09-06T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I have written the following post about <a href="https://blog.codecentric.de/en/2017/09/data-science-fraud-detection/">Data Science for Fraud Detection</a> at my company <a href="https://blog.codecentric.de/en/">codecentric</a>&rsquo;s blog:</p>

<blockquote>
<p>Fraud can be defined as “the crime of getting money by deceiving people” (Cambridge Dictionary); it is as old as humanity: whenever two parties exchange goods or conduct business there is the potential for one party scamming the other. With an ever increasing use of the internet for shopping, banking, filing insurance claims, etc. these businesses have become targets of fraud in a whole new dimension. Fraud has become a major problem in e-commerce and a lot of resources are being invested to recognize and prevent it.</p>

<p>Traditional approaches to identifying fraud have been rule-based. This means that hard and fast rules for flagging a transaction as fraudulent have to be established manually and in advance. But this system isn’t flexible and inevitably results in an arms-race between the seller’s fraud detection system and criminals finding ways to circumnavigate these rules. The modern alternative is to leverage the vast amounts of Big Data that can be collected from online transactions and model it in a way that allows us to flag or predict fraud in future transactions. For this, Data Science and Machine Learning techniques, like Deep Neural Networks (DNNs), are the obvious solution!</p>

<p>Here, I am going to show an example of how Data Science techniques can be used to identify fraud in financial transactions. I will offer some insights into the inner workings of fraud analysis, aimed at non-experts to understand.</p>
</blockquote>

<p>Continue reading at <a href="https://blog.codecentric.de/en/2017/09/data-science-fraud-detection/">https://blog.codecentric.de/en/2017/09/data-science-fraud-detection/</a>&hellip;</p>

<p>The blog post is <a href="https://blog.codecentric.de/2017/09/fraud-analyse-mit-data-science-techniken/">also available in German</a>.</p>

<p><img src="https://shiring.github.io/netlify_images/r_mse_gklfsi.png" alt="" /></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Migrating from GitHub to GitLab with RStudio (Tutorial)]]></title>
    <link href="/2017/09/migrating-github-gitlab/"/>
    <id>/2017/09/migrating-github-gitlab/</id>
    <published>2017-09-04T00:00:00+00:00</published>
    <updated>2017-09-04T00:00:00+00:00</updated>
    <content type="html"><![CDATA[

<h2 id="github-vs-gitlab">GitHub vs. GitLab</h2>

<p>Git is a distributed implementation of version control. Many people have written very eloquently about why it is a good idea to use version control, not only if you collaborate in a team but also if you work on your own; one example is <a href="https://support.rstudio.com/hc/en-us/articles/200532077?version=1.0.153&amp;mode=desktop">this article from RStudio&rsquo;s Support pages</a>.</p>

<p>In short, its main feature is that version control allows you to keep track of the changes you make to your code. It will also keep a history of all the changes you have made in the past and allows you to go back to specific versions if you made a major mistake. And Git makes collaborating on the same code very easy.</p>

<p>Most R packages are also hosted on <a href="https://github.com/">GitHub</a>. You can check out their R code in the repositories if you want to get a deeper understanding of the functions, you can install the latest development versions of packages or install packages that are not on CRAN. The issue tracker function of GitHub also makes it easy to report and respond to issues/problems with your code.</p>

<h3 id="why-would-you-want-to-leave-github">Why would you want to leave GitHub?</h3>

<p>Public repositories are free on GitHub but you need to pay for private repos (if you are a student or work in academia, you <a href="https://education.github.com/discount_requests/new">get private repos for free</a>). Since I switched from academia to industry lately and no longer fulfil these criteria, all my private repos would have to be switched to public in the future. Here, GitLab is a great alternative!</p>

<p><a href="https://gitlab.com/">GitLab</a> offers very similar functionalities as GitHub. There are <a href="https://www.slant.co/versus/532/4860/~github_vs_gitlab">many pros and cons for using GitHub versus GitLab</a> but for me, the selling point was that GitLab offers unlimited private projects and collaborators in its free plan.</p>

<p><br></p>

<h1 id="tutorial">Tutorial</h1>

<p>Migrating from GitHub to <a href="https://gitlab.com/">GitLab</a> with RStudio is very easy! Here, I will show how I migrated my GitHub repositories of R projects, that I work with from within RStudio, to GitLab.</p>

<p><img src="https://shiring.github.io/netlify_images/GitLab_logo_yej6ht.png" alt="" /></p>

<p>Beware, that ALL code snippets below show Terminal code (they are NOT from the R console)!</p>

<p><br></p>

<h2 id="migrating-existing-repositories">Migrating existing repositories</h2>

<p>You first need to set up your GitLab account (you can login with your GitHub account) and connect your old GitHub account. Under <a href="https://gitlab.com/profile/account">Settings &amp;Account</a>, you will find &ldquo;Social sign-in&rdquo;; here click on &ldquo;Connect&rdquo; next to the GitHub symbol (if you signed in with your GitHub account, it will already be connected).</p>

<p>Once you have done this, you can import all your GitHub repositories to GitLab. To do this, you first need to create a new project. Click on the drop-down arrow next to the plus sign in the top-right corner and select &ldquo;New project&rdquo;. This will open the following window:</p>

<p><img src="https://shiring.github.io/netlify_images/Bildschirmfoto1_yuc7gb.png" alt="" /></p>

<p>Here, choose &ldquo;Import project from GitHub&rdquo; and choose the repositories you want to import.</p>

<p>If you go into one of your repositories, GitLab will show you a message at the top of the site that tells you that you need to add an SSH key. The SSH key is used for secure communication between the GitLab server and your computer when you want to share information, like push/pull commits.</p>

<p><img src="https://shiring.github.io/netlify_images/Bildschirmfoto2_diwetw.png" alt="" /></p>

<p>If you already work with GitHub on your computer, you will have an SSH key set up and you can <a href="https://gitlab.com/profile/keys">copy your public SSH key to GitLab</a>. Follow the instructions <a href="https://gitlab.com/help/ssh/README">here</a>.</p>

<p>Here is how you do it on a Mac:</p>

<ol>
<li>Look for your public key and copy it to the clipboard</li>
</ol>

<!-- -->

<pre><code>cat ~/.ssh/id_rsa.pub
pbcopy &lt; ~/.ssh/id_rsa.pub
</code></pre>

<p>Then paste it into the respective field <a href="https://gitlab.com/profile/keys">here</a>.</p>

<p>The next step is to change the remote URL for pushing/pulling your project from RStudio. In your Git window (tab next to &ldquo;Environment&rdquo; and &ldquo;History&rdquo; for me), click on Settings and &ldquo;Shell&rdquo;.</p>

<p><img src="https://shiring.github.io/netlify_images/Bildschirmfoto3_ydklnw.png" alt="" /></p>

<p>Then write in the shell window that opened:</p>

<pre><code>git remote set-url origin git@&lt;GITLABHOST&gt;:&lt;ORGNAME&gt;/&lt;REPO&gt;.git
</code></pre>

<p>You can copy the link in the navigation bar of your repo on GitLab.</p>

<p><img src="https://shiring.github.io/netlify_images/Bildschirmfoto4_dheikm.png" alt="" /></p>

<p>Check that you now have the correct new gitlab path by going to &ldquo;Tools&rdquo;, &ldquo;Project Options&rdquo; and &ldquo;Git/SVN&rdquo;.</p>

<p>Also check your SSH key configuration with:</p>

<pre><code>ssh -T git@&lt;GITLABHOST&gt;
</code></pre>

<p>If you get the following message</p>

<pre><code>The authenticity of host 'gitlab.com (52.167.219.168)' can't be established.
ECDSA key fingerprint is ...
Are you sure you want to continue connecting (yes/no)?
</code></pre>

<p>type &ldquo;yes&rdquo; (and enter passphrase if prompted).</p>

<p>If everything is okay, you now get a message saying <code>Welcome to GitLab!</code></p>

<p>Now, you can commit, push and pull from within RStudio just as you have done before!</p>

<p><br></p>

<h2 id="in-case-of-problems-with-pushing-pulling">In case of problems with pushing/pulling</h2>

<p>In my case, I migrated both, my private as well as my company&rsquo;s GitHub repos to GitLab. While my private repos could be migrated without a hitch, migrating my company&rsquo;s repos was a bit more tricky (because they had additional security settings, I assume).</p>

<p>Here is how I solved this problem with my company&rsquo;s repos:</p>

<p>I have protected my SSH key with a passphrase. When pushing or pulling commits via the shell with <code>git pull</code> and <code>git push origin master</code>, I am prompted to enter my passphrase and everything works fine. Pushing/pulling from within RStudio, however, threw an error:</p>

<pre><code>ssh_askpass: exec(/usr/X11R6/bin/ssh-askpass): No such file or directory
Permission denied (publickey).
fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.
</code></pre>

<p>I am using a MacBook Pro with MacOS Sierra version 10.12.6, so this might not be an issue with another operating system.</p>

<p>The following solution worked for me:</p>

<ol>
<li>Add your SSH key</li>
</ol>

<!-- -->

<pre><code>ssh-add ~/.ssh/id_rsa
</code></pre>

<ol>
<li>And reinstall <a href="https://vscode-eastus.azurewebsites.net/docs/setup/mac">VS Code</a></li>
</ol>

<p>Now I could commit, push and pull from within RStudio just as before!</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Social Network Analysis and Topic Modeling of codecentric’s Twitter friends and followers]]></title>
    <link href="/2017/07/twitter-analysis-codecentric/"/>
    <id>/2017/07/twitter-analysis-codecentric/</id>
    <published>2017-07-28T00:00:00+00:00</published>
    <updated>2017-07-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I have written the following post about <a href="https://blog.codecentric.de/en/2017/07/combining-social-network-analysis-topic-modeling-characterize-codecentrics-twitter-friends-followers/">Social Network Analysis and Topic Modeling of codecentric&rsquo;s Twitter friends and followers</a> for <a href="https://blog.codecentric.de/en/">codecentric</a>&rsquo;s blog:</p>

<blockquote>
<p>Recently, Matthias Radtke has written a very nice blog post on Topic Modeling of the codecentric Blog Articles, where he is giving a comprehensive introduction to Topic Modeling. In this article I am showing a real-world example of how we can use Data Science to gain insights from text data and social network analysis.</p>

<p>I am using publicly available Twitter data to characterize codecentric&rsquo;s friends and followers for identifying the most &ldquo;influential&rdquo; followers and using text analysis tools like sentiment analysis to characterize their interests from their user descriptions, performing Social Network Analysis on friends, followers and a subset of second degree connections to identify key players who will be able to pass on information to a wide reach of other users and combing this network analysis with topic modeling to identify meta-groups with similar interests.</p>

<p>Knowing the interests and social network positions of our followers allows us to identify key users who are likely to retweet posts that fall within their range of interests and who will reach a wide audience.</p>

<p>&hellip;</p>
</blockquote>

<p>Continue reading at <a href="https://blog.codecentric.de/en/2017/07/combining-social-network-analysis-topic-modeling-characterize-codecentrics-twitter-friends-followers/">https://blog.codecentric.de/en/2017/07/combining-social-network-analysis-topic-modeling-characterize-codecentrics-twitter-friends-followers/</a>&hellip;</p>

<p>The entire analysis has been done in R 3.4.0 and you can find my code on <a href="https://github.com/ShirinG/blog_posts_prep/blob/master/twitter/twitter_codecentric.Rmd">Github</a>.</p>

<p><img src="https://shiring.github.io/netlify_images/twitter_net_topics_lnu3j9.png" alt="" /></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Find all my other posts on my old website!]]></title>
    <link href="/2017/07/find-all-my-other-posts-on-my-old-website/"/>
    <id>/2017/07/find-all-my-other-posts-on-my-old-website/</id>
    <published>2017-07-01T00:00:00+00:00</published>
    <updated>2017-07-01T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>For all my other posts, see my old website:
<a href="https://shiring.github.io">shiring.github.io</a></p>
]]></content>
  </entry>
</feed>