<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Shirin&#39;s playgRound</title>
  <link href="/index.xml" rel="self"/>
  <link href="/"/>
  <updated>2018-02-05T00:00:00+00:00</updated>
  <id>/</id>
  <author>
    <name>Dr. Shirin Glander</name>
  </author>
  <generator>Hugo -- gohugo.io</generator>
  <entry>
    <title type="html"><![CDATA[April 12th &amp; 13th: workshop on Deep Learning with Keras and TensorFlow in R]]></title>
    <link href="/2018/02/deep_learning_keras_tensorflow_18_04/"/>
    <id>/2018/02/deep_learning_keras_tensorflow_18_04/</id>
    <published>2018-02-05T00:00:00+00:00</published>
    <updated>2018-02-05T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Registration is now open for my 1.5-day <a href="https://www.eventbrite.de/e/workshop-deep-learning-mit-keras-und-tensorflow-tickets-42710095044?utm-medium=discovery&amp;utm-campaign=social&amp;utm-content=attendeeshare&amp;aff=escb&amp;utm-source=cp&amp;utm-term=listing">workshop on deep learning with Keras and TensorFlow using R</a>.</p>
<p>It will take place on <strong>April 12th and 13th</strong> in <strong>Hamburg, Germany</strong>.</p>
<p>In my workshop, you will learn</p>
<ul>
<li>the basics of deep learning</li>
<li>what cross-entropy and loss is</li>
<li>about activation functions</li>
<li>how to optimize weights and biases with backpropagation and gradient descent</li>
<li>how to build (deep) neural networks with Keras and TensorFlow</li>
<li>how to save and load models and model weights</li>
<li>how to visualize models with TensorBoard</li>
<li>how to make predictions on test data</li>
</ul>
<p><a href="https://www.eventbrite.de/e/workshop-deep-learning-mit-keras-und-tensorflow-tickets-42710095044?utm-medium=discovery&amp;utm-campaign=social&amp;utm-content=attendeeshare&amp;aff=escb&amp;utm-source=cp&amp;utm-term=listing">Tickets can be booked via eventbrite</a>.</p>
<p><br></p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/keras_workshop_april18.png" />

</div>
<p>Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. Keras is very convenient for fast and easy prototyping of neural networks. It is highly modular and very flexible, so that you can build basically any type of neural network you want. It supports convolutional neural networks and recurrent neural networks, as well as combinations of both. Due to its layer structure, it is highly extensible and can run on CPU or GPU.</p>
<p>The <code>keras</code> R package provides an interface to the Python library of Keras, just as the tensorflow package provides an interface to TensorFlow. Basically, R creates a conda instance and runs Keras it it, while you can still use all the functionalities of R for plotting, etc. Almost all function names are the same, so models can easily be recreated in Python for deployment.</p>
<p><br></p>
<div class="figure">
<img src="https://blog.keras.io/img/keras-tensorflow-logo.jpg" />

</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Conferences, webinars, podcasts and the likes]]></title>
    <link href="/page/conferences_podcasts_webinars/"/>
    <id>/page/conferences_podcasts_webinars/</id>
    <published>2018-02-01T16:06:06+02:00</published>
    <updated>2018-02-01T16:06:06+02:00</updated>
    <content type="html"><![CDATA[

<p>Here, you can find a list of all the talks I gave at conferences, webinars, podcasts, workshops, and all the other places you can and could hear me talk. :-)</p>

<h2 id="workshops-i-am-giving">Workshops I am giving</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2017/11/deep_learning_keras_tensorflow/">Workshop on Deep Learning with Keras and TensorFlow in R</a></li>
</ul>

<blockquote>
<p>I offer a workshop on deep learning with Keras and TensorFlow using R.
Date and place depend on who and how many people are interested, so please contact me either directly or via the workshop page: <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/">https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/</a> (the description is in German but I also offer to give the workshop in English).</p>
</blockquote>

<h2 id="upcoming-talks-webinars-podcasts-etc">Upcoming talks, webinars, podcasts, etc.</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/02/m3_2018/">Announcing my talk about explainability of machine learning models at Minds Mastering Machines Conference</a></li>
</ul>

<blockquote>
<p>On Wednesday, April 25th 2018 I am going to talk about explainability of machine learning models at the Minds Mastering Machines conference in Cologne.</p>
</blockquote>

<ul>
<li><a href="JAX 2018 talk announcement: Deep Learning - a Primer">JAX 2018 talk announcement: Deep Learning - a Primer</a></li>
</ul>

<blockquote>
<p>Deep Learning is one of the &ldquo;hot&rdquo; topics in the AI area – a lot of hype, a lot of inflated expectation, but also quite some impressive success stories. As some AI experts already predict that Deep Learning will become &ldquo;Software 2.0&rdquo;, it might be a good time to have a closer look at the topic. In this session I will try to give a comprehensive overview of Deep Learning. We will start with a bit of history and some theoretical foundations that we will use to create a little Deep Learning taxonomy. Then we will have a look at current and upcoming application areas: Where can we apply Deep Learning successfully and what does it differentiate from other approaches? Afterwards we will examine the ecosystem: Which tools and libraries are available? What are their strengths and weaknesses? And to complete the session, we will look into some practical code examples and the typical pitfalls of Deep Learning. After this session you will have a much better idea of the why, what and how of Deep Learning, including if and how you might want to apply it to your own work. <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/">https://jax.de/big-data-machine-learning/deep-learning-a-primer/</a></p>
</blockquote>

<h2 id="past-talks-webinars-podcasts-etc">Past talks, webinars, podcasts, etc.</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/02/herr_mies_wills_wissen/">I talk about machine learning with Daniel Mies (Podcast in German, though)</a></li>
</ul>

<blockquote>
<p>In January 2018 I was interviewed for a tech podcast where I talked about machine learning, neural nets, why I love R and Rstudio and how I became a Data Scientist.</p>
</blockquote>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/">Explaining Predictions of Machine Learning Models with LIME - Münster Data Science Meetup</a></li>
</ul>

<blockquote>
<p>In December 2017 I talked about Explaining Predictions of Machine Learning Models with LIME at the Münster Data Science Meetup.</p>
</blockquote>

<ul>
<li><a href="https://shiring.github.io/blogging/2017/09/20/webinar_biology_to_data_science">From Biology to Industry. A Blogger’s Journey to Data Science</a></li>
</ul>

<blockquote>
<p>In September 2017 I gave a webinar for the Applied Epidemiology Didactic of the University of Wisconsin - Madison titled “From Biology to Industry. A Blogger’s Journey to Data Science.”
I talked about how blogging about R and Data Science helped me become a Data Scientist. I also gave a short introduction to Machine Learning, Big Data and Neural Networks.</p>
</blockquote>

<ul>
<li><a href="https://shiring.github.io/machine_learning/2017/03/31/webinar_code">Building meaningful machine learning models for disease prediction</a></li>
</ul>

<blockquote>
<p>In March 2017 I gave a webinar for the ISDS R Group about my work on building machine-learning models to predict the course of different diseases. I went over building a model, evaluating its performance, and answering or addressing different disease related questions using machine learning. My talk covered the theory of machine learning as it is applied using R.</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Announcing my talk about explainability of machine learning models at Minds Mastering Machines conference]]></title>
    <link href="/2018/02/m3_2018/"/>
    <id>/2018/02/m3_2018/</id>
    <published>2018-02-01T00:00:00+00:00</published>
    <updated>2018-02-01T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>On Wednesday, April 25th 2018 I am going to <a href="https://www.m3-konferenz.de/veranstaltung-6274-erkl%C3%A4rbarkeit-von-machine-learning%3A-wie-k%C3%B6nnen-wir-vertrauen-in-komplexe-modelle-schaffen.html?id=6274">talk about explainability of machine learning models at the Minds Mastering Machines conference in Cologne</a>. The conference will be in German, though.</p>
<div class="figure">
<img src="https://www.m3-konferenz.de/common/images/konferenzen/m3.png" />

</div>
<blockquote>
<p>ERKLÄRBARKEIT VON MACHINE LEARNING: WIE KÖNNEN WIR VERTRAUEN IN KOMPLEXE MODELLE SCHAFFEN?</p>
</blockquote>
<blockquote>
<p>Mit Machine-Learning getroffene Entscheidungen sind inhärent schwierig – wenn nicht gar unmöglich – nachzuvollziehen. Die Komplexität einiger der besten Modelle, wie Neuronale Netzwerke, ist genau das, was sie so erfolgreich macht. Aber es macht sie gleichzeitig zu einer Black Box. Das kann problematisch sein, denn Geschäftsführer oder Vorstände werden weniger geneigt sein einer Entscheidung zu vertrauen und nach ihr zu handeln, wenn sie sie nicht verstehen.</p>
</blockquote>
<blockquote>
<p>Local Interpretable Model-Agnostic Explanations (LIME) ist ein Versuch, diese komplexen Modelle zumindest teilweise nachvollziehbar zu machen. In diesem Vortrag erkläre ich das Prinzip und zeige Anwendungsbeispiele von LIME.</p>
</blockquote>
<blockquote>
<p>Vorkenntnisse Grundkenntnisse Machine Learning &amp; Statistik</p>
</blockquote>
<blockquote>
<p>Lernziele * Einblick in Möglichkeit, die komplexe Modelle erklärbar machen * Vertrauen in Entscheidungen durch Machine Learning schaffen</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[I talk about machine learning with Daniel Mies (Podcast in German, though)]]></title>
    <link href="/2018/02/herr_mies_wills_wissen/"/>
    <id>/2018/02/herr_mies_wills_wissen/</id>
    <published>2018-02-01T00:00:00+00:00</published>
    <updated>2018-02-01T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>For those of you out there who speak German:</p>
<p>I was interviewed for a tech podcast where I talked about machine learning, neural nets, why I love R and Rstudio and how I became a Data Scientist.</p>
<p>You can download and listen to the podcast here: <a href="https://mies.me/2018/01/31/hmww17-machine-learning-mit-dr-shirin-glander/" class="uri">https://mies.me/2018/01/31/hmww17-machine-learning-mit-dr-shirin-glander/</a></p>
<div class="figure">
<img src="https://mies.me/wp-content/cache/podlove/09/cad1c2bcc3b506410d277c27cc12fb/herr-mies-wills-wissen_500x500.png" />

</div>
<blockquote>
<p>In der aktuellen Episode gibt Dr. Shirin Glander (Twitter, Homepage) uns ein paar Einblicke in das Thema Machine Learning. Wir klären zunächst, was Machine Learning ist und welche Möglichkeiten es bietet bevor wir etwas mehr in die Tiefe gehen. Wir beginnen mit Neuronalen Netzen und Entscheidungsbäumen und wie sich diese unterschieden. Hier kommen wir natürlich auch nicht an Supervised Learning, Unsupervised Learning und Reinforcement Learning vorbei. Wichtig bei der Arbeit mit Machine Learning sind die verwendeten Daten: Hier beginnt man mit Testdaten und Trainingsdaten, welche man mit Hilfe von Feature Engineering für die jeweilige Aufgabe optimieren kann. Shirin erzählt, wie sie mit Daten arbeitet und wie sie die richtigen Algorithmen findet. Eine wichtige Rolle spielen hier R und R Studio, welches sich besonders für statistische Analysen eignet. Gerade die Visualisierung der Daten ist hier hilfreich um selbige besser zu verstehen. Aber auch die Möglichkeiten Reports zu erzeugen und beispielsweise als PDF zu exportieren überzeugen. Wenn ihr R für Machine Learning einsetzen wollt, solltet ihr Euch auch caret ansehen. Shirin organisiert übrigens auch MünsteR, die R Users group in Münster. Wenn ihr Euch näher mit Machine Learning beschäftigen wollt, solltet ihr Euch Datacamp oder Coursera ansehen. Wenn ihr Euch für R interessiert schaut Euch die R Bloggers an Am Ende sprechen wir auch noch kurz über Deep Dreaming. Den passenden Generator hierfür, findet ihr unter deepdreamgenerator.com.</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[JAX 2018 talk announcement: Deep Learning - a Primer]]></title>
    <link href="/2018/01/jax2018/"/>
    <id>/2018/01/jax2018/</id>
    <published>2018-01-30T00:00:00+00:00</published>
    <updated>2018-01-30T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I am happy to announce that on Tuesday, April 24th 2018 Uwe Friedrichsen and I will give a talk about <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/">Deep Learning - a Primer</a> at the JAX conference in Mainz, Germany.</p>
<blockquote>
<p>Deep Learning is one of the “hot” topics in the AI area – a lot of hype, a lot of inflated expectation, but also quite some impressive success stories. As some AI experts already predict that Deep Learning will become “Software 2.0”, it might be a good time to have a closer look at the topic. In this session I will try to give a comprehensive overview of Deep Learning. We will start with a bit of history and some theoretical foundations that we will use to create a little Deep Learning taxonomy. Then we will have a look at current and upcoming application areas: Where can we apply Deep Learning successfully and what does it differentiate from other approaches? Afterwards we will examine the ecosystem: Which tools and libraries are available? What are their strengths and weaknesses? And to complete the session, we will look into some practical code examples and the typical pitfalls of Deep Learning. After this session you will have a much better idea of the why, what and how of Deep Learning, including if and how you might want to apply it to your own work. <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/" class="uri">https://jax.de/big-data-machine-learning/deep-learning-a-primer/</a></p>
</blockquote>
<div class="figure">
<img src="https://pbs.twimg.com/media/DUt3SXyUQAE3TOv.jpg" alt="https://twitter.com/jaxcon/status/957990506331557890" />
<p class="caption"><a href="https://twitter.com/jaxcon/status/957990506331557890" class="uri">https://twitter.com/jaxcon/status/957990506331557890</a></p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #94: Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley]]></title>
    <link href="/2018/01/twimlai94/"/>
    <id>/2018/01/twimlai94/</id>
    <published>2018-01-28T00:00:00+00:00</published>
    <updated>2018-01-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai94.jpg" alt="Sketchnotes from TWiMLAI talk #94: Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley" />
<p class="caption">Sketchnotes from TWiMLAI talk #94: Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-94-neuroevolution-evolving-novel-neural-network-architectures-kenneth-stanley/">here</a>.</p>
<blockquote>
<p>Kenneth studied under TWiML Talk #47 guest Risto Miikkulainen at UT Austin, and joined Uber AI Labs after Geometric Intelligence , the company he co-founded with Gary Marcus and others, was acquired in late 2016. Kenneth’s research focus is what he calls Neuroevolution, applies the idea of genetic algorithms to the challenge of evolving neural network architectures. In this conversation, we discuss the Neuroevolution of Augmenting Topologies (or NEAT) paper that Kenneth authored along with Risto, which won the 2017 International Society for Artificial Life’s Award for Outstanding Paper of the Decade 2002 – 2012. We also cover some of the extensions to that approach he’s created since, including, HyperNEAT, which can efficiently evolve very large networks with connectivity patterns that look more like those of the human and that are generally much larger than what prior approaches to neural learning could produce, and novelty search, an approach which unlike most evolutionary algorithms has no defined objective, but rather simply searches for novel behaviors. We also cover concepts like “Complexification” and “Deception”, biology vs computation including differences and similarities, and some of his other work including his book, and NERO, a video game complete with Real-time Neuroevolution. This is a meaty “Nerd Alert” interview that I think you’ll really enjoy. <a href="https://twimlai.com/twiml-talk-94-neuroevolution-evolving-novel-neural-network-architectures-kenneth-stanley/" class="uri">https://twimlai.com/twiml-talk-94-neuroevolution-evolving-novel-neural-network-architectures-kenneth-stanley/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Join MünsteR for our next meetup on obtaining functional implications of gene expression data with R]]></title>
    <link href="/2018/01/meetup_march18/"/>
    <id>/2018/01/meetup_march18/</id>
    <published>2018-01-24T00:00:00+00:00</published>
    <updated>2018-01-24T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://shiring.github.io/r_users_group/2017/05/20/map.png" />

</div>
<p>In our <a href="http://meetu.ps/e/DDY1B/w54bW/f">next MünsteR R-user group meetup</a> on <strong>March 5th, 2018</strong> Frank Rühle will talk about bioinformatics and how to analyse genome data.</p>
<p>You can RSVP here: <a href="http://meetu.ps/e/DDY1B/w54bW/f" class="uri">http://meetu.ps/e/DDY1B/w54bW/f</a></p>
<blockquote>
<p>Next-Generation sequencing and array-based technologies provided a plethora of gene expression data in the public genomics databases. But how to get meaningful information and functional implications out of this vast amount of data? Various R-packages have been published by the Bioconductor user community for distinct kinds of analysis strategies. Here, several approaches will be presented for functional gene annotation, gene enrichment analysis and co-expression network analysis. A collection of wrapper functions for streamlined analysis of expression data can be found at: <a href="https://github.com/frankRuehle/systemsbio" class="uri">https://github.com/frankRuehle/systemsbio</a>.</p>
</blockquote>
<p>Dr. Frank Rühle is a post-doctoral research fellow in the group of genetic epidemiology at the Institute of Human Genetics at the University of Münster. As biologist with focus on computational biology he aims at identifying genomic biomarker for complex cardiovascular diseases by analyzing multiomics data with respect to a systems biology view. Further research interests include the functions of long non-coding RNAs and their impact on gene regulation in heart-related phenotypes.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #92: Learning State Representations with Yael Niv]]></title>
    <link href="/2018/01/twimlai92/"/>
    <id>/2018/01/twimlai92/</id>
    <published>2018-01-19T00:00:00+00:00</published>
    <updated>2018-01-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about Learning State Representations with Yael Niv: <a href="https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/" class="uri">https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/</a></p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai92.jpg" alt="Sketchnotes from TWiMLAI talk #92: Learning State Representations with Yael Niv" />
<p class="caption">Sketchnotes from TWiMLAI talk #92: Learning State Representations with Yael Niv</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/">here</a>.</p>
<blockquote>
<p>In this interview Yael and I explore the relationship between neuroscience and machine learning. In particular, we discusses the importance of state representations in human learning, some of her experimental results in this area, and how a better understanding of representation learning can lead to insights into machine learning problems such as reinforcement and transfer learning. Did I mention this was a nerd alert show? I really enjoyed this interview and I know you will too. Be sure to send over any thoughts or feedback via the show notes page. <a href="https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/" class="uri">https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[How to make your machine learning model available as an API with the plumber package]]></title>
    <link href="/2018/01/plumber/"/>
    <id>/2018/01/plumber/</id>
    <published>2018-01-16T00:00:00+00:00</published>
    <updated>2018-01-16T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://d33wubrfki0l68.cloudfront.net/03f3d9156c39a8043be42caeee1507d43d832472/8d968/components/images/plumber.png" />

</div>
<p>The <strong>plumber</strong> package for R makes it easy to expose existing R code as a webservice via an API (<a href="https://www.rplumber.io/" class="uri">https://www.rplumber.io/</a>, Trestle Technology, LLC 2017).</p>
<p>You take an existing R script and make it accessible with <code>plumber</code> by simply adding a few lines of comments. If you have worked with Roxygen before, e.g. when building a package, you will already be familiar with the core concepts. If not, here are the most important things to know:</p>
<ul>
<li>you define the output or endpoint</li>
<li>you can add additional annotation to customize your input, output and other functionalities of your API</li>
<li>you can define every input parameter that will go into your function</li>
<li>every such annotation will begin with either <code>#'</code> or <code>#*</code></li>
</ul>
<p>With this setup, we can take a trained machine learning model and make it available as an API. With this API, other programs can access it and use it to make predictions.</p>
<div id="what-are-apis-and-webservices" class="section level2">
<h2>What are APIs and webservices?</h2>
<p>With <code>plumber</code>, we can build so called <strong>HTTP APIs</strong>. HTTP stands for Hypertext Transfer Protocol and is used to transmit information on the web; API stands for Application Programming Interface and governs the connection between some software and underlying applications. Software can then communicate via HTTP APIs. This way, our R script can be called from other software, even if the other program is not written in R and we have built a tool for machine-to-machine communication, i.e. a webservice.</p>
</div>
<div id="how-to-convert-your-r-script-into-an-api-with-plumber" class="section level2">
<h2>How to convert your R script into an API with plumber</h2>
<div id="training-and-saving-a-model" class="section level3">
<h3>Training and saving a model</h3>
<p>Let’s say we have trained a machine learning model as in <a href="https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/">this post about LIME</a>. I loaded a data set on chronic kidney disease, did some preprocessing (converting categorical features into dummy variables, scaling and centering), split it into training and test data and trained a Random Forest model with <code>caret</code>. We can use this trained model to make predictions for one test case with the following code:</p>
<pre class="r"><code>library(tidyverse)

# load test and train data
load(&quot;../../data/test_data.RData&quot;)
load(&quot;../../data/train_data.RData&quot;)

# load model
load(&quot;../../data/model_rf.RData&quot;)

# take first test case for prediction
input_data &lt;- test_data[1, ] %&gt;%
  select(-class)

# predict test case using model
pred &lt;- predict(model_rf, input_data)
cat(&quot;----------------\nTest case predicted to be&quot;, as.character(pred), &quot;\n----------------&quot;)</code></pre>
<pre><code>## ----------------
## Test case predicted to be ckd 
## ----------------</code></pre>
</div>
<div id="the-input" class="section level3">
<h3>The input</h3>
<p>For our API to work, we need to define the input, in our case the features of the test data. When we look at the model object, we see that it expects the following parameters:</p>
<pre class="r"><code>var_names &lt;- model_rf$finalModel$xNames
var_names</code></pre>
<pre><code>##  [1] &quot;age&quot;            &quot;bp&quot;             &quot;sg_1.005&quot;       &quot;sg_1.010&quot;      
##  [5] &quot;sg_1.015&quot;       &quot;sg_1.020&quot;       &quot;sg_1.025&quot;       &quot;al_0&quot;          
##  [9] &quot;al_1&quot;           &quot;al_2&quot;           &quot;al_3&quot;           &quot;al_4&quot;          
## [13] &quot;al_5&quot;           &quot;su_0&quot;           &quot;su_1&quot;           &quot;su_2&quot;          
## [17] &quot;su_3&quot;           &quot;su_4&quot;           &quot;su_5&quot;           &quot;rbc_normal&quot;    
## [21] &quot;rbc_abnormal&quot;   &quot;pc_normal&quot;      &quot;pc_abnormal&quot;    &quot;pcc_present&quot;   
## [25] &quot;pcc_notpresent&quot; &quot;ba_present&quot;     &quot;ba_notpresent&quot;  &quot;bgr&quot;           
## [29] &quot;bu&quot;             &quot;sc&quot;             &quot;sod&quot;            &quot;pot&quot;           
## [33] &quot;hemo&quot;           &quot;pcv&quot;            &quot;wbcc&quot;           &quot;rbcc&quot;          
## [37] &quot;htn_yes&quot;        &quot;htn_no&quot;         &quot;dm_yes&quot;         &quot;dm_no&quot;         
## [41] &quot;cad_yes&quot;        &quot;cad_no&quot;         &quot;appet_good&quot;     &quot;appet_poor&quot;    
## [45] &quot;pe_yes&quot;         &quot;pe_no&quot;          &quot;ane_yes&quot;        &quot;ane_no&quot;</code></pre>
<p>Good practice is to write the input parameter definition into you <a href="https://swagger.io/swagger-ui/">API Swagger UI</a>, but the code would work without these annotations. We define the parameters by annotating them with name and description in our R-script using <code>@parameter</code>. For this purpose, I want to know the type and min/max values for each of my variables in the training data. Because categorical data has been converted to dummy variables and then scaled and centered, these values will all be numeric and between 0 and 1 in this example. If I would build this script for a real case, I’d use the raw data as input and add a preprocessing function to my script, though!</p>
<pre class="r"><code># show parameter definition for the first three features
for (i in 1:3) {
# if you wanted to see it for all features, use
#for (i in 1:length(var_names)) {
  var &lt;- var_names[i]
  train_data_subs &lt;- train_data[, which(colnames(train_data) == var)]
  type &lt;- class(train_data_subs)
  
  if (type == &quot;numeric&quot;) {
    min &lt;- min(train_data_subs)
    max &lt;- max(train_data_subs)
  }
  
  cat(&quot;Variable:&quot;, var, &quot;is of type:&quot;, type, &quot;\n&quot;,
      &quot;Min value in training data =&quot;, min, &quot;\n&quot;,
      &quot;Max value in training data =&quot;, max, &quot;\n----------\n&quot;)

}</code></pre>
<pre><code>## Variable: age is of type: numeric 
##  Min value in training data = 0 
##  Max value in training data = 0.9777778 
## ----------
## Variable: bp is of type: numeric 
##  Min value in training data = 0 
##  Max value in training data = 0.7222222 
## ----------
## Variable: sg_1.005 is of type: numeric 
##  Min value in training data = 0 
##  Max value in training data = 1 
## ----------</code></pre>
<blockquote>
<p>Unless otherwise instructed, all parameters passed into plumber endpoints from query strings or dynamic paths will be character strings. <a href="https://www.rplumber.io/docs/routing-and-input.html#typed-dynamic-routes" class="uri">https://www.rplumber.io/docs/routing-and-input.html#typed-dynamic-routes</a></p>
</blockquote>
<p>This means that we need to convert numeric values before we process them further. Or we can define the parameter type explicitly, e.g. by writing <code>variable_1:numeric</code> if we want to specifiy that <em>variable_1</em> is supposed to be numeric.</p>
<p>To make sure that the model will perform as expected, it is also advisable to add a few validation functions. Here, I will validate</p>
<ul>
<li>whether every parameter is numeric/integer by checking for NAs (which would have resulted from <code>as.numeric()</code>/<code>as.integer()</code> applied to data of character type)</li>
<li>whether every parameter is between 0 and 1</li>
</ul>
<p>In order for <code>plumber</code> to work with our input, it needs to be part of the HTTP request, which can then be routed to our R function. The <a href="https://www.rplumber.io/docs/routing-and-input.html#query-strings">plumber documentation</a> describes how to use query strings as inputs. But in our case, manually writing query strings is not practical because we have so many parameters. Of course, there are programs that let us generate query strings but the easiest way to format the input from a line of data I found is to use JSON.</p>
<p>The <code>toJSON()</code> function from the <code>rjson</code> package converts our input line to JSON format:</p>
<pre class="r"><code>library(rjson)
test_case_json &lt;- toJSON(input_data)
cat(test_case_json)</code></pre>
<pre><code>## {&quot;age&quot;:0.511111111111111,&quot;bp&quot;:0.111111111111111,&quot;sg_1.005&quot;:1,&quot;sg_1.010&quot;:0,&quot;sg_1.015&quot;:0,&quot;sg_1.020&quot;:0,&quot;sg_1.025&quot;:0,&quot;al_0&quot;:0,&quot;al_1&quot;:0,&quot;al_2&quot;:0,&quot;al_3&quot;:0,&quot;al_4&quot;:1,&quot;al_5&quot;:0,&quot;su_0&quot;:1,&quot;su_1&quot;:0,&quot;su_2&quot;:0,&quot;su_3&quot;:0,&quot;su_4&quot;:0,&quot;su_5&quot;:0,&quot;rbc_normal&quot;:1,&quot;rbc_abnormal&quot;:0,&quot;pc_normal&quot;:0,&quot;pc_abnormal&quot;:1,&quot;pcc_present&quot;:1,&quot;pcc_notpresent&quot;:0,&quot;ba_present&quot;:0,&quot;ba_notpresent&quot;:1,&quot;bgr&quot;:0.193877551020408,&quot;bu&quot;:0.139386189258312,&quot;sc&quot;:0.0447368421052632,&quot;sod&quot;:0.653374233128834,&quot;pot&quot;:0,&quot;hemo&quot;:0.455056179775281,&quot;pcv&quot;:0.425925925925926,&quot;wbcc&quot;:0.170454545454545,&quot;rbcc&quot;:0.225,&quot;htn_yes&quot;:1,&quot;htn_no&quot;:0,&quot;dm_yes&quot;:0,&quot;dm_no&quot;:1,&quot;cad_yes&quot;:0,&quot;cad_no&quot;:1,&quot;appet_good&quot;:0,&quot;appet_poor&quot;:1,&quot;pe_yes&quot;:1,&quot;pe_no&quot;:0,&quot;ane_yes&quot;:1,&quot;ane_no&quot;:0}</code></pre>
</div>
<div id="defining-the-endpoint-and-output" class="section level3">
<h3>Defining the endpoint and output</h3>
<p>In order to convert this very simple script into an API, we need to define the endpoint(s). Endpoints will return an output, in our case it will return the output of the <code>predict()</code> function pasted into a line of text (e.g. “Test case predicted to be ckd”). Here, we want to have the predictions returned, so we annotate the entire function with <code>@get</code>. This endpoint in the API will get a custom name, so that we can call it later; here we call it <code>predict</code> and therefore write <code>#' @get /predict</code>.</p>
<blockquote>
<p>According to the design of the HTTP specification, GET (along with HEAD) requests are used only to read data and not change it. Therefore, when used this way, they are considered safe. That is, they can be called without risk of data modification or corruption — calling it once has the same effect as calling it 10 times, or none at all. Additionally, GET (and HEAD) is idempotent, which means that making multiple identical requests ends up having the same result as a single request. <a href="http://www.restapitutorial.com/lessons/httpmethods.html" class="uri">http://www.restapitutorial.com/lessons/httpmethods.html</a></p>
</blockquote>
<p>In this case, we could also consider using <code>@post</code> to avoid caching issues, but for this example I’ll leave it as <code>@get</code>.</p>
<blockquote>
<p>The POST verb is most-often utilized to <strong>create</strong> new resources. In particular, it’s used to create subordinate resources. That is, subordinate to some other (e.g. parent) resource. In other words, when creating a new resource, POST to the parent and the service takes care of associating the new resource with the parent, assigning an ID (new resource URI), etc. On successful creation, return HTTP status 201, returning a Location header with a link to the newly-created resource with the 201 HTTP status. POST is neither safe nor idempotent. It is therefore recommended for non-idempotent resource requests. Making two identical POST requests will most-likely result in two resources containing the same information. <a href="http://www.restapitutorial.com/lessons/httpmethods.html" class="uri">http://www.restapitutorial.com/lessons/httpmethods.html</a></p>
</blockquote>
<p>We can also customize the output. Keep in mind though, that the output should be <a href="https://www.rplumber.io/docs/rendering-and-output.html#serializers">“serialized”</a>. By default, the output will be in JSON format. Here, I want to have a text output, so I’ll specify <code>@html</code> without html formatting specifications, although I could add them if I wanted to display the text on a website. If we were to <a href="https://www.rplumber.io/docs/runtime.html#external-data-store">store the data in a database</a>, however, this would not be a good idea. In that case, it would be better to output the result as a JSON object.</p>
</div>
<div id="logging-with-filters" class="section level3">
<h3>Logging with filters</h3>
<p>It is also useful to provide some sort of logging for your API. Here, I am using the simple example from the <a href="https://www.rplumber.io/docs/routing-and-input.html#filters">plumber documentation</a> that uses filters and output the logs to the console or your API server logs. You could also <a href="https://www.rplumber.io/docs/runtime.html#file-system">write your logging output to a file</a>. In production, it would be better to use a real logging setup that stores information about each request, e.g. the time stamp, whether any errors or warnings occurred, etc. The <code>forward()</code> part of the logging function passes control on to the next handler in the pipeline, here our predict function.</p>
</div>
<div id="running-the-plumber-script" class="section level3">
<h3>Running the plumber script</h3>
<p>We need to save the entire script with annotations as an <em>.R</em> file as seen below. The regular comments <code>#</code> describe what each section does.</p>
<pre><code># script name:
# plumber.R

# set API title and description to show up in http://localhost:8000/__swagger__/

#&#39; @apiTitle Run predictions for Chronic Kidney Disease with Random Forest Model
#&#39; @apiDescription This API takes as patient data on Chronic Kidney Disease and returns a prediction whether the lab values
#&#39; indicate Chronic Kidney Disease (ckd) or not (notckd).
#&#39; For details on how the model is built, see https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/
#&#39; For further explanations of this plumber function, see https://shirinsplayground.netlify.com/2018/01/plumber/

# load model
# this path would have to be adapted if you would deploy this
load(&quot;/Users/shiringlander/Documents/Github/shirinsplayground/data/model_rf.RData&quot;)

#&#39; Log system time, request method and HTTP user agent of the incoming request
#&#39; @filter logger
function(req){
  cat(&quot;System time:&quot;, as.character(Sys.time()), &quot;\n&quot;,
      &quot;Request method:&quot;, req$REQUEST_METHOD, req$PATH_INFO, &quot;\n&quot;,
      &quot;HTTP user agent:&quot;, req$HTTP_USER_AGENT, &quot;@&quot;, req$REMOTE_ADDR, &quot;\n&quot;)
  plumber::forward()
}

# core function follows below:
# define parameters with type and description
# name endpoint
# return output as html/text
# specify 200 (okay) return

#&#39; predict Chronic Kidney Disease of test case with Random Forest model
#&#39; @param age:numeric The age of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param bp:numeric The blood pressure of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param sg_1.005:int The urinary specific gravity of the patient, integer (1: sg = 1.005, otherwise 0)
#&#39; @param sg_1.010:int The urinary specific gravity of the patient, integer (1: sg = 1.010, otherwise 0)
#&#39; @param sg_1.015:int The urinary specific gravity of the patient, integer (1: sg = 1.015, otherwise 0)
#&#39; @param sg_1.020:int The urinary specific gravity of the patient, integer (1: sg = 1.020, otherwise 0)
#&#39; @param sg_1.025:int The urinary specific gravity of the patient, integer (1: sg = 1.025, otherwise 0)
#&#39; @param al_0:int The urine albumin level of the patient, integer (1: al = 0, otherwise 0)
#&#39; @param al_1:int The urine albumin level of the patient, integer (1: al = 1, otherwise 0)
#&#39; @param al_2:int The urine albumin level of the patient, integer (1: al = 2, otherwise 0)
#&#39; @param al_3:int The urine albumin level of the patient, integer (1: al = 3, otherwise 0)
#&#39; @param al_4:int The urine albumin level of the patient, integer (1: al = 4, otherwise 0)
#&#39; @param al_5:int The urine albumin level of the patient, integer (1: al = 5, otherwise 0)
#&#39; @param su_0:int The sugar level of the patient, integer (1: su = 0, otherwise 0)
#&#39; @param su_1:int The sugar level of the patient, integer (1: su = 1, otherwise 0)
#&#39; @param su_2:int The sugar level of the patient, integer (1: su = 2, otherwise 0)
#&#39; @param su_3:int The sugar level of the patient, integer (1: su = 3, otherwise 0)
#&#39; @param su_4:int The sugar level of the patient, integer (1: su = 4, otherwise 0)
#&#39; @param su_5:int The sugar level of the patient, integer (1: su = 5, otherwise 0)
#&#39; @param rbc_normal:int The red blood cell count of the patient, integer (1: rbc = normal, otherwise 0)
#&#39; @param rbc_abnormal:int The red blood cell count of the patient, integer (1: rbc = abnormal, otherwise 0)
#&#39; @param pc_normal:int The pus cell level of the patient, integer (1: pc = normal, otherwise 0)
#&#39; @param pc_abnormal:int The pus cell level of the patient, integer (1: pc = abnormal, otherwise 0)
#&#39; @param pcc_present:int The puc cell clumps status of the patient, integer (1: pcc = present, otherwise 0)
#&#39; @param pcc_notpresent:int The puc cell clumps status of the patient, integer (1: pcc = notpresent, otherwise 0)
#&#39; @param ba_present:int The bacteria status of the patient, integer (1: ba = present, otherwise 0)
#&#39; @param ba_notpresent:int The bacteria status of the patient, integer (1: ba = notpresent, otherwise 0)
#&#39; @param bgr:numeric The blood glucose random level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param bu:numeric The blood urea level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param sc:numeric The serum creatinine level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param sod:numeric The sodium level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param pot:numeric The potassium level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param hemo:numeric The hemoglobin level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param pcv:numeric The packed cell volume of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param wbcc:numeric The white blood cell count of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param rbcc:numeric The red blood cell count of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param htn_yes:int The hypertension status of the patient, integer (1: htn = yes, otherwise 0)
#&#39; @param htn_no:int The hypertension status of the patient, integer (1: htn = no, otherwise 0)
#&#39; @param dm_yes:int The diabetes mellitus status of the patient, integer (1: dm = yes, otherwise 0)
#&#39; @param dm_no:int The diabetes mellitus status of the patient, integer (1: dm = no, otherwise 0)
#&#39; @param cad_yes:int The coronary artery disease status of the patient, integer (1: cad = yes, otherwise 0)
#&#39; @param cad_no:int The coronary artery disease status of the patient, integer (1: cad = no, otherwise 0)
#&#39; @param appet_good:int The appetite of the patient, integer (1: appet = good, otherwise 0)
#&#39; @param appet_poor:int The appetite of the patient, integer (1: appet = poor, otherwise 0)
#&#39; @param pe_yes:int The pedal edema status of the patient, integer (1: pe = yes, otherwise 0)
#&#39; @param pe_no:int The pedal edema status of the patient, integer (1: pe = no, otherwise 0)
#&#39; @param ane_yes:int The anemia status of the patient, integer (1: ane = yes, otherwise 0)
#&#39; @param ane_no:int The anemia status of the patient, integer (1: ane = no, otherwise 0)
#&#39; @get /predict
#&#39; @html
#&#39; @response 200 Returns the class (ckd or notckd) prediction from the Random Forest model; ckd = Chronic Kidney Disease
calculate_prediction &lt;- function(age, bp, sg_1.005, sg_1.010, sg_1.015, sg_1.020, sg_1.025, al_0, al_1, al_2, 
                                al_3, al_4, al_5, su_0, su_1, su_2, su_3, su_4, su_5, rbc_normal, rbc_abnormal, pc_normal, pc_abnormal,
                                pcc_present, pcc_notpresent, ba_present, ba_notpresent, bgr, bu, sc, sod, pot, hemo, pcv, 
                                wbcc, rbcc, htn_yes, htn_no, dm_yes, dm_no, cad_yes, cad_no, appet_good, appet_poor, pe_yes, pe_no, 
                                ane_yes, ane_no) {
  
  # make data frame from numeric parameters
  input_data_num &lt;&lt;- data.frame(age, bp, bgr, bu, sc, sod, pot, hemo, pcv, wbcc, rbcc,
                     stringsAsFactors = FALSE)
  # and make sure they really are numeric
  input_data_num &lt;&lt;- as.data.frame(t(sapply(input_data_num, as.numeric)))
  
  # make data frame from (binary) integer parameters
  input_data_int &lt;&lt;- data.frame(sg_1.005, sg_1.010, sg_1.015, sg_1.020, sg_1.025, al_0, al_1, al_2, 
                                al_3, al_4, al_5, su_0, su_1, su_2, su_3, su_4, su_5, rbc_normal, rbc_abnormal, pc_normal, pc_abnormal,
                                pcc_present, pcc_notpresent, ba_present, ba_notpresent, htn_yes, htn_no, dm_yes, dm_no, 
                                cad_yes, cad_no, appet_good, appet_poor, pe_yes, pe_no, ane_yes, ane_no,
                                stringsAsFactors = FALSE)
  # and make sure they really are numeric
  input_data_int &lt;&lt;- as.data.frame(t(sapply(input_data_int, as.integer)))
  # combine into one data frame
  input_data &lt;&lt;- as.data.frame(cbind(input_data_num, input_data_int))
  
  # validation for parameter
  if (any(is.na(input_data))) {
    res$status &lt;- 400
    res$body &lt;- &quot;Parameters have to be numeric or integers&quot;
  }
  
  if (any(input_data &lt; 0) || any(input_data &gt; 1)) {
    res$status &lt;- 400
    res$body &lt;- &quot;Parameters have to be between 0 and 1&quot;
  }

  # predict and return result
  pred_rf &lt;&lt;- predict(model_rf, input_data)
  paste(&quot;----------------\nTest case predicted to be&quot;, as.character(pred_rf), &quot;\n----------------\n&quot;)
}
</code></pre>
<p>Note that I am using the “double-assignment” operator <code>&lt;&lt;-</code> in my function, because I want to make sure that objects are overwritten at the top level (i.e. globally). This would have been relevant had I set a global parameter, but to show it the example, I decided to use it here as well.</p>
<p>We can now call our script with the <code>plumb()</code> function, run it with <code>run()</code> and open it on port 800. Calling <code>plumb()</code> creates an environment in which all our functions are evaluated.</p>
<pre class="r"><code>library(plumber)</code></pre>
<pre class="r"><code>r &lt;- plumb(&quot;/Users/shiringlander/Documents/Github/shirinsplayground/static/scripts/plumber.R&quot;)
r$run(port = 8000)</code></pre>
<p>We will now see the following message in our R console:</p>
<pre><code>Starting server to listen on port 8000
Running the swagger UI at http://127.0.0.1:8000/__swagger__/</code></pre>
<p>If you go to *<a href="http://localhost:8000/__swagger__/*" class="uri">http://localhost:8000/__swagger__/*</a>, you could now try out the function by manually choosing values for all the parameters we defined in the script.</p>
<p><img src="https://shiring.github.io/netlify_images/swagger1.png" alt="http://localhost:8000/__swagger__/" /> … <img src="https://shiring.github.io/netlify_images/swagger2.png" alt="http://localhost:8000/__swagger__/ continued" /></p>
<p>Because we annotated the <code>calculate_prediction()</code> function in our script with <code>#' @get /predict</code> we can access it via *<a href="http://localhost:8000/predict*" class="uri">http://localhost:8000/predict*</a>. But because we have no input specified as of yet, we will only see an error on this site. So, we still need to put our JSON formatted input into the function. To do this, we can use <a href="https://en.wikipedia.org/wiki/CURL"><em>curl</em></a> from the terminal and feed in the JSON string from above. If you are using RStudio in the latest version, you have a handy terminal window open in your working directory. You find it right next to the Console.</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/terminal_rstudio.png" alt="Terminal in RStudio" />
<p class="caption">Terminal in RStudio</p>
</div>
<pre><code>curl -H &quot;Content-Type: application/json&quot; -X GET -d &#39;{&quot;age&quot;:0.511111111111111,&quot;bp&quot;:0.111111111111111,&quot;sg_1.005&quot;:1,&quot;sg_1.010&quot;:0,&quot;sg_1.015&quot;:0,&quot;sg_1.020&quot;:0,&quot;sg_1.025&quot;:0,&quot;al_0&quot;:0,&quot;al_1&quot;:0,&quot;al_2&quot;:0,&quot;al_3&quot;:0,&quot;al_4&quot;:1,&quot;al_5&quot;:0,&quot;su_0&quot;:1,&quot;su_1&quot;:0,&quot;su_2&quot;:0,&quot;su_3&quot;:0,&quot;su_4&quot;:0,&quot;su_5&quot;:0,&quot;rbc_normal&quot;:1,&quot;rbc_abnormal&quot;:0,&quot;pc_normal&quot;:0,&quot;pc_abnormal&quot;:1,&quot;pcc_present&quot;:1,&quot;pcc_notpresent&quot;:0,&quot;ba_present&quot;:0,&quot;ba_notpresent&quot;:1,&quot;bgr&quot;:0.193877551020408,&quot;bu&quot;:0.139386189258312,&quot;sc&quot;:0.0447368421052632,&quot;sod&quot;:0.653374233128834,&quot;pot&quot;:0,&quot;hemo&quot;:0.455056179775281,&quot;pcv&quot;:0.425925925925926,&quot;wbcc&quot;:0.170454545454545,&quot;rbcc&quot;:0.225,&quot;htn_yes&quot;:1,&quot;htn_no&quot;:0,&quot;dm_yes&quot;:0,&quot;dm_no&quot;:1,&quot;cad_yes&quot;:0,&quot;cad_no&quot;:1,&quot;appet_good&quot;:0,&quot;appet_poor&quot;:1,&quot;pe_yes&quot;:1,&quot;pe_no&quot;:0,&quot;ane_yes&quot;:1,&quot;ane_no&quot;:0}&#39; &quot;http://localhost:8000/predict&quot;</code></pre>
<blockquote>
<p><strong>-H</strong> defines an extra header to include in the request when sending HTTP to a server (<a href="https://curl.haxx.se/docs/manpage.html#-H" class="uri">https://curl.haxx.se/docs/manpage.html#-H</a>).</p>
</blockquote>
<blockquote>
<p><strong>-X</strong> pecifies a custom request method to use when communicating with the HTTP server (<a href="https://curl.haxx.se/docs/manpage.html#-X" class="uri">https://curl.haxx.se/docs/manpage.html#-X</a>).</p>
</blockquote>
<blockquote>
<p><strong>-d</strong> sends the specified data in a request to the HTTP server, in the same way that a browser does when a user has filled in an HTML form and presses the submit button. This will cause curl to pass the data to the server using the content-type application/x-www-form-urlencoded (<a href="https://curl.haxx.se/docs/manpage.html#-d" class="uri">https://curl.haxx.se/docs/manpage.html#-d</a>).</p>
</blockquote>
<p>This will return the following output:</p>
<ul>
<li><code>cat()</code> outputs to the R console if you use R interactively; if you use R on a server, it will be included in the server logs.</li>
</ul>
<pre><code>System time: 2018-01-15 13:34:32 
 Request method: GET /predict 
 HTTP user agent: curl/7.54.0 @ 127.0.0.1 </code></pre>
<ul>
<li><code>paste</code> outputs to the terminal</li>
</ul>
<pre><code>----------------
Test case predicted to be ckd 
----------------</code></pre>
</div>
<div id="security" class="section level3">
<h3>Security</h3>
<p>This example shows a pretty simply R-script API. But if you plan on deploying your API to production, you should consider the <a href="https://www.rplumber.io/docs/security.html">security section of the plumber documentation</a>. It give additional information about how you can make your code (more) secure.</p>
</div>
<div id="finalize" class="section level3">
<h3>Finalize</h3>
<p>If you wanted to deploy this API you would need to <a href="https://www.rplumber.io/docs/hosting.html">host</a> it, i.e. provide the model and run an R environment with plumber, ideally on a server. A good way to do this, would be to package everything in a <a href="https://www.rplumber.io/docs/hosting.html#docker">Docker</a> container and run this. Docker will ensure that you have a working snapshot of the system settings, R and package versions that won’t change. For more information on dockerizing your API, check out <a href="https://hub.docker.com/r/trestletech/plumber/" class="uri">https://hub.docker.com/r/trestletech/plumber/</a>.</p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.2
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] methods   stats     graphics  grDevices utils     datasets  base     
## 
## other attached packages:
##  [1] plumber_0.4.4   rjson_0.2.15    forcats_0.2.0   stringr_1.2.0  
##  [5] dplyr_0.7.4     purrr_0.2.4     readr_1.1.1     tidyr_0.7.2    
##  [9] tibble_1.4.2    ggplot2_2.2.1   tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##  [1] httr_1.3.1          ddalpha_1.3.1       sfsmisc_1.1-1      
##  [4] jsonlite_1.5        splines_3.4.3       foreach_1.4.4      
##  [7] prodlim_1.6.1       modelr_0.1.1        assertthat_0.2.0   
## [10] stats4_3.4.3        DRR_0.0.3           cellranger_1.1.0   
## [13] yaml_2.1.16         robustbase_0.92-8   ipred_0.9-6        
## [16] pillar_1.1.0        backports_1.1.2     lattice_0.20-35    
## [19] glue_1.2.0          digest_0.6.14       randomForest_4.6-12
## [22] rvest_0.3.2         colorspace_1.3-2    recipes_0.1.2      
## [25] httpuv_1.3.5        htmltools_0.3.6     Matrix_1.2-12      
## [28] plyr_1.8.4          psych_1.7.8         timeDate_3042.101  
## [31] pkgconfig_2.0.1     CVST_0.2-1          broom_0.4.3        
## [34] haven_1.1.1         caret_6.0-78        bookdown_0.5       
## [37] scales_0.5.0        gower_0.1.2         lava_1.6           
## [40] withr_2.1.1         nnet_7.3-12         lazyeval_0.2.1     
## [43] cli_1.0.0           mnormt_1.5-5        survival_2.41-3    
## [46] magrittr_1.5        crayon_1.3.4        readxl_1.0.0       
## [49] evaluate_0.10.1     nlme_3.1-131        MASS_7.3-48        
## [52] xml2_1.1.1          dimRed_0.1.0        foreign_0.8-69     
## [55] class_7.3-14        blogdown_0.4        tools_3.4.3        
## [58] hms_0.4.0           kernlab_0.9-25      munsell_0.4.3      
## [61] bindrcpp_0.2        compiler_3.4.3      RcppRoll_0.2.2     
## [64] rlang_0.1.6         grid_3.4.3          iterators_1.0.9    
## [67] rstudioapi_0.7      rmarkdown_1.8       gtable_0.2.0       
## [70] ModelMetrics_1.1.0  codetools_0.2-15    reshape2_1.4.3     
## [73] R6_2.2.2            lubridate_1.7.1     knitr_1.18         
## [76] bindr_0.1           rprojroot_1.3-2     stringi_1.1.6      
## [79] parallel_3.4.3      Rcpp_0.12.15        rpart_4.1-12       
## [82] tidyselect_0.2.3    DEoptimR_1.0-8</code></pre>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #91: Philosophy of Intelligence with Matthew Crosby]]></title>
    <link href="/2018/01/twimlai91/"/>
    <id>/2018/01/twimlai91/</id>
    <published>2018-01-14T00:00:00+00:00</published>
    <updated>2018-01-14T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about Philosophy of Intelligence with Matthew Crosby: <a href="https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/" class="uri">https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/</a></p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai91.jpg" alt="Sketchnotes from TWiMLAI talk #92: Philosophy of Intelligence with Matthew Crosby" />
<p class="caption">Sketchnotes from TWiMLAI talk #92: Philosophy of Intelligence with Matthew Crosby</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-91-philosophy-intelligence-matthew-crosby/">here</a>.</p>
<blockquote>
<p>This week on the podcast we’re featuring a series of conversations from the NIPs conference in Long Beach, California. I attended a bunch of talks and learned a ton, organized an impromptu roundtable on Building AI Products, and met a bunch of great people, including some former TWiML Talk guests.This time around i’m joined by Matthew Crosby, a researcher at Imperial College London, working on the Kinds of Intelligence Project. Matthew joined me after the NIPS Symposium of the same name, an event that brought researchers from a variety of disciplines together towards three aims: a broader perspective of the possible types of intelligence beyond human intelligence, better measurements of intelligence, and a more purposeful analysis of where progress should be made in AI to best benefit society. Matthew’s research explores intelligence from a philosophical perspective, exploring ideas like predictive processing and controlled hallucination, and how these theories of intelligence impact the way we approach creating artificial intelligence. This was a very interesting conversation, i’m sure you’ll enjoy. <a href="https://twimlai.com/twiml-talk-91-philosophy-intelligence-matthew-crosby/" class="uri">https://twimlai.com/twiml-talk-91-philosophy-intelligence-matthew-crosby/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Looking beyond accuracy to improve trust in machine learning]]></title>
    <link href="/2018/01/looking_beyond_accuracy_to_improve_trust_in_ml/"/>
    <id>/2018/01/looking_beyond_accuracy_to_improve_trust_in_ml/</id>
    <published>2018-01-10T00:00:00+00:00</published>
    <updated>2018-01-10T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I have written another blogpost about <a href="https://blog.codecentric.de/en/2018/01/look-beyond-accuracy-improve-trust-machine-learning/">Looking beyond accuracy to improve trust in machine learning</a> at my company <a href="https://blog.codecentric.de/en/">codecentric</a>&rsquo;s blog:</p>

<blockquote>
<p>Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex machine learning models are essentially black boxes and too complicated to understand, we need to use approximations to get a better sense of how they work. One such approach is LIME, which stands for Local Interpretable Model-agnostic Explanations and is a tool that helps understand and explain the decisions made by complex machine learning models.</p>
</blockquote>

<p>Continue reading at <a href="https://blog.codecentric.de/en/2018/01/look-beyond-accuracy-improve-trust-machine-learning/">https://blog.codecentric.de/en/2018/01/look-beyond-accuracy-improve-trust-machine-learning/</a>&hellip;</p>

<p>Links to the entire example code and more info are given at the end of the blog post.</p>

<p>The blog post is <a href="https://blog.codecentric.de/2018/01/vertrauen-und-vorurteile-maschinellem-lernen/">also available in German</a>.</p>

<p><img src="https://blog.codecentric.de/files/2018/01/lime_output_figure.png" alt="" /></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[TWiMLAI talk 88 sketchnotes: Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru]]></title>
    <link href="/2018/01/twimlai88_sketchnotes/"/>
    <id>/2018/01/twimlai88_sketchnotes/</id>
    <published>2018-01-10T00:00:00+00:00</published>
    <updated>2018-01-10T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes taken from the <a href="https://twimlai.com/twiml-talk-88-using-deep-learning-google-street-view-estimate-demographics-timnit-gebru/">“This week in Machine Learning &amp; AI” podcast number 88 about Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru</a>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai88_sketchnotes_vhjzac.jpg" alt="Sketchnotes from TWiMLAI talk #88: Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru" />
<p class="caption">Sketchnotes from TWiMLAI talk #88: Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru</p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Registration now open for workshop on Deep Learning with Keras and TensorFlow using R]]></title>
    <link href="/2017/12/keras_sketchnotes/"/>
    <id>/2017/12/keras_sketchnotes/</id>
    <published>2017-12-20T00:00:00+00:00</published>
    <updated>2017-12-20T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Recently, I announced <a href="https://shirinsplayground.netlify.com/2017/11/deep_learning_keras_tensorflow/">my workshop on Deep Learning with Keras and TensorFlow</a>.</p>
<p>The next dates for it are <strong>January 18th and 19th</strong> in Solingen, Germany.</p>
<p>You can register now by following this link: <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow" class="uri">https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow</a></p>
<p>If any non-German-speaking people want to attend, I’m happy to give the course in English!</p>
<p><a href="mailto:shirin.glander@codecentric.de">Contact me if you have further questions.</a></p>
<hr />
<p>As a little bonus, I am also sharing my sketch notes from a Podcast I listened to when first getting into Keras:</p>
<ul>
<li><a href="https://softwareengineeringdaily.com/2016/01/29/deep-learning-and-keras-with-francois-chollet/">Software Engineering Daily with Francois Chollet</a></li>
</ul>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/keras_sketchnotes_lgrnvo.jpg" alt="Sketchnotes: Software Engineering Daily - Podcast from Jan 29th 2016" />
<p class="caption">Sketchnotes: Software Engineering Daily - Podcast from Jan 29th 2016</p>
</div>
<p>Links from the notes:</p>
<ul>
<li><a href="https://keras.io/">Keras for Python</a></li>
<li><a href="https://keras.rstudio.com/">Keras for R</a></li>
<li><a href="https://github.com/maxpumperla/elephas">elephas</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Explaining Predictions of Machine Learning Models with LIME - Münster Data Science Meetup]]></title>
    <link href="/2017/12/lime_sketchnotes/"/>
    <id>/2017/12/lime_sketchnotes/</id>
    <published>2017-12-12T00:00:00+00:00</published>
    <updated>2017-12-12T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="slides-from-munster-data-science-meetup" class="section level2">
<h2>Slides from Münster Data Science Meetup</h2>
<p><a href="https://shiring.github.io/netlify_images/lime_meetup_slides_wvsh6s.pdf">These are my slides</a> from the <a href="https://www.meetup.com/Data-Science-Meetup-Muenster/events/244173239/">Münster Data Science Meetup on December 12th, 2017</a>.</p>
<pre class="r"><code>knitr::include_url(&quot;https://shiring.github.io/netlify_images/lime_meetup_slides_wvsh6s.pdf&quot;)</code></pre>
<iframe src="https://shiring.github.io/netlify_images/lime_meetup_slides_wvsh6s.pdf" width="672" height="400px">
</iframe>
<p><br></p>
<p>My sketchnotes were collected from these two podcasts:</p>
<ul>
<li><a href="https://twimlai.com/twiml-talk-7-carlos-guestrin-explaining-predictions-machine-learning-models/" class="uri">https://twimlai.com/twiml-talk-7-carlos-guestrin-explaining-predictions-machine-learning-models/</a></li>
<li><a href="https://dataskeptic.com/blog/episodes/2016/trusting-machine-learning-models-with-lime" class="uri">https://dataskeptic.com/blog/episodes/2016/trusting-machine-learning-models-with-lime</a></li>
</ul>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/lime_sketchnotes_guq6u5.jpg" alt="Sketchnotes: TWiML Talk #7 with Carlos Guestrin – Explaining the Predictions of Machine Learning Models &amp; Data Skeptic Podcast - Trusting Machine Learning Models with Lime" />
<p class="caption">Sketchnotes: TWiML Talk #7 with Carlos Guestrin – Explaining the Predictions of Machine Learning Models &amp; Data Skeptic Podcast - Trusting Machine Learning Models with Lime</p>
</div>
<hr />
</div>
<div id="example-code" class="section level2">
<h2>Example Code</h2>
<ul>
<li>the following libraries were loaded:</li>
</ul>
<pre class="r"><code>library(tidyverse)  # for tidy data analysis
library(farff)      # for reading arff file
library(missForest) # for imputing missing values
library(dummies)    # for creating dummy variables
library(caret)      # for modeling
library(lime)       # for explaining predictions</code></pre>
<div id="data" class="section level3">
<h3>Data</h3>
<p>The Chronic Kidney Disease dataset was downloaded from UC Irvine’s Machine Learning repository: <a href="http://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease" class="uri">http://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease</a></p>
<pre class="r"><code>data_file &lt;- file.path(&quot;path/to/chronic_kidney_disease_full.arff&quot;)</code></pre>
<ul>
<li>load data with the <code>farff</code> package</li>
</ul>
<pre class="r"><code>data &lt;- readARFF(data_file)</code></pre>
<div id="features" class="section level4">
<h4>Features</h4>
<ul>
<li>age - age</li>
<li>bp - blood pressure</li>
<li>sg - specific gravity</li>
<li>al - albumin</li>
<li>su - sugar</li>
<li>rbc - red blood cells</li>
<li>pc - pus cell</li>
<li>pcc - pus cell clumps</li>
<li>ba - bacteria</li>
<li>bgr - blood glucose random</li>
<li>bu - blood urea</li>
<li>sc - serum creatinine</li>
<li>sod - sodium</li>
<li>pot - potassium</li>
<li>hemo - hemoglobin</li>
<li>pcv - packed cell volume</li>
<li>wc - white blood cell count</li>
<li>rc - red blood cell count</li>
<li>htn - hypertension</li>
<li>dm - diabetes mellitus</li>
<li>cad - coronary artery disease</li>
<li>appet - appetite</li>
<li>pe - pedal edema</li>
<li>ane - anemia</li>
<li>class - class</li>
</ul>
</div>
</div>
<div id="missing-data" class="section level3">
<h3>Missing data</h3>
<ul>
<li>impute missing data with Nonparametric Missing Value Imputation using Random Forest (<code>missForest</code> package)</li>
</ul>
<pre class="r"><code>data_imp &lt;- missForest(data)</code></pre>
</div>
<div id="one-hot-encoding" class="section level3">
<h3>One-hot encoding</h3>
<ul>
<li>create dummy variables (<code>caret::dummy.data.frame()</code>)</li>
<li>scale and center</li>
</ul>
<pre class="r"><code>data_imp_final &lt;- data_imp$ximp
data_dummy &lt;- dummy.data.frame(dplyr::select(data_imp_final, -class), sep = &quot;_&quot;)
data &lt;- cbind(dplyr::select(data_imp_final, class), scale(data_dummy, 
                                                   center = apply(data_dummy, 2, min),
                                                   scale = apply(data_dummy, 2, max)))</code></pre>
</div>
<div id="modeling" class="section level3">
<h3>Modeling</h3>
<pre class="r"><code># training and test set
set.seed(42)
index &lt;- createDataPartition(data$class, p = 0.9, list = FALSE)
train_data &lt;- data[index, ]
test_data  &lt;- data[-index, ]

# modeling
model_rf &lt;- caret::train(class ~ .,
  data = train_data,
  method = &quot;rf&quot;, # random forest
  trControl = trainControl(method = &quot;repeatedcv&quot;, 
       number = 10, 
       repeats = 5, 
       verboseIter = FALSE))</code></pre>
<pre class="r"><code>model_rf</code></pre>
<pre><code>## Random Forest 
## 
## 360 samples
##  48 predictor
##   2 classes: &#39;ckd&#39;, &#39;notckd&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 324, 324, 324, 324, 325, 324, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    2    0.9922647  0.9838466
##   25    0.9917392  0.9826070
##   48    0.9872930  0.9729881
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<pre class="r"><code># predictions
pred &lt;- data.frame(sample_id = 1:nrow(test_data), predict(model_rf, test_data, type = &quot;prob&quot;), actual = test_data$class) %&gt;%
  mutate(prediction = colnames(.)[2:3][apply(.[, 2:3], 1, which.max)], correct = ifelse(actual == prediction, &quot;correct&quot;, &quot;wrong&quot;))

confusionMatrix(pred$actual, pred$prediction)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction ckd notckd
##     ckd     23      2
##     notckd   0     15
##                                           
##                Accuracy : 0.95            
##                  95% CI : (0.8308, 0.9939)
##     No Information Rate : 0.575           
##     P-Value [Acc &gt; NIR] : 1.113e-07       
##                                           
##                   Kappa : 0.8961          
##  Mcnemar&#39;s Test P-Value : 0.4795          
##                                           
##             Sensitivity : 1.0000          
##             Specificity : 0.8824          
##          Pos Pred Value : 0.9200          
##          Neg Pred Value : 1.0000          
##              Prevalence : 0.5750          
##          Detection Rate : 0.5750          
##    Detection Prevalence : 0.6250          
##       Balanced Accuracy : 0.9412          
##                                           
##        &#39;Positive&#39; Class : ckd             
## </code></pre>
</div>
<div id="lime" class="section level3">
<h3>LIME</h3>
<ul>
<li>LIME needs data without response variable</li>
</ul>
<pre class="r"><code>train_x &lt;- dplyr::select(train_data, -class)
test_x &lt;- dplyr::select(test_data, -class)

train_y &lt;- dplyr::select(train_data, class)
test_y &lt;- dplyr::select(test_data, class)</code></pre>
<ul>
<li>build explainer</li>
</ul>
<pre class="r"><code>explainer &lt;- lime(train_x, model_rf, n_bins = 5, quantile_bins = TRUE)</code></pre>
<ul>
<li>run <code>explain()</code> function</li>
</ul>
<pre class="r"><code>explanation_df &lt;- lime::explain(test_x, explainer, n_labels = 1, n_features = 8, n_permutations = 1000, feature_select = &quot;forward_selection&quot;)</code></pre>
<ul>
<li>model reliability</li>
</ul>
<pre class="r"><code>explanation_df %&gt;%
  ggplot(aes(x = model_r2, fill = label)) +
    geom_density(alpha = 0.5)</code></pre>
<p><img src="/post/2017-12-12_lime_sketchnotes_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<ul>
<li>plot explanations</li>
</ul>
<pre class="r"><code>plot_features(explanation_df[1:24, ], ncol = 1)</code></pre>
<p><img src="/post/2017-12-12_lime_sketchnotes_files/figure-html/unnamed-chunk-15-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="session-info" class="section level3">
<h3>Session Info</h3>
<pre><code>## Session info -------------------------------------------------------------</code></pre>
<pre><code>##  setting  value                       
##  version  R version 3.4.3 (2017-11-30)
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  de_DE.UTF-8                 
##  tz       Europe/Berlin               
##  date     2018-02-05</code></pre>
<pre><code>## Packages -----------------------------------------------------------------</code></pre>
<pre><code>##  package      * version  date       source        
##  assertthat     0.2.0    2017-04-11 CRAN (R 3.4.0)
##  backports      1.1.2    2017-12-13 CRAN (R 3.4.3)
##  base         * 3.4.3    2017-12-07 local         
##  BBmisc         1.11     2017-03-10 CRAN (R 3.4.0)
##  bindr          0.1      2016-11-13 CRAN (R 3.4.0)
##  bindrcpp     * 0.2      2017-06-17 CRAN (R 3.4.0)
##  blogdown       0.4      2017-12-12 CRAN (R 3.4.3)
##  bookdown       0.5      2017-08-20 CRAN (R 3.4.1)
##  broom          0.4.3    2017-11-20 CRAN (R 3.4.2)
##  caret        * 6.0-78   2017-12-10 CRAN (R 3.4.3)
##  cellranger     1.1.0    2016-07-27 CRAN (R 3.4.0)
##  checkmate      1.8.5    2017-10-24 CRAN (R 3.4.2)
##  class          7.3-14   2015-08-30 CRAN (R 3.4.3)
##  cli            1.0.0    2017-11-05 CRAN (R 3.4.2)
##  codetools      0.2-15   2016-10-05 CRAN (R 3.4.3)
##  colorspace     1.3-2    2016-12-14 CRAN (R 3.4.0)
##  compiler       3.4.3    2017-12-07 local         
##  crayon         1.3.4    2017-09-16 cran (@1.3.4) 
##  CVST           0.2-1    2013-12-10 CRAN (R 3.4.0)
##  datasets     * 3.4.3    2017-12-07 local         
##  ddalpha        1.3.1    2017-09-27 CRAN (R 3.4.2)
##  DEoptimR       1.0-8    2016-11-19 CRAN (R 3.4.0)
##  devtools       1.13.4   2017-11-09 CRAN (R 3.4.2)
##  digest         0.6.14   2018-01-14 CRAN (R 3.4.3)
##  dimRed         0.1.0    2017-05-04 CRAN (R 3.4.0)
##  dplyr        * 0.7.4    2017-09-28 CRAN (R 3.4.2)
##  DRR            0.0.3    2018-01-06 CRAN (R 3.4.3)
##  dummies      * 1.5.6    2012-06-14 CRAN (R 3.4.0)
##  e1071          1.6-8    2017-02-02 CRAN (R 3.4.0)
##  evaluate       0.10.1   2017-06-24 CRAN (R 3.4.0)
##  farff        * 1.0      2016-09-11 CRAN (R 3.4.0)
##  forcats      * 0.2.0    2017-01-23 CRAN (R 3.4.0)
##  foreach      * 1.4.4    2017-12-12 CRAN (R 3.4.3)
##  foreign        0.8-69   2017-06-22 CRAN (R 3.4.3)
##  ggplot2      * 2.2.1    2016-12-30 CRAN (R 3.4.0)
##  glmnet         2.0-13   2017-09-22 CRAN (R 3.4.2)
##  glue           1.2.0    2017-10-29 CRAN (R 3.4.2)
##  gower          0.1.2    2017-02-23 CRAN (R 3.4.0)
##  graphics     * 3.4.3    2017-12-07 local         
##  grDevices    * 3.4.3    2017-12-07 local         
##  grid           3.4.3    2017-12-07 local         
##  gtable         0.2.0    2016-02-26 CRAN (R 3.4.0)
##  haven          1.1.1    2018-01-18 CRAN (R 3.4.3)
##  highr          0.6      2016-05-09 CRAN (R 3.4.0)
##  hms            0.4.0    2017-11-23 CRAN (R 3.4.3)
##  htmltools      0.3.6    2017-04-28 CRAN (R 3.4.0)
##  htmlwidgets    1.0      2018-01-20 CRAN (R 3.4.3)
##  httpuv         1.3.5    2017-07-04 CRAN (R 3.4.1)
##  httr           1.3.1    2017-08-20 CRAN (R 3.4.1)
##  ipred          0.9-6    2017-03-01 CRAN (R 3.4.0)
##  iterators    * 1.0.9    2017-12-12 CRAN (R 3.4.3)
##  itertools    * 0.1-3    2014-03-12 CRAN (R 3.4.0)
##  jsonlite       1.5      2017-06-01 CRAN (R 3.4.0)
##  kernlab        0.9-25   2016-10-03 CRAN (R 3.4.0)
##  knitr          1.18     2017-12-27 CRAN (R 3.4.3)
##  labeling       0.3      2014-08-23 CRAN (R 3.4.0)
##  lattice      * 0.20-35  2017-03-25 CRAN (R 3.4.3)
##  lava           1.6      2018-01-13 CRAN (R 3.4.3)
##  lazyeval       0.2.1    2017-10-29 CRAN (R 3.4.2)
##  lime         * 0.3.1    2017-11-24 CRAN (R 3.4.3)
##  lubridate      1.7.1    2017-11-03 CRAN (R 3.4.2)
##  magrittr       1.5      2014-11-22 CRAN (R 3.4.0)
##  MASS           7.3-48   2017-12-25 CRAN (R 3.4.3)
##  Matrix         1.2-12   2017-11-20 CRAN (R 3.4.3)
##  memoise        1.1.0    2017-04-21 CRAN (R 3.4.0)
##  methods      * 3.4.3    2017-12-07 local         
##  mime           0.5      2016-07-07 CRAN (R 3.4.0)
##  missForest   * 1.4      2013-12-31 CRAN (R 3.4.0)
##  mnormt         1.5-5    2016-10-15 CRAN (R 3.4.0)
##  ModelMetrics   1.1.0    2016-08-26 CRAN (R 3.4.0)
##  modelr         0.1.1    2017-07-24 CRAN (R 3.4.1)
##  munsell        0.4.3    2016-02-13 CRAN (R 3.4.0)
##  nlme           3.1-131  2017-02-06 CRAN (R 3.4.3)
##  nnet           7.3-12   2016-02-02 CRAN (R 3.4.3)
##  parallel       3.4.3    2017-12-07 local         
##  pillar         1.1.0    2018-01-14 CRAN (R 3.4.3)
##  pkgconfig      2.0.1    2017-03-21 CRAN (R 3.4.0)
##  plyr           1.8.4    2016-06-08 CRAN (R 3.4.0)
##  prodlim        1.6.1    2017-03-06 CRAN (R 3.4.0)
##  psych          1.7.8    2017-09-09 CRAN (R 3.4.1)
##  purrr        * 0.2.4    2017-10-18 CRAN (R 3.4.2)
##  R6             2.2.2    2017-06-17 CRAN (R 3.4.0)
##  randomForest * 4.6-12   2015-10-07 CRAN (R 3.4.0)
##  Rcpp           0.12.15  2018-01-20 CRAN (R 3.4.3)
##  RcppRoll       0.2.2    2015-04-05 CRAN (R 3.4.0)
##  readr        * 1.1.1    2017-05-16 CRAN (R 3.4.0)
##  readxl         1.0.0    2017-04-18 CRAN (R 3.4.0)
##  recipes        0.1.2    2018-01-11 CRAN (R 3.4.3)
##  reshape2       1.4.3    2017-12-11 CRAN (R 3.4.3)
##  rlang          0.1.6    2017-12-21 CRAN (R 3.4.3)
##  rmarkdown      1.8      2017-11-17 CRAN (R 3.4.2)
##  robustbase     0.92-8   2017-11-01 CRAN (R 3.4.2)
##  rpart          4.1-12   2018-01-12 CRAN (R 3.4.3)
##  rprojroot      1.3-2    2018-01-03 CRAN (R 3.4.3)
##  rstudioapi     0.7      2017-09-07 CRAN (R 3.4.1)
##  rvest          0.3.2    2016-06-17 CRAN (R 3.4.0)
##  scales         0.5.0    2017-08-24 CRAN (R 3.4.1)
##  sfsmisc        1.1-1    2017-06-08 CRAN (R 3.4.0)
##  shiny          1.0.5    2017-08-23 CRAN (R 3.4.1)
##  shinythemes    1.1.1    2016-10-12 CRAN (R 3.4.0)
##  splines        3.4.3    2017-12-07 local         
##  stats        * 3.4.3    2017-12-07 local         
##  stats4         3.4.3    2017-12-07 local         
##  stringdist     0.9.4.6  2017-07-31 CRAN (R 3.4.1)
##  stringi        1.1.6    2017-11-17 CRAN (R 3.4.2)
##  stringr      * 1.2.0    2017-02-18 CRAN (R 3.4.0)
##  survival       2.41-3   2017-04-04 CRAN (R 3.4.3)
##  tibble       * 1.4.2    2018-01-22 CRAN (R 3.4.3)
##  tidyr        * 0.7.2    2017-10-16 CRAN (R 3.4.2)
##  tidyselect     0.2.3    2017-11-06 CRAN (R 3.4.2)
##  tidyverse    * 1.2.1    2017-11-14 CRAN (R 3.4.2)
##  timeDate       3042.101 2017-11-16 CRAN (R 3.4.2)
##  tools          3.4.3    2017-12-07 local         
##  utils        * 3.4.3    2017-12-07 local         
##  withr          2.1.1    2017-12-19 CRAN (R 3.4.3)
##  xml2           1.1.1    2017-01-24 CRAN (R 3.4.0)
##  xtable         1.8-2    2016-02-05 CRAN (R 3.4.0)
##  yaml           2.1.16   2017-12-12 CRAN (R 3.4.3)</code></pre>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[MICE (Multiple Imputation by Chained Equations) in R - sketchnotes from MünsteR Meetup]]></title>
    <link href="/2017/11/mice_sketchnotes/"/>
    <id>/2017/11/mice_sketchnotes/</id>
    <published>2017-11-28T00:00:00+00:00</published>
    <updated>2017-11-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Last night, the <a href="http://meetu.ps/c/3ffGL/w54bW/f">MünsteR R user-group</a> had <a href="https://www.meetup.com/Munster-R-Users-Group/events/243388360/">another great meetup</a>:</p>
<p>Karin Groothuis-Oudshoorn, Assistant Professor at the University of Twente, presented her R package <code>mice</code> about Multivariate Imputation by Chained Equations.</p>
<p>It was a very interesting talk and here are my sketchnotes that I took during it:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/mice_sketchnote_gxjsgc.jpg" alt="MICE talk sketchnotes" />
<p class="caption">MICE talk sketchnotes</p>
</div>
<p>Here is the link to the paper referenced in my notes: <a href="https://www.jstatsoft.org/article/view/v045i03" class="uri">https://www.jstatsoft.org/article/view/v045i03</a></p>
<blockquote>
<p>“The mice package implements a method to deal with missing data. The package creates multiple imputations (replacement values) for multivariate missing data. The method is based on Fully Conditional Specification, where each incomplete variable is imputed by a separate model. The MICE algorithm can impute mixes of continuous, binary, unordered categorical and ordered categorical data. In addition, MICE can impute continuous two-level data, and maintain consistency between imputations by means of passive imputation. Many diagnostic plots are implemented to inspect the quality of the imputations.”&quot; (<a href="https://cran.r-project.org/web/packages/mice/README.html" class="uri">https://cran.r-project.org/web/packages/mice/README.html</a>)</p>
</blockquote>
<p>For more information on the package go to <a href="http://stefvanbuuren.github.io/mice/" class="uri">http://stefvanbuuren.github.io/mice/</a>.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Workshop on Deep Learning with Keras and TensorFlow in R]]></title>
    <link href="/2017/11/deep_learning_keras_tensorflow/"/>
    <id>/2017/11/deep_learning_keras_tensorflow/</id>
    <published>2017-11-20T00:00:00+00:00</published>
    <updated>2017-11-20T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>You can now book me and my 1-day workshop on <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/">deep learning with Keras and TensorFlow using R</a>.</p>
<p>In my workshop, you will learn</p>
<ul>
<li>the basics of deep learning</li>
<li>what cross-entropy and loss is</li>
<li>about activation functions</li>
<li>how to optimize weights and biases with backpropagation and gradient descent</li>
<li>how to build (deep) neural networks with Keras and TensorFlow</li>
<li>how to save and load models and model weights</li>
<li>how to visualize models with TensorBoard</li>
<li>how to make predictions on test data</li>
</ul>
<p>Date and place depend on who and how many people are interested, so please contact me either directly or via the workshop page: <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/" class="uri">https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/</a> (the description is in German but I also offer to give the workshop in English).</p>
<p><br></p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/mlp_r7pv7z.jpg" alt="Neural Network with three densely connected hidden layers" />
<p class="caption">Neural Network with three densely connected hidden layers</p>
</div>
<p>Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. Keras is very convenient for fast and easy prototyping of neural networks. It is highly modular and very flexible, so that you can build basically any type of neural network you want. It supports convolutional neural networks and recurrent neural networks, as well as combinations of both. Due to its layer structure, it is highly extensible and can run on CPU or GPU.</p>
<p>The <code>keras</code> R package provides an interface to the Python library of Keras, just as the tensorflow package provides an interface to TensorFlow. Basically, R creates a conda instance and runs Keras it it, while you can still use all the functionalities of R for plotting, etc. Almost all function names are the same, so models can easily be recreated in Python for deployment.</p>
<p><br></p>
<div class="figure">
<img src="https://blog.keras.io/img/keras-tensorflow-logo.jpg" />

</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[How to combine point and boxplots in timeline charts with ggplot2 facets]]></title>
    <link href="/2017/11/combine_point_boxplot_ggplot/"/>
    <id>/2017/11/combine_point_boxplot_ggplot/</id>
    <published>2017-11-18T00:00:00+00:00</published>
    <updated>2017-11-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>In a recent project, I was looking to plot data from different variables along the same time axis. The difficulty was, that some of these variables I wanted to have as point plots, while others I wanted as box-plots.</p>
<p>Because I work with the tidyverse, I wanted to produce these plots with ggplot2. Faceting was the obvious first step but it took me quite a while to figure out how to best combine facets with point plots (where I have one value per time point) with and box-plots (where I have multiple values per time point).</p>
<p>The reason why this isn’t trivial is that box plots require groups or factors on the x-axis, while points can be plotted over a continuous range of x-values. If your alarm bells are ringing right now, you are absolutely right: before you try to combine plots with different x-axis properties, you should think long and hard whether this is an accurate representation of the data and if its a good idea to do so! Here, I had multiple values per time point for one variable and I wanted to make the median + variation explicitly clear, while also showing the continuous changes of other variables over the same range of time.</p>
<p>So, I am writing this short tutorial here in hopes that it saves the next person trying to do something similar from spending an entire morning on stackoverflow. ;-)</p>
<p>For this demonstration, I am creating some fake data:</p>
<pre class="r"><code>library(tidyverse)
dates &lt;- seq(as.POSIXct(&quot;2017-10-01 07:00&quot;), as.POSIXct(&quot;2017-10-01 10:30&quot;), by = 180) # 180 seconds == 3 minutes
fake_data &lt;- data.frame(time = dates,
                        var1_1 = runif(length(dates)),
                        var1_2 = runif(length(dates)),
                        var1_3 = runif(length(dates)),
                        var2 = runif(length(dates))) %&gt;%
  sample_frac(size = 0.33)
head(fake_data)</code></pre>
<pre><code>##                   time    var1_1    var1_2    var1_3      var2
## 64 2017-10-01 10:09:00 0.6621389 0.5521539 0.9148288 0.6078481
## 67 2017-10-01 10:18:00 0.2538266 0.1958265 0.8239181 0.5116218
## 15 2017-10-01 07:42:00 0.6220942 0.9945485 0.9075784 0.7552689
## 27 2017-10-01 08:18:00 0.9209701 0.7804722 0.1649222 0.9315722
## 50 2017-10-01 09:27:00 0.2937356 0.2903439 0.1449462 0.2220199
## 16 2017-10-01 07:45:00 0.2189083 0.4640567 0.3110658 0.8932355</code></pre>
<p>Here, variable 1 (<code>var1</code>) has three measurements per time point, while variable 2 (<code>var2</code>) has one.</p>
<p>First, for plotting with ggplot2 we want our data in a tidy long format. I also add another column for faceting that groups the variables from <code>var1</code> together.</p>
<pre class="r"><code>fake_data_long &lt;- fake_data %&gt;%
  gather(x, y, var1_1:var2) %&gt;%
  mutate(facet = ifelse(x %in% c(&quot;var1_1&quot;, &quot;var1_2&quot;, &quot;var1_3&quot;), &quot;var1&quot;, x))
head(fake_data_long)</code></pre>
<pre><code>##                  time      x         y facet
## 1 2017-10-01 10:09:00 var1_1 0.6621389  var1
## 2 2017-10-01 10:18:00 var1_1 0.2538266  var1
## 3 2017-10-01 07:42:00 var1_1 0.6220942  var1
## 4 2017-10-01 08:18:00 var1_1 0.9209701  var1
## 5 2017-10-01 09:27:00 var1_1 0.2937356  var1
## 6 2017-10-01 07:45:00 var1_1 0.2189083  var1</code></pre>
<p>Now, we can plot this the following way:</p>
<ul>
<li>facet by variable</li>
<li>subset data to facets for point plots and give aesthetics in <code>geom_point()</code></li>
<li>subset data to facets for box plots and give aesthetics in <code>geom_boxplot()</code>. Here we also need to set the <code>group</code> aesthetic; if we don’t specifically give that, we will get a plot with one big box, instead of a box for every time point.</li>
</ul>
<pre class="r"><code>fake_data_long %&gt;%
  ggplot() +
    facet_grid(facet ~ ., scales = &quot;free&quot;) +
    geom_point(data = subset(fake_data_long, facet == &quot;var2&quot;), 
               aes(x = time, y = y),
               size = 1) +
    geom_line(data = subset(fake_data_long, facet == &quot;var2&quot;), 
               aes(x = time, y = y)) +
    geom_boxplot(data = subset(fake_data_long, facet == &quot;var1&quot;), 
               aes(x = time, y = y, group = time))</code></pre>
<p><img src="/post/2017-11-18-combine_point_boxplot_ggplot_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.2
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] methods   stats     graphics  grDevices utils     datasets  base     
## 
## other attached packages:
##  [1] bindrcpp_0.2    forcats_0.2.0   stringr_1.2.0   dplyr_0.7.4    
##  [5] purrr_0.2.4     readr_1.1.1     tidyr_0.7.2     tibble_1.4.2   
##  [9] ggplot2_2.2.1   tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_0.2.3 reshape2_1.4.3   haven_1.1.1      lattice_0.20-35 
##  [5] colorspace_1.3-2 htmltools_0.3.6  yaml_2.1.16      rlang_0.1.6     
##  [9] pillar_1.1.0     foreign_0.8-69   glue_1.2.0       modelr_0.1.1    
## [13] readxl_1.0.0     bindr_0.1        plyr_1.8.4       munsell_0.4.3   
## [17] blogdown_0.4     gtable_0.2.0     cellranger_1.1.0 rvest_0.3.2     
## [21] psych_1.7.8      evaluate_0.10.1  labeling_0.3     knitr_1.18      
## [25] parallel_3.4.3   broom_0.4.3      Rcpp_0.12.15     backports_1.1.2 
## [29] scales_0.5.0     jsonlite_1.5     mnormt_1.5-5     hms_0.4.0       
## [33] digest_0.6.14    stringi_1.1.6    bookdown_0.5     grid_3.4.3      
## [37] rprojroot_1.3-2  cli_1.0.0        tools_3.4.3      magrittr_1.5    
## [41] lazyeval_0.2.1   crayon_1.3.4     pkgconfig_2.0.1  xml2_1.1.1      
## [45] lubridate_1.7.1  assertthat_0.2.0 rmarkdown_1.8    httr_1.3.1      
## [49] rstudioapi_0.7   R6_2.2.2         nlme_3.1-131     compiler_3.4.3</code></pre>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Explore Predictive Maintenance with flexdashboard]]></title>
    <link href="/2017/11/predictive_maintenance_dashboard/"/>
    <id>/2017/11/predictive_maintenance_dashboard/</id>
    <published>2017-11-02T00:00:00+00:00</published>
    <updated>2017-11-02T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I have written the following post about <a href="https://blog.codecentric.de/en/2017/11/explore-predictive-maintenance-flexdashboard/">Predictive Maintenance and flexdashboard</a> at my company <a href="https://blog.codecentric.de/en/">codecentric</a>’s blog:</p>
<blockquote>
<p>Predictive Maintenance is an increasingly popular strategy associated with Industry 4.0; it uses advanced analytics and machine learning to optimize machine costs and output (see Google Trends plot below). A common use-case for Predictive Maintenance is to proactively monitor machines, so as to predict when a check-up is needed to reduce failure and maximize performance. In contrast to traditional maintenance, where each machine has to undergo regular routine check-ups, Predictive Maintenance can save costs and reduce downtime. A machine learning approach to such a problem would be to analyze machine failure over time to train a supervised classification model that predicts failure. Data from sensors and weather information is often used as features in modeling.</p>
</blockquote>
<blockquote>
<p>…</p>
</blockquote>
<blockquote>
<p>With flexdashboard RStudio provides a great way to create interactive dashboards with R. It is an easy and very fast way to present analyses or create story maps. Here, I have used it to demonstrate different analysis techniques for Predictive Maintenance. It uses Shiny run-time to create interactive content.</p>
</blockquote>
<blockquote>
<p>…</p>
</blockquote>
<p>Continue reading at <a href="https://blog.codecentric.de/en/2017/11/explore-predictive-maintenance-flexdashboard/" class="uri">https://blog.codecentric.de/en/2017/11/explore-predictive-maintenance-flexdashboard/</a></p>
<div class="figure">
<img src="https://blog.codecentric.de/files/2017/10/dashboard_screenshot.png" />

</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Blockchain &amp; distributed ML - my report from the data2day conference]]></title>
    <link href="/2017/09/data2day/"/>
    <id>/2017/09/data2day/</id>
    <published>2017-09-28T00:00:00+00:00</published>
    <updated>2017-09-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://www.data2day.de/common/images/konferenzen/data2day2017.svg" />

</div>
<p>Yesterday and today I attended the <a href="www.data2day.de">data2day</a>, a conference about Big Data, Machine Learning and Data Science in Heidelberg, Germany. Topics and workshops covered a range of topics surrounding (big) data analysis and Machine Learning, like Deep Learning, Reinforcement Learning, TensorFlow applications, etc. Distributed systems and scalability were a major part of a lot of the talks as well, reflecting the growing desire to build bigger and more complex models that can’t (or would take too long to) run on a single computer. Most of the application examples were built in Python but one talk by Andreas Prawitt was specifically titled “Using R for Predictive Maintenance: an example from the TRUMPF Laser GmbH”. I also saw quite a few graphs that were obviously made with ggplot!</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="de" dir="ltr">
Guten Morgen auf der <a href="https://twitter.com/data2day"><span class="citation">@data2day</span></a> Kommt uns doch mal am Stand besuchen :-) <a href="https://t.co/YK46ACdNj9">pic.twitter.com/YK46ACdNj9</a>
</p>
— codecentric AG (<span class="citation">@codecentric</span>) <a href="https://twitter.com/codecentric/status/912928993279606784">September 27, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><br></p>
<p>The keynote lecture on Wednesday about <strong>Blockchains for AI</strong> was given by Trent McConaghy. <a href="https://www.sitepen.com/blog/2017/09/21/blockchain-basics/">Blockchain technology</a> is based on a decentralized system of storing and validating data and changes in data. It experiences a huge hype at the moment but it is only starting to gain track in Data Science and Machine Learning as well. I therefore found it a very fitting topic for the keynote lecture! Trent and his colleagues at <a href="www.bigchaindb.com">BigchainDB</a> are implementing an “internet-scale blockchain database for the world” - the Interplanetary Database (IPDB).</p>
<blockquote>
<p>“IPDB is a blockchain database that offers decentralized control, immutability and the creation and trading of digital assets. […] As a database for the world, IPDB offers decentralized control, strong governance and universal accessibility. IPDB relies on “caretaker” organizations around the world, who share responsibility for managing the network and governing the IPDB Foundation. Anyone in the world will be able to use IPDB. […]” <a href="https://blog.bigchaindb.com/ipdb-announced-as-public-planetary-scale-blockchain-database-7a363824fc14" class="uri">https://blog.bigchaindb.com/ipdb-announced-as-public-planetary-scale-blockchain-database-7a363824fc14</a></p>
</blockquote>
<p>He presented a number of examples where blockchain technology for decentralized data storage/access can be beneficial to Machine Learning and AI, like exchanging data from self-driving cars, of online market places and for generating art with computers. You can learn more about him <a href="https://blog.oceanprotocol.com/from-ai-to-blockchain-to-data-meet-ocean-f210ff460465">here</a>:</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="en" dir="ltr">
It's always been about the data.<br>Announcing Ocean.<a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a> <a href="https://twitter.com/hashtag/Blockchain?src=hash&amp;ref_src=twsrc%5Etfw">#Blockchain</a> <a href="https://twitter.com/oceanprotocol?ref_src=twsrc%5Etfw"><span class="citation">@OceanProtocol</span></a><a href="https://t.co/Do4XNn3ucN">https://t.co/Do4XNn3ucN</a>
</p>
— Trent McConaghy (<span class="citation">@trentmc0</span>) <a href="https://twitter.com/trentmc0/status/909793166416662528?ref_src=twsrc%5Etfw">September 18, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><br></p>
<p>The other talks were a mix of high- and low-level topics: from introductions to machine learning, Apache Spark and data analysis with Python to (distributed) data streaming with Kappa architecture or Apache Kafka, containerization with Docker and Kubernetes, data archiving with Apache Cassandra, relevance tuning with Solr and much more. While I spent most of the time at my company’s conference stand, I did hear three of the talks. I summarize each of them below…</p>
<p><br></p>
<ol style="list-style-type: decimal">
<li><strong>Scalable Machine Learning with Apache Spark for Fraud Detection</strong></li>
</ol>
<p>In this first talk I heard, Dr. Patrick Baier and Dr. Stanimir Dragiev presented their work at <a href="www.zalando.de/">Zalando</a>. They built a scalable machine learning framework with Apache Spark, Scala and AWS to assess and predict fraud in online transactions. <a href="www.zalando.de/">Zalando</a> is a German online store that sells clothes, shoes and accessories. Normally, they allow registered customers to buy via invoice, i.e. they receive their ordered items before they pay them. This leaves them vulnerable to fraud where item are not paid for. The goal of their data science team is to use customer and basket data to obtain a probability score for how likely a transaction is going to be fraudulent. High-risk payment options, like invoice, can then be disabled in transactions with high fraud probability. To build and run such machine learning models, the Zalando data science team uses a combination of Spark, Scala, R, AWS, SQL, Python, Docker, etc. In their workflow, they use a combination of static and dynamic features, imputing missing values and building a decision model. In order to scale their modeling workflow to process more requests, use more data in training, update models more frequently and/or run more models, they described a workflow that uses Spark, Scala and Amazon Web Services (AWS). Spark’s machine learning library can be used for modeling and scaled horizontally by increasing the number of clusters on which to run the models. Scala provides multi-threading functionality and AWS is used for storing data in S3 and extending computation power depending on changing needs. Finally, they include a model inspector into their workflow to assure comparability of training and test data and check that the model is behaving as expected. Problems that they are dealing with include highly unbalanced data (which is getting even worse the better their models work as they keep reducing the number of fraud cases), delayed labeling due to the long process of the transactions, seasonality in data.</p>
<p><br></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Sparse Data: Don’t Mind the Gap!</strong></li>
</ol>
<p>In this talk, my colleagues from <a href="www.codecentric.de">codecentric</a> Dr. Daniel Pape and Dr. Michael Plümacher showed an example from ad targeting of how to deal with sparse data. Sparse data occurs in many areas, e.g. as rare events over a long period of time or in areas where there are many items and few occurrences per item, like in recommender systems or in natural language processing (NLP). In ad targeting, the measure of success is the rate of the click-through rate (CRT): this is the number of clicks on a given advertisement displayed to a user on a website divided by the total number of advertisements, or impressions. Because financial revenue comes from a high CTR, advertisements should be placed in a way that maximizes their chance of being clicked, i.e. we want to recommend advertisements for specific users that match their interests or are of actual relevance. Sparsity come into play with ad targeting because the number of clicks is very low compared to two metrics: a) from all the potential ads that a user could see, only a small proportion is actually shown to her/him and b) of the ads that a user sees, she/he only clicks on very few. This means that, a CTR matrix of advertisements x targets will have very few combinations that have been clicked (the mean CTR is 0.003) and contain many missing values. The approach they took was to impute the missing values and predict for each target/user the most similar ads from the imputed CTR matrix. This approach worked well for a reasonably large data set but it didn’t perform so well with smaller (and therefore even sparser) data. They then talked about alternative approaches, like grouping users and/or ads into groups in order to reduce the sparsity of the data. Their take-home messages were that 1) there is no one-size-fits-all solution, what works depends on the context and 2) if the underlying data is of bad quality, the results will be sub-optimal - no matter how sophisticated the model.</p>
<p><br></p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Distributed TensorFlow with Kubernetes</strong></li>
</ol>
<p>In the third talk, another colleague of mine from <a href="www.codecentric.de">codecentric</a>, Jakob Karalus, explained in detail how to set up a distributed machine learning modelling set-up with <a href="https://www.tensorflow.org/">TensorFlow</a> and <a href="https://kubernetes.io/">Kubernetes</a>. TensorFlow is used to build neural networks in a graph-based manner. Distributed and parallel machine learning can be necessary when training big neural networks with a lot of training data, very deep neural networks, with complex parameters, grid search for hyper-parameter tuning, etc. A good way to build neural networks in a controlled and stable environment is to use <a href="https://www.docker.com/">Docker</a> containers. Kubernetes is a container orchestration tool that can set up distribution of nodes from our TensorFlow modeling container. Setting up this distributed system is quite complex, though and Jakob recommended to try to stay on one CPU/GPU as long as possible.</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="de" dir="ltr">
Tag 2 auf der <a href="https://twitter.com/data2day?ref_src=twsrc%5Etfw"><span class="citation">@data2day</span></a> kommt am Stand vorbei, wir haben noch ein paar T-Shirts und Softwerker für euch :-) <a href="https://t.co/xyG8Leg3lF">pic.twitter.com/xyG8Leg3lF</a>
</p>
— codecentric AG (<span class="citation">@codecentric</span>) <a href="https://twitter.com/codecentric/status/913301091755941888?ref_src=twsrc%5Etfw">September 28, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="de" dir="ltr">
Verteiltes Deep Learning mit TensorFlow und Kubernetes - <a href="https://twitter.com/krallistic"><span class="citation">@krallistic</span></a> auf der <a href="https://twitter.com/data2day"><span class="citation">@data2day</span></a> <a href="https://t.co/5AGJdhL5U1">pic.twitter.com/5AGJdhL5U1</a>
</p>
— codecentric AG (<span class="citation">@codecentric</span>) <a href="https://twitter.com/codecentric/status/913041395128111105">September 27, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[From Biology to Industry. A Blogger’s Journey to Data Science.]]></title>
    <link href="/2017/09/from-biology-to-industry.-a-bloggers-journey-to-data-science./"/>
    <id>/2017/09/from-biology-to-industry.-a-bloggers-journey-to-data-science./</id>
    <published>2017-09-20T00:00:00+00:00</published>
    <updated>2017-09-20T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Today, I have given a webinar for the Applied Epidemiology Didactic of the University of Wisconsin - Madison titled “From Biology to Industry. A Blogger’s Journey to Data Science.”</p>
<p>I talked about how blogging about R and Data Science helped me become a Data Scientist. I also gave a short introduction to Machine Learning, Big Data and Neural Networks.</p>
<p>My slides can be found here: <a href="https://www.slideshare.net/ShirinGlander/from-biology-to-industry-a-bloggers-journey-to-data-science" class="uri">https://www.slideshare.net/ShirinGlander/from-biology-to-industry-a-bloggers-journey-to-data-science</a></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Why I use R for Data Science - An Ode to R]]></title>
    <link href="/2017/09/ode_to_r/"/>
    <id>/2017/09/ode_to_r/</id>
    <published>2017-09-19T00:00:00+00:00</published>
    <updated>2017-09-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Working in Data Science, I often feel like I have to justify using R over Python. And while I do use Python for running scripts in production, I am much more comfortable with the R environment. Basically, whenever I can, I use R for prototyping, testing, visualizing and teaching. But because personal gut-feeling preference isn’t a very good reason to give to (scientifically minded) people, I’ve thought a lot about the pros and cons of using R. This is what I came up with why I still prefer R…</p>
<p><em>Disclaimer:</em> I have “grown up” with R and I’m much more familiar with it, so I admit that I am quite biased in my assessment. If you think I’m not doing other languages justice, I’ll be happy to hear your pros and cons!</p>
<p><br></p>
<ol style="list-style-type: decimal">
<li><p>First of, <a href="https://www.r-project.org/">R</a> is an <a href="https://cran.r-project.org/">open-source, cross-platform</a> language, so it’s free to use by any- and everybody. This in itself doesn’t make it special, though, because so are other languages, like Python.</p></li>
<li><p>It is an established language, so that there are lots and lots of packages for basically every type of analysis you can think of. You find packages for <a href="https://www.analyticsvidhya.com/blog/2015/08/list-r-packages-data-analysis/">data analysis</a>, <a href="http://www.kdnuggets.com/2017/02/top-r-packages-machine-learning.html">machine learning</a>, <a href="https://support.rstudio.com/hc/en-us/articles/201057987-Quick-list-of-useful-R-packages">visualization</a>, <a href="https://www.computerworld.com/article/2921176/business-intelligence/great-r-packages-for-data-import-wrangling-visualization.html">data wrangling</a>, <a href="https://cran.r-project.org/web/views/Spatial.html">spatial analysis</a>, <a href="https://www.bioconductor.org/">bioinformatics</a> and much more. But, same as with Python, this plethora of packages can sometimes make things a bit confusing: you would often need to test and compare several similar packages in order to find the best one.</p></li>
<li><p>Most of the packages are of very high quality. And when a package is on <a href="https://cran.r-project.org/web/packages/available_packages_by_name.html">CRAN</a> or <a href="https://www.bioconductor.org/">Bioconductor</a> (as most are), you can be sure that it has been checked, that you will get proper documentation and that you won’t have problems with installation, dependencies, etc. In my experience, R package and function documentation generally tends to be better than, say, of Python packages.</p></li>
<li><p>R’s graphics capabilities are superior to any other I know. Especially <a href="http://ggplot2.org/">ggplot2</a> with all its <a href="http://www.ggplot2-exts.org/">extensions</a> provides a structured, yet powerful set of tools for producing <a href="http://www.r-graph-gallery.com/portfolio/ggplot2-package/">high-quality publication-ready graphs and figures</a>. Moreover, ggplot2 is part of the <a href="https://www.tidyverse.org/">tidyverse</a> and works well with <a href="https://cran.r-project.org/web/packages/broom/vignettes/broom.html">broom</a>. This has made data wrangling and analysis much more convenient and structured and structured for me.</p></li>
<li><p>The suite of tools around <a href="https://www.rstudio.com/">R Studio</a> make it perfect for documenting data analysis workflows and for teaching. You can provide easy instructions for installation and <a href="http://rmarkdown.rstudio.com/">R Markdown</a> files for your students to follow along. Everybody is going to use the same system. In Python, you are always dealing with questions like version 2 vs version 3, Spyder vs Jupyter Notebook, pip vs conda, etc. <a href="https://www.rstudio.com/products/rpackages/">Everything around R Studio</a> is very well maintained and comes with extensive documentation and detailed tutorials. You find add-ins for <a href="https://support.rstudio.com/hc/en-us/articles/200532077-Version-Control-with-Git-and-SVN">version control</a>, <a href="http://shiny.rstudio.com/">Shiny</a> apps, writing books or other documents (<a href="https://bookdown.org/yihui/bookdown/">bookdown</a>) and you can write presentations directly in R Markdown, including code + output and everything as <a href="http://rmarkdown.rstudio.com/beamer_presentation_format.html">LaTeX beamer presentations</a>, <a href="http://rmarkdown.rstudio.com/ioslides_presentation_format.html">ioslides</a> or <a href="http://rmarkdown.rstudio.com/revealjs_presentation_format.html">reveal.js</a>. You can also create <a href="http://rmarkdown.rstudio.com/flexdashboard/">Dashboards</a>, include interactive <a href="http://rmarkdown.rstudio.com/developer_html_widgets.html">HTML widgets</a> and you can even build your blog (as this one is) with <a href="https://bookdown.org/yihui/blogdown/">blogdown</a> conveniently from within RStudio!</p></li>
<li><p>If you are looking for advanced functionality, it is very likely that somebody has already written a package for it. There are packages that allow you to access <a href="https://spark.rstudio.com/">Spark</a>, <a href="https://cran.r-project.org/web/packages/h2o/index.html">H2O</a>, <a href="https://ropensci.org/tutorials/elastic_tutorial.html">elasticsearch</a>, <a href="https://tensorflow.rstudio.com/">TensorFlow</a>, <a href="https://tensorflow.rstudio.com/keras/">Keras</a>, <a href="https://ropensci.org/blog/blog/2016/11/16/tesseract">tesseract</a>, and so many more with no hassle at all. And you can even run <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/system2.html">bash</a>, <a href="https://github.com/rstudio/reticulate">Python</a> from within R!</p></li>
<li><p>There is a big - and very active - community! This is one of the things I most enjoy about working with R. You can find many high-quality <a href="https://cran.r-project.org/manuals.html">manuals</a>, <a href="https://cran.r-project.org/other-docs.html">resources</a> and tutorials for all kinds of topics. Most of them provided free of charge by people who often dedicate their spare time to help others. The same goes for asking questions on <a href="https://stackoverflow.com/questions/tagged/r">Stack Overflow</a>, putting up issues on <a href="https://github.com/">Github</a> or <a href="https://groups.google.com/forum/#!forum/r-help-archive">Google groups</a>: usually you will get several answers within a short period of time (from my experience minutes to hours). What other community is so supportive and so helpful!? But for most things, you wouldn’t even need to ask for help because many of the packages come with absolutely amazing vignettes, that describe the functions and workflows in a detailed, yet easy to understand way. If that’s not enough, you will very likely find additional tutorials on <a href="https://www.r-bloggers.com/">R-bloggers</a>, a site maintained by Tal Galili that aggregates hundreds of R-blogs. There are several <a href="https://www.r-project.org/conferences.html">R Conferences</a>, like the <a href="https://user2018.r-project.org/">useR</a>, <a href="https://ropensci.org/community/events.html">rOpenSci Unconference</a> and many <a href="https://jumpingrivers.github.io/meetingsR/r-user-groups.html">R-user groups</a> all around the globe.</p></li>
</ol>
<p>I can’t stress enough how much I appreciate all the people who are involved in the R-community; who write packages, tutorials, blogs, who share information, provide support and who think about how to make data analysis easy, more convenient and - dare I say - fun!</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/circle-159252_1280_mfs0ku.png" alt="Community is everything!" />
<p class="caption">Community is everything!</p>
</div>
<p>The main drawbacks I experience with R are that scripts tends to be harder to deploy than Python (<a href="https://www.microsoft.com/en-us/cloud-platform/r-server">R-server</a> might be a solution, but I don’t know enough about it to really judge). Dealing with memory, space and security issues is often difficult in R. But there has already been a vast improvement over the last months/years, so I’m sure we will see development there in the future…</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Welcome to my page!]]></title>
    <link href="/page/about/"/>
    <id>/page/about/</id>
    <published>2017-09-12T16:06:06+02:00</published>
    <updated>2017-09-12T16:06:06+02:00</updated>
    <content type="html"><![CDATA[

<p><img src="/img/Bewerbungsfoto_klein.jpg" alt="" /></p>

<p>I&rsquo;m Shirin, a biologist turned bioinformatician turned data scientist.</p>

<p>I&rsquo;m especially interested in machine learning and data visualization. While I am using R most every day at work, I wanted to have an incentive to regularly explore other types of analyses and other types of data that I don&rsquo;t normally work with. I have also very often benefited from other people&rsquo;s published code in that it gave me ideas for my own work; and I hope that sharing my own analyses will inspire others as much as I often am by what can be be done with data.  It&rsquo;s amazing to me what can be learned from analyzing and visualizing data!</p>

<p>My tool of choice for data analysis so far has been R. I also organize the <a href="https://shiring.github.io/r_users_group/2017/05/20/muenster_r_user_group">MünsteR R-users group on meetup.com</a>.</p>

<p><img src="http://res.cloudinary.com/shiring/image/upload/v1511852499/my_story_wml3zm.png" alt="My journey to Data Science" /></p>

<p>I love dancing and used to do competitive ballroom and latin dancing. Even though I don&rsquo;t have time for that anymore, I still enjoy teaching &ldquo;social dances&rdquo; once a week with the Hochschulsport (university sports courses).</p>

<p>I created the R package <a href="https://github.com/ShirinG/exprAnalysis">exprAnalysis</a>, designed to streamline my RNA-seq data analysis pipeline. It is available via Github. Instructions for installation and usage can be found <a href="https://shiring.github.io/rna-seq/microarray/2016/09/28/exprAnalysis">here</a>.</p>

<p>This blog will showcase some of the analyses I have been doing with different data sets (all freely available). I will also host teaching materials for students to access in conjunction with R courses I am giving.</p>

<hr />

<h2 id="contact-me">Contact me:</h2>

<ul>
<li><a href="https://www.codecentric.de/team/shirin-glander/">Codecentric AG</a></li>
<li><a href="mailto:shirin.glander@gmail.com">Email</a></li>
<li><a href="http://www.xing.com/profile/Shirin_Glander">Xing</a></li>
<li><a href="http://de.linkedin.com/in/shirin-glander-01120881">Linkedin</a></li>
<li><a href="http://twitter.com/ShirinGlander">Twitter</a></li>
</ul>

<hr />

<p>Also check out <a href="http://www.R-bloggers.com">R-bloggers</a> for lots of cool R stuff!</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Moving my blog to blogdown]]></title>
    <link href="/2017/09/moving-my-blog-to-blogdown/"/>
    <id>/2017/09/moving-my-blog-to-blogdown/</id>
    <published>2017-09-12T00:00:00+00:00</published>
    <updated>2017-09-12T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>It’s been a long time coming but I finally moved my blog from Jekyll/Bootstrap on Github pages to blogdown, Hugo and <a href="https://www.netlify.com/">Netlify</a>! Moreover, I also now have my own domain name <a href="https://www.shirin-glander.de">www.shirin-glander.de</a>. :-)</p>
<p>I followed the <a href="https://bookdown.org/yihui/blogdown/">blogdown ebook</a> to set up my blog. I chose Thibaud Leprêtre’s <a href="https://themes.gohugo.io/hugo-tranquilpeak-theme/">tranquilpeak theme</a>. It looks much more polished than my old blog.</p>
<p>My old blog will remain where it is, so that all the links that are out there will still work (and I don’t have to go through the hassle of migrating all my posts to my new site). You find a link to my old site in the sidebar.</p>
<p><br></p>
<hr />
<p>Just to test that everything works, I run the example code:</p>
<div id="r-markdown" class="section level1">
<h1>R Markdown</h1>
<p>This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <a href="http://rmarkdown.rstudio.com" class="uri">http://rmarkdown.rstudio.com</a>.</p>
<p>You can embed an R code chunk like this:</p>
<pre class="r"><code>summary(cars)
##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00
fit &lt;- lm(dist ~ speed, data = cars)
fit
## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Coefficients:
## (Intercept)        speed  
##     -17.579        3.932</code></pre>
</div>
<div id="including-plots" class="section level1">
<h1>Including Plots</h1>
<p>You can also embed plots. See Figure <a href="#fig:pie">1</a> for example:</p>
<pre class="r"><code>par(mar = c(0, 1, 0, 1))
pie(
  c(280, 60, 20),
  c(&#39;Sky&#39;, &#39;Sunny side of pyramid&#39;, &#39;Shady side of pyramid&#39;),
  col = c(&#39;#0292D8&#39;, &#39;#F7EA39&#39;, &#39;#C4B632&#39;),
  init.angle = -50, border = NA
)</code></pre>
<div class="figure"><span id="fig:pie"></span>
<img src="/post/2017-09-12-moving-my-blog-to-blogdown_files/figure-html/pie-1.png" alt="A fancy pie chart." width="672" />
<p class="caption">
Figure 1: A fancy pie chart.
</p>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Data Science for Fraud Detection]]></title>
    <link href="/2017/09/data-science-fraud-detection/"/>
    <id>/2017/09/data-science-fraud-detection/</id>
    <published>2017-09-06T00:00:00+00:00</published>
    <updated>2017-09-06T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I have written the following post about <a href="https://blog.codecentric.de/en/2017/09/data-science-fraud-detection/">Data Science for Fraud Detection</a> at my company <a href="https://blog.codecentric.de/en/">codecentric</a>&rsquo;s blog:</p>

<blockquote>
<p>Fraud can be defined as “the crime of getting money by deceiving people” (Cambridge Dictionary); it is as old as humanity: whenever two parties exchange goods or conduct business there is the potential for one party scamming the other. With an ever increasing use of the internet for shopping, banking, filing insurance claims, etc. these businesses have become targets of fraud in a whole new dimension. Fraud has become a major problem in e-commerce and a lot of resources are being invested to recognize and prevent it.</p>

<p>Traditional approaches to identifying fraud have been rule-based. This means that hard and fast rules for flagging a transaction as fraudulent have to be established manually and in advance. But this system isn’t flexible and inevitably results in an arms-race between the seller’s fraud detection system and criminals finding ways to circumnavigate these rules. The modern alternative is to leverage the vast amounts of Big Data that can be collected from online transactions and model it in a way that allows us to flag or predict fraud in future transactions. For this, Data Science and Machine Learning techniques, like Deep Neural Networks (DNNs), are the obvious solution!</p>

<p>Here, I am going to show an example of how Data Science techniques can be used to identify fraud in financial transactions. I will offer some insights into the inner workings of fraud analysis, aimed at non-experts to understand.</p>
</blockquote>

<p>Continue reading at <a href="https://blog.codecentric.de/en/2017/09/data-science-fraud-detection/">https://blog.codecentric.de/en/2017/09/data-science-fraud-detection/</a>&hellip;</p>

<p>The blog post is <a href="https://blog.codecentric.de/2017/09/fraud-analyse-mit-data-science-techniken/">also available in German</a>.</p>

<p><img src="https://shiring.github.io/netlify_images/r_mse_gklfsi.png" alt="" /></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Migrating from GitHub to GitLab with RStudio (Tutorial)]]></title>
    <link href="/2017/09/migrating-github-gitlab/"/>
    <id>/2017/09/migrating-github-gitlab/</id>
    <published>2017-09-04T00:00:00+00:00</published>
    <updated>2017-09-04T00:00:00+00:00</updated>
    <content type="html"><![CDATA[

<h2 id="github-vs-gitlab">GitHub vs. GitLab</h2>

<p>Git is a distributed implementation of version control. Many people have written very eloquently about why it is a good idea to use version control, not only if you collaborate in a team but also if you work on your own; one example is <a href="https://support.rstudio.com/hc/en-us/articles/200532077?version=1.0.153&amp;mode=desktop">this article from RStudio&rsquo;s Support pages</a>.</p>

<p>In short, its main feature is that version control allows you to keep track of the changes you make to your code. It will also keep a history of all the changes you have made in the past and allows you to go back to specific versions if you made a major mistake. And Git makes collaborating on the same code very easy.</p>

<p>Most R packages are also hosted on <a href="https://github.com/">GitHub</a>. You can check out their R code in the repositories if you want to get a deeper understanding of the functions, you can install the latest development versions of packages or install packages that are not on CRAN. The issue tracker function of GitHub also makes it easy to report and respond to issues/problems with your code.</p>

<h3 id="why-would-you-want-to-leave-github">Why would you want to leave GitHub?</h3>

<p>Public repositories are free on GitHub but you need to pay for private repos (if you are a student or work in academia, you <a href="https://education.github.com/discount_requests/new">get private repos for free</a>). Since I switched from academia to industry lately and no longer fulfil these criteria, all my private repos would have to be switched to public in the future. Here, GitLab is a great alternative!</p>

<p><a href="https://gitlab.com/">GitLab</a> offers very similar functionalities as GitHub. There are <a href="https://www.slant.co/versus/532/4860/~github_vs_gitlab">many pros and cons for using GitHub versus GitLab</a> but for me, the selling point was that GitLab offers unlimited private projects and collaborators in its free plan.</p>

<p><br></p>

<h1 id="tutorial">Tutorial</h1>

<p>Migrating from GitHub to <a href="https://gitlab.com/">GitLab</a> with RStudio is very easy! Here, I will show how I migrated my GitHub repositories of R projects, that I work with from within RStudio, to GitLab.</p>

<p><img src="https://shiring.github.io/netlify_images/GitLab_logo_yej6ht.png" alt="" /></p>

<p>Beware, that ALL code snippets below show Terminal code (they are NOT from the R console)!</p>

<p><br></p>

<h2 id="migrating-existing-repositories">Migrating existing repositories</h2>

<p>You first need to set up your GitLab account (you can login with your GitHub account) and connect your old GitHub account. Under <a href="https://gitlab.com/profile/account">Settings &amp;Account</a>, you will find &ldquo;Social sign-in&rdquo;; here click on &ldquo;Connect&rdquo; next to the GitHub symbol (if you signed in with your GitHub account, it will already be connected).</p>

<p>Once you have done this, you can import all your GitHub repositories to GitLab. To do this, you first need to create a new project. Click on the drop-down arrow next to the plus sign in the top-right corner and select &ldquo;New project&rdquo;. This will open the following window:</p>

<p><img src="https://shiring.github.io/netlify_images/Bildschirmfoto1_yuc7gb.png" alt="" /></p>

<p>Here, choose &ldquo;Import project from GitHub&rdquo; and choose the repositories you want to import.</p>

<p>If you go into one of your repositories, GitLab will show you a message at the top of the site that tells you that you need to add an SSH key. The SSH key is used for secure communication between the GitLab server and your computer when you want to share information, like push/pull commits.</p>

<p><img src="https://shiring.github.io/netlify_images/Bildschirmfoto2_diwetw.png" alt="" /></p>

<p>If you already work with GitHub on your computer, you will have an SSH key set up and you can <a href="https://gitlab.com/profile/keys">copy your public SSH key to GitLab</a>. Follow the instructions <a href="https://gitlab.com/help/ssh/README">here</a>.</p>

<p>Here is how you do it on a Mac:</p>

<ol>
<li>Look for your public key and copy it to the clipboard</li>
</ol>

<!-- -->

<pre><code>cat ~/.ssh/id_rsa.pub
pbcopy &lt; ~/.ssh/id_rsa.pub
</code></pre>

<p>Then paste it into the respective field <a href="https://gitlab.com/profile/keys">here</a>.</p>

<p>The next step is to change the remote URL for pushing/pulling your project from RStudio. In your Git window (tab next to &ldquo;Environment&rdquo; and &ldquo;History&rdquo; for me), click on Settings and &ldquo;Shell&rdquo;.</p>

<p><img src="https://shiring.github.io/netlify_images/Bildschirmfoto3_ydklnw.png" alt="" /></p>

<p>Then write in the shell window that opened:</p>

<pre><code>git remote set-url origin git@&lt;GITLABHOST&gt;:&lt;ORGNAME&gt;/&lt;REPO&gt;.git
</code></pre>

<p>You can copy the link in the navigation bar of your repo on GitLab.</p>

<p><img src="https://shiring.github.io/netlify_images/Bildschirmfoto4_dheikm.png" alt="" /></p>

<p>Check that you now have the correct new gitlab path by going to &ldquo;Tools&rdquo;, &ldquo;Project Options&rdquo; and &ldquo;Git/SVN&rdquo;.</p>

<p>Also check your SSH key configuration with:</p>

<pre><code>ssh -T git@&lt;GITLABHOST&gt;
</code></pre>

<p>If you get the following message</p>

<pre><code>The authenticity of host 'gitlab.com (52.167.219.168)' can't be established.
ECDSA key fingerprint is ...
Are you sure you want to continue connecting (yes/no)?
</code></pre>

<p>type &ldquo;yes&rdquo; (and enter passphrase if prompted).</p>

<p>If everything is okay, you now get a message saying <code>Welcome to GitLab!</code></p>

<p>Now, you can commit, push and pull from within RStudio just as you have done before!</p>

<p><br></p>

<h2 id="in-case-of-problems-with-pushing-pulling">In case of problems with pushing/pulling</h2>

<p>In my case, I migrated both, my private as well as my company&rsquo;s GitHub repos to GitLab. While my private repos could be migrated without a hitch, migrating my company&rsquo;s repos was a bit more tricky (because they had additional security settings, I assume).</p>

<p>Here is how I solved this problem with my company&rsquo;s repos:</p>

<p>I have protected my SSH key with a passphrase. When pushing or pulling commits via the shell with <code>git pull</code> and <code>git push origin master</code>, I am prompted to enter my passphrase and everything works fine. Pushing/pulling from within RStudio, however, threw an error:</p>

<pre><code>ssh_askpass: exec(/usr/X11R6/bin/ssh-askpass): No such file or directory
Permission denied (publickey).
fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.
</code></pre>

<p>I am using a MacBook Pro with MacOS Sierra version 10.12.6, so this might not be an issue with another operating system.</p>

<p>The following solution worked for me:</p>

<ol>
<li>Add your SSH key</li>
</ol>

<!-- -->

<pre><code>ssh-add ~/.ssh/id_rsa
</code></pre>

<ol>
<li>And reinstall <a href="https://vscode-eastus.azurewebsites.net/docs/setup/mac">VS Code</a></li>
</ol>

<p>Now I could commit, push and pull from within RStudio just as before!</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Social Network Analysis and Topic Modeling of codecentric’s Twitter friends and followers]]></title>
    <link href="/2017/07/twitter-analysis-codecentric/"/>
    <id>/2017/07/twitter-analysis-codecentric/</id>
    <published>2017-07-28T00:00:00+00:00</published>
    <updated>2017-07-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I have written the following post about <a href="https://blog.codecentric.de/en/2017/07/combining-social-network-analysis-topic-modeling-characterize-codecentrics-twitter-friends-followers/">Social Network Analysis and Topic Modeling of codecentric&rsquo;s Twitter friends and followers</a> for <a href="https://blog.codecentric.de/en/">codecentric</a>&rsquo;s blog:</p>

<blockquote>
<p>Recently, Matthias Radtke has written a very nice blog post on Topic Modeling of the codecentric Blog Articles, where he is giving a comprehensive introduction to Topic Modeling. In this article I am showing a real-world example of how we can use Data Science to gain insights from text data and social network analysis.</p>

<p>I am using publicly available Twitter data to characterize codecentric&rsquo;s friends and followers for identifying the most &ldquo;influential&rdquo; followers and using text analysis tools like sentiment analysis to characterize their interests from their user descriptions, performing Social Network Analysis on friends, followers and a subset of second degree connections to identify key players who will be able to pass on information to a wide reach of other users and combing this network analysis with topic modeling to identify meta-groups with similar interests.</p>

<p>Knowing the interests and social network positions of our followers allows us to identify key users who are likely to retweet posts that fall within their range of interests and who will reach a wide audience.</p>

<p>&hellip;</p>
</blockquote>

<p>Continue reading at <a href="https://blog.codecentric.de/en/2017/07/combining-social-network-analysis-topic-modeling-characterize-codecentrics-twitter-friends-followers/">https://blog.codecentric.de/en/2017/07/combining-social-network-analysis-topic-modeling-characterize-codecentrics-twitter-friends-followers/</a>&hellip;</p>

<p>The entire analysis has been done in R 3.4.0 and you can find my code on <a href="https://github.com/ShirinG/blog_posts_prep/blob/master/twitter/twitter_codecentric.Rmd">Github</a>.</p>

<p><img src="https://shiring.github.io/netlify_images/twitter_net_topics_lnu3j9.png" alt="" /></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Find all my other posts on my old website!]]></title>
    <link href="/2017/07/find-all-my-other-posts-on-my-old-website/"/>
    <id>/2017/07/find-all-my-other-posts-on-my-old-website/</id>
    <published>2017-07-01T00:00:00+00:00</published>
    <updated>2017-07-01T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>For all my other posts, see my old website:
<a href="https://shiring.github.io">shiring.github.io</a></p>
]]></content>
  </entry>
</feed>