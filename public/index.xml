<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Shirin&#39;s playgRound</title>
  <link href="/index.xml" rel="self"/>
  <link href="/"/>
  <updated>2018-12-12T00:00:00+00:00</updated>
  <id>/</id>
  <author>
    <name>Dr. Shirin Glander</name>
  </author>
  <generator>Hugo -- gohugo.io</generator>
  <entry>
    <title type="html"><![CDATA[Code for case study - Customer Churn with Keras/TensorFlow and H2O]]></title>
    <link href="/2018/12/customer_churn_code/"/>
    <id>/2018/12/customer_churn_code/</id>
    <published>2018-12-12T00:00:00+00:00</published>
    <updated>2018-12-12T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>This is code that accompanies a <a href="https://www.dpunkt.de/buecher/13208/9783864906107-data-science.html">book</a> chapter on customer churn that I have written for the German dpunkt Verlag. The book is in German and will probably appear in February: <a href="https://www.dpunkt.de/buecher/13208/9783864906107-data-science.html" class="uri">https://www.dpunkt.de/buecher/13208/9783864906107-data-science.html</a>.</p>
<p>The code you find below can be used to recreate all figures and analyses from this book chapter. Because the content is exclusively for the book, my descriptions around the code had to be minimal. But I’m sure, you can get the gist, even without the book. ;-)</p>
<div id="inspiration-sources" class="section level1">
<h1>Inspiration &amp; Sources</h1>
<p>Thank you to the following people for providing excellent code examples about customer churn:</p>
<ul>
<li>Matt Dancho: <a href="http://www.business-science.io/business/2017/11/28/customer_churn_analysis_keras.html" class="uri">http://www.business-science.io/business/2017/11/28/customer_churn_analysis_keras.html</a></li>
<li>JJ Allaire: <a href="https://github.com/rstudio/keras-customer-churn" class="uri">https://github.com/rstudio/keras-customer-churn</a></li>
<li>Susan Li: <a href="https://towardsdatascience.com/predict-customer-churn-with-r-9e62357d47b4" class="uri">https://towardsdatascience.com/predict-customer-churn-with-r-9e62357d47b4</a></li>
<li>John Sullivan: <a href="https://jtsulliv.github.io/churn-eda/" class="uri">https://jtsulliv.github.io/churn-eda/</a></li>
</ul>
</div>
<div id="setup" class="section level1">
<h1>Setup</h1>
<p>All analyses are done in R using RStudio. For detailed session information including R version, operating system and package versions, see the <code>sessionInfo()</code> output at the end of this document.</p>
<p>All figures are produced with ggplot2.</p>
<ul>
<li>Libraries</li>
</ul>
<pre class="r"><code># Load libraries
library(tidyverse) # for tidy data analysis
library(readr)     # for fast reading of input files
library(caret)     # for convenient splitting
library(mice)      # mice package for Multivariate Imputation by Chained Equations (MICE)
library(keras)     # for neural nets
library(lime)      # for explaining neural nets
library(rsample)   # for splitting training and test data
library(recipes)   # for preprocessing
library(yardstick) # for evaluation
library(ggthemes)  # for additional plotting themes
library(corrplot)  # for correlation

theme_set(theme_minimal())</code></pre>
<pre class="r"><code># Install Keras if you have not installed it before
# follow instructions if you haven&#39;t installed TensorFlow
install_keras()</code></pre>
<p><br></p>
</div>
<div id="data-preparation" class="section level1">
<h1>Data preparation</h1>
<div id="the-dataset" class="section level2">
<h2>The dataset</h2>
<p>The Telco Customer Churn data set is the same one that Matt Dancho used in his post (see above). It was downloaded from <a href="https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/">IBM Watson</a></p>
<pre class="r"><code>churn_data_raw &lt;- read_csv(&quot;WA_Fn-UseC_-Telco-Customer-Churn.csv&quot;)</code></pre>
<pre class="r"><code>glimpse(churn_data_raw)</code></pre>
<pre><code>## Observations: 7,043
## Variables: 21
## $ customerID       &lt;chr&gt; &quot;7590-VHVEG&quot;, &quot;5575-GNVDE&quot;, &quot;3668-QPYBK&quot;, &quot;77...
## $ gender           &lt;chr&gt; &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;...
## $ SeniorCitizen    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
## $ Partner          &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;N...
## $ Dependents       &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;N...
## $ tenure           &lt;dbl&gt; 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 5...
## $ PhoneService     &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;...
## $ MultipleLines    &lt;chr&gt; &quot;No phone service&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No phone ser...
## $ InternetService  &lt;chr&gt; &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;Fiber optic&quot;, &quot;F...
## $ OnlineSecurity   &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, ...
## $ OnlineBackup     &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, ...
## $ DeviceProtection &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, ...
## $ TechSupport      &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;N...
## $ StreamingTV      &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;...
## $ StreamingMovies  &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;N...
## $ Contract         &lt;chr&gt; &quot;Month-to-month&quot;, &quot;One year&quot;, &quot;Month-to-month...
## $ PaperlessBilling &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;...
## $ PaymentMethod    &lt;chr&gt; &quot;Electronic check&quot;, &quot;Mailed check&quot;, &quot;Mailed c...
## $ MonthlyCharges   &lt;dbl&gt; 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89....
## $ TotalCharges     &lt;dbl&gt; 29.85, 1889.50, 108.15, 1840.75, 151.65, 820....
## $ Churn            &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, ...</code></pre>
<div id="eda" class="section level3">
<h3>EDA</h3>
<ul>
<li>Proportion of churn</li>
</ul>
<pre class="r"><code>churn_data_raw %&gt;%
  count(Churn)</code></pre>
<pre><code>## # A tibble: 2 x 2
##   Churn     n
##   &lt;chr&gt; &lt;int&gt;
## 1 No     5174
## 2 Yes    1869</code></pre>
<ul>
<li>Plot categorical features</li>
</ul>
<pre class="r"><code>churn_data_raw %&gt;%
  mutate(SeniorCitizen = as.character(SeniorCitizen)) %&gt;%
  select(-customerID) %&gt;%
  select_if(is.character) %&gt;%
  select(Churn, everything()) %&gt;%
  gather(x, y, gender:PaymentMethod) %&gt;%
  count(Churn, x, y) %&gt;%
  ggplot(aes(x = y, y = n, fill = Churn, color = Churn)) +
    facet_wrap(~ x, ncol = 4, scales = &quot;free&quot;) +
    geom_bar(stat = &quot;identity&quot;, alpha = 0.5) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
          legend.position = &quot;top&quot;) +
    scale_color_tableau() +
    scale_fill_tableau()</code></pre>
<p><img src="/post/2018-12-12_customer_churn_code_files/figure-html/eda_chr-1.png" width="1152" /></p>
<ul>
<li>Plot numerical features</li>
</ul>
<pre class="r"><code>churn_data_raw %&gt;%
  select(-customerID) %&gt;%
  #select_if(is.numeric) %&gt;%
  select(Churn, MonthlyCharges, tenure, TotalCharges) %&gt;%
  gather(x, y, MonthlyCharges:TotalCharges) %&gt;%
  ggplot(aes(x = y, fill = Churn, color = Churn)) +
    facet_wrap(~ x, ncol = 3, scales = &quot;free&quot;) +
    geom_density(alpha = 0.5) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1),
          legend.position = &quot;top&quot;) +
    scale_color_tableau() +
    scale_fill_tableau()</code></pre>
<p><img src="/post/2018-12-12_customer_churn_code_files/figure-html/eda_num-1.png" width="1152" /></p>
<ul>
<li>Remove customer ID as it doesn’t provide information</li>
</ul>
<pre class="r"><code>churn_data &lt;- churn_data_raw %&gt;%
  select(-customerID)</code></pre>
</div>
<div id="dealing-with-missing-values" class="section level3">
<h3>Dealing with missing values</h3>
<ul>
<li>Pattern of missing data</li>
</ul>
<pre class="r"><code>md.pattern(churn_data, plot = FALSE)</code></pre>
<pre><code>##      gender SeniorCitizen Partner Dependents tenure PhoneService
## 7032      1             1       1          1      1            1
## 11        1             1       1          1      1            1
##           0             0       0          0      0            0
##      MultipleLines InternetService OnlineSecurity OnlineBackup
## 7032             1               1              1            1
## 11               1               1              1            1
##                  0               0              0            0
##      DeviceProtection TechSupport StreamingTV StreamingMovies Contract
## 7032                1           1           1               1        1
## 11                  1           1           1               1        1
##                     0           0           0               0        0
##      PaperlessBilling PaymentMethod MonthlyCharges Churn TotalCharges   
## 7032                1             1              1     1            1  0
## 11                  1             1              1     1            0  1
##                     0             0              0     0           11 11</code></pre>
<ul>
<li>Option 1: impute missing data =&gt; NOT done here!</li>
</ul>
<pre class="r"><code>imp &lt;- mice(data = churn_data,  print = FALSE)
train_data_impute &lt;- complete(imp, &quot;long&quot;)</code></pre>
<ul>
<li>Option 2: drop missing data =&gt; done here because not too much information is lost by removing it</li>
</ul>
<pre class="r"><code>churn_data &lt;- churn_data %&gt;%
  drop_na()</code></pre>
</div>
</div>
<div id="training-and-test-split" class="section level2">
<h2>Training and test split</h2>
<ul>
<li>Partition data into training and test set</li>
</ul>
<pre class="r"><code>set.seed(42)
index &lt;- createDataPartition(churn_data$Churn, p = 0.7, list = FALSE)</code></pre>
<ul>
<li>Partition test set again into validation and test set</li>
</ul>
<pre class="r"><code>train_data &lt;- churn_data[index, ]
test_data  &lt;- churn_data[-index, ]

index2 &lt;- createDataPartition(test_data$Churn, p = 0.5, list = FALSE)

valid_data &lt;- test_data[-index2, ]
test_data &lt;- test_data[index2, ]</code></pre>
<pre class="r"><code>nrow(train_data)</code></pre>
<pre><code>## [1] 4924</code></pre>
<pre class="r"><code>nrow(valid_data)</code></pre>
<pre><code>## [1] 1054</code></pre>
<pre class="r"><code>nrow(test_data)</code></pre>
<pre><code>## [1] 1054</code></pre>
</div>
<div id="pre-processing" class="section level2">
<h2>Pre-Processing</h2>
<ul>
<li>Create recipe for preprocessing</li>
</ul>
<blockquote>
<p>A recipe is a description of what steps should be applied to a data set in order to get it ready for data analysis.</p>
</blockquote>
<pre class="r"><code>recipe_churn &lt;- recipe(Churn ~ ., train_data) %&gt;%
  step_dummy(all_nominal(), -all_outcomes()) %&gt;%
  step_center(all_predictors(), -all_outcomes()) %&gt;%
  step_scale(all_predictors(), -all_outcomes()) %&gt;%
  prep(data = train_data)</code></pre>
<ul>
<li>Apply recipe to three datasets</li>
</ul>
<pre class="r"><code>train_data &lt;- bake(recipe_churn, new_data = train_data) %&gt;%
  select(Churn, everything())

valid_data &lt;- bake(recipe_churn, new_data = valid_data) %&gt;%
  select(Churn, everything())

test_data &lt;- bake(recipe_churn, new_data = test_data) %&gt;%
  select(Churn, everything())</code></pre>
<ul>
<li>For Keras create response variable as one-hot encoded matrix</li>
</ul>
<pre class="r"><code>train_y_drop &lt;- to_categorical(as.integer(as.factor(train_data$Churn)) - 1, 2)
colnames(train_y_drop) &lt;- c(&quot;No&quot;, &quot;Yes&quot;)

valid_y_drop &lt;- to_categorical(as.integer(as.factor(valid_data$Churn)) - 1, 2)
colnames(valid_y_drop) &lt;- c(&quot;No&quot;, &quot;Yes&quot;)

test_y_drop &lt;- to_categorical(as.integer(as.factor(test_data$Churn)) - 1, 2)
colnames(test_y_drop) &lt;- c(&quot;No&quot;, &quot;Yes&quot;)</code></pre>
<ul>
<li>Because we want to train on a binary outcome, we can delete the “No” column</li>
</ul>
<pre class="r"><code># if training with binary crossentropy
train_y_drop &lt;- train_y_drop[, 2, drop = FALSE]
head(train_y_drop)</code></pre>
<pre><code>##      Yes
## [1,]   0
## [2,]   1
## [3,]   1
## [4,]   0
## [5,]   1
## [6,]   0</code></pre>
<pre class="r"><code>valid_y_drop &lt;- valid_y_drop[, 2, drop = FALSE]
test_y_drop &lt;- test_y_drop[, 2, drop = FALSE]</code></pre>
<ul>
<li>Remove response variable from preprocessed data (for Keras)</li>
</ul>
<pre class="r"><code>train_data_bk &lt;- select(train_data, -Churn)
head(train_data_bk)</code></pre>
<pre><code>## # A tibble: 6 x 30
##   SeniorCitizen  tenure MonthlyCharges TotalCharges gender_Male Partner_Yes
##           &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;
## 1        -0.439  0.0765         -0.256       -0.163       0.979      -0.966
## 2        -0.439 -1.23           -0.359       -0.949       0.979      -0.966
## 3        -0.439 -0.983           1.17        -0.635      -1.02       -0.966
## 4        -0.439 -0.901          -1.16        -0.863      -1.02       -0.966
## 5        -0.439 -0.168           1.34         0.347      -1.02        1.03 
## 6        -0.439  1.22           -0.282        0.542       0.979      -0.966
## # ... with 24 more variables: Dependents_Yes &lt;dbl&gt;,
## #   PhoneService_Yes &lt;dbl&gt;, MultipleLines_No.phone.service &lt;dbl&gt;,
## #   MultipleLines_Yes &lt;dbl&gt;, InternetService_Fiber.optic &lt;dbl&gt;,
## #   InternetService_No &lt;dbl&gt;, OnlineSecurity_No.internet.service &lt;dbl&gt;,
## #   OnlineSecurity_Yes &lt;dbl&gt;, OnlineBackup_No.internet.service &lt;dbl&gt;,
## #   OnlineBackup_Yes &lt;dbl&gt;, DeviceProtection_No.internet.service &lt;dbl&gt;,
## #   DeviceProtection_Yes &lt;dbl&gt;, TechSupport_No.internet.service &lt;dbl&gt;,
## #   TechSupport_Yes &lt;dbl&gt;, StreamingTV_No.internet.service &lt;dbl&gt;,
## #   StreamingTV_Yes &lt;dbl&gt;, StreamingMovies_No.internet.service &lt;dbl&gt;,
## #   StreamingMovies_Yes &lt;dbl&gt;, Contract_One.year &lt;dbl&gt;,
## #   Contract_Two.year &lt;dbl&gt;, PaperlessBilling_Yes &lt;dbl&gt;,
## #   PaymentMethod_Credit.card..automatic. &lt;dbl&gt;,
## #   PaymentMethod_Electronic.check &lt;dbl&gt;, PaymentMethod_Mailed.check &lt;dbl&gt;</code></pre>
<pre class="r"><code>valid_data_bk &lt;- select(valid_data, -Churn)
test_data_bk &lt;- select(test_data, -Churn)</code></pre>
<ul>
<li>Alternative to above, to convert response variable into numeric format where 1 = Yes and 0 = No</li>
</ul>
<pre class="r"><code>train_data$Churn &lt;- ifelse(train_data$Churn == &quot;Yes&quot;, 1, 0)
valid_data$Churn &lt;- ifelse(valid_data$Churn == &quot;Yes&quot;, 1, 0)
test_data$Churn &lt;- ifelse(test_data$Churn == &quot;Yes&quot;, 1, 0)</code></pre>
</div>
<div id="modeling-with-keras" class="section level2">
<h2>Modeling with Keras</h2>
<ul>
<li>Define a simple MLP</li>
</ul>
<pre class="r"><code>model_keras &lt;- keras_model_sequential()

model_keras %&gt;% 
  layer_dense(units = 32, kernel_initializer = &quot;uniform&quot;, activation = &quot;relu&quot;, 
              input_shape = ncol(train_data_bk)) %&gt;% 
  layer_dropout(rate = 0.2) %&gt;%
  
  layer_dense(units = 16, kernel_initializer = &quot;uniform&quot;, activation = &quot;relu&quot;) %&gt;% 
  layer_dropout(rate = 0.2) %&gt;%
  
  layer_dense(units = 8, kernel_initializer = &quot;uniform&quot;, activation = &quot;relu&quot;) %&gt;% 
  layer_dropout(rate = 0.2) %&gt;%

  layer_dense(units = 1,
              kernel_initializer = &quot;uniform&quot;, activation = &quot;sigmoid&quot;) %&gt;%
  
  compile(
        optimizer = &#39;adamax&#39;,
        loss      = &#39;binary_crossentropy&#39;,
        metrics   = c(&quot;binary_accuracy&quot;, &quot;mse&quot;)
    )

summary(model_keras)</code></pre>
<pre><code>## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## dense_1 (Dense)                  (None, 32)                    992         
## ___________________________________________________________________________
## dropout_1 (Dropout)              (None, 32)                    0           
## ___________________________________________________________________________
## dense_2 (Dense)                  (None, 16)                    528         
## ___________________________________________________________________________
## dropout_2 (Dropout)              (None, 16)                    0           
## ___________________________________________________________________________
## dense_3 (Dense)                  (None, 8)                     136         
## ___________________________________________________________________________
## dropout_3 (Dropout)              (None, 8)                     0           
## ___________________________________________________________________________
## dense_4 (Dense)                  (None, 1)                     9           
## ===========================================================================
## Total params: 1,665
## Trainable params: 1,665
## Non-trainable params: 0
## ___________________________________________________________________________</code></pre>
<ul>
<li>Fit model (we could have used validation split on the trainings data instead of creating a validation set =&gt; see #)</li>
</ul>
<pre class="r"><code>fit_keras &lt;- fit(model_keras, 
    x = as.matrix(train_data_bk), 
    y = train_y_drop,
    batch_size = 32, 
    epochs = 20,
    #validation_split = 0.30,
    validation_data = list(as.matrix(valid_data_bk), valid_y_drop),
    verbose = 2
    )</code></pre>
<ul>
<li>Plot Keras training results</li>
</ul>
<pre class="r"><code>plot(fit_keras) +
  scale_color_tableau() +
  scale_fill_tableau()</code></pre>
<p><img src="/post/2018-12-12_customer_churn_code_files/figure-html/plot_fit_keras-1.png" width="960" /></p>
</div>
<div id="evaluation" class="section level2">
<h2>Evaluation</h2>
<ul>
<li>Predict classes and probabilities</li>
</ul>
<pre class="r"><code>pred_classes_test &lt;- predict_classes(object = model_keras, x = as.matrix(test_data_bk))
pred_proba_test  &lt;- predict_proba(object = model_keras, x = as.matrix(test_data_bk))</code></pre>
<ul>
<li>Create results table</li>
</ul>
<pre class="r"><code>test_results &lt;- tibble(
  actual_yes = as.factor(as.vector(test_y_drop)),
  pred_classes_test = as.factor(as.vector(pred_classes_test)),
  Yes = as.vector(pred_proba_test), 
  No = 1 - as.vector(pred_proba_test))
head(test_results)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   actual_yes pred_classes_test     Yes    No
##   &lt;fct&gt;      &lt;fct&gt;               &lt;dbl&gt; &lt;dbl&gt;
## 1 0          1                 0.567   0.433
## 2 0          0                 0.0200  0.980
## 3 1          1                 0.683   0.317
## 4 1          0                 0.330   0.670
## 5 0          0                 0.00589 0.994
## 6 0          0                 0.0459  0.954</code></pre>
<ul>
<li>Calculate confusion matrix</li>
</ul>
<pre class="r"><code>test_results %&gt;% 
  conf_mat(actual_yes, pred_classes_test)</code></pre>
<pre><code>##           Truth
## Prediction   0   1
##          0 694 125
##          1  80 155</code></pre>
<ul>
<li>Calculate metrics</li>
</ul>
<pre class="r"><code>test_results %&gt;% 
  metrics(actual_yes, pred_classes_test)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.806
## 2 kap      binary         0.475</code></pre>
<ul>
<li>Are under the ROC curve</li>
</ul>
<pre class="r"><code>test_results %&gt;% 
  roc_auc(actual_yes, Yes)</code></pre>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 roc_auc binary         0.839</code></pre>
<ul>
<li>Precision and recall</li>
</ul>
<pre class="r"><code>tibble(
    precision = test_results %&gt;% yardstick::precision(actual_yes, pred_classes_test) %&gt;% select(.estimate) %&gt;% as.numeric(),
    recall    = test_results %&gt;% yardstick::recall(actual_yes, pred_classes_test) %&gt;% select(.estimate) %&gt;% as.numeric()
)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   precision recall
##       &lt;dbl&gt;  &lt;dbl&gt;
## 1     0.847  0.897</code></pre>
<ul>
<li>F1-Statistic</li>
</ul>
<pre class="r"><code>test_results %&gt;% yardstick::f_meas(actual_yes, pred_classes_test, beta = 1)</code></pre>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 f_meas  binary         0.871</code></pre>
</div>
<div id="h2o" class="section level2">
<h2>H2O</h2>
<p>Shows an alternative to Keras!</p>
<ul>
<li>Initialise H2O instance and convert data to h2o frame</li>
</ul>
<pre class="r"><code>library(h2o)
h2o.init(nthreads = -1)</code></pre>
<pre><code>##  Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         4 minutes 37 seconds 
##     H2O cluster timezone:       Europe/Berlin 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.20.0.8 
##     H2O cluster version age:    2 months and 24 days  
##     H2O cluster name:           H2O_started_from_R_shiringlander_bzx929 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   3.39 GB 
##     H2O cluster total cores:    8 
##     H2O cluster allowed cores:  8 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.1 (2018-07-02)</code></pre>
<pre class="r"><code>h2o.no_progress()

train_hf &lt;- as.h2o(train_data)
valid_hf &lt;- as.h2o(valid_data)
test_hf &lt;- as.h2o(test_data)

response &lt;- &quot;Churn&quot;
features &lt;- setdiff(colnames(train_hf), response)</code></pre>
<pre class="r"><code># For binary classification, response should be a factor
train_hf[, response] &lt;- as.factor(train_hf[, response])
valid_hf[, response] &lt;- as.factor(valid_hf[, response])
test_hf[, response] &lt;- as.factor(test_hf[, response])</code></pre>
<pre class="r"><code>summary(train_hf$Churn, exact_quantiles = TRUE)</code></pre>
<pre><code>##  Churn  
##  0:3615 
##  1:1309</code></pre>
<pre class="r"><code>summary(valid_hf$Churn, exact_quantiles = TRUE)</code></pre>
<pre><code>##  Churn 
##  0:774 
##  1:280</code></pre>
<pre class="r"><code>summary(test_hf$Churn, exact_quantiles = TRUE)</code></pre>
<pre><code>##  Churn 
##  0:774 
##  1:280</code></pre>
<ul>
<li>Train model with <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html">AutoML</a>.</li>
</ul>
<blockquote>
<p>“During model training, you might find that the majority of your data belongs in a single class. For example, consider a binary classification model that has 100 rows, with 80 rows labeled as class 1 and the remaining 20 rows labeled as class 2. This is a common scenario, given that machine learning attempts to predict class 1 with the highest accuracy. It can also be an example of an imbalanced dataset, in this case, with a ratio of 4:1. The balance_classes option can be used to balance the class distribution. When enabled, H2O will either undersample the majority classes or oversample the minority classes. Note that the resulting model will also correct the final probabilities (“undo the sampling”) using a monotonic transform, so the predicted probabilities of the first model will differ from a second model. However, because AUC only cares about ordering, it won’t be affected. If this option is enabled, then you can also specify a value for the class_sampling_factors and max_after_balance_size options.” <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/balance_classes.html" class="uri">http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/balance_classes.html</a></p>
</blockquote>
<pre class="r"><code>aml &lt;- h2o.automl(x = features, 
                  y = response,
                  training_frame = train_hf,
                  validation_frame = valid_hf,
                  balance_classes = TRUE,
                  max_runtime_secs = 3600)

# View the AutoML Leaderboard
lb &lt;- aml@leaderboard

best_model &lt;- aml@leader

h2o.saveModel(best_model, &quot;/Users/shiringlander/Documents/Github/Data&quot;)</code></pre>
<ul>
<li>Prediction</li>
</ul>
<pre class="r"><code>pred &lt;- h2o.predict(best_model, test_hf[, -1])</code></pre>
<ul>
<li>Mean per class error</li>
</ul>
<pre class="r"><code>h2o.mean_per_class_error(best_model, train = TRUE, valid = TRUE, xval = TRUE)</code></pre>
<pre><code>##     train     valid      xval 
## 0.1717911 0.2172683 0.2350682</code></pre>
<ul>
<li>Confusion matrix on validation data</li>
</ul>
<pre class="r"><code>h2o.confusionMatrix(best_model, valid = TRUE)</code></pre>
<pre><code>## Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.258586059604449:
##          0   1    Error      Rate
## 0      478 130 0.213816  =130/608
## 1       49 173 0.220721   =49/222
## Totals 527 303 0.215663  =179/830</code></pre>
<pre class="r"><code>h2o.auc(best_model, train = TRUE)</code></pre>
<pre><code>## [1] 0.9039696</code></pre>
<pre class="r"><code>h2o.auc(best_model, valid = TRUE)</code></pre>
<pre><code>## [1] 0.8509068</code></pre>
<pre class="r"><code>h2o.auc(best_model, xval = TRUE)</code></pre>
<pre><code>## [1] 0.8397085</code></pre>
<ul>
<li>Performance and confusion matrix on test data</li>
</ul>
<pre class="r"><code>perf &lt;- h2o.performance(best_model, test_hf)
h2o.confusionMatrix(perf)</code></pre>
<pre><code>## Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.40790847476229:
##          0   1    Error       Rate
## 0      667 107 0.138243   =107/774
## 1       86 194 0.307143    =86/280
## Totals 753 301 0.183112  =193/1054</code></pre>
<ul>
<li>Plot performance</li>
</ul>
<pre class="r"><code>plot(perf)</code></pre>
<ul>
<li>More performance metrics extracted</li>
</ul>
<pre class="r"><code>h2o.logloss(perf)</code></pre>
<pre><code>## [1] 0.4008041</code></pre>
<pre class="r"><code>h2o.mse(perf)</code></pre>
<pre><code>## [1] 0.1278505</code></pre>
<pre class="r"><code>h2o.auc(perf)</code></pre>
<pre><code>## [1] 0.8622301</code></pre>
<pre class="r"><code>metrics &lt;- as.data.frame(h2o.metric(perf))
head(metrics)</code></pre>
<pre><code>##   threshold         f1          f2   f0point5  accuracy precision
## 1 0.8278177 0.01418440 0.008912656 0.03472222 0.7362429         1
## 2 0.8203439 0.02816901 0.017793594 0.06756757 0.7381404         1
## 3 0.8173635 0.04195804 0.026642984 0.09868421 0.7400380         1
## 4 0.8160146 0.04878049 0.031055901 0.11363636 0.7409867         1
## 5 0.8139018 0.05555556 0.035460993 0.12820513 0.7419355         1
## 6 0.8112067 0.07560137 0.048629531 0.16975309 0.7447818         1
##        recall specificity absolute_mcc min_per_class_accuracy
## 1 0.007142857           1   0.07249342            0.007142857
## 2 0.014285714           1   0.10261877            0.014285714
## 3 0.021428571           1   0.12580168            0.021428571
## 4 0.025000000           1   0.13594622            0.025000000
## 5 0.028571429           1   0.14540208            0.028571429
## 6 0.039285714           1   0.17074408            0.039285714
##   mean_per_class_accuracy tns fns fps tps tnr       fnr fpr         tpr
## 1               0.5035714 774 278   0   2   1 0.9928571   0 0.007142857
## 2               0.5071429 774 276   0   4   1 0.9857143   0 0.014285714
## 3               0.5107143 774 274   0   6   1 0.9785714   0 0.021428571
## 4               0.5125000 774 273   0   7   1 0.9750000   0 0.025000000
## 5               0.5142857 774 272   0   8   1 0.9714286   0 0.028571429
## 6               0.5196429 774 269   0  11   1 0.9607143   0 0.039285714
##   idx
## 1   0
## 2   1
## 3   2
## 4   3
## 5   4
## 6   5</code></pre>
<ul>
<li>Plot performance metrics</li>
</ul>
<pre class="r"><code>metrics %&gt;%
  gather(x, y, f1:tpr) %&gt;%
  ggplot(aes(x = threshold, y = y, group = x)) +
    facet_wrap(~ x, ncol = 2, scales = &quot;free&quot;) +
    geom_line()</code></pre>
<p><img src="/post/2018-12-12_customer_churn_code_files/figure-html/metrics-1.png" width="768" /></p>
<ul>
<li>Examine prediction thresholds</li>
</ul>
<pre class="r"><code>threshold &lt;- metrics[order(-metrics$accuracy), &quot;threshold&quot;][1]

finalRf_predictions &lt;- data.frame(actual = as.vector(test_hf$Churn), 
                                  as.data.frame(h2o.predict(object = best_model, 
                                                            newdata = test_hf)))

finalRf_predictions$accurate &lt;- ifelse(finalRf_predictions$actual == 
                                         finalRf_predictions$predict, &quot;ja&quot;, &quot;nein&quot;)

finalRf_predictions$predict_stringent &lt;- ifelse(finalRf_predictions$p1 &gt; threshold, 1, 
                                                ifelse(finalRf_predictions$p0 &gt; threshold, 0, &quot;unsicher&quot;))
finalRf_predictions$accurate_stringent &lt;- ifelse(finalRf_predictions$actual == 
                                                   finalRf_predictions$predict_stringent, &quot;ja&quot;, 
                                       ifelse(finalRf_predictions$predict_stringent == 
                                                &quot;unsicher&quot;, &quot;unsicher&quot;, &quot;nein&quot;))

finalRf_predictions %&gt;%
  group_by(actual, predict) %&gt;%
  dplyr::summarise(n = n())</code></pre>
<pre><code>## # A tibble: 4 x 3
## # Groups:   actual [?]
##   actual predict     n
##   &lt;fct&gt;  &lt;fct&gt;   &lt;int&gt;
## 1 0      0         602
## 2 0      1         172
## 3 1      0          63
## 4 1      1         217</code></pre>
<pre class="r"><code>finalRf_predictions %&gt;%
  group_by(actual, predict_stringent) %&gt;%
  dplyr::summarise(n = n())</code></pre>
<pre><code>## # A tibble: 6 x 3
## # Groups:   actual [?]
##   actual predict_stringent     n
##   &lt;fct&gt;  &lt;chr&gt;             &lt;int&gt;
## 1 0      0                   683
## 2 0      1                    63
## 3 0      unsicher             28
## 4 1      0                   101
## 5 1      1                   152
## 6 1      unsicher             27</code></pre>
<pre class="r"><code>finalRf_predictions %&gt;%
  gather(x, y, accurate, accurate_stringent) %&gt;%
  mutate(x = ifelse(x == &quot;accurate&quot;, &quot;Default Schwelle: 0.5&quot;, 
                    paste(&quot;Angepasste Schwelle:&quot;, round(threshold, digits = 2)))) %&gt;%
  ggplot(aes(x = actual, fill = y)) +
    facet_grid(~ x) +
    geom_bar(position = &quot;dodge&quot;) +
    scale_fill_tableau()</code></pre>
<p><img src="/post/2018-12-12_customer_churn_code_files/figure-html/default_vs_stringent-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>df &lt;- finalRf_predictions[, c(1, 3, 4)]

thresholds &lt;- seq(from = 0, to = 1, by = 0.1)

prop_table &lt;- data.frame(threshold = thresholds, 
                         prop_p0_true = NA, prop_p0_false = NA,
                         prop_p1_true = NA, prop_p1_false = NA)

for (threshold in thresholds) {

  pred_1 &lt;- ifelse(df$p1 &gt; threshold, 1, 0)
  pred_1_t &lt;- ifelse(pred_1 == df$actual, TRUE, FALSE)
  
  group &lt;- data.frame(df, 
                      &quot;pred_true&quot; = pred_1_t) %&gt;%
    group_by(actual, pred_true) %&gt;%
    dplyr::summarise(n = n())
  
  group_p0 &lt;- filter(group, actual == &quot;0&quot;)
  
  prop_p0_t &lt;- sum(filter(group_p0, pred_true == TRUE)$n) / sum(group_p0$n)
  prop_p0_f &lt;- sum(filter(group_p0, pred_true == FALSE)$n) / sum(group_p0$n)
  prop_table[prop_table$threshold == threshold, &quot;prop_p0_true&quot;] &lt;- prop_p0_t
  prop_table[prop_table$threshold == threshold, &quot;prop_p0_false&quot;] &lt;- prop_p0_f
  
  group_p1 &lt;- filter(group, actual == &quot;1&quot;)
  
  prop_p1_t &lt;- sum(filter(group_p1, pred_true == TRUE)$n) / sum(group_p1$n)
  prop_p1_f &lt;- sum(filter(group_p1, pred_true == FALSE)$n) / sum(group_p1$n)
  prop_table[prop_table$threshold == threshold, &quot;prop_p1_true&quot;] &lt;- prop_p1_t
  prop_table[prop_table$threshold == threshold, &quot;prop_p1_false&quot;] &lt;- prop_p1_f
}</code></pre>
<pre class="r"><code>prop_table %&gt;%
  gather(x, y, prop_p0_true, prop_p1_true) %&gt;%
  rename(Schwellenwert = threshold) %&gt;%
  mutate(x = ifelse(x == &quot;prop_p0_true&quot;, &quot;prop true p0&quot;,
         &quot;prop true p1&quot;)) %&gt;%
  ggplot(aes(x = Schwellenwert, y = y, color = x)) +
    geom_point() +
    geom_line() +
    scale_color_tableau()</code></pre>
<p><img src="/post/2018-12-12_customer_churn_code_files/figure-html/prop_table-1.png" width="576" style="display: block; margin: auto;" /></p>
<div id="costrevenue-calculation" class="section level3">
<h3>Cost/revenue calculation</h3>
<p>Let’s assume that</p>
<ol style="list-style-type: decimal">
<li>a marketing campaign + employee time will cost the company 1000€ per year for every customer that is included in the campaign.</li>
<li>the annual average revenue per customer is 2000€ (in more complex scenarios customers could be further divided into revenue groups to calculate how “valuable” they are and how harmful loosing them would be)</li>
<li>investing into unnecessary marketing doesn’t cause churn by itself (i.e. a customer who isn’t going to churn isn’t reacting negatively to the add campaign - which could happen in more complex scenarios).</li>
<li>without a customer churn model the company would target half of their customer (by chance) for ad-campaigns</li>
<li>without a customer churn model the company would lose about 25% of their customers to churn</li>
</ol>
<p>This would mean that compared to no intervention we would have</p>
<ul>
<li>prop_p0_true == customers who were correctly predicted to not churn did not cost anything (no marketing money was spent): +/-0€</li>
<li>prop_p0_false == customers that did not churn who are predicted to churn will be an empty investment: +/-0€ - 1500€</li>
<li>prop_p1_false == customer that were predicted to stay but churned: -2000€</li>
<li>prop_p1_true == customers that were correctly predicted to churn:</li>
<li>let’s say 100% of those could be kept by investing into marketing: +2000€ -1500€</li>
<li>let’s say 50% could be kept by investing into marketing: +2000€ * 0.5 -1500€</li>
</ul>
<p><br></p>
<ul>
<li>Let’s play around with some values:</li>
</ul>
<pre class="r"><code># Baseline
revenue &lt;- 2000
cost &lt;- 1000

customers_churn &lt;- filter(test_data, Churn == 1)
customers_churn_n &lt;- nrow(customers_churn)

customers_no_churn &lt;- filter(filter(test_data, Churn == 0))
customers_no_churn_n &lt;- nrow(customers_no_churn)

customers &lt;- customers_churn_n + customers_no_churn_n

ad_target_rate &lt;- 0.5
ad_cost_default &lt;- customers * ad_target_rate * cost

churn_rate_default &lt;- customers_churn_n / customers_no_churn_n
ann_revenue_default &lt;- customers_no_churn_n * revenue

net_win_default &lt;- ann_revenue_default - ad_cost_default
net_win_default</code></pre>
<pre><code>## [1] 1021000</code></pre>
<ul>
<li>How much revenue can we gain from predicting customer churn (assuming conversionr rate of 0.7):</li>
</ul>
<pre class="r"><code>conversion &lt;- 0.7

net_win_table &lt;- prop_table %&gt;%
  mutate(prop_p0_true_X = prop_p0_true * customers_no_churn_n * revenue,
         prop_p0_false_X = prop_p0_false * customers_no_churn_n * (revenue -cost),
         prop_p1_false_X = prop_p1_false * customers_churn_n * 0,
         prop_p1_true_X = prop_p1_true * customers_churn_n * ((revenue * conversion) - cost)) %&gt;%
  group_by(threshold) %&gt;%
  summarise(net_win = sum(prop_p0_true_X + prop_p0_false_X + prop_p1_false_X + prop_p1_true_X),
            net_win_compared = net_win - net_win_default) %&gt;%
  arrange(-net_win_compared)

net_win_table</code></pre>
<pre><code>## # A tibble: 11 x 3
##    threshold net_win net_win_compared
##        &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;
##  1       0.7 1558000           537000
##  2       0.8 1554000           533000
##  3       0.6 1551000           530000
##  4       0.9 1548000           527000
##  5       1   1548000           527000
##  6       0.5 1534000           513000
##  7       0.4 1515600           494600
##  8       0.3 1483400           462400
##  9       0.2 1417200           396200
## 10       0.1 1288200           267200
## 11       0    886000          -135000</code></pre>
</div>
</div>
<div id="lime" class="section level2">
<h2>LIME</h2>
<ul>
<li>Explaining predictions</li>
</ul>
<pre class="r"><code>Xtrain &lt;- as.data.frame(train_hf)
Xtest &lt;- as.data.frame(test_hf)

# run lime() on training set
explainer &lt;- lime::lime(x = Xtrain, 
                        model = best_model)

# run explain() on the explainer
explanation &lt;- lime::explain(x = Xtest[1:9, ], 
                             explainer = explainer, 
                             n_labels = 1,
                             n_features = 4,
                             kernel_width = 0.5)</code></pre>
<pre class="r"><code>plot_explanations(explanation)</code></pre>
<p><img src="/post/2018-12-12_customer_churn_code_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
<pre class="r"><code>explanation %&gt;%
  plot_features(ncol = 3)</code></pre>
<p><img src="/post/2018-12-12_customer_churn_code_files/figure-html/unnamed-chunk-50-1.png" width="1248" /></p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS  10.14.2
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] h2o_3.20.0.8    bindrcpp_0.2.2  corrplot_0.84   ggthemes_4.0.1 
##  [5] yardstick_0.0.2 recipes_0.1.4   rsample_0.0.3   lime_0.4.1     
##  [9] keras_2.2.4     mice_3.3.0      caret_6.0-81    lattice_0.20-38
## [13] forcats_0.3.0   stringr_1.3.1   dplyr_0.7.8     purrr_0.2.5    
## [17] readr_1.2.1     tidyr_0.8.2     tibble_1.4.2    ggplot2_3.1.0  
## [21] tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##  [1] minqa_1.2.4        colorspace_1.3-2   class_7.3-14      
##  [4] base64enc_0.1-3    rstudioapi_0.8     prodlim_2018.04.18
##  [7] fansi_0.4.0        lubridate_1.7.4    xml2_1.2.0        
## [10] codetools_0.2-15   splines_3.5.1      knitr_1.21        
## [13] shinythemes_1.1.2  zeallot_0.1.0      jsonlite_1.6      
## [16] nloptr_1.2.1       pROC_1.13.0        broom_0.5.1       
## [19] tfruns_1.4         shiny_1.2.0        compiler_3.5.1    
## [22] httr_1.3.1         backports_1.1.2    assertthat_0.2.0  
## [25] Matrix_1.2-15      lazyeval_0.2.1     cli_1.0.1         
## [28] later_0.7.5        htmltools_0.3.6    tools_3.5.1       
## [31] gtable_0.2.0       glue_1.3.0         reshape2_1.4.3    
## [34] Rcpp_1.0.0         cellranger_1.1.0   nlme_3.1-137      
## [37] blogdown_0.9       iterators_1.0.10   timeDate_3043.102 
## [40] gower_0.1.2        xfun_0.4           lme4_1.1-19       
## [43] rvest_0.3.2        mime_0.6           pan_1.6           
## [46] MASS_7.3-51.1      scales_1.0.0       ipred_0.9-8       
## [49] hms_0.4.2          promises_1.0.1     parallel_3.5.1    
## [52] yaml_2.2.0         reticulate_1.10    rpart_4.1-13      
## [55] stringi_1.2.4      tensorflow_1.10    foreach_1.4.4     
## [58] lava_1.6.4         bitops_1.0-6       rlang_0.3.0.1     
## [61] pkgconfig_2.0.2    evaluate_0.12      bindr_0.1.1       
## [64] labeling_0.3       htmlwidgets_1.3    tidyselect_0.2.5  
## [67] plyr_1.8.4         magrittr_1.5       bookdown_0.8      
## [70] R6_2.3.0           generics_0.0.2     mitml_0.3-6       
## [73] pillar_1.3.0       haven_2.0.0        whisker_0.3-2     
## [76] withr_2.1.2        RCurl_1.95-4.11    survival_2.43-3   
## [79] nnet_7.3-12        modelr_0.1.2       crayon_1.3.4      
## [82] jomo_2.6-5         utf8_1.1.4         rmarkdown_1.11    
## [85] grid_3.5.1         readxl_1.1.0       data.table_1.11.8 
## [88] ModelMetrics_1.2.2 digest_0.6.18      xtable_1.8-3      
## [91] httpuv_1.4.5       stats4_3.5.1       munsell_0.5.0     
## [94] glmnet_2.0-16</code></pre>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Trust in ML models. Slides from TWiML &amp; AI EMEA Meetup &#43; iX Articles]]></title>
    <link href="/2018/12/trust_in_ml_slides_ix/"/>
    <id>/2018/12/trust_in_ml_slides_ix/</id>
    <published>2018-12-06T00:00:00+00:00</published>
    <updated>2018-12-06T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><strong>Update:</strong> There is now a recording of the meetup up on <a href="https://youtu.be/jTFACgdhpi4">YouTube</a>.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/jTFACgdhpi4?start=464" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>Here you find my slides the <a href="https://twimlai.com/meetup/">TWiML &amp; AI EMEA Meetup about <strong>Trust in ML models</strong></a>, where I presented the <a href="https://homes.cs.washington.edu/~marcotcr/aaai18.pdf">Anchors paper by Carlos Guestrin et al.</a>.</p>
<div style="position:relative;width:100%;height:0;padding-bottom:calc(56.25% + 40px);">
<iframe allowfullscreen style="position:absolute; width: 100%; height: 100%;border: solid 1px #333;" src="https://www.beautiful.ai/player/-LSiZRKH9RqNHKGaj6o1/TWiMLAI-EMEA-Meetup?utm_source=beautiful_player&amp;utm_medium=embed&amp;utm_campaign=-LSiZRKH9RqNHKGaj6o1">
</iframe>
</div>
<hr />
<p>I have also just written two articles for the German IT magazin <strong>iX</strong> about the same topic of <strong>Explaining Black-Box Machine Learning Models</strong>:</p>
<ul>
<li><p><a href="https://shop.heise.de/katalog/ix-12-2018">A short article in the <strong>iX 12/2018</strong></a></p></li>
<li><p><a href="https://shop.heise.de/katalog/blick-in-die-blackbox">The full article in <strong>iX Developer Machine Learning</strong></a></p></li>
</ul>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/ix_article_01.jpeg" />

</div>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/ix_article_2.jpeg" />

</div>
<hr />
<p>You can find the code for reproducing all plots in the blogposts here:</p>
<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/07/explaining_ml_models_code_caret_iml/">Explaining Black-Box Machine Learning Models - Code Part 1: tabular data + caret + iml</a></li>
<li><a href="https://shirinsplayground.netlify.com/2018/07/explaining_ml_models_code_text_lime/">Explaining Black-Box Machine Learning Models - Code Part 2: Text classification with LIME</a></li>
<li><a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits_lime/">Explaining Black-Box Machine Learning Models - Code Part 3: Keras image classification models with LIME</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[How do Convolutional Neural Nets (CNNs) learn? &#43; Keras example]]></title>
    <link href="/2018/11/how_cnns_learn/"/>
    <id>/2018/11/how_cnns_learn/</id>
    <published>2018-11-29T00:00:00+00:00</published>
    <updated>2018-11-29T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>As with the other videos from our <a href="https://bootcamp.codecentric.ai/">codecentric.ai Bootcamp</a> (<a href="https://shirinsplayground.netlify.com/2018/10/ml_basics_rf/">Random Forests</a>, <a href="https://shirinsplayground.netlify.com/2018/11/neural_nets_explained/">Neural Nets</a> &amp; <a href="https://shirinsplayground.netlify.com/2018/11/ml_basics_gbm/">Gradient Boosting</a>), I am again sharing an English version of the script (plus R code) for this most recent addition on <a href="https://youtu.be/MWPohcMtFLo">How Convolutional Neural Nets work</a>.</p>
<hr />
<p>In this lesson, I am going to explain <strong>how computers learn to see</strong>; meaning, how do they learn to recognize images or object on images? One of the most commonly used approaches to teach computers “vision” are <strong>Convolutional Neural Nets</strong>.</p>
<p>This lesson builds on top of two other lessons: <a href="https://youtu.be/JS4E04dJj0I">Computer Vision Basics</a> and <a href="https://shirinsplayground.netlify.com/2018/11/neural_nets_explained/">Neural Nets</a>. In the first video, Oli explains what computer vision is, how images are read by computers and how they can be analyzed with traditional approaches, like <strong>Histograms of Oriented Gradients</strong> and more. He also shows a very cool project, that he and colleagues worked on, where they programmed a small drone to recognize and avoid obstacles, like people. This video is only available in German, though. In the Neural Nets blog post, I show how Neural Nets work by explaining what <strong>Multi-Layer Perceptrons (MLPs)</strong> are and how they learn, using techniques like <strong>gradient descent</strong>, <strong>backpropagation</strong>, <strong>loss</strong> and <strong>activation functions</strong>.</p>
<p>Convolutional Neural Nets are usually abbreviated either <strong>CNNs</strong> or <strong>ConvNets</strong>. They are a specific type of neural network that has very particular differences compared to MLPs. Basically, you can think of CNNs as working similarly to the <strong>receptive fields of photoreceptors</strong> in the human eye. Receptive fields in our eyes are small connected areas on the retina where groups of many photoreceptors stimulate much fewer ganglion cells. Thus, each ganglion cell can be stimulated by a large number of receptors, so that a complex input is condensed into a <strong>compressed output</strong> before it is further processed in the brain.</p>
<p>Before we dive deeper into CNNs, I briefly want to recap how images can take on a numerical format. We need a numerical representation of our image because just like any other machine learning model or neural net, CNNs need data in form of numbers in order to learn! With images, these numbers are <strong>pixel values</strong>; when we have a greyscale image, these values represent a range of “greyness” from 0 (black) to 255 (white).</p>
<p>Here is an example image from the fruits datasets, which is used in the <a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits/">practical example for this lesson</a>. In general, data can be represented in different formats, e.g. as vectors, tables or matrices. I am using the <code>imager</code> package to read the image and have a look at the pixel values, which are represented as a <strong>matrix with the dimensions image width x image height</strong>.</p>
<pre class="r"><code>library(imager)
im &lt;- load.image(&quot;/Users/shiringlander/Documents/Github/codecentric.AI-bootcamp/data/fruits-360/Training/Strawberry/100_100.jpg&quot;)</code></pre>
<pre class="r"><code>plot(im)</code></pre>
<p><img src="/post/how_cnns_learn_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>But when we look at the <code>dim()</code> function with our image, we see that there are actually four dimensions and only the first two represent image width and image height. The third dimension is for the depth, which means in case of videos the time or order of the frames; with regular images, we don’t need this dimension. The third dimension shows the number of <strong>color channels</strong>; in this case, we have a color image, so there are three channels for red, green and blue. The values remain in the same between 0 and 255 but now they don’t represent greyscales but color intensity of the respective channel. This 3-dimensional format (a stack of three matrices) is also called a 3-dimensional <strong>array</strong>.</p>
<pre class="r"><code>dim(im)</code></pre>
<pre><code>## [1] 100 100   1   3</code></pre>
<p>Let’s see what happens if we convert our image to greyscale:</p>
<pre class="r"><code>im_grey &lt;- grayscale(im)
plot(im_grey)</code></pre>
<p><img src="/post/how_cnns_learn_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Our grey image has only one channel.</p>
<pre class="r"><code>dim(im_grey)</code></pre>
<pre><code>## [1] 100 100   1   1</code></pre>
<p>When we look at the actual matrix of pixel values (below, shown with a subset), we see that our values are not shown as raw values, but as scaled values between 0 and 1.</p>
<pre class="r"><code>head(as.array(im_grey)[25:75, 25:75, 1, 1])</code></pre>
<pre><code>##           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]
## [1,] 0.2015294 0.1923529 0.2043529 0.2354902 0.2021961 0.2389804 0.2431373
## [2,] 0.2597647 0.2009804 0.2522745 0.3812941 0.2243137 0.2439608 0.2054902
## [3,] 0.2872941 0.2397255 0.3251765 0.5479608 0.3723922 0.2525882 0.2714510
## [4,] 0.2212549 0.2596078 0.5109020 0.2871765 0.5529412 0.2162745 0.5660000
## [5,] 0.2725882 0.3765882 0.2081569 0.1924314 0.3110196 0.3767843 0.6663922
## [6,] 0.4154118 0.2168627 0.2979216 0.1883922 0.1836471 0.5210196 0.4032549
##           [,8]      [,9]     [,10]     [,11]     [,12]     [,13]     [,14]
## [1,] 0.2787059 0.2401961 0.2547451 0.2709020 0.2475686 0.2474118 0.2561961
## [2,] 0.2407451 0.2520392 0.3678039 0.3932941 0.3570588 0.3727843 0.3171765
## [3,] 0.5352157 0.4680392 0.2788627 0.2087451 0.2096471 0.2569412 0.2856863
## [4,] 0.2663137 0.1769020 0.2441961 0.2172549 0.2004314 0.2517255 0.2801961
## [5,] 0.2470980 0.1892549 0.2169020 0.2211765 0.2041569 0.1972549 0.1933725
## [6,] 0.2209412 0.1961961 0.2166275 0.2123137 0.2503922 0.3057255 0.3998431
##          [,15]     [,16]     [,17]     [,18]     [,19]     [,20]     [,21]
## [1,] 0.2330980 0.2163529 0.2244706 0.2161961 0.1913725 0.2833725 0.1994902
## [2,] 0.2316863 0.2426275 0.2131765 0.2018431 0.2054902 0.2452157 0.2080392
## [3,] 0.2290196 0.2086667 0.2161176 0.2283922 0.2447059 0.2281176 0.2908627
## [4,] 0.2605882 0.2009412 0.2431765 0.4591765 0.6387843 0.3078824 0.2486275
## [5,] 0.1975686 0.2092549 0.2742745 0.4005882 0.3773333 0.2245490 0.2474902
## [6,] 0.3936471 0.1815294 0.1930980 0.2084706 0.5097647 0.3130196 0.2153333
##          [,22]     [,23]     [,24]     [,25]     [,26]     [,27]     [,28]
## [1,] 0.2122353 0.2283529 0.4250980 0.4372157 0.2789020 0.2011373 0.2278431
## [2,] 0.1925098 0.2745098 0.3172157 0.4366667 0.3427451 0.2161176 0.2557647
## [3,] 0.2150588 0.2788627 0.2544314 0.3665882 0.3292157 0.2121176 0.2092157
## [4,] 0.2119216 0.2029020 0.2005098 0.2485882 0.2550588 0.2402745 0.2172549
## [5,] 0.2466275 0.1983137 0.2108627 0.2305098 0.3066667 0.3615686 0.3726275
## [6,] 0.2040000 0.2472549 0.2114510 0.1891765 0.2429020 0.2867451 0.2863529
##          [,29]     [,30]     [,31]     [,32]     [,33]     [,34]     [,35]
## [1,] 0.2782353 0.3150980 0.3993725 0.3683922 0.3249804 0.3210588 0.3150588
## [2,] 0.2593333 0.2162353 0.2950588 0.4864706 0.4195294 0.4238039 0.3776863
## [3,] 0.2314510 0.2311765 0.2737255 0.3915686 0.3851765 0.4050588 0.4233725
## [4,] 0.2583922 0.2953333 0.3530196 0.3609412 0.4549020 0.4880000 0.4905882
## [5,] 0.4509804 0.5030980 0.4882745 0.4000784 0.4856863 0.6270196 0.5930196
## [6,] 0.2034118 0.1965882 0.2072157 0.2238824 0.2080392 0.2009804 0.5564314
##          [,36]     [,37]     [,38]     [,39]     [,40]     [,41]     [,42]
## [1,] 0.2461961 0.2352549 0.2726275 0.2752549 0.2603529 0.3112549 0.3981176
## [2,] 0.2441961 0.2152157 0.2407059 0.2647451 0.2650196 0.2767451 0.3592549
## [3,] 0.2318039 0.2348235 0.2612157 0.2647059 0.2647059 0.2958431 0.3112549
## [4,] 0.2630588 0.1901176 0.2414510 0.2483529 0.2601961 0.2713725 0.3139216
## [5,] 0.3403529 0.2250588 0.2315294 0.1954510 0.2704314 0.3076078 0.3111765
## [6,] 0.4018039 0.2904706 0.3806275 0.4549020 0.3765098 0.4278824 0.4952941
##          [,43]     [,44]     [,45]     [,46]     [,47]     [,48]     [,49]
## [1,] 0.3724706 0.3154902 0.3728627 0.3653333 0.3758824 0.4943922 0.4682353
## [2,] 0.3664706 0.3616863 0.3263922 0.2882745 0.2752157 0.2451373 0.3379608
## [3,] 0.3309804 0.2837647 0.2366275 0.2718039 0.2713725 0.2832549 0.2749020
## [4,] 0.3819216 0.3143137 0.2364706 0.2324314 0.2685098 0.2722745 0.2324706
## [5,] 0.2989804 0.2561176 0.2748627 0.3621961 0.5355686 0.4248235 0.6004314
## [6,] 0.4528627 0.3580392 0.2934118 0.4385098 0.2146275 0.2045882 0.2243922
##          [,50]     [,51]
## [1,] 0.3378039 0.2782353
## [2,] 0.2750980 0.3264314
## [3,] 0.2761961 0.3800000
## [4,] 0.3410980 0.5016863
## [5,] 0.6163922 0.6553333
## [6,] 0.2436471 0.2944706</code></pre>
<p>The same applies to the color image, which if multiplied with 255 shows raw pixel values:</p>
<pre class="r"><code>head(as.array(im)[25:75, 25:75, 1, 1]  * 255)</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
## [1,]  138  142  150  161  151  155  153  161  158   156   155   144   143
## [2,]  159  139  147  183  152  159  135  127  144   174   177   164   162
## [3,]  170  140  143  200  172  150  139  184  185   148   139   133   134
## [4,]  142  138  189  130  204  119  200  114  114   148   152   141   140
## [5,]  138  172  139  133  145  146  220  122  132   149   153   140   127
## [6,]  170  141  184  155  127  190  162  129  147   150   144   148   155
##      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24]
## [1,]   148   149   150   152   143   130   156   145   154   151   191
## [2,]   148   136   156   158   148   135   141   139   143   161   165
## [3,]   140   139   148   153   146   138   132   155   143   161   153
## [4,]   147   157   152   155   190   226   147   146   145   141   137
## [5,]   128   143   157   164   172   151   120   147   161   143   134
## [6,]   179   187   144   144   132   190   143   136   147   156   135
##      [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35]
## [1,]   190   157   141   139   144   152   176   167   150   149   159
## [2,]   177   170   153   162   152   136   155   197   167   164   162
## [3,]   162   165   148   147   146   140   144   163   147   151   170
## [4,]   143   149   147   138   143   148   157   149   165   172   185
## [5,]   150   165   172   169   185   197   193   169   187   216   206
## [6,]   143   149   152   147   125   123   127   137   133   120   198
##      [,36] [,37] [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46]
## [1,]   154   155   159   153   145   155   170   159   149   171   170
## [2,]   148   150   152   147   139   149   163   162   168   166   153
## [3,]   142   155   159   153   146   165   162   165   159   150   153
## [4,]   144   138   154   153   153   156   166   184   170   150   143
## [5,]   151   137   150   138   152   147   152   156   151   156   173
## [6,]   161   150   185   199   166   171   189   184   168   159   196
##      [,47] [,48] [,49] [,50] [,51]
## [1,]   170   200   200   174   165
## [2,]   143   135   167   160   176
## [3,]   146   148   150   150   174
## [4,]   148   149   139   159   190
## [5,]   215   191   237   233   229
## [6,]   136   133   141   143   148</code></pre>
<p>These pixel arrays of our images are now the input to our CNN, which can now learn to recognize e.g. which fruit is on each image (a classification task). This is accomplished by learning different levels of abstraction of the images. In the first few hidden layers, the CNN ususally detects general patterns, like edges; the deeper we go into the CNN, these learned abstractions become more specific, like textures, patterns and (parts of) objects.</p>
<p>We could also train MLPs on our images but usually, they are not very good at this sort of task. So, what’s the magic behind CNNs, that makes them so much more powerful at detecting images and object?</p>
<div id="the-most-important-difference-is-that-mlps-consider-each-pixel-position-as-an-independent-feature-it-does-not-know-neighboring-pixels-thats-why-mlps-will-not-be-able-to-detect-images-where-the-objects-have-a-different-orientation-position-etc.-moreover-because-we-often-deal-with-large-images-the-sheer-number-of-trainable-parameters-in-an-mlp-will-quickly-escalate-so-that-training-such-a-network-isnt-exactly-efficient.-in-mlps-weights-are-learned-e.g.with-gradient-descent-and-backpropagation." class="section level2">
<h2>The most important difference is that MLPs consider each pixel position as an independent feature; it does not know neighboring pixels! That’s why MLPs will not be able to detect images where the objects have a different orientation, position, etc. Moreover, because we often deal with large images, the sheer number of trainable parameters in an MLP will quickly escalate, so that training such a network isn’t exactly efficient. In MLPs, <strong>weights</strong> are learned, e.g. with gradient descent and backpropagation.</h2>
<p>Bei CNNs werden dagegen Gruppen aus benachbarten Pixeln betrachtet. Das bezeichnen wir als “Local connectivity”, denn in unserem Neuronalen Netz sind Neuronen später zunächst nur mit den lokal entsprechenden Neuronen in den Folgeschichten verbinden. Dadurch kann das Netz effizient und relativ schnell den Kontext von Mustern und Objekten lernen und diese auch in anderen Postionen auf dem Bild erkennen. Konkret funktionert das mit sogenannten Sliding Windows, also Fenstern, die eine Gruppe von Pixeln betrachten und dabei von links oben nach rechts unten das Bild abscannen. Gelernt werden dabei dann auch keine Gewichte, sondern Filter. Was diese Filter sind, erkläre ich im nächsten Schritt. Übrigens können wir CNNs nicht nur für Bilder, sondern auch sehr gut für andere Aufgaben, wie Textklassifikation verwenden.</p>
<hr />
<p>Auf jedem Fenster des Sliding Windows,</p>
<p>das zu Beispiel eine Größe</p>
<p>von 3 x 3 Pixeln haben kann,</p>
<p>wird nämlich nun eine bestimmt mathematische Operation durchgeführt, die sogenannte Faltung - oder auf Englisch: convolution. Daher also auch der Name… ;-)</p>
<p>Diese Faltung passiert für jedes Fenster des gesamten Bildes.</p>
<p>…</p>
<p>In diesem Video, das ihr vielleicht schon aus den Computer Vision Basics kennt, ist das einmal animiert gezeigt.</p>
<p>Aber was ist nun diese Faltung? Die Faltung passiert über das Multiplizieren der Pixelwerte in unserem Fenster mit einem sogenannten Filter. Dieser Filter ist nichts anderes als eine Matrix mit den selben Dimensionen des Sliding Windows, hier 3 x 3. Je nachdem, welche Werte in einem Filter stehen, führt die Faltung zu einer bestimmten Transformation des Originalbildes. Hier ist der Filter für das Schärfen von Bildern zu sehen.</p>
<p>Das Ganze läuft nun so ab, wie in dieser großartigen Abbildung von setosa.io zu sehen ist. Das linke Bild ist das Original, das rechte zeigt das transformierte Bild nach Anwendung des Filters.</p>
<p>Die Pixel des Fensters, das gerade betrachtete wird sind in den Kästchen in der Mitte zu sehen. Jeder Pixelwert in dem Fenster wird nun mit dem Wert an der entprechenden Stelle im Filter multipliziert und mit allen anderen so multiplizierten Werten aufaddiert. Das Ergebnis ist hier -97 und ist der neue Wert für das transformierte Bild.</p>
<hr />
<p>Zwei wichtige Hyperparameter für CNNs sind Padding und die Schrittgröße.</p>
<p>Padding bezeichnet das Hinzufügen von extra Pixelschichten am Rand des Bildes, damit jeder Pixel des eigentlichen Bildes beim scannen des Sliding Windows gleich häufig gefaltet wird. Würden wir kein Padding anwenden, würden die Randpixel weniger häufig betrachtet, als die Pixel in der Mitte des Bildes - und unser Bild würde nach der Transformation kleiner werden. Es gibt mehrere Arten von Padding. Bei “same” werden die Randpixel dupliziert</p>
<p>und an an den Rand</p>
<p>hinzugefügt. Alternativ könnten wir die Ränder auch mit Nullern auffüllen, dem sogenannten Zero-Padding.</p>
<p>Jetzt kann das Sliding Window oben link starten.</p>
<p>Die Schrittgröße bezeichnet nun, wie weit das Sliding Window bei jedem Schritt vorrückt. Meisten wird eine Schrittgröße von 1 verwendet, d.h. dass das Sliding Window nach jeder Faltung jeweils einen Pixel weiterwandert. Würden wir die Schrittgröße vergrößern, würde die Berechnung des neuronalen Netzes schneller gehen, wir würden aber auch nicht so detaillierte Muster erkennen können. Und unser Bild würde sich im Output verkleinern.</p>
<hr />
<p>Kommen wir aber noch mal zurück zu den Filtern. Manchmal werden Filter, bzw. Fenster auch als Kernel oder Filter Kernel bezeichnet. Tatsächlich sind Filter Sammlungen von Kernels, d.h. wenn wir mit Farbbildern und 3 Kanälen - also 3 Dimensionen - arbeiten, haben wir einen Kernel pro Kanal, aus dem sich ein Filter zusammensetzt. Pro Filter bekommen wir dann EINEN Ergebniswert aus dem Skalarprodukt zurück! Beispiele für andere Filter können zum Beispiel das Originalbild verwaschen,</p>
<p>untere</p>
<p>oder obere horizontale Kanten erkennen.</p>
<p>Im Prinzip können aber beliebige Werte in die Filter eingesetzt werden,</p>
<p>so dass verschiedenste Muster in dem Bild hervortreten. Jedes Muster stellt eine sogenannte Feature Map oder Activation Map dar. Die Werte, die an den einzelnen Stellen des Filters stehen, sollen jetzt von unserem neuronalen Netz gelernt werden. Das CNN lernt also, welche Transformation es wann durchführen muss, um die richtigen Muster und Objekte in den Bildern zu erkennen. Dafür lernt das CNN nicht nur einen Filter, sondern sehr sehr viele. Es lernt sogar in jeder versteckten SCHICHT mehrere Filter parallel.</p>
<p>Diese unabhängig voneinander parallel gelernten Filter, bzw. die transformierten Output-Bilder nach der Faltung, produzieren die sogenannten</p>
<p>Stacks of Feature Maps oder Activation Maps. Die Anzahl der zu trainierenden Parameter ist bei Faltungen deutlich geringer als bei voll verknüpften Neuronen in MLPs. In den Beispielen, die ich hier gezeigt habe, haben wir Graustufenbilder und 2-dimensionale Filter gesehen. Bei Farbbildern mit 3 Farbkanälen hätten wir entsprechend 3-dimensionale Filter, die das Ergbenis ebenfalls mit dem Skalarprodukt aus Filter und Bildausschnitt berechnen.</p>
<hr />
<p>Wie CNNs LERNEN Bilder mit Hilfe von Filtern und Faltungen zu erkennen, haben wir jetzt gesehen. CNN-Architekturen bestehen aber nicht nur aus Schichten, in denen Faltung passiert, den Convolutional Layern, sondern sie haben zusätzlich sogenannte Pooling Schichten. In den Pooling Schichten werden die Bilder (also die transformierten Outputs aus vorhergehenden Faltungsschichten) verkleinert.</p>
<p>Dafür wird auch wieder ein Sliding Window verwendet, dieses muss nicht die selbe Größe haben, wie das Sliding Window aus den Faltungsschichten aber hier zeige ich das Prinzip beispielhaft ebenfalls an einem 3 x 3 Fenster. Wichtig ist dabei, dass das Sliding Window beim Pooling meist nicht überlappt, sondern jeder Pixel genau 1x betrachtet wird, damit das Output-Bild entsprechend kleiner wird.</p>
<p>Es gibt mehrere Arten, wie Bilder mit Pooling verkleinert werden können. Die häufigsten sind</p>
<p>Max Pooling, wo nur der größte Wert jedes Fensters behalten wird,</p>
<p>Average Pooling, wo der Durschschnittswert aus jedem Fenster gebildet wird</p>
<p>und Sum Pooling, wo die Summe aller Werte gebildet wird. Pooling arbeitet dabei unabhängig auf jeder einzelnen Feature Map und sorgt nicht nur für eine Reduktion des Bildes und damit an zu optimierenden Parametern, sondern hilft auch dabei, allgemeingültige Feature zu extrahieren, die robust gegenüber kleineren Änderungen des Inputs sind.</p>
<hr />
<p>Okay, setzen wir nun also die Faltungsschichten und Pooling-Schichten zu einer typischen CNN-Architektur zusammen.</p>
<p>Wir starten natürlich wie immer mit dem Input, in unserem Fall Bilder.</p>
<p>Nun kommt in der Regel 1 oder 2 Faltungsschichten (plus Aktivierungsfunktion nach jeder Faltung, z.B. Rectified Linear Units),</p>
<p>gefolgt von einer Pooling-Schicht.</p>
<p>Von diesen Blöcken aus Convolution und Pooling können nun mehrere hintereinander gefügt werden. In der nächsten Lektion werden wir uns konkrete Beispiele mit unterschiedlich vielen Schichten und Weiterentwicklungen, wie AlexNet, VGG, Inception und ResNets, angucken. In dieser Lektion werden wir uns außerdem angucken, wie wir Activation Maps in unserem Modell visualisieren können.</p>
<p>Bevor wir das Endergbnis, zum Beispiel eine Klasse bestimmen lassen, folgen eine oder ein paar wenige Dense Layer, als würden wir ein kleines MLP hinten dran hängen. In den Dense Layern werden deshab wieder Gewichte gelernt. Im Wesentlichen haben die Faltungsschichten in unserem Netz relevante Feature gefunden, und die Pooling-Schichten habe diese Information soweit verdichtet, dass es nun sinnvoll ist ein MLP zu trainieren, dass in der Lage ist die finale Klassifikation zu lernen.</p>
<p>Und zuletzt haben wir eine Output-Schicht, ebenfalls wie bei einem einfachen MLP.</p>
<hr />
<p>Sicher wollt ihr jetzt wissen, wie ihr selber ein CNN bauen und verwenden könnt. Dafür haben wir euch wieder ein Notebook mit einem Beispiel erstellt. In dieser Lektion verwenden wir Keras und TensorFlow um ein CNN zu trainieren, das Bilder von verschiedenen Früchten erkennen kann.</p>
<p>Hier nur ein kleiner Ausschnitt aus dem Code. Vielleicht erkennt ihr schon Teile aus diesem Video, wie Convolution, padding und pooling?</p>
<p>Die vollständige Erklärung mit Code zum Mitmachen findet ihr in dem Notebook zu diesem Video!</p>
<p>You can find the R version of the Python code, which we provide for this course in <a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits/">this blog article</a>.</p>
<hr />
<p><a href="https://youtu.be/MWPohcMtFLo">Video</a>:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/MWPohcMtFLo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p><a href="https://codecentric.slides.com/shiringlander/intro_cnns">Slides</a>:</p>
<iframe src="//codecentric.slides.com/shiringlander/intro_cnns/embed" width="576" height="420" scrolling="no" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen>
</iframe>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS  10.14.2
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] imager_0.41.1 magrittr_1.5 
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_1.0.0       bookdown_0.8     png_0.1-7        digest_0.6.18   
##  [5] tiff_0.1-5       plyr_1.8.4       evaluate_0.12    blogdown_0.9    
##  [9] rlang_0.3.0.1    stringi_1.2.4    bmp_0.3          rmarkdown_1.11  
## [13] tools_3.5.1      stringr_1.3.1    purrr_0.2.5      igraph_1.2.2    
## [17] jpeg_0.1-8       xfun_0.4         yaml_2.2.0       compiler_3.5.1  
## [21] pkgconfig_2.0.2  htmltools_0.3.6  readbitmap_0.1.5 knitr_1.21</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Machine Learning Basics - Gradient Boosting &amp; XGBoost]]></title>
    <link href="/2018/11/ml_basics_gbm/"/>
    <id>/2018/11/ml_basics_gbm/</id>
    <published>2018-11-29T00:00:00+00:00</published>
    <updated>2018-11-29T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>In a recent video, I covered <a href="https://shirinsplayground.netlify.com/2018/10/ml_basics_rf/">Random Forests</a> and <a href="https://shirinsplayground.netlify.com/2018/11/neural_nets_explained/">Neural Nets</a> as part of the <a href="https://bootcamp.codecentric.ai/">codecentric.ai Bootcamp</a>.</p>
<p>In the most recent video, I covered <strong>Gradient Boosting and XGBoost.</strong></p>
<p>You can find the <a href="https://youtu.be/xXZeVKP74ao">video on YouTube</a> and the <a href="https://codecentric.slides.com/shiringlander/ml_basics_gbm">slides on slides.com</a>. Both are again in German with code examples in Python.</p>
<p>But below, you find the English version of the content, plus code examples in R for <code>caret</code>, <code>xgboost</code> and <code>h2o</code>. :-)</p>
<hr />
<div class="figure">
<img src="https://shiring.github.io/netlify_images/gbm_yt_video.png" />

</div>
<p>Like Random Forest, Gradient Boosting is another technique for performing supervised machine learning tasks, like classification and regression. The implementations of this technique can have different names, most commonly you encounter Gradient Boosting machines (abbreviated GBM) and XGBoost. XGBoost is particularly popular because it has been the winning algorithm in a number of recent <a href="kaggle.com">Kaggle</a> competitions.</p>
<p>Similar to Random Forests, Gradient Boosting is an <strong>ensemble learner</strong>. This means it will create a final model based on a collection of individual models. The predictive power of these individual models is weak and prone to overfitting but combining many such weak models in an ensemble will lead to an overall much improved result. In Gradient Boosting machines, the most common type of weak model used is decision trees - another parallel to Random Forests.</p>
<div id="how-gradient-boosting-works" class="section level2">
<h2>How Gradient Boosting works</h2>
<p>Let’s look at how Gradient Boosting works. Most of the magic is described in the name: “Gradient” plus “Boosting”.</p>
<p><strong>Boosting</strong> builds models from individual so called “weak learners” in an iterative way. In the <a href="https://shirinsplayground.netlify.com/2018/10/ml_basics_rf/">Random Forests</a> part, I had already discussed the differences between <strong>Bagging</strong> and <strong>Boosting</strong> as tree ensemble methods. In boosting, the individual models are not built on completely random subsets of data and features but sequentially by putting more weight on instances with wrong predictions and high errors. The general idea behind this is that instances, which are hard to predict correctly (“difficult” cases) will be focused on during learning, so that the model learns from past mistakes. When we train each ensemble on a subset of the training set, we also call this <strong>Stochastic Gradient Boosting</strong>, which can help improve generalizability of our model.</p>
<p>The <strong>gradient</strong> is used to minimize a <strong>loss function</strong>, similar to how <a href="https://shirinsplayground.netlify.com/2018/11/neural_nets_explained/">Neural Nets</a> utilize gradient descent to optimize (“learn”) weights. In each round of training, the weak learner is built and its predictions are compared to the correct outcome that we expect. The distance between prediction and truth represents the error rate of our model. These errors can now be used to calculate the gradient. The gradient is nothing fancy, it is basically the partial derivative of our loss function - so it describes the steepness of our error function. The gradient can be used to find the direction in which to change the model parameters in order to (maximally) reduce the error in the next round of training by “descending the gradient”.</p>
<p>In Neural nets, gradient descent is used to look for the minimum of the loss function, i.e. learning the model parameters (e.g. weights) for which the prediction error is lowest in <strong>a single model</strong>. In Gradient Boosting we are combining the predictions of <strong>multiple models</strong>, so we are not optimizing the model parameters directly but the boosted model predictions. Therefore, the gradients will be added to the running training process by fitting the next tree also to these values.</p>
<p>Because we apply gradient descent, we will find <strong>learning rate</strong> (the “step size” with which we descend the gradient), <strong>shrinkage</strong> (reduction of the learning rate) and <strong>loss function</strong> as hyperparameters in Gradient Boosting models - just as with Neural Nets. Other <a href="https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters">hyperparameters</a> of Gradient Boosting are similar to those of Random Forests:</p>
<ul>
<li>the number of iterations (i.e. the number of trees to ensemble),</li>
<li>the number of observations in each leaf,</li>
<li>tree complexity and depth,</li>
<li>the proportion of samples and</li>
<li>the proportion of features on which to train on.</li>
</ul>
</div>
<div id="gradient-boosting-machines-vs.xgboost" class="section level2">
<h2>Gradient Boosting Machines vs. XGBoost</h2>
<p><a href="https://github.com/dmlc/xgboost">XGBoost</a> stands for Extreme Gradient Boosting; it is a specific implementation of the Gradient Boosting method which uses more accurate approximations to find the best tree model. It employs a number of nifty tricks that make it exceptionally successful, particularly with structured data. The most important are</p>
<p>1.) computing <strong>second-order gradients, i.e. second partial derivatives</strong> of the loss function (similar to <strong>Newton’s method</strong>), which provides more information about the direction of gradients and how to get to the minimum of our loss function. While regular gradient boosting uses the loss function of our base model (e.g. decision tree) as a proxy for minimizing the error of the overall model, XGBoost uses the 2nd order derivative as an approximation.</p>
<p>2.) And advanced <strong>regularization</strong> (L1 &amp; L2), which improves model generalization.</p>
<p>XGBoost has additional advantages: training is very fast and can be parallelized / distributed across clusters.</p>
<hr />
</div>
<div id="code-in-r" class="section level1">
<h1>Code in R</h1>
<p>Here is a very quick run through how to train Gradient Boosting and XGBoost models in R with <code>caret</code>, <code>xgboost</code> and <code>h2o</code>.</p>
<div id="data" class="section level2">
<h2>Data</h2>
<p>First, data: I’ll be using the <code>ISLR</code> package, which contains a number of datasets, one of them is <code>College</code>.</p>
<blockquote>
<p>Statistics for a large number of US Colleges from the 1995 issue of US News and World Report.</p>
</blockquote>
<pre class="r"><code>library(tidyverse)
library(ISLR)

ml_data &lt;- College
ml_data %&gt;%
  glimpse()</code></pre>
<pre><code>## Observations: 777
## Variables: 18
## $ Private     &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, ...
## $ Apps        &lt;dbl&gt; 1660, 2186, 1428, 417, 193, 587, 353, 1899, 1038, ...
## $ Accept      &lt;dbl&gt; 1232, 1924, 1097, 349, 146, 479, 340, 1720, 839, 4...
## $ Enroll      &lt;dbl&gt; 721, 512, 336, 137, 55, 158, 103, 489, 227, 172, 4...
## $ Top10perc   &lt;dbl&gt; 23, 16, 22, 60, 16, 38, 17, 37, 30, 21, 37, 44, 38...
## $ Top25perc   &lt;dbl&gt; 52, 29, 50, 89, 44, 62, 45, 68, 63, 44, 75, 77, 64...
## $ F.Undergrad &lt;dbl&gt; 2885, 2683, 1036, 510, 249, 678, 416, 1594, 973, 7...
## $ P.Undergrad &lt;dbl&gt; 537, 1227, 99, 63, 869, 41, 230, 32, 306, 78, 110,...
## $ Outstate    &lt;dbl&gt; 7440, 12280, 11250, 12960, 7560, 13500, 13290, 138...
## $ Room.Board  &lt;dbl&gt; 3300, 6450, 3750, 5450, 4120, 3335, 5720, 4826, 44...
## $ Books       &lt;dbl&gt; 450, 750, 400, 450, 800, 500, 500, 450, 300, 660, ...
## $ Personal    &lt;dbl&gt; 2200, 1500, 1165, 875, 1500, 675, 1500, 850, 500, ...
## $ PhD         &lt;dbl&gt; 70, 29, 53, 92, 76, 67, 90, 89, 79, 40, 82, 73, 60...
## $ Terminal    &lt;dbl&gt; 78, 30, 66, 97, 72, 73, 93, 100, 84, 41, 88, 91, 8...
## $ S.F.Ratio   &lt;dbl&gt; 18.1, 12.2, 12.9, 7.7, 11.9, 9.4, 11.5, 13.7, 11.3...
## $ perc.alumni &lt;dbl&gt; 12, 16, 30, 37, 2, 11, 26, 37, 23, 15, 31, 41, 21,...
## $ Expend      &lt;dbl&gt; 7041, 10527, 8735, 19016, 10922, 9727, 8861, 11487...
## $ Grad.Rate   &lt;dbl&gt; 60, 56, 54, 59, 15, 55, 63, 73, 80, 52, 73, 76, 74...</code></pre>
</div>
<div id="gradient-boosting-in-caret" class="section level2">
<h2>Gradient Boosting in caret</h2>
<p>The most flexible R package for machine learning is <code>caret</code>. If you go to the <a href="https://topepo.github.io/caret/available-models.html">Available Models section in the online documentation</a> and search for “Gradient Boosting”, this is what you’ll find:</p>
<table style="width:72%;">
<colgroup>
<col width="8%" />
<col width="18%" />
<col width="6%" />
<col width="13%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>method Value</th>
<th>Type</th>
<th>Libraries</th>
<th>Tuning Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>eXtreme Gradient Boosting</td>
<td>xgbDART</td>
<td>Classification, Regression</td>
<td>xgboost, plyr</td>
<td>nrounds, max_depth, eta, gamma, subsample, colsample_bytree, rate_drop, skip_drop, min_child_weight</td>
</tr>
<tr class="even">
<td>eXtreme Gradient Boosting</td>
<td>xgbLinear</td>
<td>Classification, Regression</td>
<td>xgboost</td>
<td>nrounds, lambda, alpha, eta</td>
</tr>
<tr class="odd">
<td>eXtreme Gradient Boosting</td>
<td>xgbTree</td>
<td>Classification, Regression</td>
<td>xgboost, plyr</td>
<td>nrounds, max_depth, eta, gamma, colsample_bytree, min_child_weight, subsample</td>
</tr>
<tr class="even">
<td>Gradient Boosting Machines</td>
<td>gbm_h2o</td>
<td>Classification, Regression</td>
<td>h2o</td>
<td>ntrees, max_depth, min_rows, learn_rate, col_sample_rate</td>
</tr>
<tr class="odd">
<td>Stochastic Gradient Boosting</td>
<td>gbm</td>
<td>Classification, Regression</td>
<td>gbm, plyr</td>
<td>n.trees, interaction.depth, shrinkage, n.minobsinnode</td>
</tr>
</tbody>
</table>
<p>A table with the different Gradient Boosting implementations, you can use with <code>caret</code>. Here I’ll show a very simple Stochastic Gradient Boosting example:</p>
<pre class="r"><code>library(caret)

# Partition into training and test data
set.seed(42)
index &lt;- createDataPartition(ml_data$Private, p = 0.7, list = FALSE)
train_data &lt;- ml_data[index, ]
test_data  &lt;- ml_data[-index, ]

# Train model with preprocessing &amp; repeated cv
model_gbm &lt;- caret::train(Private ~ .,
                          data = train_data,
                          method = &quot;gbm&quot;,
                          trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 5, 
                                                  repeats = 3, 
                                                  verboseIter = FALSE),
                          verbose = 0)
model_gbm</code></pre>
<pre><code>## Stochastic Gradient Boosting 
## 
## 545 samples
##  17 predictor
##   2 classes: &#39;No&#39;, &#39;Yes&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold, repeated 3 times) 
## Summary of sample sizes: 437, 436, 435, 436, 436, 436, ... 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  Accuracy   Kappa    
##   1                   50      0.9217830  0.7940197
##   1                  100      0.9327980  0.8264864
##   1                  150      0.9370795  0.8389860
##   2                   50      0.9334095  0.8275826
##   2                  100      0.9364341  0.8373727
##   2                  150      0.9333872  0.8298388
##   3                   50      0.9370627  0.8373028
##   3                  100      0.9376629  0.8398466
##   3                  150      0.9370401  0.8395797
## 
## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1
## 
## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were n.trees = 100,
##  interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10.</code></pre>
<p>With <code>predict()</code>, we can use this model to make predictions on test data. Here, I’ll be feeding this directly to the <code>confusionMatrix</code> function:</p>
<pre class="r"><code>caret::confusionMatrix(
  data = predict(model_gbm, test_data),
  reference = test_data$Private
  )</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  No Yes
##        No   57   9
##        Yes   6 160
##                                           
##                Accuracy : 0.9353          
##                  95% CI : (0.8956, 0.9634)
##     No Information Rate : 0.7284          
##     P-Value [Acc &gt; NIR] : 7.952e-16       
##                                           
##                   Kappa : 0.839           
##  Mcnemar&#39;s Test P-Value : 0.6056          
##                                           
##             Sensitivity : 0.9048          
##             Specificity : 0.9467          
##          Pos Pred Value : 0.8636          
##          Neg Pred Value : 0.9639          
##              Prevalence : 0.2716          
##          Detection Rate : 0.2457          
##    Detection Prevalence : 0.2845          
##       Balanced Accuracy : 0.9258          
##                                           
##        &#39;Positive&#39; Class : No              
## </code></pre>
</div>
<div id="the-xgboost-library" class="section level2">
<h2>The xgboost library</h2>
<p>We can also directly work with the <a href="https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html">xgboost</a> package in R. It’s a bit more involved but also includes advanced possibilities.</p>
<p>The easiest way to work with <code>xgboost</code> is with the <code>xgboost()</code> function. The four most important arguments to give are</p>
<ul>
<li><code>data</code>: a <strong>matrix</strong> of the training data</li>
<li><code>label</code>: the response variable in numeric format (for binary classification 0 &amp; 1)</li>
<li><code>objective</code>: defines what learning task should be trained, here binary classification</li>
<li><code>nrounds</code>: number of boosting iterations</li>
</ul>
<pre class="r"><code>library(xgboost)

xgboost_model &lt;- xgboost(data = as.matrix(train_data[, -1]), 
                         label = as.numeric(train_data$Private)-1,
                         max_depth = 3, 
                         objective = &quot;binary:logistic&quot;, 
                         nrounds = 10, 
                         verbose = FALSE,
                         prediction = TRUE)
xgboost_model</code></pre>
<pre><code>## ##### xgb.Booster
## raw: 6.7 Kb 
## call:
##   xgb.train(params = params, data = dtrain, nrounds = nrounds, 
##     watchlist = watchlist, verbose = verbose, print_every_n = print_every_n, 
##     early_stopping_rounds = early_stopping_rounds, maximize = maximize, 
##     save_period = save_period, save_name = save_name, xgb_model = xgb_model, 
##     callbacks = callbacks, max_depth = 3, objective = &quot;binary:logistic&quot;, 
##     prediction = TRUE)
## params (as set within xgb.train):
##   max_depth = &quot;3&quot;, objective = &quot;binary:logistic&quot;, prediction = &quot;TRUE&quot;, silent = &quot;1&quot;
## xgb.attributes:
##   niter
## callbacks:
##   cb.evaluation.log()
## # of features: 17 
## niter: 10
## nfeatures : 17 
## evaluation_log:
##     iter train_error
##        1    0.064220
##        2    0.051376
## ---                 
##        9    0.036697
##       10    0.033028</code></pre>
<p>We can again use <code>predict()</code>; because here, we will get prediction probabilities, we need to convert them into labels to compare them with the true class:</p>
<pre class="r"><code>predict(xgboost_model, 
        as.matrix(test_data[, -1])) %&gt;%
  as.tibble() %&gt;%
  mutate(prediction = round(value),
         label = as.numeric(test_data$Private)-1) %&gt;%
  count(prediction, label)</code></pre>
<pre><code>## # A tibble: 4 x 3
##   prediction label     n
##        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;
## 1          0     0    56
## 2          0     1     6
## 3          1     0     7
## 4          1     1   163</code></pre>
<p>Alternatively, we can use <code>xgb.train()</code>, which is more flexible and allows for more advanced settings compared to <code>xgboost()</code>. Here, we first need to create a so called DMatrix from the data. Optionally, we can define a watchlist for evaluating model performance during the training run. I am also creating a parameter set as a list object, which I am feeding to the <code>params</code> argument.</p>
<pre class="r"><code>dtrain &lt;- xgb.DMatrix(as.matrix(train_data[, -1]), 
                      label = as.numeric(train_data$Private)-1)
dtest &lt;- xgb.DMatrix(as.matrix(test_data[, -1]), 
                      label = as.numeric(test_data$Private)-1)

params &lt;- list(max_depth = 3, 
               objective = &quot;binary:logistic&quot;,
               silent = 0)

watchlist &lt;- list(train = dtrain, eval = dtest)

bst_model &lt;- xgb.train(params = params, 
                       data = dtrain, 
                       nrounds = 10, 
                       watchlist = watchlist,
                       verbose = FALSE,
                       prediction = TRUE)
bst_model</code></pre>
<pre><code>## ##### xgb.Booster
## raw: 6.7 Kb 
## call:
##   xgb.train(params = params, data = dtrain, nrounds = 10, watchlist = watchlist, 
##     verbose = FALSE, prediction = TRUE)
## params (as set within xgb.train):
##   max_depth = &quot;3&quot;, objective = &quot;binary:logistic&quot;, silent = &quot;0&quot;, prediction = &quot;TRUE&quot;, silent = &quot;1&quot;
## xgb.attributes:
##   niter
## callbacks:
##   cb.evaluation.log()
## # of features: 17 
## niter: 10
## nfeatures : 17 
## evaluation_log:
##     iter train_error eval_error
##        1    0.064220   0.099138
##        2    0.051376   0.077586
## ---                            
##        9    0.036697   0.060345
##       10    0.033028   0.056034</code></pre>
<p>The model can be used just as before:</p>
<pre class="r"><code>predict(bst_model, 
        as.matrix(test_data[, -1])) %&gt;%
  as.tibble() %&gt;%
  mutate(prediction = round(value),
         label = as.numeric(test_data$Private)-1) %&gt;%
  count(prediction, label)</code></pre>
<pre><code>## # A tibble: 4 x 3
##   prediction label     n
##        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;
## 1          0     0    56
## 2          0     1     6
## 3          1     0     7
## 4          1     1   163</code></pre>
<p>The third option, is to use <code>xgb.cv</code>, which will perform cross-validation. This function does not return a model, it is rather used to find optimal hyperparameters, particularly for <code>nrounds</code>.</p>
<pre class="r"><code>cv_model &lt;- xgb.cv(params = params,
                   data = dtrain, 
                   nrounds = 100, 
                   watchlist = watchlist,
                   nfold = 5,
                   verbose = FALSE,
                   prediction = TRUE) # prediction of cv folds</code></pre>
<p>Here, we can see after how many rounds, we achieved the smallest test error:</p>
<pre class="r"><code>cv_model$evaluation_log %&gt;%
  filter(test_error_mean == min(test_error_mean))</code></pre>
<pre><code>##   iter train_error_mean train_error_std test_error_mean test_error_std
## 1   17        0.0082568     0.002338999       0.0550458     0.01160461
## 2   25        0.0018350     0.001716352       0.0550458     0.01004998
## 3   29        0.0009176     0.001123826       0.0550458     0.01421269
## 4   32        0.0009176     0.001123826       0.0550458     0.01535140
## 5   33        0.0004588     0.000917600       0.0550458     0.01535140
## 6   80        0.0000000     0.000000000       0.0550458     0.01004998</code></pre>
</div>
<div id="h2o" class="section level2">
<h2>H2O</h2>
<p>H2O is another popular package for machine learning in R. We will first set up the session and create training and test data:</p>
<pre class="r"><code>library(h2o)
h2o.init(nthreads = -1)</code></pre>
<pre><code>## 
## H2O is not running yet, starting it now...
## 
## Note:  In case of errors look at the following log files:
##     /var/folders/5j/v30zfr7s14qfhqwqdmqmpxw80000gn/T//RtmpWCdBYk/h2o_shiringlander_started_from_r.out
##     /var/folders/5j/v30zfr7s14qfhqwqdmqmpxw80000gn/T//RtmpWCdBYk/h2o_shiringlander_started_from_r.err
## 
## 
## Starting H2O JVM and connecting: ... Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         2 seconds 105 milliseconds 
##     H2O cluster timezone:       Europe/Berlin 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.20.0.8 
##     H2O cluster version age:    2 months and 8 days  
##     H2O cluster name:           H2O_started_from_R_shiringlander_phb668 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   3.56 GB 
##     H2O cluster total cores:    8 
##     H2O cluster allowed cores:  8 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.1 (2018-07-02)</code></pre>
<pre class="r"><code>h2o.no_progress()

data_hf &lt;- as.h2o(ml_data)

splits &lt;- h2o.splitFrame(data_hf, 
                         ratios = 0.75, 
                         seed = 1)

train &lt;- splits[[1]]
test &lt;- splits[[2]]

response &lt;- &quot;Private&quot;
features &lt;- setdiff(colnames(train), response)</code></pre>
<div id="gradient-boosting" class="section level3">
<h3>Gradient Boosting</h3>
<p>The <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html">Gradient Boosting</a> implementation can be used as such:</p>
<pre class="r"><code>h2o_gbm &lt;- h2o.gbm(x = features, 
                   y = response, 
                   training_frame = train,
                   nfolds = 3) # cross-validation
h2o_gbm</code></pre>
<pre><code>## Model Details:
## ==============
## 
## H2OBinomialModel: gbm
## Model ID:  GBM_model_R_1543572213551_1 
## Model Summary: 
##   number_of_trees number_of_internal_trees model_size_in_bytes min_depth
## 1              50                       50               12998         5
##   max_depth mean_depth min_leaves max_leaves mean_leaves
## 1         5    5.00000          8         21    15.74000
## 
## 
## H2OBinomialMetrics: gbm
## ** Reported on training data. **
## 
## MSE:  0.00244139
## RMSE:  0.04941043
## LogLoss:  0.02582422
## Mean Per-Class Error:  0
## AUC:  1
## Gini:  1
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##         No Yes    Error    Rate
## No     160   0 0.000000  =0/160
## Yes      0 419 0.000000  =0/419
## Totals 160 419 0.000000  =0/579
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.671121 1.000000 246
## 2                       max f2  0.671121 1.000000 246
## 3                 max f0point5  0.671121 1.000000 246
## 4                 max accuracy  0.671121 1.000000 246
## 5                max precision  0.996764 1.000000   0
## 6                   max recall  0.671121 1.000000 246
## 7              max specificity  0.996764 1.000000   0
## 8             max absolute_mcc  0.671121 1.000000 246
## 9   max min_per_class_accuracy  0.671121 1.000000 246
## 10 max mean_per_class_accuracy  0.671121 1.000000 246
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`
## 
## H2OBinomialMetrics: gbm
## ** Reported on cross-validation data. **
## ** 3-fold cross-validation on training data (Metrics computed for combined holdout predictions) **
## 
## MSE:  0.05794659
## RMSE:  0.240721
## LogLoss:  0.1971785
## Mean Per-Class Error:  0.1030131
## AUC:  0.9741125
## Gini:  0.9482249
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##         No Yes    Error     Rate
## No     132  28 0.175000  =28/160
## Yes     13 406 0.031026  =13/419
## Totals 145 434 0.070812  =41/579
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.345249 0.951934 265
## 2                       max f2  0.149750 0.969939 284
## 3                 max f0point5  0.971035 0.958493 184
## 4                 max accuracy  0.345249 0.929188 265
## 5                max precision  0.997741 1.000000   0
## 6                   max recall  0.009001 1.000000 385
## 7              max specificity  0.997741 1.000000   0
## 8             max absolute_mcc  0.345249 0.819491 265
## 9   max min_per_class_accuracy  0.893580 0.904535 223
## 10 max mean_per_class_accuracy  0.917039 0.916982 213
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`
## Cross-Validation Metrics Summary: 
##                                mean           sd cv_1_valid  cv_2_valid
## accuracy                  0.9278624  0.008904516   0.921466  0.94545454
## auc                       0.9762384 0.0051301476 0.96743006   0.9851994
## err                      0.07213761  0.008904516 0.07853403 0.054545455
## err_count                 13.666667    0.8819171       15.0        12.0
## f0point5                  0.9352853  0.013447009 0.92972183   0.9608541
## f1                        0.9512787 0.0065108957 0.94423795  0.96428573
## f2                        0.9681131 0.0052480404  0.9592145   0.9677419
## lift_top_group            1.3879367  0.040602904  1.4580153   1.3173653
## logloss                  0.20110694  0.028338892 0.23033275   0.1444385
## max_per_class_error      0.20442705  0.049009725 0.18333334  0.13207547
## mcc                      0.81914276  0.016271077  0.8149471   0.8491877
## mean_per_class_accuracy   0.8877074   0.01979144 0.89306617   0.9189922
## mean_per_class_error      0.1122926   0.01979144 0.10693384  0.08100779
## mse                     0.059073452  0.007476475 0.06384816 0.044414397
## precision                 0.9250553   0.01813692  0.9202899   0.9585799
## r2                        0.7061854   0.02871026 0.70365846  0.75712836
## recall                    0.9798418  0.010080538  0.9694657   0.9700599
## rmse                     0.24200912  0.015890896 0.25268194  0.21074724
## specificity              0.79557294  0.049009725 0.81666666   0.8679245
##                          cv_3_valid
## accuracy                  0.9166667
## auc                       0.9760858
## err                     0.083333336
## err_count                      14.0
## f0point5                 0.91527987
## f1                        0.9453125
## f2                        0.9773829
## lift_top_group            1.3884298
## logloss                  0.22854955
## max_per_class_error      0.29787233
## mcc                       0.7932934
## mean_per_class_accuracy  0.85106385
## mean_per_class_error     0.14893617
## mse                     0.068957806
## precision                 0.8962963
## r2                       0.65776944
## recall                          1.0
## rmse                      0.2625982
## specificity              0.70212764</code></pre>
<p>We can calculate performance on test data with <code>h2o.performance()</code>:</p>
<pre class="r"><code>h2o.performance(h2o_gbm, test)</code></pre>
<pre><code>## H2OBinomialMetrics: gbm
## 
## MSE:  0.03509102
## RMSE:  0.187326
## LogLoss:  0.1350709
## Mean Per-Class Error:  0.05216017
## AUC:  0.9770811
## Gini:  0.9541623
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##        No Yes    Error    Rate
## No     48   4 0.076923   =4/52
## Yes     4 142 0.027397  =4/146
## Totals 52 146 0.040404  =8/198
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.580377 0.972603 136
## 2                       max f2  0.214459 0.979730 146
## 3                 max f0point5  0.907699 0.979827 127
## 4                 max accuracy  0.580377 0.959596 136
## 5                max precision  0.997449 1.000000   0
## 6                   max recall  0.006710 1.000000 187
## 7              max specificity  0.997449 1.000000   0
## 8             max absolute_mcc  0.580377 0.895680 136
## 9   max min_per_class_accuracy  0.821398 0.952055 131
## 10 max mean_per_class_accuracy  0.821398 0.956797 131
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`</code></pre>
</div>
<div id="xgboost" class="section level3">
<h3>XGBoost</h3>
<p>Alternatively, we can also use the <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.html">XGBoost</a> implementation of H2O:</p>
<pre class="r"><code>h2o_xgb &lt;- h2o.xgboost(x = features, 
                       y = response, 
                       training_frame = train,
                       nfolds = 3)
h2o_xgb</code></pre>
<pre><code>## Model Details:
## ==============
## 
## H2OBinomialModel: xgboost
## Model ID:  XGBoost_model_R_1543572213551_364 
## Model Summary: 
##   number_of_trees
## 1              50
## 
## 
## H2OBinomialMetrics: xgboost
## ** Reported on training data. **
## 
## MSE:  0.25
## RMSE:  0.5
## LogLoss:  0.6931472
## Mean Per-Class Error:  0.5
## AUC:  0.5
## Gini:  0
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##        No Yes    Error      Rate
## No      0 160 1.000000  =160/160
## Yes     0 419 0.000000    =0/419
## Totals  0 579 0.276339  =160/579
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.500000 0.839679   0
## 2                       max f2  0.500000 0.929047   0
## 3                 max f0point5  0.500000 0.765996   0
## 4                 max accuracy  0.500000 0.723661   0
## 5                max precision  0.500000 0.723661   0
## 6                   max recall  0.500000 1.000000   0
## 7              max specificity  0.500000 0.000000   0
## 8             max absolute_mcc  0.500000 0.000000   0
## 9   max min_per_class_accuracy  0.500000 0.000000   0
## 10 max mean_per_class_accuracy  0.500000 0.500000   0
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`
## 
## H2OBinomialMetrics: xgboost
## ** Reported on cross-validation data. **
## ** 3-fold cross-validation on training data (Metrics computed for combined holdout predictions) **
## 
## MSE:  0.25
## RMSE:  0.5
## LogLoss:  0.6931472
## Mean Per-Class Error:  0.5
## AUC:  0.5
## Gini:  0
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##        No Yes    Error      Rate
## No      0 160 1.000000  =160/160
## Yes     0 419 0.000000    =0/419
## Totals  0 579 0.276339  =160/579
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.500000 0.839679   0
## 2                       max f2  0.500000 0.929047   0
## 3                 max f0point5  0.500000 0.765996   0
## 4                 max accuracy  0.500000 0.723661   0
## 5                max precision  0.500000 0.723661   0
## 6                   max recall  0.500000 1.000000   0
## 7              max specificity  0.500000 0.000000   0
## 8             max absolute_mcc  0.500000 0.000000   0
## 9   max min_per_class_accuracy  0.500000 0.000000   0
## 10 max mean_per_class_accuracy  0.500000 0.500000   0
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`
## Cross-Validation Metrics Summary: 
##                                mean          sd  cv_1_valid  cv_2_valid
## accuracy                  0.7260711  0.01583762  0.73595506   0.6950673
## auc                             0.5         0.0         0.5         0.5
## err                      0.27392888  0.01583762  0.26404494  0.30493274
## err_count                 53.333332   7.3560257        47.0        68.0
## f0point5                  0.7680598 0.014220628  0.77698696   0.7402101
## f1                        0.8411026 0.010714028  0.84789646   0.8201058
## f2                       0.92966795 0.005267985   0.9330484   0.9193357
## lift_top_group                  1.0         0.0         1.0         1.0
## logloss                   0.6931472         0.0   0.6931472   0.6931472
## max_per_class_error             1.0         0.0         1.0         1.0
## mcc                             0.0         NaN         NaN         NaN
## mean_per_class_accuracy         0.5         0.0         0.5         0.5
## mean_per_class_error            0.5         0.0         0.5         0.5
## mse                            0.25         0.0        0.25        0.25
## precision                 0.7260711  0.01583762  0.73595506   0.6950673
## r2                      -0.26316962 0.043160092 -0.28650317 -0.17953037
## recall                          1.0         0.0         1.0         1.0
## rmse                            0.5         0.0         0.5         0.5
## specificity                     0.0         0.0         0.0         0.0
##                          cv_3_valid
## accuracy                   0.747191
## auc                             0.5
## err                        0.252809
## err_count                      45.0
## f0point5                 0.78698224
## f1                        0.8553055
## f2                        0.9366197
## lift_top_group                  1.0
## logloss                   0.6931472
## max_per_class_error             1.0
## mcc                             NaN
## mean_per_class_accuracy         0.5
## mean_per_class_error            0.5
## mse                            0.25
## precision                  0.747191
## r2                      -0.32347536
## recall                          1.0
## rmse                            0.5
## specificity                     0.0</code></pre>
<p>And use it just as before:</p>
<pre class="r"><code>h2o.performance(h2o_xgb, test)</code></pre>
<pre><code>## H2OBinomialMetrics: xgboost
## 
## MSE:  0.25
## RMSE:  0.5
## LogLoss:  0.6931472
## Mean Per-Class Error:  0.5
## AUC:  0.5
## Gini:  0
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##        No Yes    Error     Rate
## No      0  52 1.000000   =52/52
## Yes     0 146 0.000000   =0/146
## Totals  0 198 0.262626  =52/198
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.500000 0.848837   0
## 2                       max f2  0.500000 0.933504   0
## 3                 max f0point5  0.500000 0.778252   0
## 4                 max accuracy  0.500000 0.737374   0
## 5                max precision  0.500000 0.737374   0
## 6                   max recall  0.500000 1.000000   0
## 7              max specificity  0.500000 0.000000   0
## 8             max absolute_mcc  0.500000 0.000000   0
## 9   max min_per_class_accuracy  0.500000 0.000000   0
## 10 max mean_per_class_accuracy  0.500000 0.500000   0
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`</code></pre>
<hr />
</div>
</div>
</div>
<div id="video" class="section level1">
<h1>Video</h1>
<iframe width="560" height="315" src="https://www.youtube.com/embed/xXZeVKP74ao" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
<div id="slides" class="section level1">
<h1>Slides</h1>
<iframe src="//codecentric.slides.com/shiringlander/ml_basics_gbm/embed" width="576" height="420" scrolling="no" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen>
</iframe>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS  10.14.1
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] h2o_3.20.0.8    bindrcpp_0.2.2  xgboost_0.71.2  caret_6.0-80   
##  [5] lattice_0.20-38 ISLR_1.2        forcats_0.3.0   stringr_1.3.1  
##  [9] dplyr_0.7.7     purrr_0.2.5     readr_1.1.1     tidyr_0.8.2    
## [13] tibble_1.4.2    ggplot2_3.1.0   tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##  [1] nlme_3.1-137       bitops_1.0-6       lubridate_1.7.4   
##  [4] dimRed_0.1.0       httr_1.3.1         rprojroot_1.3-2   
##  [7] tools_3.5.1        backports_1.1.2    utf8_1.1.4        
## [10] R6_2.3.0           rpart_4.1-13       lazyeval_0.2.1    
## [13] colorspace_1.3-2   nnet_7.3-12        withr_2.1.2       
## [16] gbm_2.1.4          gridExtra_2.3      tidyselect_0.2.5  
## [19] compiler_3.5.1     cli_1.0.1          rvest_0.3.2       
## [22] xml2_1.2.0         bookdown_0.7       scales_1.0.0      
## [25] sfsmisc_1.1-2      DEoptimR_1.0-8     robustbase_0.93-3 
## [28] digest_0.6.18      rmarkdown_1.10     pkgconfig_2.0.2   
## [31] htmltools_0.3.6    rlang_0.3.0.1      readxl_1.1.0      
## [34] ddalpha_1.3.4      rstudioapi_0.8     bindr_0.1.1       
## [37] jsonlite_1.5       ModelMetrics_1.2.2 RCurl_1.95-4.11   
## [40] magrittr_1.5       Matrix_1.2-15      fansi_0.4.0       
## [43] Rcpp_0.12.19       munsell_0.5.0      abind_1.4-5       
## [46] stringi_1.2.4      yaml_2.2.0         MASS_7.3-51.1     
## [49] plyr_1.8.4         recipes_0.1.3      grid_3.5.1        
## [52] pls_2.7-0          crayon_1.3.4       haven_1.1.2       
## [55] splines_3.5.1      hms_0.4.2          knitr_1.20        
## [58] pillar_1.3.0       reshape2_1.4.3     codetools_0.2-15  
## [61] stats4_3.5.1       CVST_0.2-2         magic_1.5-9       
## [64] glue_1.3.0         evaluate_0.12      blogdown_0.9      
## [67] data.table_1.11.8  modelr_0.1.2       foreach_1.4.4     
## [70] cellranger_1.1.0   gtable_0.2.0       kernlab_0.9-27    
## [73] assertthat_0.2.0   DRR_0.0.3          xfun_0.4          
## [76] gower_0.1.2        prodlim_2018.04.18 broom_0.5.0       
## [79] e1071_1.7-0        class_7.3-14       survival_2.43-1   
## [82] geometry_0.3-6     timeDate_3043.102  RcppRoll_0.3.0    
## [85] iterators_1.0.10   lava_1.6.3         ipred_0.9-8</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Slides from my talks about Demystifying Big Data and Deep Learning (and how to get started)]]></title>
    <link href="/2018/11/slides_demystifying_dl/"/>
    <id>/2018/11/slides_demystifying_dl/</id>
    <published>2018-11-20T00:00:00+00:00</published>
    <updated>2018-11-20T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>On November 7th, Uwe Friedrichsen and I gave our talk from the <a href="https://shirinsplayground.netlify.com/2018/01/jax2018/">JAX conference 2018: Deep Learning - a Primer</a> again at the <a href="https://jax.de/">W-JAX</a> in Munich.</p>
<p>A few weeks before, I gave a similar talk at two events about <strong>Demystifying Big Data and Deep Learning (and how to get started)</strong>.</p>
<p>Here are the two very similar presentations from these talks:</p>
<div style="position:relative;width:100%;height:0;padding-bottom:calc(56.25% + 40px);">
<iframe allowfullscreen style="position:absolute; width: 100%; height: 100%;border: solid 1px #333;" src="https://www.beautiful.ai/player/-LPyhpe7dzEbn6g70401/Techlabs?utm_source=beautiful_player&amp;utm_medium=embed&amp;utm_campaign=-LPyhpe7dzEbn6g70401">
</iframe>
</div>
<div style="position:relative;width:100%;height:0;padding-bottom:calc(56.25% + 40px);">
<iframe allowfullscreen style="position:absolute; width: 100%; height: 100%;border: solid 1px #333;" src="https://www.beautiful.ai/player/-LO3lXsKLz_Q6RXNT-Ed/W-JAX2018?utm_source=beautiful_player&amp;utm_medium=embed&amp;utm_campaign=-LO3lXsKLz_Q6RXNT-Ed">
</iframe>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[How to use cross-validation with the image data generator in Keras and TensorFlow]]></title>
    <link href="/2018/11/keras_fruits_crossvalidation/"/>
    <id>/2018/11/keras_fruits_crossvalidation/</id>
    <published>2018-11-19T00:00:00+00:00</published>
    <updated>2018-11-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<blockquote>
<p>I’ve been using keras and TensorFlow for a while now - and love its simplicity and straight-forward way to modeling. As part of the latest update to my <a href="https://shirinsplayground.netlify.com/2018/05/deep_learning_keras_tensorflow_18_07/">Workshop about deep learning with R and keras</a> I’ve added a new example analysis:</p>
</blockquote>
<p><a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits/" class="uri">https://shirinsplayground.netlify.com/2018/06/keras_fruits/</a></p>
<pre class="r"><code>library(keras)
library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──</code></pre>
<pre><code>## ✔ ggplot2 3.1.0     ✔ purrr   0.2.5
## ✔ tibble  1.4.2     ✔ dplyr   0.7.7
## ✔ tidyr   0.8.2     ✔ stringr 1.3.1
## ✔ readr   1.1.1     ✔ forcats 0.3.0</code></pre>
<pre><code>## ── Conflicts ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code># list of fruits to modle
fruit_list &lt;- c(&quot;Kiwi&quot;, &quot;Banana&quot;, &quot;Apricot&quot;, &quot;Avocado&quot;, &quot;Cocos&quot;, &quot;Clementine&quot;, &quot;Mandarine&quot;, &quot;Orange&quot;,
                &quot;Limes&quot;, &quot;Lemon&quot;, &quot;Peach&quot;, &quot;Plum&quot;, &quot;Raspberry&quot;, &quot;Strawberry&quot;, &quot;Pineapple&quot;, &quot;Pomegranate&quot;)

# number of output classes (i.e. fruits)
output_n &lt;- length(fruit_list)

# image size to scale down to (original images are 100 x 100 px)
img_width &lt;- 20
img_height &lt;- 20
target_size &lt;- c(img_width, img_height)

# RGB = 3 channels
channels &lt;- 3

# path to image folders
train_image_files_path &lt;- &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Training/&quot;</code></pre>
<pre class="r"><code>train_data_gen = image_data_generator(
  rescale = 1/255,
  validation_split = 0.3
)</code></pre>
<pre class="r"><code># training images
train_image_array_gen &lt;- flow_images_from_directory(train_image_files_path, 
                                          train_data_gen,
                                          subset = &#39;training&#39;,
                                          target_size = target_size,
                                          class_mode = &quot;categorical&quot;,
                                          classes = fruit_list,
                                          seed = 42)

# validation images
valid_image_array_gen &lt;- flow_images_from_directory(train_image_files_path, 
                                          train_data_gen,
                                          subset = &#39;validation&#39;,
                                          target_size = target_size,
                                          class_mode = &quot;categorical&quot;,
                                          classes = fruit_list,
                                          seed = 42)</code></pre>
<pre class="r"><code>cat(&quot;Number of images per class:&quot;)</code></pre>
<pre><code>## Number of images per class:</code></pre>
<pre class="r"><code>table(factor(train_image_array_gen$classes))</code></pre>
<pre><code>## 
##   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15 
## 327 343 345 299 343 343 343 336 343 345 345 313 343 345 343 345</code></pre>
<pre class="r"><code>cat(&quot;\nClass label vs index mapping:\n&quot;)</code></pre>
<pre><code>## 
## Class label vs index mapping:</code></pre>
<pre class="r"><code>train_image_array_gen$class_indices</code></pre>
<pre><code>## $Lemon
## [1] 9
## 
## $Peach
## [1] 10
## 
## $Limes
## [1] 8
## 
## $Apricot
## [1] 2
## 
## $Plum
## [1] 11
## 
## $Avocado
## [1] 3
## 
## $Strawberry
## [1] 13
## 
## $Pineapple
## [1] 14
## 
## $Orange
## [1] 7
## 
## $Mandarine
## [1] 6
## 
## $Banana
## [1] 1
## 
## $Clementine
## [1] 5
## 
## $Kiwi
## [1] 0
## 
## $Cocos
## [1] 4
## 
## $Pomegranate
## [1] 15
## 
## $Raspberry
## [1] 12</code></pre>
<div id="define-model" class="section level3">
<h3>Define model</h3>
<pre class="r"><code># number of training samples
train_samples &lt;- train_image_array_gen$n
# number of validation samples
valid_samples &lt;- valid_image_array_gen$n

# define batch size and number of epochs
batch_size &lt;- 32
epochs &lt;- 10</code></pre>
<pre class="r"><code># initialise model
model &lt;- keras_model_sequential()

# add layers
model %&gt;%
  layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = &quot;same&quot;, input_shape = c(img_width, img_height, channels)) %&gt;%
  layer_activation(&quot;relu&quot;) %&gt;%
  
  # Second hidden layer
  layer_conv_2d(filter = 16, kernel_size = c(3,3), padding = &quot;same&quot;) %&gt;%
  layer_activation_leaky_relu(0.5) %&gt;%
  layer_batch_normalization() %&gt;%

  # Use max pooling
  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;%
  layer_dropout(0.25) %&gt;%
  
  # Flatten max filtered output into feature vector 
  # and feed into dense layer
  layer_flatten() %&gt;%
  layer_dense(100) %&gt;%
  layer_activation(&quot;relu&quot;) %&gt;%
  layer_dropout(0.5) %&gt;%

  # Outputs from dense layer are projected onto output layer
  layer_dense(output_n) %&gt;% 
  layer_activation(&quot;softmax&quot;)

# compile
model %&gt;% compile(
  loss = &quot;categorical_crossentropy&quot;,
  optimizer = optimizer_rmsprop(lr = 0.0001, decay = 1e-6),
  metrics = &quot;accuracy&quot;
)</code></pre>
<pre class="r"><code># fit
hist &lt;- model %&gt;% fit_generator(
  # training data
  train_image_array_gen,
  
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = epochs, 
  
  # validation data
  validation_data = valid_image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size)
)</code></pre>
<pre class="r"><code>plot(hist)</code></pre>
<p><img src="/post/2018-11-19_keras_fruits_crossvalidation_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code>test_image_files_path = &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Validation/&quot;

test_datagen &lt;- image_data_generator(rescale = 1/255)

test_generator &lt;- flow_images_from_directory(
        test_image_files_path,
        test_datagen,
        target_size = target_size,
        class_mode = &quot;categorical&quot;,
        classes = fruit_list,
        seed = 42)</code></pre>
<pre class="r"><code>indices &lt;- train_image_array_gen$class_indices %&gt;%
  as.data.frame() %&gt;%
  gather() %&gt;%
  arrange(value)</code></pre>
<pre class="r"><code>predictions &lt;- as.data.frame(predict_generator(model, test_generator, steps = as.integer(test_generator$n / batch_size)))</code></pre>
<pre class="r"><code>colnames(predictions) &lt;- indices$key
predictions &lt;- predictions %&gt;%
  mutate(truth_idx = test_generator$classes) %&gt;%
  left_join(indices, by = c(&quot;truth_idx&quot; = &quot;value&quot;)) %&gt;%
  rename(truth_lbl = key)</code></pre>
<pre class="r"><code>predictions %&gt;%
  mutate(id = seq(1:test_generator$n)) %&gt;%
  gather(pred_lbl, y, Kiwi:Pomegranate) %&gt;%
  ggplot(aes(x = id, y = y), color = truth_lbl) +
    facet_wrap( ~ pred_lbl, scales = &quot;free&quot;, ncol = 2) +
    geom_jitter()</code></pre>
<pre class="r"><code>evaluate_generator(model, test_generator, steps = as.integer(test_generator$n / batch_size))</code></pre>
<pre><code>## $loss
## [1] 0.02086813
## 
## $acc
## [1] 0.996142</code></pre>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS  10.14.1
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] bindrcpp_0.2.2  forcats_0.3.0   stringr_1.3.1   dplyr_0.7.7    
##  [5] purrr_0.2.5     readr_1.1.1     tidyr_0.8.2     tibble_1.4.2   
##  [9] ggplot2_3.1.0   tidyverse_1.2.1 keras_2.2.0    
## 
## loaded via a namespace (and not attached):
##  [1] reticulate_1.10  tidyselect_0.2.5 xfun_0.4         reshape2_1.4.3  
##  [5] haven_1.1.2      lattice_0.20-38  colorspace_1.3-2 htmltools_0.3.6 
##  [9] yaml_2.2.0       base64enc_0.1-3  rlang_0.3.0.1    pillar_1.3.0    
## [13] withr_2.1.2      glue_1.3.0       readxl_1.1.0     modelr_0.1.2    
## [17] bindr_0.1.1      plyr_1.8.4       tensorflow_1.9   cellranger_1.1.0
## [21] munsell_0.5.0    blogdown_0.9     gtable_0.2.0     rvest_0.3.2     
## [25] evaluate_0.12    labeling_0.3     knitr_1.20       tfruns_1.4      
## [29] broom_0.5.0      Rcpp_0.12.19     backports_1.1.2  scales_1.0.0    
## [33] jsonlite_1.5     hms_0.4.2        digest_0.6.18    stringi_1.2.4   
## [37] bookdown_0.7     grid_3.5.1       rprojroot_1.3-2  cli_1.0.1       
## [41] tools_3.5.1      magrittr_1.5     lazyeval_0.2.1   crayon_1.3.4    
## [45] whisker_0.3-2    pkgconfig_2.0.2  zeallot_0.1.0    Matrix_1.2-15   
## [49] xml2_1.2.0       lubridate_1.7.4  rstudioapi_0.8   assertthat_0.2.0
## [53] rmarkdown_1.10   httr_1.3.1       R6_2.3.0         nlme_3.1-137    
## [57] compiler_3.5.1</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[TWIMLAI European Online Meetup about Trust in Predictions of ML Models]]></title>
    <link href="/2018/11/twimlai_meetup/"/>
    <id>/2018/11/twimlai_meetup/</id>
    <published>2018-11-13T00:00:00+00:00</published>
    <updated>2018-11-13T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>At the upcoming <a href="https://twimlai.com/meetups/trust-in-predictions-of-ml-models/">This week in machine learning and AI European online Meetup</a>, I’ll be presenting and leading a discussion about the <a href="https://homes.cs.washington.edu/~marcotcr/aaai18.pdf">Anchors paper</a>, the next generation of machine learning interpretability tools. Come and join the fun! :-)</p>
<ul>
<li>Date: Tuesday 4th December 2018</li>
<li>Time: 19:00 PM CET/CEST</li>
</ul>
<p>Join: <a href="https://twimlai.com/meetups/trust-in-predictions-of-ml-models/" class="uri">https://twimlai.com/meetups/trust-in-predictions-of-ml-models/</a></p>
<div class="figure">
<img src="https://twimlai.com/files/2017/09/TWiML_banner_wide.png" />

</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[How to prepare data for NLP (text classification) with Keras and TensorFlow]]></title>
    <link href="/2018/11/text_classification_keras_data_prep/"/>
    <id>/2018/11/text_classification_keras_data_prep/</id>
    <published>2018-11-12T00:00:00+00:00</published>
    <updated>2018-11-12T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>In the past, I have written and taught quite a bit about image classification with Keras (<a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits/">e.g. here</a>). Text classification isn’t too different in terms of using the Keras principles to train a sequential or function model. You can even use Convolutional Neural Nets (CNNs) for text classification.</p>
<p>What is very different, however, is how to prepare raw text data for modeling. When you look at the <a href="https://jjallaire.github.io/deep-learning-with-r-notebooks/notebooks/3.4-classifying-movie-reviews.nb.html">IMDB example from the Deep Learning with R Book</a>, you get a great explanation of how to train the model. But because the <strong>preprocessed</strong> IMDB dataset comes with the <code>keras</code> package, it isn’t so straight-forward to use what you learned on your own data.</p>
<div id="how-can-a-computer-work-with-text" class="section level2">
<h2>How can a computer work with text?</h2>
<p>As with any neural network, we need to convert our data into a numeric format; in Keras and TensorFlow we work with tensors. The IMDB example data from the <code>keras</code> package has been preprocessed to a list of integers, where every integer corresponds to a word arranged by descending word frequency.</p>
<p>So, how do we make it from raw text to such a list of integers? Luckily, Keras offers a few convenience functions that make our lives much easier.</p>
<pre class="r"><code>library(keras)
library(tidyverse)</code></pre>
</div>
<div id="data" class="section level2">
<h2>Data</h2>
<p>In the example below, I am using a Kaggle dataset: <a href="https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews">Women’s e-commerce cloting reviews</a>. The data contains a text review of different items of clothing, as well as some additional information, like rating, division, etc. I will use the review title and text in order to classify whether or not the item was liked. I am creating the response variable from the rating: every item rates with 5 stars is considered “liked” (1), the rest as “not liked” (0). I am also combining review title and text.</p>
<pre class="r"><code>clothing_reviews &lt;- read_csv(&quot;/Users/shiringlander/Documents/Github/ix_lime_etc/Womens Clothing E-Commerce Reviews.csv&quot;) %&gt;%
  mutate(Liked = ifelse(Rating == 5, 1, 0),
         text = paste(Title, `Review Text`),
         text = gsub(&quot;NA&quot;, &quot;&quot;, text))</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   X1 = col_integer(),
##   `Clothing ID` = col_integer(),
##   Age = col_integer(),
##   Title = col_character(),
##   `Review Text` = col_character(),
##   Rating = col_integer(),
##   `Recommended IND` = col_integer(),
##   `Positive Feedback Count` = col_integer(),
##   `Division Name` = col_character(),
##   `Department Name` = col_character(),
##   `Class Name` = col_character()
## )</code></pre>
<pre class="r"><code>glimpse(clothing_reviews)</code></pre>
<pre><code>## Observations: 23,486
## Variables: 13
## $ X1                        &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...
## $ `Clothing ID`             &lt;int&gt; 767, 1080, 1077, 1049, 847, 1080, 85...
## $ Age                       &lt;int&gt; 33, 34, 60, 50, 47, 49, 39, 39, 24, ...
## $ Title                     &lt;chr&gt; NA, NA, &quot;Some major design flaws&quot;, &quot;...
## $ `Review Text`             &lt;chr&gt; &quot;Absolutely wonderful - silky and se...
## $ Rating                    &lt;int&gt; 4, 5, 3, 5, 5, 2, 5, 4, 5, 5, 3, 5, ...
## $ `Recommended IND`         &lt;int&gt; 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...
## $ `Positive Feedback Count` &lt;int&gt; 0, 4, 0, 0, 6, 4, 1, 4, 0, 0, 14, 2,...
## $ `Division Name`           &lt;chr&gt; &quot;Initmates&quot;, &quot;General&quot;, &quot;General&quot;, &quot;...
## $ `Department Name`         &lt;chr&gt; &quot;Intimate&quot;, &quot;Dresses&quot;, &quot;Dresses&quot;, &quot;B...
## $ `Class Name`              &lt;chr&gt; &quot;Intimates&quot;, &quot;Dresses&quot;, &quot;Dresses&quot;, &quot;...
## $ Liked                     &lt;dbl&gt; 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, ...
## $ text                      &lt;chr&gt; &quot; Absolutely wonderful - silky and s...</code></pre>
<p>Whether an item was liked or not will be the response variable or label for classification of the reviews.</p>
<pre class="r"><code>clothing_reviews %&gt;%
  ggplot(aes(x = factor(Liked), fill = Liked)) +
    geom_bar(alpha = 0.8) +
    guides(fill = FALSE)</code></pre>
<p><img src="/post/2018-11-12_text_classification_keras_data_prep_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="tokenizers" class="section level2">
<h2>Tokenizers</h2>
<p>The first step is to tokenize the text. This means, converting our text into a sequence of integers where each integer corresponds to a word in the dictionary.</p>
<pre class="r"><code>text &lt;- clothing_reviews$text</code></pre>
<p>The <code>num_words</code> argument defines the number of words we want to consider (this will be our feature space). Because the output integers will be sorted according to decreasing word frequency, if we set 1000, we will only get the 1000 most frequent words in our corpus.</p>
<pre class="r"><code>max_features &lt;- 1000
tokenizer &lt;- text_tokenizer(num_words = max_features)</code></pre>
<p>Next, we need to fit the tokenizer to our text data. Note, that the <code>tokenizer</code> object is modified in place (as are models in Keras)!</p>
<pre class="r"><code>tokenizer %&gt;% 
  fit_text_tokenizer(text)</code></pre>
<p>After fitting the tokenizer, we can extract the following information: the number of documents …</p>
<pre class="r"><code>tokenizer$document_count</code></pre>
<pre><code>## [1] 23486</code></pre>
<p>… and the word-index list. Notice, that even though we set the maximum number of words to 1000, our index contains many more words. In fact, the index will keep all words in the index but when converting our reviews to vectors, the stored value <code>tokenizer$num_words</code> will be used to restrict to the most common words.</p>
<pre class="r"><code>tokenizer$word_index %&gt;%
  head()</code></pre>
<pre><code>## $raining
## [1] 13788
## 
## $yellow
## [1] 553
## 
## $four
## [1] 1501
## 
## $bottons
## [1] 7837
## 
## $woods
## [1] 7896
## 
## $`friend&#39;s`
## [1] 3525</code></pre>
<p>We now have the dictionary of integers and which words they should replace in our text. But we still don’t have a list of integers for our reviews. So, now we use the <code>texts_to_sequences</code> functions, which will do just that! Words, which weren’t among the top 1000 were excluded.</p>
<pre class="r"><code>text_seqs &lt;- texts_to_sequences(tokenizer, text)
text_seqs %&gt;%
  head()</code></pre>
<pre><code>## [[1]]
## [1] 249 494 924   3 595   3  63
## 
## [[2]]
##  [1]  19   7  17  35  84   2   8 221   5   9   4 114   3  37 328   2 135
## [18]   2 421  43  25  57   5 139  35  95   2  75   4  95   3  39 518   2
## [35]  19   1  88  11  31 423  38   4  56 474   1 401  43 160  30   4 132
## [52]  11 447 444   6 761  95
## 
## [[3]]
##  [1] 156 134   2  68 314 180  12   7  17   3  53 183   5   8  98  12  31
## [18]   2  57   1  95  42  18 240  22  10   2 230   7   8  30  42  15  42
## [35]   9 683  21   2 122  20 803   5  45   2   5   9  95  99  86  16  38
## [52] 581 256   1  24 673  16  63   3  26 267  10   1 182 673  68   4  23
## [69] 148 285 489   3 543 738 481 157 997   4 134  16   1 157 489 846 326
## [86]   1 455   5 706
## 
## [[4]]
##  [1]  18 292 220   2  19  19  19   7 592  35 209   3 652 310 189   2  33
## [18]   5   2 120 530  10  27 212
## 
## [[5]]
##  [1]  55  71   7  71   6  23  55   8  76 504   8   1 163 484   5   6   1
## [18]  49  88   8  33  14 262   3   5   6  15   5 855  64  14 257 376  19
## [35]   7  71
## 
## [[6]]
##  [1]  20  12   1  23  95   2  19 244  10   7  60   6  20  12   1  23  95
## [18]   2  39  38 285 278 324   3 115  33   4   9   7 492   7  17  16  23
## [35]  84  66  13   1  10 250   4 245  13  17   1 100   6  90   3  23 321
## [52]  15   5  18  42 428  20   4   8   3   1 100  43 378 506 111   1  13
## [69]   1   2  19   1  46   3   1 686  13   1 124  10   5  38 135  20  98
## [86]  11  31   2 370   7  17</code></pre>
<p>So, there we have it! From here on out, we can simply follow the <a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_cnn.R">IMDB example from the Keras documentation</a>:</p>
<pre class="r"><code># Set parameters:
maxlen &lt;- 100
batch_size &lt;- 32
embedding_dims &lt;- 50
filters &lt;- 64
kernel_size &lt;- 3
hidden_dims &lt;- 50
epochs &lt;- 5</code></pre>
<p>Because we can’t directly use this list of integers in our neural network, there is still some preprocessing to do. In the IMDB example, the lists are padded so that they all have the same length. The <code>pad_sequences</code> function will return a matrix, with columns for a given maximum number of words (or the number of words in the longest sequence). Here, we have 400 columns in our matrix. Reviews with fewer words were padded with zeros at the beginning before the indices. Longer reviews are cut after 400 words.</p>
<pre class="r"><code>x_train &lt;- text_seqs %&gt;%
  pad_sequences(maxlen = maxlen)
dim(x_train)</code></pre>
<pre><code>## [1] 23486   100</code></pre>
<p>Our response variable will be encoded with 1s (5-star review) and 0s (not 5-star reviews). Because we have a binary outcome, we only need this one vector.</p>
<pre class="r"><code>y_train &lt;- clothing_reviews$Liked
length(y_train)</code></pre>
<pre><code>## [1] 23486</code></pre>
</div>
<div id="embeddings" class="section level2">
<h2>Embeddings</h2>
<p>These padded word index matrices now need to be converted into something that gives information about the features (i.e. words) in a way that can be used for learning. Currently, the state-of-the-art for text models are word embeddings or word vectors, which are learned from the text data. Word embeddings encode the context of words in relatively few dimensions while maximizing the information that is contained in these vectors. Basically, word embeddings are values that are learned by a neural net.</p>
<p>In our model below, we want to learn the word embeddings from our (padded) word vectors and directly use these learned embeddings for classification</p>
<p>word embeddings vs. one hot encoding</p>
<p>There are two ways to obtain word embeddings:</p>
<p>Learn word embeddings jointly with the main task you care about (e.g. document classification or sentiment prediction). In this setup, you would start with random word vectors, then learn your word vectors in the same way that you learn the weights of a neural network. Load into your model word embeddings that were pre-computed using a different machine learning task than the one you are trying to solve. These are called “pre-trained word embeddings”.</p>
<p>The simplest way to associate a dense vector to a word would be to pick the vector at random. The problem with this approach is that the resulting embedding space would have no structure: for instance, the words “accurate” and “exact” may end up with completely different embeddings, even though they are interchangeable in most sentences. It would be very difficult for a deep neural network to make sense of such a noisy, unstructured embedding space.</p>
<p>To get a bit more abstract: the geometric relationships between word vectors should reflect the semantic relationships between these words. Word embeddings are meant to map human language into a geometric space. For instance, in a reasonable embedding space, we would expect synonyms to be embedded into similar word vectors, and in general we would expect the geometric distance (e.g. L2 distance) between any two word vectors to relate to the semantic distance of the associated words (words meaning very different things would be embedded to points far away from each other, while related words would be closer). Even beyond mere distance, we may want specific directions in the embedding space to be meaningful.</p>
<p>In real-world word embedding spaces, common examples of meaningful geometric transformations are “gender vectors” and “plural vector”. For instance, by adding a “female vector” to the vector “king”, one obtain the vector “queen”. By adding a “plural vector”, one obtain “kings”. Word embedding spaces typically feature thousands of such interpretable and potentially useful vectors.</p>
<p>Is there some “ideal” word embedding space that would perfectly map human language and could be used for any natural language processing task? Possibly, but in any case, we have yet to compute anything of the sort. Also, there isn’t such a thing as “human language”, there are many different languages and they are not isomorphic, as a language is the reflection of a specific culture and a specific context. But more pragmatically, what makes a good word embedding space depends heavily on your task: the perfect word embedding space for an English-language movie review sentiment analysis model may look very different from the perfect embedding space for an English-language legal document classification model, because the importance of certain semantic relationships varies from task to task.</p>
<p>It’s thus reasonable to learn a new embedding space with every new task. Fortunately, backpropagation makes this easy, and Keras makes it even easier. It’s about learning the weights of a layer using layer_embedding()</p>
<p>A layer_embedding() is best understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors. It takes integers as input, it looks up these integers in an internal dictionary, and it returns the associated vectors. It’s effectively a dictionary lookup (see figure 6.4).</p>
<p>An embedding layer takes as input a 2D tensor of integers, of shape (samples, sequence_length), where each entry is a sequence of integers. It can embed sequences of variable lengths: for instance, you could feed into the embedding layer in the previous example batches with shapes (32, 10) (batch of 32 sequences of length 10) or (64, 15) (batch of 64 sequences of length 15). All sequences in a batch must have the same length, though (because you need to pack them into a single tensor), so sequences that are shorter than others should be padded with zeros, and sequences that are longer should be truncated.</p>
<p>This layer returns a 3D floating-point tensor, of shape (samples, sequence_length, embedding_dimensionality). Such a 3D tensor can then be processed by an RNN layer or a 1D convolution layer (both will be introduced in the following sections).</p>
<p>When you instantiate an embedding layer, its weights (its internal dictionary of token vectors) are initially random, just as with any other layer. During training, these word vectors are gradually adjusted via backpropagation, structuring the space into something the downstream model can exploit. Once fully trained, the embedding space will show a lot of structure – a kind of structure specialized for the specific problem for which you were training your model.</p>
<p>Let’s apply this idea to the IMDB movie-review sentiment-prediction task that you’re already familiar with. First, you’ll quickly prepare the data. You’ll restrict the movie reviews to the top 10,000 most common words (as you did the first time you worked with this dataset) and cut off the reviews after only 20 words. The network will learn 8-dimensional embeddings for each of the 10,000 words, turn the input integer sequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the tensor to 2D, and train a single dense layer on top for classification.</p>
<pre class="r"><code>#Initialize model
model &lt;- keras_model_sequential()

model %&gt;% 
  # Start off with an efficient embedding layer which maps
  # the vocab indices into embedding_dims dimensions
  layer_embedding(max_features, embedding_dims, input_length = maxlen) %&gt;%
  layer_dropout(0.2) %&gt;%

  # Add a Convolution1D, which will learn filters
    # Word group filters of size filter_length:
  layer_conv_1d(
    filters, kernel_size, 
    padding = &quot;valid&quot;, activation = &quot;relu&quot;, strides = 1
  ) %&gt;%
  # Apply max pooling:
  layer_global_max_pooling_1d() %&gt;%

  # Add a vanilla hidden layer:
  layer_dense(hidden_dims) %&gt;%

  # Apply 20% layer dropout
  layer_dropout(0.2) %&gt;%
  layer_activation(&quot;relu&quot;) %&gt;%

  # Project onto a single unit output layer, and squash it with a sigmoid

  layer_dense(1) %&gt;%
  layer_activation(&quot;sigmoid&quot;)

# Compile model
model %&gt;% compile(
  loss = &quot;binary_crossentropy&quot;,
  optimizer = &quot;adam&quot;,
  metrics = &quot;accuracy&quot;
)</code></pre>
<pre class="r"><code>model %&gt;%
  fit(
    x_train,
    y_train,
    batch_size = batch_size,
    epochs = epochs,
    validation_split = 0.3
  )</code></pre>
<p><a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_cnn_lstm.R" class="uri">https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_cnn_lstm.R</a> <a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_lstm.R" class="uri">https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_lstm.R</a></p>
</div>
<div id="alternative-preprocessing-functions" class="section level2">
<h2>Alternative preprocessing functions</h2>
<p>The above example follows the IMDB example from the Keras documentation, but there are alternative ways to preprocess your text for modeling with Keras.</p>
<p><a href="https://jjallaire.github.io/deep-learning-with-r-notebooks/notebooks/6.1-one-hot-encoding-of-words-or-characters.nb.html">one-hot-encoding</a></p>
<pre class="r"><code>one_hot_results &lt;- texts_to_matrix(tokenizer, text, mode = &quot;binary&quot;)
dim(one_hot_results)</code></pre>
<pre><code>## [1] 23486  1000</code></pre>
<p>A variant of one-hot encoding is the so-called “one-hot hashing trick”, which can be used when the number of unique tokens in your vocabulary is too large to handle explicitly. Instead of explicitly assigning an index to each word and keeping a reference of these indices in a dictionary, one may hash words into vectors of fixed size. This is typically done with a very lightweight hashing function. The main advantage of this method is that it does away with maintaining an explicit word index, which saves memory and allows online encoding of the data (starting to generate token vectors right away, before having seen all of the available data). The one drawback of this method is that it is susceptible to “hash collisions”: two different words may end up with the same hash, and subsequently any machine learning model looking at these hashes won’t be able to tell the difference between these words. The likelihood of hash collisions decreases when the dimensionality of the hashing space is much larger than the total number of unique tokens being hashed.</p>
<p>Word-level one-hot encoding with hashing trick (toy example):</p>
<pre class="r"><code>hashing_results &lt;- text_hashing_trick(text[1], n = 100)
hashing_results</code></pre>
<pre><code>## [1] 88 75 18 90  7 90 23</code></pre>
<p>Using pre-trained word embeddings Sometimes, you have so little training data available that could never use your data alone to learn an appropriate task-specific embedding of your vocabulary. What to do then?</p>
<p>Instead of learning word embeddings jointly with the problem you want to solve, you could be loading embedding vectors from a pre-computed embedding space known to be highly structured and to exhibit useful properties – that captures generic aspects of language structure. The rationale behind using pre-trained word embeddings in natural language processing is very much the same as for using pre-trained convnets in image classification: we don’t have enough data available to learn truly powerful features on our own, but we expect the features that we need to be fairly generic, i.e. common visual features or semantic features. In this case it makes sense to reuse features learned on a different problem.</p>
<p>Such word embeddings are generally computed using word occurrence statistics (observations about what words co-occur in sentences or documents), using a variety of techniques, some involving neural networks, others not. The idea of a dense, low-dimensional embedding space for words, computed in an unsupervised way, was initially explored by Bengio et al. in the early 2000s, but it only started really taking off in research and industry applications after the release of one of the most famous and successful word embedding scheme: the Word2Vec algorithm, developed by Mikolov at Google in 2013. Word2Vec dimensions capture specific semantic properties, e.g. gender.</p>
<p>There are various pre-computed databases of word embeddings that can download and start using in a Keras embedding layer. Word2Vec is one of them. Another popular one is called “GloVe”, developed by Stanford researchers in 2014. It stands for “Global Vectors for Word Representation”, and it is an embedding technique based on factorizing a matrix of word co-occurrence statistics. Its developers have made available pre-computed embeddings for millions of English tokens, obtained from Wikipedia data or from Common Crawl data.</p>
<p>Let’s take a look at how you can get started using GloVe embeddings in a Keras model. The same method will of course be valid for Word2Vec embeddings or any other word embedding database that you can download. We will also use this example to refresh the text tokenization techniques we introduced a few paragraphs ago: we will start from raw text, and work our way up.</p>
<p><a href="https://jjallaire.github.io/deep-learning-with-r-notebooks/notebooks/6.1-using-word-embeddings.nb.html" class="uri">https://jjallaire.github.io/deep-learning-with-r-notebooks/notebooks/6.1-using-word-embeddings.nb.html</a></p>
<p><a href="https://www.r-bloggers.com/word-embeddings-with-keras/" class="uri">https://www.r-bloggers.com/word-embeddings-with-keras/</a></p>
<blockquote>
<p>Word embedding is a method used to map words of a vocabulary to dense vectors of real numbers where semanticaly similar words are mapped to nearby points. Representing words in this vector space help algorithms achieve better performance in natural language processing tasks like syntatic parsing and sentiment analysis by grouping similar words. For example, we expect that in the embedding space “cats” and “dogs” are mapped to nearby points since they are both animals, mammals, pets, etc. In this tutorial we will implement the skip-gram model created by Mikolov et al in R using the keras package. The skip-gram model is a flavor of word2vec, a class of computationally-efficient predictive models for learning word embeddings from raw text.</p>
</blockquote>
<blockquote>
<p>You can’t feed lists of integers into a neural network. You have to turn your lists into tensors. There are two ways to do that: - Pad your lists so that they all have the same length, turn them into an integer tensor of shape (samples, word_indices), and then use as the first layer in your network a layer capable of handling such integer tensors (the “embedding” layer, which we’ll cover in detail later in the book). - One-hot-encode your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vector that would be all zeros except for indices 3 and 5, which would be ones. Then you could use as the first layer in your network a dense layer, capable of handling floating-point vector data.</p>
</blockquote>
</div>
<div id="use-pretrained-glove-embeddings-separate-post" class="section level2">
<h2>Use pretrained GLove embeddings (separate post)</h2>
<p><a href="https://keras.rstudio.com/articles/examples/pretrained_word_embeddings.html" class="uri">https://keras.rstudio.com/articles/examples/pretrained_word_embeddings.html</a></p>
<pre class="r"><code>library(keras)

GLOVE_DIR &lt;- &#39;/Users/shiringlander/Documents/Github/Data/glove.6B&#39;
TEXT_DATA_DIR &lt;- &#39;/Users/shiringlander/Documents/Github/Data/20_newsgroup&#39;
MAX_SEQUENCE_LENGTH &lt;- 1000
MAX_NUM_WORDS &lt;- 20000
EMBEDDING_DIM &lt;- 100
VALIDATION_SPLIT &lt;- 0.2

# download data if necessary
download_data &lt;- function(data_dir, url_path, data_file) {
  if (!dir.exists(data_dir)) {
    download.file(paste0(url_path, data_file), data_file, mode = &quot;wb&quot;)
    if (tools::file_ext(data_file) == &quot;zip&quot;)
      unzip(data_file, exdir = tools::file_path_sans_ext(data_file))
    else
      untar(data_file)
    unlink(data_file)
  }
}
download_data(GLOVE_DIR, &#39;http://nlp.stanford.edu/data/&#39;, &#39;glove.6B.zip&#39;)
download_data(TEXT_DATA_DIR, &quot;http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/&quot;, &quot;news20.tar.gz&quot;)

# first, build index mapping words in the embeddings set
# to their embedding vector

cat(&#39;Indexing word vectors.\n&#39;)</code></pre>
<pre><code>## Indexing word vectors.</code></pre>
<pre class="r"><code>embeddings_index &lt;- new.env(parent = emptyenv())
lines &lt;- readLines(file.path(GLOVE_DIR, &#39;glove.6B.100d.txt&#39;))
for (line in lines) {
  values &lt;- strsplit(line, &#39; &#39;, fixed = TRUE)[[1]]
  word &lt;- values[[1]]
  coefs &lt;- as.numeric(values[-1])
  embeddings_index[[word]] &lt;- coefs
}

cat(sprintf(&#39;Found %s word vectors.\n&#39;, length(embeddings_index)))</code></pre>
<pre><code>## Found 400000 word vectors.</code></pre>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS  10.14.1
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] bindrcpp_0.2.2  forcats_0.3.0   stringr_1.3.1   dplyr_0.7.7    
##  [5] purrr_0.2.5     readr_1.1.1     tidyr_0.8.2     tibble_1.4.2   
##  [9] ggplot2_3.1.0   tidyverse_1.2.1 keras_2.2.0    
## 
## loaded via a namespace (and not attached):
##  [1] reticulate_1.10  tidyselect_0.2.5 xfun_0.4         haven_1.1.2     
##  [5] lattice_0.20-38  colorspace_1.3-2 htmltools_0.3.6  yaml_2.2.0      
##  [9] base64enc_0.1-3  rlang_0.3.0.1    pillar_1.3.0     withr_2.1.2     
## [13] glue_1.3.0       readxl_1.1.0     modelr_0.1.2     bindr_0.1.1     
## [17] plyr_1.8.4       tensorflow_1.9   cellranger_1.1.0 munsell_0.5.0   
## [21] blogdown_0.9     gtable_0.2.0     rvest_0.3.2      evaluate_0.12   
## [25] labeling_0.3     knitr_1.20       tfruns_1.4       broom_0.5.0     
## [29] Rcpp_0.12.19     backports_1.1.2  scales_1.0.0     jsonlite_1.5    
## [33] hms_0.4.2        digest_0.6.18    stringi_1.2.4    bookdown_0.7    
## [37] grid_3.5.1       rprojroot_1.3-2  cli_1.0.1        tools_3.5.1     
## [41] magrittr_1.5     lazyeval_0.2.1   crayon_1.3.4     whisker_0.3-2   
## [45] pkgconfig_2.0.2  zeallot_0.1.0    Matrix_1.2-15    xml2_1.2.0      
## [49] lubridate_1.7.4  rstudioapi_0.8   assertthat_0.2.0 rmarkdown_1.10  
## [53] httr_1.3.1       R6_2.3.0         nlme_3.1-137     compiler_3.5.1</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[&#39;How do neural nets learn?&#39; A step by step explanation using the H2O Deep Learning algorithm.]]></title>
    <link href="/2018/11/neural_nets_explained/"/>
    <id>/2018/11/neural_nets_explained/</id>
    <published>2018-11-06T00:00:00+00:00</published>
    <updated>2018-11-06T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>In my <a href="https://shirinsplayground.netlify.com/2018/10/ml_basics_rf/">last blogpost about Random Forests</a> I introduced the <a href="https://www.codecentric.de/kuenstliche-intelligenz/">codecentric.ai Bootcamp</a>. The next part I published was about <strong>Neural Networks</strong> and <strong>Deep Learning</strong>. Every video of our bootcamp will have example code and tasks to promote hands-on learning. While the practical parts of the bootcamp will be using Python, below you will find the English R version of this <strong>Neural Nets Practical Example</strong>, where I explain how neural nets learn and how the concepts and techniques translate to training neural nets in R with the <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/deep-learning.html">H2O Deep Learning function</a>.</p>
<p>You can find the video on <a href="https://youtu.be/Z42fE0MGoDQ">YouTube</a> but as, as before, it is only available in German. Same goes for the <a href="https://codecentric.slides.com/shiringlander/dl_bootcamp_nn">slides</a>, which are also currently German only. See the end of this article for the embedded video and slides.</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/nn_explained/neural_nets_explained.png" />

</div>
<hr />
<div id="neural-nets-and-deep-learning" class="section level2">
<h2>Neural Nets and Deep Learning</h2>
<p>Just like <a href="https://shirinsplayground.netlify.com/2018/10/ml_basics_rf/">Random Forests</a>, neural nets are a method for machine learning and can be used for supervised, unsupervised and reinforcement learning. The idea behind neural nets has already been developed back in the 1940s as a way to mimic how our human brain learns. That’s way neural nets in machine learning are also called ANNs (Artificial Neural Networks).</p>
<p>When we say <strong>Deep Learning</strong>, we talk about big and complex neural nets, which are able to solve complex tasks, like image or language understanding. Deep Learning has gained traction and success particularly with the recent developments in GPUs and TPUs (Tensor Processing Units), the increase in computing power and data in general, as well as the development of easy-to-use frameworks, like Keras and TensorFlow. We find Deep Learning in our everyday lives, e.g. in voice recognition, computer vision, recommender systems, reinforcement learning and many more.</p>
<p>The easiest type of ANN has only node (also called neuron) and is called <strong>perceptron</strong>. Incoming data flows into this neuron, where a result is calculated, e.g. by summing up all incoming data. Each of the incoming data points is multiplied with a weight; <strong>weights</strong> can basically be any number and are used to modify the results that are calculated by a neuron: if we change the weight, the result will change also. Optionally, we can add a so called bias to the data points to modify the results even further.</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/nn_explained/perceptron.jpg" />

</div>
<p>But how do neural nets learn? Below, I will show with an example that uses common techniques and principles.</p>
</div>
<div id="libraries" class="section level2">
<h2>Libraries</h2>
<p>First, we will load all the packages we need:</p>
<ul>
<li><em>tidyverse</em> for data wrangling and plotting</li>
<li><em>readr</em> for reading in a csv</li>
<li><em>h2o</em> for Deep Learning (<code>h2o.init</code> initializes the cluster)</li>
</ul>
<pre class="r"><code>library(tidyverse)
library(readr)
library(h2o)
h2o.init(nthreads = -1)</code></pre>
<pre><code>##  Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         3 hours 46 minutes 
##     H2O cluster timezone:       Europe/Berlin 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.20.0.8 
##     H2O cluster version age:    1 month and 16 days  
##     H2O cluster name:           H2O_started_from_R_shiringlander_jpa775 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   3.16 GB 
##     H2O cluster total cores:    8 
##     H2O cluster allowed cores:  8 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.1 (2018-07-02)</code></pre>
</div>
<div id="data" class="section level2">
<h2>Data</h2>
<p>The dataset used in this example is a <a href="https://www.kaggle.com/blastchar/telco-customer-churn">customer churn dataset from Kaggle</a>.</p>
<blockquote>
<p>Each row represents a customer, each column contains customer’s attributes</p>
</blockquote>
<p>We will load the data from a csv file:</p>
<pre class="r"><code>telco_data &lt;- read_csv(&quot;../../../Data/Churn/WA_Fn-UseC_-Telco-Customer-Churn.csv&quot;)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   .default = col_character(),
##   SeniorCitizen = col_integer(),
##   tenure = col_integer(),
##   MonthlyCharges = col_double(),
##   TotalCharges = col_double()
## )</code></pre>
<pre><code>## See spec(...) for full column specifications.</code></pre>
<pre class="r"><code>head(telco_data)</code></pre>
<pre><code>## # A tibble: 6 x 21
##   customerID gender SeniorCitizen Partner Dependents tenure PhoneService
##   &lt;chr&gt;      &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;int&gt; &lt;chr&gt;       
## 1 7590-VHVEG Female             0 Yes     No              1 No          
## 2 5575-GNVDE Male               0 No      No             34 Yes         
## 3 3668-QPYBK Male               0 No      No              2 Yes         
## 4 7795-CFOCW Male               0 No      No             45 No          
## 5 9237-HQITU Female             0 No      No              2 Yes         
## 6 9305-CDSKC Female             0 No      No              8 Yes         
## # ... with 14 more variables: MultipleLines &lt;chr&gt;, InternetService &lt;chr&gt;,
## #   OnlineSecurity &lt;chr&gt;, OnlineBackup &lt;chr&gt;, DeviceProtection &lt;chr&gt;,
## #   TechSupport &lt;chr&gt;, StreamingTV &lt;chr&gt;, StreamingMovies &lt;chr&gt;,
## #   Contract &lt;chr&gt;, PaperlessBilling &lt;chr&gt;, PaymentMethod &lt;chr&gt;,
## #   MonthlyCharges &lt;dbl&gt;, TotalCharges &lt;dbl&gt;, Churn &lt;chr&gt;</code></pre>
<p>Let’s quickly examine the data by plotting density distributions of numeric variables…</p>
<pre class="r"><code>telco_data %&gt;%
  select_if(is.numeric) %&gt;%
  gather() %&gt;%
  ggplot(aes(x = value)) +
    facet_wrap(~ key, scales = &quot;free&quot;, ncol = 4) +
    geom_density()</code></pre>
<pre><code>## Warning: Removed 11 rows containing non-finite values (stat_density).</code></pre>
<p><img src="/post/2018-11-06_neural_nets_explained_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>… and barcharts for categorical variables.</p>
<pre class="r"><code>telco_data %&gt;%
  select_if(is.character) %&gt;%
  select(-customerID) %&gt;%
  gather() %&gt;%
  ggplot(aes(x = value)) +
    facet_wrap(~ key, scales = &quot;free&quot;, ncol = 3) +
    geom_bar()</code></pre>
<p><img src="/post/2018-11-06_neural_nets_explained_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Before we can work with h2o, we need to convert our data into an h2o frame object. Note, that I am also converting character columns to categorical columns, otherwise h2o will ignore them. Moreover, we will need our response variable to be in categorical format in order to perform classification on this data.</p>
<pre class="r"><code>hf &lt;- telco_data %&gt;%
  mutate_if(is.character, as.factor) %&gt;%
  as.h2o</code></pre>
<p>Next, I’ll create a vector of the feature names I want to use for modeling (I am leaving out the customer ID because it doesn’t add useful information about customer churn).</p>
<pre class="r"><code>hf_X &lt;- colnames(telco_data)[2:20]
hf_X</code></pre>
<pre><code>##  [1] &quot;gender&quot;           &quot;SeniorCitizen&quot;    &quot;Partner&quot;         
##  [4] &quot;Dependents&quot;       &quot;tenure&quot;           &quot;PhoneService&quot;    
##  [7] &quot;MultipleLines&quot;    &quot;InternetService&quot;  &quot;OnlineSecurity&quot;  
## [10] &quot;OnlineBackup&quot;     &quot;DeviceProtection&quot; &quot;TechSupport&quot;     
## [13] &quot;StreamingTV&quot;      &quot;StreamingMovies&quot;  &quot;Contract&quot;        
## [16] &quot;PaperlessBilling&quot; &quot;PaymentMethod&quot;    &quot;MonthlyCharges&quot;  
## [19] &quot;TotalCharges&quot;</code></pre>
<p>I am doing the same for the response variable:</p>
<pre class="r"><code>hf_y &lt;- colnames(telco_data)[21]
hf_y</code></pre>
<pre><code>## [1] &quot;Churn&quot;</code></pre>
<p>Now, we are ready to use the <code>h2o.deeplearning</code> function, which has a number of arguments and hyperparameters, which we can set to define our neural net and the learning process. The most essential arguments are</p>
<ul>
<li><code>y</code>: the name of the response variable</li>
<li><code>training_frame</code>: the training data</li>
<li><code>x</code>: the vector of features to use is optional and all remaining columns would be used if we don’t specify it, but since we don’t want to include customer id in our model, we need to give this vector here.</li>
</ul>
<p>The <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/deep-learning.html#defining-a-deep-learning-model">H2O documentation for the Deep Learning function</a> gives an overview over all arguments and hyperparameters you can use. I will explain a few of the most important ones.</p>
<pre class="r"><code>dl_model &lt;- h2o.deeplearning(x = hf_X,
                             y = hf_y,
                             training_frame = hf)</code></pre>
</div>
<div id="activation-functions" class="section level2">
<h2>Activation functions</h2>
<p>Before, when describing the simple <strong>perceptron</strong>, I said that a result is calculated in a neuron, e.g. by summing up all the incoming data multiplied by weights. However, this has one big disadvantage: such an approach would only enable our neural net to learn <strong>linear</strong> relationships between data. In order to be able to learn (you can also say approximate) any mathematical problem - no matter how complex - we use <strong>activation functions</strong>. Activation functions normalize the output of a neuron, e.g. to values between -1 and 1, (Tanh), 0 and 1 (Sigmoid) or by setting negative values to 0 (Rectified Linear Units, ReLU). In H2O we can choose between Tanh, Tanh with Dropout, Rectifier (default), Rectifier with Dropout, Maxout and Maxout with Dropout. Let’s choose Rectifier with Dropout. <strong>Dropout</strong> is used to improve the generalizability of neural nets by randomly setting a given proportion of nodes to 0. The dropout rate in H2O is specified with two arguments: <code>hidden_dropout_ratios</code>, which per default sets 50% of hidden (more on that in a minute) nodes to 0. Here, I want to reduce that proportion to 20% but let’s talk about hidden layers and hidden nodes first. In addition to hidden dropout, H2O let’s us specify a dropout for the input layer with <code>input_dropout_ratio</code>. This argument is deactivated by default and this is how we will leave it.</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/nn_explained/activation_functions.jpg" />

</div>
<pre class="r"><code>dl_model &lt;- h2o.deeplearning(x = hf_X,
                             y = hf_y,
                             training_frame = hf,
                             activation = &quot;RectifierWithDropout&quot;)</code></pre>
</div>
<div id="hidden-layers" class="section level2">
<h2>Hidden layers</h2>
<p>In more complex neural nets, neurons are arranged in layers. The first layer is the input layer with our data that is flowing into the neural net. Then we have a number of <strong>hidden layers</strong> and finally an output layer with the final prediction of our neural net. There are many different types and architectures for neural nets, like LSTMs, CNNs, GANs, etc. A simple architecture is the <strong>Multi-Layer-Perceptron (MLP)</strong> in which every node is connected to all other nodes in the preceding and the following layers; such layers are also called dense layers. We can train such an MLP with H2O by specifying the number hidden layers and the number of nodes in each hidden layer with the <code>hidden</code> argument. Let’s try out three hidden layers with 100, 80 and 100 nodes in each. Now, we can define the <code>hidden_dropout_ratios</code> for every hidden layer with 20% dropout.</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/mlp_r7pv7z.jpg" />

</div>
<pre class="r"><code>dl_model &lt;- h2o.deeplearning(x = hf_X,
                             y = hf_y,
                             training_frame = hf,
                             activation = &quot;RectifierWithDropout&quot;,
                             hidden = c(100, 80, 100),
                             hidden_dropout_ratios = c(0.2, 0.2, 0.2))</code></pre>
</div>
<div id="learning-through-optimisation" class="section level2">
<h2>Learning through optimisation</h2>
<p>Just as in the simple perceptron from the beginning, our MLP has <strong>weights</strong> associated with every node. And just as in the simple perceptron, we want to find the most optimal combination of weights in order to get a desired result from the output layer. The desired result in a supervised learning task is for the neural net to calculate predictions that are as close to reality as possible.</p>
<p>How that works has a lot to do with mathematical optimization and the following concepts:</p>
<ul>
<li>difference between prediction and reality</li>
<li>loss functions</li>
<li>backpropagation</li>
<li>optimization methods, like gradient descent</li>
</ul>
<div id="difference-between-prediction-and-reality" class="section level3">
<h3>Difference between prediction and reality</h3>
<p>When our network calculates a predictions, what the output nodes will return before applying an activation function is a numeric value of any size - this is called the score. Just as before, we will now apply an activation function to this score in order to normalize it. In the output of a classification task, we would like to get values between 0 and 1, that’s why we most commonly use the <strong>softmax</strong> function; this function converts the scores for every possible outcome/class into a probability distribution with values between 0 and 1 and a sum of 1.</p>
</div>
<div id="one-hot-encoding" class="section level3">
<h3>One-Hot-Encoding</h3>
<p>In order to compare this probability distribution of predictions with the true outcome/class, we use a special format to encode our outcome: <strong>One-Hot-Encoding</strong>. For every instance, we will generate a vector with either 0 or 1 for every possible class: the true class of the instance will get a 1, all other classes will get 0s. This one-hot-encoded vector now looks very similar to a probability distribution: it contains values between 0 and 1 and sums up to 1. In fact, our one-hot-encoded vector looks like the probability distribution if our network had predicted the correct class with 100% certainty!</p>
<p>We can now use this similarity between probability distribution and one-hot-encoded vector to calculate the difference between the two; this difference tells us how close to reality our network is: if the difference is small, it is close to reality, if the difference is big, it wasn’t very accurate in predicting the outcome. The goal of the learning process is now to find the combination of weights that make the difference between probability distribution and one-hot-encoded vector as small as possible.</p>
<p>One-Hot-Encoding will also be applied to categorical feature variables, because our neural nets need numeric values to learn from - strings and categories in their raw format are not useful per se. Fortunately, many machine learning packages - H2O being one of them - perform this one-hot-encoding automatically in the background for you. We could still change the <code>categorical_encoding</code> argument to “Enum”, “OneHotInternal”, “OneHotExplicit”, “Binary”, “Eigen”, “LabelEncoder”, “SortByResponse” and “EnumLimited” but we will leave the default setting of “AUTO”.</p>
</div>
<div id="loss-functions" class="section level3">
<h3>Loss-Functions</h3>
<p>This minimization of the difference between prediction and reality for the entire training set is also called <strong>minimising the loss function</strong>. There are many different loss functions (and in some cases, you will even write your own specific loss function), in H2O we have “CrossEntropy”, “Quadratic”, “Huber”, “Absolute” and “Quantile”. In classification tasks, the loss function we want to minimize is usually <strong>cross-entropy</strong>.</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/nn_explained/cross-entropy.jpg" />

</div>
<pre class="r"><code>dl_model &lt;- h2o.deeplearning(x = hf_X,
                             y = hf_y,
                             training_frame = hf,
                             activation = &quot;RectifierWithDropout&quot;,
                             hidden = c(100, 80, 100),
                             hidden_dropout_ratios = c(0.2, 0.2, 0.2),
                             loss = &quot;CrossEntropy&quot;)</code></pre>
</div>
<div id="backpropagation" class="section level3">
<h3>Backpropagation</h3>
<p>With backpropagation, the calculated error (from the cross entropy in our case) will be propagated back through the network to calculate the proportion of error for every neuron. Based on this proportion of error, we get an error landscape for every neuron. This error landscape can be thought of as a hilly landscape in the Alps, for example. The positions in this landscape are different weights, so that we get weights with high error at the peaks and weights with low error in valleys. In order to minimize the error, we want to find the position in this landscape (i.e. the weight) that is in the deepest valley.</p>
</div>
<div id="gradient-descent" class="section level3">
<h3>Gradient descent</h3>
<p>Let’s imagine we were a hiker, who is left at a random place in this landscape - while being blindfolded - and we are tasked with finding our way to this valley. We would start by feeling around and looking for the direction with the steepest downward slope. This is also what our neural net does, just that this “feeling around” is called “calculating the <strong>gradient</strong>”. And just as we would then make a step in that direction with the steepest downwards slope, our neural net makes a step in the direction with the steepest gradient. This is called <strong>gradient descent</strong>. This procedure will be repeated until we find a place, where we can’t go any further down. In our neural net, this number of repeated rounds is called the number of <strong>epochs</strong> and we define it in H2O with the <code>epochs</code> argument.</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/nn_explained/gradient_descent.jpg" />

</div>
<pre class="r"><code>dl_model &lt;- h2o.deeplearning(x = hf_X,
                             y = hf_y,
                             training_frame = hf,
                             activation = &quot;RectifierWithDropout&quot;,
                             hidden = c(100, 80, 100),
                             hidden_dropout_ratios = c(0.2, 0.2, 0.2),
                             loss = &quot;CrossEntropy&quot;,
                             epochs = 200)</code></pre>
</div>
<div id="adaptive-learning-rate-and-momentum" class="section level3">
<h3>Adaptive learning rate and momentum</h3>
<p>One common problem with this simple approach is, that we might end up in a place, where there is no direction to go down any more but the steepest valley in the entire landscape is somewhere else. In our neural net, this would be called getting stuck in local minima or on saddle points. In order to be able to overcome these points and find the <strong>global minimum</strong>, several advanced techniques have been developed in recent years.</p>
<p>One of them is the <strong>adaptive learning rate</strong>. Learning rate can be though of as the step size of our hiker. In H2O, this is defined with the <code>rate</code> argument. With an adaptive learning rate, we can e.g. start out with a big learning rate reduce it the closer we get to the end of our model training run. If you wanted to have an adaptive learning rate in your neural net, you would use the following functions in H2O: <code>adaptive_rate = TRUE</code>, <code>rho</code> (rate of learning rate decay) and <code>epsilon</code> (a smoothing factor).</p>
<p>In this example, I am not using adaptive learning rate, however, but something called <strong>momentum</strong>. If you imagine a ball being pushed from some point in the landscape, it will gain momentum that propels it into a general direction and won’t make big jumps into opposing directions. This principle is applied with momentum in neural nets. In our H2O function we define <code>momentum_start</code> (momentum at the beginning of training), <code>momentum_ramp</code> (number of instances for which momentum is supposed to increase) and <code>momentum_stable</code> (final momentum).</p>
<p>The <strong>Nesterov accelerated gradient</strong> is an addition to momentum in which the next step is guessed before taking it. In this way, the previous error will influence the direction of the next step.</p>
<pre class="r"><code>dl_model &lt;- h2o.deeplearning(x = hf_X,
                             y = hf_y,
                             training_frame = hf,
                             activation = &quot;RectifierWithDropout&quot;,
                             hidden = c(100, 80, 100),
                             hidden_dropout_ratios = c(0.2, 0.2, 0.2),
                             loss = &quot;CrossEntropy&quot;,
                             epochs = 200,
                             rate = 0.005,
                             adaptive_rate = FALSE,
                             momentum_start = 0.5,
                             momentum_ramp = 100,
                             momentum_stable = 0.99,
                             nesterov_accelerated_gradient = TRUE)</code></pre>
</div>
<div id="l1-and-l2-regularization" class="section level3">
<h3>L1 and L2 Regularization</h3>
<p>In addition to <strong>dropout</strong>, we can apply <strong>regularization</strong> to improve the generalizability of our neural network. In H2O, we can use L1 and L2 regularization. With L1, many nodes will be set to 0 and with L2 many nodes will get low weights.</p>
<pre class="r"><code>dl_model &lt;- h2o.deeplearning(x = hf_X,
                             y = hf_y,
                             training_frame = hf,
                             activation = &quot;RectifierWithDropout&quot;,
                             hidden = c(100, 80, 100),
                             hidden_dropout_ratios = c(0.2, 0.2, 0.2),
                             loss = &quot;CrossEntropy&quot;,
                             epochs = 200,
                             rate = 0.005,
                             adaptive_rate = TRUE,
                             momentum_start = 0.5,
                             momentum_ramp = 100,
                             momentum_stable = 0.99,
                             nesterov_accelerated_gradient = TRUE,
                             l1 = 0,
                             l2 = 0)</code></pre>
</div>
</div>
<div id="crossvalidation" class="section level2">
<h2>Crossvalidation</h2>
<p>Now you know the main principles of how neural nets learn!</p>
<p>Before we finally train our model, we want to have a way to judge how well our model learned and if learning improves over the epochs. Here, we want to use validation data, to make these performance measures less biased compared to using the training data. In H2O, we could either give a specific validation set with <code>validation_frame</code> or we use <strong>crossvalidation</strong>. The argument <code>nfolds</code> specifies how many folds shall be generated and with <code>fold_assignment = &quot;Stratified&quot;</code> we tell H2O to make sure to keep the class ratios of our response variable the same in all folds. We also specify to save the cross validation predictions for examination.</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/nn_explained/validation.jpg" />

</div>
<pre class="r"><code>dl_model &lt;- h2o.deeplearning(x = hf_X,
                             y = hf_y,
                             training_frame = hf,
                             activation = &quot;RectifierWithDropout&quot;,
                             hidden = c(100, 80, 100),
                             hidden_dropout_ratios = c(0.2, 0.2, 0.2),
                             loss = &quot;CrossEntropy&quot;,
                             epochs = 200,
                             rate = 0.005,
                             adaptive_rate = FALSE,
                             momentum_start = 0.5,
                             momentum_ramp = 100,
                             momentum_stable = 0.99,
                             nesterov_accelerated_gradient = TRUE,
                             l1 = 0,
                             l2 = 0,
                             nfolds = 3,
                             fold_assignment = &quot;Stratified&quot;,
                             keep_cross_validation_predictions = TRUE,
                             balance_classes = TRUE,
                             seed = 42)</code></pre>
<p>Finally, we are balancing the class proportions and we set a seed for pseudo-number-generation - and we train our model!</p>
<p>The resulting model object can be used just like any other H2O model: for predicting new/test data, for calculating performance metrics, saving it, or plotting the training results:</p>
<pre class="r"><code>plot(dl_model)</code></pre>
<p><img src="/post/2018-11-06_neural_nets_explained_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>The crossvalidation predictions can be accessed with <code>h2o.cross_validation_predictions()</code> (returns one data frame for every fold) and <code>h2o.cross_validation_holdout_predictions()</code> (combined in one data frame).</p>
<pre class="r"><code>h2o.cross_validation_predictions(dl_model)</code></pre>
<pre><code>## [[1]]
##   predict        No        Yes
## 1      No 0.0000000 0.00000000
## 2      No 0.9784236 0.02157642
## 3      No 0.0000000 0.00000000
## 4      No 0.9808329 0.01916711
## 5     Yes 0.1512552 0.84874476
## 6     Yes 0.1590060 0.84099396
## 
## [7043 rows x 3 columns] 
## 
## [[2]]
##   predict        No       Yes
## 1     Yes 0.2484017 0.7515983
## 2      No 0.0000000 0.0000000
## 3      No 0.8107972 0.1892028
## 4      No 0.0000000 0.0000000
## 5      No 0.0000000 0.0000000
## 6      No 0.0000000 0.0000000
## 
## [7043 rows x 3 columns] 
## 
## [[3]]
##   predict No Yes
## 1      No  0   0
## 2      No  0   0
## 3      No  0   0
## 4      No  0   0
## 5      No  0   0
## 6      No  0   0
## 
## [7043 rows x 3 columns]</code></pre>
<pre class="r"><code>h2o.cross_validation_holdout_predictions(dl_model)</code></pre>
<pre><code>##   predict        No        Yes
## 1     Yes 0.2484017 0.75159829
## 2      No 0.9784236 0.02157642
## 3      No 0.8107972 0.18920283
## 4      No 0.9808329 0.01916711
## 5     Yes 0.1512552 0.84874476
## 6     Yes 0.1590060 0.84099396
## 
## [7043 rows x 3 columns]</code></pre>
<hr />
</div>
<div id="video" class="section level1">
<h1>Video</h1>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Z42fE0MGoDQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
</div>
<div id="slides" class="section level1">
<h1>Slides</h1>
<iframe src="//codecentric.slides.com/shiringlander/dl_bootcamp_nn/embed" width="576" height="420" scrolling="no" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen>
</iframe>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS  10.14
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] bindrcpp_0.2.2  h2o_3.20.0.8    forcats_0.3.0   stringr_1.3.1  
##  [5] dplyr_0.7.7     purrr_0.2.5     readr_1.1.1     tidyr_0.8.1    
##  [9] tibble_1.4.2    ggplot2_3.1.0   tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_0.2.5 xfun_0.4         haven_1.1.2      lattice_0.20-35 
##  [5] colorspace_1.3-2 htmltools_0.3.6  yaml_2.2.0       utf8_1.1.4      
##  [9] rlang_0.3.0.1    pillar_1.3.0     glue_1.3.0       withr_2.1.2     
## [13] modelr_0.1.2     readxl_1.1.0     bindr_0.1.1      plyr_1.8.4      
## [17] munsell_0.5.0    blogdown_0.9     gtable_0.2.0     cellranger_1.1.0
## [21] rvest_0.3.2      evaluate_0.12    labeling_0.3     knitr_1.20      
## [25] fansi_0.4.0      broom_0.5.0      Rcpp_0.12.19     scales_1.0.0    
## [29] backports_1.1.2  jsonlite_1.5     hms_0.4.2        digest_0.6.18   
## [33] stringi_1.2.4    bookdown_0.7     grid_3.5.1       rprojroot_1.3-2 
## [37] bitops_1.0-6     cli_1.0.1        tools_3.5.1      magrittr_1.5    
## [41] lazyeval_0.2.1   RCurl_1.95-4.11  crayon_1.3.4     pkgconfig_2.0.2 
## [45] xml2_1.2.0       lubridate_1.7.4  assertthat_0.2.0 rmarkdown_1.10  
## [49] httr_1.3.1       rstudioapi_0.8   R6_2.3.0         nlme_3.1-137    
## [53] compiler_3.5.1</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Machine Learning Basics - Random Forest]]></title>
    <link href="/2018/10/ml_basics_rf/"/>
    <id>/2018/10/ml_basics_rf/</id>
    <published>2018-10-30T00:00:00+00:00</published>
    <updated>2018-10-30T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>A few colleagues of mine and I from <a href="https://www.codecentric.de/kuenstliche-intelligenz/">codecentric.ai</a> are currently working on developing a free online course about machine learning and deep learning. As part of this course, I am developing a <a href="https://www.youtube.com/codecentricAI">series of videos</a> about machine learning basics - the first video in this series was about Random Forests.</p>
<p>You can find the video on <a href="https://youtu.be/ieF_QjVUNEQ">YouTube</a> but as of now, it is only available in German. Same goes for the <a href="https://codecentric.slides.com/shiringlander/ml_basics_rf">slides</a>, which are also currently German only.</p>
<p>I did however translate my script:</p>
<hr />
<div class="figure">
<img src="https://shiring.github.io/netlify_images/ml_basics_rf.png" />

</div>
<p>Random Forest (RF) is one of the many machine learning algorithms used for supervised learning, this means for learning from labelled data and making predictions based on the learned patterns. RF can be used for both classification and regression tasks.</p>
<div id="decision-trees" class="section level2">
<h2>Decision trees</h2>
<p>RF is based on decision trees. In machine learning decision trees are a technique for creating predictive models. They are called decision <strong>trees</strong> because the prediction follows several branches of “if… then…” decision splits - similar to the branches of a tree. If we imagine that we start with a sample, which we want to predict a class for, we would start at the bottom of a tree and travel up the trunk until we come to the first split-off branch. This split can be thought of as a feature in machine learning, let’s say it would be “age”; we would now make a decision about which branch to follow: “if our sample has an age bigger than 30, continue along the left branch, else continue along the right branch”. This we would do until we come to the next branch and repeat the same decision process until there are no more branches before us. This endpoint is called a leaf and in decision trees would represent the final result: a predicted class or value.</p>
<p>At each branch, the feature thresholds that best split the (remaining) samples locally is found. The most common metrics for defining the “best split” are <strong>gini impurity</strong> and <strong>information gain</strong> for classification tasks and <strong>variance reduction</strong> for regression.</p>
<p>Single decision trees are very easy to visualize and understand because they follow a method of decision-making that is very similar to how we humans make decisions: with a chain of simple rules. However, they are not very robust, i.e. they don’t generalize well to unseen samples. Here is where Random Forests come into play.</p>
</div>
<div id="ensemble-learning" class="section level2">
<h2>Ensemble learning</h2>
<p>RF makes predictions by combining the results from many individual decision trees - so we cal them a <strong>forest</strong> of decision trees. Because RF combines multiple models, it falls under the category of ensemble learning. Other ensemble learning methods are gradient boosting and stacked ensembles.</p>
</div>
<div id="combining-decision-trees" class="section level2">
<h2>Combining decision trees</h2>
<p>There are two main ways for combining the outputs of multiple decision trees into a random forest:</p>
<ol style="list-style-type: decimal">
<li>Bagging, which is also called Bootstrap aggregation (used in Random Forests)</li>
<li>Boosting (used in Gradient Boosting Machines)</li>
</ol>
<p>Bagging works the following way: decision trees are trained on randomly sampled subsets of the data, while sampling is being done with replacement. Bagging is the default method used with Random Forests. A big advantage of bagging over individual trees is that it decrease the variance of the model. Individual trees are very prone to overfitting and are very sensitive to noise in the data. As long as our individual trees are not correlated, combining them with bagging will make them more robust without increasing the bias. The part about correlation is important, though! We remove (most of) the correlation by randomly sampling subsets of data and training the different decision trees on this subsets instead of on the entire dataset. In addition to randomly sampling instances from our data, RF also uses <strong>feature bagging</strong>. With feature bagging, at each split in the decision tree only a random subset of features is considered. This technique reduces correlation even more because it helps reduce the impact of very strong predictor variables (i.e. features that have a very strong influence on predicting the target or response variable).</p>
<p>Boosting works similarly but with one major difference: the samples are weighted for sampling so that samples, which were predicted incorrectly get a higher weight and are therefore sampled more often. The idea behind this is that difficult cases should be emphasized during learning compared to easy cases. Because of this difference bagging can be easily paralleled, while boosting is performed sequentially.</p>
<p>The final result of our model is calculated by averaging over all predictions from these sampled trees or by majority vote.</p>
<div class="figure">
<img src="https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png" alt="https://commons.wikimedia.org/wiki/File:Random_forest_diagram_complete.png" />
<p class="caption"><a href="https://commons.wikimedia.org/wiki/File:Random_forest_diagram_complete.png" class="uri">https://commons.wikimedia.org/wiki/File:Random_forest_diagram_complete.png</a></p>
</div>
</div>
<div id="hyperparameters-to-be-tuned" class="section level2">
<h2>Hyperparameters to be tuned</h2>
<p>Hyperparameters are the arguments that can be set before training and which define how the training is done. The main hyperparameters in Random Forests are</p>
<ul>
<li>The number of decision trees to be combined</li>
<li>The maximum depth of the trees</li>
<li>The maximum number of features considered at each split</li>
<li>Whether bagging/bootstrapping is performed with or without replacement</li>
</ul>
</div>
<div id="training-random-forest-models" class="section level2">
<h2>Training Random Forest models</h2>
<p>Random Forest implementations are available in many machine learning libraries for R and Python, like <code>caret</code> (R, imports the <code>randomForest</code> and other RF packages), Scikit-learn (Python) and H2O (R and Python).</p>
<p>Examples in R can be found here: <a href="https://shirinsplayground.netlify.com/2018/06/intro_to_ml_workshop_heidelberg/" class="uri">https://shirinsplayground.netlify.com/2018/06/intro_to_ml_workshop_heidelberg/</a>.</p>
</div>
<div id="other-tree-based-machine-learning-algorithms" class="section level2">
<h2>Other tree-based machine learning algorithms</h2>
<p>The pros of Random Forests are that they are a relatively fast and powerful algorithm for classification and regression learning. Calculations can be parallelized and perform well on many problems, even with small datasets and the output returns prediction probabilities.</p>
<p>Downsides of Random Forests are that they are black-boxes, meaning that we can’t interpret the decisions made by the model because they are too complex. RF are also somewhat prone to overfitting and they tend to be bad at predicting underrepresented classes in unbalanced datasets.</p>
<p>Other tree-based algorithms are (Extreme) Gradient Boosting and Rotation Forests.</p>
<hr />
</div>
<div id="video" class="section level1">
<h1>Video</h1>
<iframe width="560" height="315" src="https://www.youtube.com/embed/ieF_QjVUNEQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
</div>
<div id="slides" class="section level1">
<h1>Slides</h1>
<iframe src="//codecentric.slides.com/shiringlander/ml_basics_rf/embed" width="576" height="420" scrolling="no" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen>
</iframe>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Slides from my talk at the R-Ladies Meetup about Interpretable Deep Learning with R, Keras and LIME]]></title>
    <link href="/2018/10/rladieslondon_slides/"/>
    <id>/2018/10/rladieslondon_slides/</id>
    <published>2018-10-17T00:00:00+00:00</published>
    <updated>2018-10-17T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>During my stay in London for the <a href="https://www.mcubed.london/sessions/explaining-complex-machine-learning-models-lime/">m3 conference</a>, I also gave a talk at the R-Ladies London Meetup on Tuesday, October 16th, about one of my favorite topics: <a href="https://www.meetup.com/de-DE/rladies-london/events/254581599/">Interpretable Deep Learning with R, Keras and LIME</a>.</p>
<blockquote>
<p>Keras is a high-level open-source deep learning framework that by default works on top of TensorFlow. Keras is minimalistic, efficient and highly flexible because it works with a modular layer system to define, compile and fit neural networks. It has been written in Python but can also be used from within R. Because the underlying backend can be changed from TensorFlow to Theano and CNTK (with more options being developed right now) it is designed to be framework-independent. Models can be trained on CPU or GPU, locally or in the cloud. Shirin will show an example of how to build an image classifier with Keras. We’ll be using a convolutional neural net to classify fruits in images. But that’s not all! We not only want to judge our black-box model based on accuracy and loss measures - we want to get a better understanding of how the model works. We will use an algorithm called LIME (local interpretable model-agnostic explanations) to find out what part of the different test images contributed most strongly to the classification that was made by our model. Shirin will introduce LIME and explain how it works. And finally, she will show how to apply LIME to the image classifier we built before, as well as to a pre-trained Imagenet model.</p>
</blockquote>
<div style="position:relative;width:100%;height:0;padding-bottom:calc(56.25% + 40px);">
<iframe allowfullscreen style="position:absolute; width: 100%; height: 100%;border: solid 1px #333;" src="https://www.beautiful.ai/player/-LMb1LCnguOqFZNKGx80/R-Ladies-London-Meetup?utm_source=beautiful_player&amp;utm_medium=embed&amp;utm_campaign=-LMb1LCnguOqFZNKGx80">
</iframe>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Slides from my m-cubed talk about Explaining complex machine learning models with LIME]]></title>
    <link href="/2018/10/mcubed_slides/"/>
    <id>/2018/10/mcubed_slides/</id>
    <published>2018-10-16T00:00:00+00:00</published>
    <updated>2018-10-16T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>The last two days, I was in London for the <a href="https://www.mcubed.london/">M-cubed conference</a>.</p>
<p>Here are the slides from my talk about <a href="https://www.mcubed.london/sessions/explaining-complex-machine-learning-models-lime/">Explaining complex machine learning models with LIME</a>:</p>
<blockquote>
<p>Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex machine learning models are essentially black boxes and too complicated to understand, we need to use approximations.</p>
</blockquote>
<div style="position:relative;width:100%;height:0;padding-bottom:calc(56.25% + 40px);">
<iframe allowfullscreen style="position:absolute; width: 100%; height: 100%;border: solid 1px #333;" src="https://www.beautiful.ai/player/-LMb2EcqL2my0oK1uOBw/m3-conference?utm_source=beautiful_player&amp;utm_medium=embed&amp;utm_campaign=-LMb2EcqL2my0oK1uOBw">
</iframe>
</div>
<p><br></p>
<p>I also took sketchnotes from the two keynote lectures:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/mcubed_keynote_1.JPG" alt="HUMAN MINDS AND MACHINE INTELLIGENCE – WHO’S THE MASTER? by Dr Joanna Bryson" />
<p class="caption">HUMAN MINDS AND MACHINE INTELLIGENCE – WHO’S THE MASTER? by Dr Joanna Bryson</p>
</div>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/mcubed_keynote_2.JPG" alt="A QUESTION OF MACHINE INTELLIGENCE – STRONG ARGUMENTS IN SUPPORT OF AND AGAINST DIFFERENT DEFINITIONS by Prof. Dagmar Monett Diaz" />
<p class="caption">A QUESTION OF MACHINE INTELLIGENCE – STRONG ARGUMENTS IN SUPPORT OF AND AGAINST DIFFERENT DEFINITIONS by Prof. Dagmar Monett Diaz</p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI: Evaluating Model Explainability Methods with Sara Hooker]]></title>
    <link href="/2018/10/twimlai189/"/>
    <id>/2018/10/twimlai189/</id>
    <published>2018-10-12T00:00:00+00:00</published>
    <updated>2018-10-12T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Evaluating Model Explainability Methods with Sara Hooker</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai189.jpg" alt="Sketchnotes from TWiMLAI talk: Evaluating Model Explainability Methods with Sara Hooker" />
<p class="caption">Sketchnotes from TWiMLAI talk: Evaluating Model Explainability Methods with Sara Hooker</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-189-evaluating-model-explainability-methods-with-sara-hooker/">here</a>.</p>
<iframe style="border: none" src="//html5-player.libsyn.com/embed/episode/id/7152485/height/90/theme/custom/autoplay/no/autonext/no/thumbnail/no/preload/no/no_addthis/no/direction/backward/render-playlist/no/custom-color/3e85b1/" height="90" width="100%" scrolling="no" allowfullscreen webkitallowfullscreen mozallowfullscreen oallowfullscreen msallowfullscreen>
</iframe>
<blockquote>
<p>In this, the first episode of the Deep Learning Indaba series, we’re joined by Sara Hooker, AI Resident at Google Brain. I had the pleasure of speaking with Sara in the run-up to the Indaba about her work on interpretability in deep neural networks. We discuss what interpretability means and when it’s important, and explore some nuances like the distinction between interpreting model decisions vs model function. We also dig into her paper Evaluating Feature Importance Estimates and look at the relationship between this work and interpretability approaches like LIME. We also talk a bit about Google, in particular, the relationship between Brain and the rest of the Google AI landscape and the significance of the recently announced Google AI Lab in Accra, Ghana, being led by friend of the show Moustapha Cisse. And, of course, we chat a bit about the Indaba as well. <a href="https://twimlai.com/twiml-talk-189-evaluating-model-explainability-methods-with-sara-hooker/" class="uri">https://twimlai.com/twiml-talk-189-evaluating-model-explainability-methods-with-sara-hooker/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Using R to help plan the future of transport. Join MünsteR for our next meetup!]]></title>
    <link href="/2018/10/meetup_nov18/"/>
    <id>/2018/10/meetup_nov18/</id>
    <published>2018-10-10T00:00:00+00:00</published>
    <updated>2018-10-10T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://shiring.github.io/r_users_group/2017/05/20/map.png" />

</div>
<p>In our <a href="http://meetu.ps/e/FWvp6/w54bW/f">next MünsteR R-user group meetup</a> on <strong>Tuesday, November 20th, 2018</strong>, titled <strong>Using R to help plan the future of transport</strong>, Mark Padgham will provide an overview of several inter-related R packages for analysing urban dynamics.</p>
<p>You can RSVP here: <a href="http://meetu.ps/e/FWvp6/w54bW/f">http://meetu.ps/e/F7zDN/w54bW/f</a></p>
<blockquote>
<p>The primary motivation for developing these packages has been their use in Active Transport Futures - a group of researchers and coders striving to aid cities to better plan for futures in which active travel, particularly walking and cycling, plays an increasingly prominent role (lots of open source code at github.com/ATFutures). We will take a close look at the packages “osmdata” and “dodgr”, the first of which imports Open Street Map (OSM) data directly into R, while the second is a very powerful, large-scale routing engine. These two packages enable accurate estimates of densities of different kinds of transport along all individual street segments of a city. The development of these packages has been partly motivated by their use in the Urban Health Agenda of the World Health Organization. We will also demonstrate a few other tools currently being developed for this work, including a brand new package to very easily set up a web server for large amounts of spatial data.</p>
</blockquote>
<div id="about-the-speaker" class="section level2">
<h2>About the speaker:</h2>
<blockquote>
<p>Mark Padgham has programmed C++ for many years, and started using R as a way to get his code noticed by people, which seems to have worked. Along with several others, he is in the process of establishing Active Transport Futures as a GmbH, and hopes that the company will play a significant role in aiding transitions away from environmentally and socially unsustainable modes of transport.</p>
</blockquote>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Image clustering with Keras and k-Means]]></title>
    <link href="/2018/10/keras_fruits_cluster/"/>
    <id>/2018/10/keras_fruits_cluster/</id>
    <published>2018-10-06T00:00:00+00:00</published>
    <updated>2018-10-06T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>A while ago, I wrote two blogposts about <a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits/">image classification with Keras</a> and about <a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits_lime/">how to use your own models or pretrained models for predictions and using LIME to explain to predictions</a>.</p>
<p>Recently, I came across <a href="https://medium.com/@franky07724_57962/using-keras-pre-trained-models-for-feature-extraction-in-image-clustering-a142c6cdf5b1">this blogpost</a> on using Keras to extract learned features from models and use those to cluster images. It is written in Python, though - so I adapted the code to R. You find the results below.</p>
<p>One use-case for image clustering could be that it can make labelling images easier because - ideally - the clusters would pre-sort your images, so that you only need to go over them quickly and check that they make sense.</p>
<p><br></p>
<div id="libraries" class="section level2">
<h2>Libraries</h2>
<p>Okay, let’s get started by loading the packages we need.</p>
<pre class="r"><code>library(keras)
library(magick)  # for preprocessing images
library(tidyverse)
library(imager)</code></pre>
<p><br></p>
</div>
<div id="pretrained-model" class="section level2">
<h2>Pretrained model</h2>
<p>And we load the VGG16 pretrained model but we exclude the laste layers.</p>
<pre class="r"><code>model &lt;- application_vgg16(weights = &quot;imagenet&quot;, 
                           include_top = FALSE)
model</code></pre>
<pre><code>## Model
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## input_1 (InputLayer)             (None, None, None, 3)         0           
## ___________________________________________________________________________
## block1_conv1 (Conv2D)            (None, None, None, 64)        1792        
## ___________________________________________________________________________
## block1_conv2 (Conv2D)            (None, None, None, 64)        36928       
## ___________________________________________________________________________
## block1_pool (MaxPooling2D)       (None, None, None, 64)        0           
## ___________________________________________________________________________
## block2_conv1 (Conv2D)            (None, None, None, 128)       73856       
## ___________________________________________________________________________
## block2_conv2 (Conv2D)            (None, None, None, 128)       147584      
## ___________________________________________________________________________
## block2_pool (MaxPooling2D)       (None, None, None, 128)       0           
## ___________________________________________________________________________
## block3_conv1 (Conv2D)            (None, None, None, 256)       295168      
## ___________________________________________________________________________
## block3_conv2 (Conv2D)            (None, None, None, 256)       590080      
## ___________________________________________________________________________
## block3_conv3 (Conv2D)            (None, None, None, 256)       590080      
## ___________________________________________________________________________
## block3_pool (MaxPooling2D)       (None, None, None, 256)       0           
## ___________________________________________________________________________
## block4_conv1 (Conv2D)            (None, None, None, 512)       1180160     
## ___________________________________________________________________________
## block4_conv2 (Conv2D)            (None, None, None, 512)       2359808     
## ___________________________________________________________________________
## block4_conv3 (Conv2D)            (None, None, None, 512)       2359808     
## ___________________________________________________________________________
## block4_pool (MaxPooling2D)       (None, None, None, 512)       0           
## ___________________________________________________________________________
## block5_conv1 (Conv2D)            (None, None, None, 512)       2359808     
## ___________________________________________________________________________
## block5_conv2 (Conv2D)            (None, None, None, 512)       2359808     
## ___________________________________________________________________________
## block5_conv3 (Conv2D)            (None, None, None, 512)       2359808     
## ___________________________________________________________________________
## block5_pool (MaxPooling2D)       (None, None, None, 512)       0           
## ===========================================================================
## Total params: 14,714,688
## Trainable params: 14,714,688
## Non-trainable params: 0
## ___________________________________________________________________________</code></pre>
<p><br></p>
</div>
<div id="training-images" class="section level2">
<h2>Training images</h2>
<p>Next, I am writting a helper function for reading in images and preprocessing them.</p>
<pre class="r"><code>image_prep &lt;- function(x) {
  arrays &lt;- lapply(x, function(path) {
    img &lt;- image_load(path, target_size = c(224, 224))
    x &lt;- image_to_array(img)
    x &lt;- array_reshape(x, c(1, dim(x)))
    x &lt;- imagenet_preprocess_input(x)
  })
  do.call(abind::abind, c(arrays, list(along = 1)))
}</code></pre>
<p>My images are in the following folder:</p>
<pre class="r"><code>image_files_path &lt;- &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Training&quot;
sample_fruits &lt;- sample(list.dirs(image_files_path), 2)</code></pre>
<p>Because running the clustering on all images would take very long, I am randomly sampling 5 image classes.</p>
<pre class="r"><code>file_list &lt;- list.files(sample_fruits, full.names = TRUE, recursive = TRUE)
head(file_list)</code></pre>
<pre><code>## [1] &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Training/Apple Red 2/0_100.jpg&quot;  
## [2] &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Training/Apple Red 2/1_100.jpg&quot;  
## [3] &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Training/Apple Red 2/10_100.jpg&quot; 
## [4] &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Training/Apple Red 2/100_100.jpg&quot;
## [5] &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Training/Apple Red 2/101_100.jpg&quot;
## [6] &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Training/Apple Red 2/102_100.jpg&quot;</code></pre>
<p>For each of these images, I am running the <code>predict()</code> function of Keras with the VGG16 model. Because I excluded the last layers of the model, this function will not actually return any class predictions as it would normally do; instead we will get the output of the last layer: <code>block5_pool (MaxPooling2D)</code>.</p>
<p>These, we can use as learned features (or abstractions) of the images. Running this part of the code takes several minutes, so I save the output to a RData file (because I samples randomly, the classes you see below might not be the same as in the <code>sample_fruits</code> list above).</p>
<pre class="r"><code>vgg16_feature_list &lt;- data.frame()

for (image in file_list) {
  
  print(image)
  cat(&quot;Image&quot;, which(file_list == image), &quot;from&quot;, length(file_list))
  
  vgg16_feature &lt;- predict(model, image_prep(image))
  
  flatten &lt;- as.data.frame.table(vgg16_feature, responseName = &quot;value&quot;) %&gt;%
    select(value)
  flatten &lt;- cbind(image, as.data.frame(t(flatten)))
  
  vgg16_feature_list &lt;- rbind(vgg16_feature_list, flatten)
}
save(vgg16_feature_list, file = &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/vgg16_feature_list.RData&quot;)</code></pre>
<pre class="r"><code>load(&quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/vgg16_feature_list.RData&quot;)</code></pre>
<pre class="r"><code>dim(vgg16_feature_list)</code></pre>
<pre><code>## [1]   980 25089</code></pre>
</div>
<div id="clustering" class="section level2">
<h2>Clustering</h2>
<p>Next, I’m comparing two clustering attempts:</p>
<ol style="list-style-type: decimal">
<li>clustering original data and</li>
<li>clustering first 10 principal components of the data</li>
</ol>
<p>Here as well, I saved the output to RData because calculation takes some time.</p>
<pre class="r"><code>pca &lt;- prcomp(vgg16_feature_list[, -1],
              center = TRUE,
              scale = FALSE)
save(pca, file = &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/pca.RData&quot;)</code></pre>
<pre class="r"><code>load(&quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/pca.RData&quot;)</code></pre>
<p>Plotting the first two principal components suggests that the images fall into 4 clusters.</p>
<pre class="r"><code>data.frame(PC1 = pca$x[, 1], 
           PC2 = pca$x[, 2]) %&gt;%
  ggplot(aes(x = PC1, y = PC2)) +
    geom_point()</code></pre>
<p><img src="/post/2018-10-06_keras_fruits_cluster_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>The <code>kMeans</code> function let’s us do <a href="https://en.wikipedia.org/wiki/K-means_clustering">k-Means clustering</a>.</p>
<pre class="r"><code>set.seed(50)
cluster_pca &lt;- kmeans(pca$x[, 1:10], 4)
cluster_feature &lt;- kmeans(vgg16_feature_list[, -1], 4)</code></pre>
<p>Let’s combine the resulting cluster information back with the image information and create a column class (abbreviated with the first three letters).</p>
<pre class="r"><code>cluster_list &lt;- data.frame(cluster_pca = cluster_pca$cluster, 
                           cluster_feature = cluster_feature$cluster,
                           vgg16_feature_list) %&gt;%
  select(cluster_pca, cluster_feature, image) %&gt;%
  mutate(class = gsub(&quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Training/&quot;, &quot;&quot;, image),
         class = substr(class, start = 1, stop = 3))
head(cluster_list)</code></pre>
<pre><code>##   cluster_pca cluster_feature
## 1           3               2
## 2           3               2
## 3           3               2
## 4           3               2
## 5           3               2
## 6           3               2
##                                                                                            image
## 1 /Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Training/Cocos/100_100.jpg
## 2 /Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Training/Cocos/101_100.jpg
## 3 /Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Training/Cocos/102_100.jpg
## 4 /Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Training/Cocos/103_100.jpg
## 5 /Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Training/Cocos/104_100.jpg
## 6 /Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Training/Cocos/105_100.jpg
##   class
## 1   Coc
## 2   Coc
## 3   Coc
## 4   Coc
## 5   Coc
## 6   Coc</code></pre>
<p>And let’s count the number of images in each cluster, as well their class.</p>
<ul>
<li>PCA:</li>
</ul>
<pre class="r"><code>cluster_list %&gt;%
  count(cluster_pca, class)</code></pre>
<pre><code>## # A tibble: 4 x 3
##   cluster_pca class     n
##         &lt;int&gt; &lt;chr&gt; &lt;int&gt;
## 1           1 Sal     245
## 2           2 Coc     245
## 3           3 Coc     245
## 4           4 Sal     245</code></pre>
<ul>
<li>Original feature map:</li>
</ul>
<pre class="r"><code>cluster_list %&gt;%
  count(cluster_feature, class)</code></pre>
<pre><code>## # A tibble: 4 x 3
##   cluster_feature class     n
##             &lt;int&gt; &lt;chr&gt; &lt;int&gt;
## 1               1 Sal     245
## 2               2 Coc     490
## 3               3 Sal     157
## 4               4 Sal      88</code></pre>
<p>The classes map pretty clearly to the four clusters from the PCA.</p>
<pre class="r"><code>cluster_list %&gt;%
  mutate(PC1 = pca$x[, 1],
         PC2 = pca$x[, 2]) %&gt;%
  ggplot(aes(x = PC1, y = PC2, color = class, shape = factor(cluster_pca))) +
    geom_point()</code></pre>
<p><img src="/post/2018-10-06_keras_fruits_cluster_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>So, let’s plot a few of the images from each cluster so that maybe we’ll be able to see a pattern that explains why our fruits fall into four instead of 2 clusters.</p>
<pre class="r"><code>plot_random_images &lt;- function(n_img = 49,
                               cluster = 1,
                               rows = 7,
                               cols = 7) {
  cluster_list_random_cl_1 &lt;- cluster_list %&gt;%
    filter(cluster_pca == cluster) %&gt;%
    sample_n(n_img)
  
  graphics::layout(matrix(c(1:n_img), rows, cols, byrow = TRUE))
  for (i in 1:n_img) {
    path &lt;- as.character(cluster_list_random_cl_1$image[i])
    img &lt;- load.image(path)
    plot(img, axes = FALSE)
    title(main = paste(&quot;Cluster PCA&quot;, cluster))
  }
}</code></pre>
<pre class="r"><code>sapply(c(1:4), function(x) plot_random_images(cluster = x))</code></pre>
<p><img src="/post/2018-10-06_keras_fruits_cluster_files/figure-html/unnamed-chunk-18-1.png" width="1920" /><img src="/post/2018-10-06_keras_fruits_cluster_files/figure-html/unnamed-chunk-18-2.png" width="1920" /><img src="/post/2018-10-06_keras_fruits_cluster_files/figure-html/unnamed-chunk-18-3.png" width="1920" /><img src="/post/2018-10-06_keras_fruits_cluster_files/figure-html/unnamed-chunk-18-4.png" width="1920" /></p>
<pre><code>## [[1]]
## NULL
## 
## [[2]]
## NULL
## 
## [[3]]
## NULL
## 
## [[4]]
## NULL</code></pre>
<p>Obviously, the clusters reflect the fruits AND the orientation of the fruits. In that way, our clustering represents intuitive patterns in the images that we can understand.</p>
<p>If we didn’t know the classes, labelling our fruits would be much easier now than manually going through each image individually!</p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] bindrcpp_0.2.2  imager_0.41.1   magrittr_1.5    forcats_0.3.0  
##  [5] stringr_1.3.1   dplyr_0.7.6     purrr_0.2.5     readr_1.1.1    
##  [9] tidyr_0.8.1     tibble_1.4.2    ggplot2_3.0.0   tidyverse_1.2.1
## [13] magick_2.0      keras_2.2.0    
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_0.12.19     lubridate_1.7.4  lattice_0.20-35  png_0.1-7       
##  [5] utf8_1.1.4       assertthat_0.2.0 zeallot_0.1.0    rprojroot_1.3-2 
##  [9] digest_0.6.17    tiff_0.1-5       R6_2.3.0         cellranger_1.1.0
## [13] plyr_1.8.4       backports_1.1.2  evaluate_0.11    httr_1.3.1      
## [17] blogdown_0.8     pillar_1.3.0     tfruns_1.4       rlang_0.2.2     
## [21] lazyeval_0.2.1   readxl_1.1.0     rstudioapi_0.8   whisker_0.3-2   
## [25] bmp_0.3          Matrix_1.2-14    reticulate_1.10  rmarkdown_1.10  
## [29] labeling_0.3     igraph_1.2.2     munsell_0.5.0    broom_0.5.0     
## [33] compiler_3.5.1   modelr_0.1.2     xfun_0.3         pkgconfig_2.0.2 
## [37] base64enc_0.1-3  tensorflow_1.9   htmltools_0.3.6  readbitmap_0.1.5
## [41] tidyselect_0.2.4 bookdown_0.7     fansi_0.4.0      crayon_1.3.4    
## [45] withr_2.1.2      grid_3.5.1       nlme_3.1-137     jsonlite_1.5    
## [49] gtable_0.2.0     scales_1.0.0     cli_1.0.1        stringi_1.2.4   
## [53] xml2_1.2.0       tools_3.5.1      glue_1.3.0       hms_0.4.2       
## [57] jpeg_0.1-8       yaml_2.2.0       colorspace_1.3-2 rvest_0.3.2     
## [61] knitr_1.20       bindr_0.1.1      haven_1.1.2</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Slides from talk: &#39;Decoding The Black Box&#39; at the Frankfurt Data Science Meetup]]></title>
    <link href="/2018/09/ffm_datascience_meetup_slides/"/>
    <id>/2018/09/ffm_datascience_meetup_slides/</id>
    <published>2018-09-27T00:00:00+00:00</published>
    <updated>2018-09-27T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>On Wednesday, September 26th, I gave a talk about <a href="http://meetu.ps/e/DQBZx/w54bW/f">‘Decoding The Black Box’ at the Frankfurt Data Science Meetup</a>.</p>
<p>My slides were created with <a href="https://www.beautiful.ai/">beautiful.ai</a> and can be found <a href="https://www.beautiful.ai/deck/-LMqJI021mvFKYprvdPr/FFM-Data-Science-Meetup">here</a>.</p>
<div style="position:relative;width:100%;height:0;padding-bottom:calc(56.25% + 40px);">
<iframe allowfullscreen style="position:absolute; width: 100%; height: 100%;border: solid 1px #333;" src="https://www.beautiful.ai/player/-LMqJI021mvFKYprvdPr/FFM-Data-Science-Meetup?utm_source=beautiful_player&amp;utm_medium=embed&amp;utm_campaign=-LMqJI021mvFKYprvdPr">
</iframe>
</div>
<blockquote>
<p>DECODING THE BLACK BOX</p>
</blockquote>
<blockquote>
<p>And finally we will have with us Dr.Shirin Glander, whom we were inviting for a long time back. Shirin lives in Münster and works as a Data Scientist at codecentric, she has lots of practical experience. Besides crunching data, she trains her creativity by sketching information. Visit her blog and you will find lots of interesting stuff there, like experimenting with Keras, TensorFlow, LIME, caret, lots of R and also her beautiful sketches. We recommend: www.shirin-glander.de Besides all that she is an organiser of MünsteR - R User Group: www.meetup.com/Munster-R-Users-Group</p>
</blockquote>
<blockquote>
<p>Traditional ML workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex ML models are essentially black boxes and too complicated to understand, we need to use approximations, like LIME.</p>
</blockquote>
<p><br></p>
<p>Here are also some sketchnotes I took during the second talk:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/ffm_datascience_meetup.jpg" />

</div>
<blockquote>
<p>HOW IT REALLY WORKS! ADVANCED STREAMING AND EDGE ANALYTICS METHODS IN INDUSTRY 4.0</p>
</blockquote>
<blockquote>
<p>We will have this evening as our guest - Nicole Tschauder, who works as a Manufacturing Analytics Expert at SAS. She focuses on the use of ML in production, logistics and other IoT scenarios. From her background Nicole is a mathematician.</p>
</blockquote>
<blockquote>
<p>In order to transform gigantic IoT data flows into usable insights, two things are key: The right technology to obtain sensor data and functioning analytical methods to analyze data either at the edge or in-stream. During this presentation, Nicole will give an introduction to new methods that are specifically tailored to the analysis of sensor data. You’ll learn how that is different from classical analytical methods and how to apply the knowledge in areas like predictive maintenance, anomaly detection or signal processing.</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[I&#39;ll be talking about &#39;Decoding The Black Box&#39; at the Frankfurt Data Science Meetup]]></title>
    <link href="/2018/09/ffm_datascience_meetup/"/>
    <id>/2018/09/ffm_datascience_meetup/</id>
    <published>2018-09-19T00:00:00+00:00</published>
    <updated>2018-09-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I have yet another Meetup talk to announce:</p>
<p>On Wednesday, September 26th, I’ll be talking about <a href="http://meetu.ps/e/DQBZx/w54bW/f">‘Decoding The Black Box’ at the Frankfurt Data Science Meetup</a>.</p>
<p>Particularly cool with this meetup is that they will livestream the event at <a href="www.youtube.com/c/FrankfurtDataScience" class="uri">www.youtube.com/c/FrankfurtDataScience</a>!</p>
<blockquote>
<p>TALK#2: DECODING THE BLACK BOX</p>
</blockquote>
<blockquote>
<p>And finally we will have with us Dr.Shirin Glander, whom we were inviting for a long time back. Shirin lives in Münster and works as a Data Scientist at codecentric, she has lots of practical experience. Besides crunching data, she trains her creativity by sketching information. Visit her blog and you will find lots of interesting stuff there, like experimenting with Keras, TensorFlow, LIME, caret, lots of R and also her beautiful sketches. We recommend: www.shirin-glander.de Besides all that she is an organiser of MünsteR - R User Group: www.meetup.com/Munster-R-Users-Group</p>
</blockquote>
<blockquote>
<p>Traditional ML workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex ML models are essentially black boxes and too complicated to understand, we need to use approximations, like LIME.</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[November 8th &amp; 9th in Munich: Workshop on Deep Learning with Keras and TensorFlow in R]]></title>
    <link href="/2018/09/deep_learning_keras_tensorflow_18_09/"/>
    <id>/2018/09/deep_learning_keras_tensorflow_18_09/</id>
    <published>2018-09-19T00:00:00+00:00</published>
    <updated>2018-09-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Registration is now open for my 1.5-day <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/?utm_content=77252894&amp;utm_medium=social&amp;utm_source=twitter#schulung-detail">workshop on deep learning with Keras and TensorFlow using R</a>.</p>
<p>It will take place on <strong>November 8th &amp; 9th</strong> in <strong>Munich, Germany</strong>.</p>
<p><br></p>
<div class="figure">
<img src="https://blog.keras.io/img/keras-tensorflow-logo.jpg" />

</div>
<p><br></p>
<p><a href="https://blog.codecentric.de/en/2018/02/deep-learning-workshop-bei-der-codecentric-ag-solingen/">You can read about one participant’s experience in my workshop:</a></p>
<blockquote>
<p>Big Data – a buzz word you can find everywhere these days, from nerdy blogs to scientific research papers and even in the news. But how does Big Data Analysis work, exactly? In order to find that out, I attended the workshop on “Deep Learning with Keras and TensorFlow”. On a stormy Thursday afternoon, we arrived at the modern and light-flooded codecentric AG headquarters. There, we met performance expert Dieter Dirkes and Data Scientist Dr. Shirin Glander. In the following two days, Shirin gave us a hands-on introduction into the secrets of Deep Learning and helped us to program our first Neural Net. After a short round of introduction of the participants, it became clear that many different areas and domains are interested in Deep Learning: geologists want to classify (satellite) images, energy providers want to analyse time-series, insurers want to predict numbers and I – a humanities major – want to classify text. And codecentric employees were also interested in getting to know the possibilities of Deep Learning, so that a third of the participants were employees from the company itself.</p>
</blockquote>
<p><a href="https://blog.codecentric.de/en/2018/02/deep-learning-workshop-bei-der-codecentric-ag-solingen/">Continue reading…</a></p>
<p>In my workshop, you will learn</p>
<ul>
<li>the basics of deep learning</li>
<li>what cross-entropy and loss is</li>
<li>about activation functions</li>
<li>how to optimize weights and biases with backpropagation and gradient descent</li>
<li>how to build (deep) neural networks with Keras and TensorFlow</li>
<li>how to save and load models and model weights</li>
<li>how to visualize models with TensorBoard</li>
<li>how to make predictions on test data</li>
</ul>
<p><br></p>
<div class="figure">
<img src="https://pbs.twimg.com/media/DnYlz0XW0AAWB-b.png" />

</div>
<p>Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. Keras is very convenient for fast and easy prototyping of neural networks. It is highly modular and very flexible, so that you can build basically any type of neural network you want. It supports convolutional neural networks and recurrent neural networks, as well as combinations of both. Due to its layer structure, it is highly extensible and can run on CPU or GPU.</p>
<p>The <code>keras</code> R package provides an interface to the Python library of Keras, just as the tensorflow package provides an interface to TensorFlow. Basically, R creates a conda instance and runs Keras it it, while you can still use all the functionalities of R for plotting, etc. Almost all function names are the same, so models can easily be recreated in Python for deployment.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[I&#39;ll be talking at the R-Ladies Meetup about Interpretable Deep Learning with R, Keras and LIME]]></title>
    <link href="/2018/09/rladieslondontalk/"/>
    <id>/2018/09/rladieslondontalk/</id>
    <published>2018-09-17T00:00:00+00:00</published>
    <updated>2018-09-17T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Today I am very happy to announce that during my stay in London for the <a href="https://www.mcubed.london/sessions/explaining-complex-machine-learning-models-lime/">m3 conference</a>, I’ll also be giving a talk at the R-Ladies London Meetup on Tuesday, October 16th, about one of my favorite topics: <a href="https://www.meetup.com/de-DE/rladies-london/events/254581599/">Interpretable Deep Learning with R, Keras and LIME</a>.</p>
<p>You can register via Eventbrite: <a href="https://www.eventbrite.co.uk/e/interpretable-deep-learning-with-r-lime-and-keras-tickets-50118369392" class="uri">https://www.eventbrite.co.uk/e/interpretable-deep-learning-with-r-lime-and-keras-tickets-50118369392</a></p>
<blockquote>
<p>ABOUT THE TALK</p>
</blockquote>
<blockquote>
<p>Keras is a high-level open-source deep learning framework that by default works on top of TensorFlow. Keras is minimalistic, efficient and highly flexible because it works with a modular layer system to define, compile and fit neural networks. It has been written in Python but can also be used from within R. Because the underlying backend can be changed from TensorFlow to Theano and CNTK (with more options being developed right now) it is designed to be framework-independent. Models can be trained on CPU or GPU, locally or in the cloud. Shirin will show an example of how to build an image classifier with Keras. We’ll be using a convolutional neural net to classify fruits in images. But that’s not all! We not only want to judge our black-box model based on accuracy and loss measures - we want to get a better understanding of how the model works. We will use an algorithm called LIME (local interpretable model-agnostic explanations) to find out what part of the different test images contributed most strongly to the classification that was made by our model. Shirin will introduce LIME and explain how it works. And finally, she will show how to apply LIME to the image classifier we built before, as well as to a pre-trained Imagenet model.</p>
</blockquote>
<blockquote>
<p>MEETUP AGENDA</p>
</blockquote>
<blockquote>
<p>18.00 - 19.00: Doors open and networking. 19.00 - 19.15: Intro in the Data Science at Farfetch 19.15 - 20.30: Shirin’s talk + questions ~20.30 - ~21.00: networking</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[W-JAX 2018 talk: Deep Learning - a Primer]]></title>
    <link href="/2018/08/wjax2018/"/>
    <id>/2018/08/wjax2018/</id>
    <published>2018-08-29T00:00:00+00:00</published>
    <updated>2018-08-29T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>On November 7th, I’ll be in Munich for the W-JAX conference where I’ll be giving the talk that my colleague Uwe Friedrichsen and I gave at the JAX conference this April again: <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/">Deep Learning - a Primer</a>.</p>
<p>Let me know if any of you here are going to be there and would like to meet up!</p>
<p><br></p>
<p>Slides from the original talk can be found here: <a href="https://www.slideshare.net/ShirinGlander/deep-learning-a-primer-95197733" class="uri">https://www.slideshare.net/ShirinGlander/deep-learning-a-primer-95197733</a></p>
<blockquote>
<p>Deep Learning is one of the “hot” topics in the AI area – a lot of hype, a lot of inflated expectation, but also quite some impressive success stories. As some AI experts already predict that Deep Learning will become “Software 2.0”, it might be a good time to have a closer look at the topic. In this session I will try to give a comprehensive overview of Deep Learning. We will start with a bit of history and some theoretical foundations that we will use to create a little Deep Learning taxonomy. Then we will have a look at current and upcoming application areas: Where can we apply Deep Learning successfully and what does it differentiate from other approaches? Afterwards we will examine the ecosystem: Which tools and libraries are available? What are their strengths and weaknesses? And to complete the session, we will look into some practical code examples and the typical pitfalls of Deep Learning. After this session you will have a much better idea of the why, what and how of Deep Learning, including if and how you might want to apply it to your own work. <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/" class="uri">https://jax.de/big-data-machine-learning/deep-learning-a-primer/</a></p>
</blockquote>
<div class="figure">
<img src="https://pbs.twimg.com/media/DUt3SXyUQAE3TOv.jpg" alt="https://twitter.com/jaxcon/status/957990506331557890" />
<p class="caption"><a href="https://twitter.com/jaxcon/status/957990506331557890" class="uri">https://twitter.com/jaxcon/status/957990506331557890</a></p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Slides from my SAP webinar: Explaining Keras Image Classification Models with LIME]]></title>
    <link href="/2018/08/sap_webinar_slides/"/>
    <id>/2018/08/sap_webinar_slides/</id>
    <published>2018-08-21T00:00:00+00:00</published>
    <updated>2018-08-21T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Here I am sharing the slides for a webinar I gave for SAP about <strong>Explaining Keras Image Classification Models with LIME</strong>.</p>
<p>Slides can be found here: <a href="https://www.slideshare.net/ShirinGlander/sap-webinar-explaining-keras-image-classification-models-with-lime" class="uri">https://www.slideshare.net/ShirinGlander/sap-webinar-explaining-keras-image-classification-models-with-lime</a></p>
<blockquote>
<p>Keras is a high-level open-source deep learning framework that by default works on top of TensorFlow. Keras is minimalistic, efficient and highly flexible because it works with a modular layer system to define, compile and fit neural networks. It has been written in Python but can also be used from within R. Because the underlying backend can be changed from TensorFlow to Theano and CNTK (with more options being developed right now) it is designed to be framework-independent. Models can be trained on CPU or GPU, locally or in the cloud.</p>
</blockquote>
<blockquote>
<p>I will show an example how to build an image classifier with Keras. We’ll be using a convolutional neural net to classify fruits in images. But that’s not all! We not only want to judge our black-box model based on accuracy and loss measures - we want to get a better understanding of how the model works. We will use an algorithm called LIME (local interpretable model-agnostic explanations) to find out what part of the different test images contributed most strongly to the classification that was made by our model. I will introduce LIME and explain how it works. And finally, I will show how to apply LIME to the image classifier we built before, as well as to a pretrained Imagenet model.</p>
</blockquote>
<blockquote>
<p>You will get:</p>
</blockquote>
<ul>
<li>an introduction to Keras</li>
<li>an overview about deep learning and neural nets</li>
<li>a demo how to build an image classifier with Keras</li>
<li>an introduction to explaining black box models, specifically to the LIME algorithm</li>
<li>a demo how to apply LIME to explain the predictions of our own Keras image classifier, as well as of a pretrained Imagenet</li>
</ul>
<blockquote>
<p>Further Information:</p>
</blockquote>
<ul>
<li><a href="www.shirin-glander.de" class="uri">www.shirin-glander.de</a></li>
<li><a href="https://blog.codecentric.de/author/shirin-glander/" class="uri">https://blog.codecentric.de/author/shirin-glander/</a></li>
<li><a href="www.youtube.com/codecentricAI" class="uri">www.youtube.com/codecentricAI</a></li>
</ul>
<blockquote>
<p>Links mentioned:</p>
</blockquote>
<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits/" class="uri">https://shirinsplayground.netlify.com/2018/06/keras_fruits/</a></li>
<li><a href="https://blog.codecentric.de/2018/01/%20vertrauen-und-vorurteile-maschinellem-lernen/">https://blog.codecentric.de/2018/01/ vertrauen-und-vorurteile-maschinellem-lernen/</a></li>
<li><a href="https://shirinsplayground.netlify.com/2018/07/%20explaining_ml_models_code_text_lime/">https://shirinsplayground.netlify.com/2018/07/ explaining_ml_models_code_text_lime/</a></li>
<li><a href="www.codecentric.ai" class="uri">www.codecentric.ai</a></li>
<li><a href="https://www.youtube.com/codecentricAI" class="uri">https://www.youtube.com/codecentricAI</a></li>
</ul>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/M9htTiB6ObhMqI" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen>
</iframe>
<div style="margin-bottom:5px">
<strong> <a href="//www.slideshare.net/ShirinGlander/sap-webinar-explaining-keras-image-classification-models-with-lime" title="SAP webinar: Explaining Keras Image Classification Models with LIME" target="_blank">SAP webinar: Explaining Keras Image Classification Models with LIME</a> </strong> from <strong><a href="//www.slideshare.net/ShirinGlander" target="_blank">Shirin Glander</a></strong>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[MünsteR Meetup on Blog Mining: Deriving the success of blog posts from metadata and text data.]]></title>
    <link href="/2018/08/meetup_august18/"/>
    <id>/2018/08/meetup_august18/</id>
    <published>2018-08-01T00:00:00+00:00</published>
    <updated>2018-08-01T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://shiring.github.io/r_users_group/2017/05/20/map.png" />

</div>
<p>In our <a href="http://meetu.ps/e/FDZ1D/w54bW/f">next MünsteR R-user group meetup</a> on <strong>Tuesday, August 28th, 2018</strong> Jenny Saatkamp will give a talk titled <strong>Blog Mining: Deriving the success of blog posts from metadata and text data</strong>. You can RSVP here: <a href="http://meetu.ps/e/FDZ1D/w54bW/f">http://meetu.ps/e/F7zDN/w54bW/f</a></p>
<blockquote>
<p>In our next MünsteR Meetup, Jenny Saatkamp will present her Blog Mining analysis, which is based on 1.500 blog posts from the codecentric company blog (<a href="https://blog.codecentric.de/" class="uri">https://blog.codecentric.de/</a>) and makes use of different mining techniques for metadata and text data. The presentation is divided into two parts: First, new insights into the blog posts and their success is gained using metadata. Therefore, diverse methods and R-packages are used such as H2O automatic machine learning, lime, clustering and basic explorative statistics. The sources of metadata are the blog itself, Google Analytics and the SEO-Tool Ahrefs. Second, a text mining analysis is performed using the texts and titles of each blog post. You will learn about common preprocessing steps like stemming, removal of stop words and bag of words and how to visualize text data. As the text mining analysis is still in progress, Jenny might also be able to show you results of a machine learning algorithm that is able to derive the success of blog posts based on text data.</p>
</blockquote>
<blockquote>
<p>About the speaker: Jenny Saatkamp is currently studying business informatics at the University of Applied Science in Münster. She is a junior Data Scientist, but enthusiastic about it and has already done several Data Science projects in her study and internships. One of her projects is the presented analysis of metadata, which is the topic of her bachelor thesis. The text analysis is also a project as a part of her study.</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Explaining Black-Box Machine Learning Models - Code Part 2: Text classification with LIME]]></title>
    <link href="/2018/07/explaining_ml_models_code_text_lime/"/>
    <id>/2018/07/explaining_ml_models_code_text_lime/</id>
    <published>2018-07-26T00:00:00+00:00</published>
    <updated>2018-07-26T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>This is code that will encompany an article that will appear in a special edition of a German IT magazine. The article is about explaining black-box machine learning models. In that article I’m showcasing three practical examples:</p>
<p><br></p>
<ol style="list-style-type: decimal">
<li>Explaining supervised classification models built on tabular data using <code>caret</code> and the <code>iml</code> package</li>
<li>Explaining image classification models with <code>keras</code> and <code>lime</code></li>
<li>Explaining text classification models with <code>xgboost</code> and <code>lime</code></li>
</ol>
<p><br></p>
<ul>
<li>The first part has been published <a href="https://shirinsplayground.netlify.com/2018/07/explaining_ml_models_code_caret_iml/">here</a>.</li>
<li>The second part has been published <a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits_lime/">here</a>.</li>
</ul>
<p>Below, you will find the code for the third part: Text classification with <a href="https://cran.r-project.org/web/packages/lime/index.html">lime</a>.</p>
<pre class="r"><code># data wrangling
library(tidyverse)
library(readr)

# plotting
library(ggthemes)
theme_set(theme_minimal())

# text prep
library(text2vec)

# ml
library(caret)
library(xgboost)

# explanation
library(lime)</code></pre>
<div id="text-classification-models" class="section level2">
<h2>Text classification models</h2>
<p>Here I am using another Kaggle dataset: <a href="https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews">Women’s e-commerce cloting reviews</a>. The data contains a text review of different items of clothing, as well as some additional information, like rating, division, etc.</p>
<p>In this example, I will use the review title and text in order to classify whether or not the item was liked. I am creating the response variable from the rating: every item rates with 5 stars is considered “liked” (1), the rest as “not liked” (0). I am also combining review title and text.</p>
<pre class="r"><code>clothing_reviews &lt;- read_csv(&quot;/Users/shiringlander/Documents/Github/ix_lime_etc/Womens Clothing E-Commerce Reviews.csv&quot;) %&gt;%
  mutate(Liked = as.factor(ifelse(Rating == 5, 1, 0)),
         text = paste(Title, `Review Text`),
         text = gsub(&quot;NA&quot;, &quot;&quot;, text))</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   X1 = col_integer(),
##   `Clothing ID` = col_integer(),
##   Age = col_integer(),
##   Title = col_character(),
##   `Review Text` = col_character(),
##   Rating = col_integer(),
##   `Recommended IND` = col_integer(),
##   `Positive Feedback Count` = col_integer(),
##   `Division Name` = col_character(),
##   `Department Name` = col_character(),
##   `Class Name` = col_character()
## )</code></pre>
<pre class="r"><code>glimpse(clothing_reviews)</code></pre>
<pre><code>## Observations: 23,486
## Variables: 13
## $ X1                        &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...
## $ `Clothing ID`             &lt;int&gt; 767, 1080, 1077, 1049, 847, 1080, 85...
## $ Age                       &lt;int&gt; 33, 34, 60, 50, 47, 49, 39, 39, 24, ...
## $ Title                     &lt;chr&gt; NA, NA, &quot;Some major design flaws&quot;, &quot;...
## $ `Review Text`             &lt;chr&gt; &quot;Absolutely wonderful - silky and se...
## $ Rating                    &lt;int&gt; 4, 5, 3, 5, 5, 2, 5, 4, 5, 5, 3, 5, ...
## $ `Recommended IND`         &lt;int&gt; 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...
## $ `Positive Feedback Count` &lt;int&gt; 0, 4, 0, 0, 6, 4, 1, 4, 0, 0, 14, 2,...
## $ `Division Name`           &lt;chr&gt; &quot;Initmates&quot;, &quot;General&quot;, &quot;General&quot;, &quot;...
## $ `Department Name`         &lt;chr&gt; &quot;Intimate&quot;, &quot;Dresses&quot;, &quot;Dresses&quot;, &quot;B...
## $ `Class Name`              &lt;chr&gt; &quot;Intimates&quot;, &quot;Dresses&quot;, &quot;Dresses&quot;, &quot;...
## $ Liked                     &lt;fct&gt; 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, ...
## $ text                      &lt;chr&gt; &quot; Absolutely wonderful - silky and s...</code></pre>
<p>Whether an item was liked or not will thus be my response variable or label for classification.</p>
<pre class="r"><code>clothing_reviews %&gt;%
  ggplot(aes(x = Liked, fill = Liked)) +
    geom_bar(alpha = 0.8) +
    scale_fill_tableau(palette = &quot;tableau20&quot;) +
    guides(fill = FALSE)</code></pre>
<p><img src="/post/2018-07-26_explaining_ml_models_code_text_lime_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Let’s split the data into train and test sets:</p>
<pre class="r"><code>set.seed(42)
idx &lt;- createDataPartition(clothing_reviews$Liked, 
                           p = 0.8, 
                           list = FALSE, 
                           times = 1)

clothing_reviews_train &lt;- clothing_reviews[ idx,]
clothing_reviews_test  &lt;- clothing_reviews[-idx,]</code></pre>
</div>
<div id="lets-start-simple" class="section level2">
<h2>Let’s start simple</h2>
<p>The first text model I’m looking at has been built similarly to the example model in the help for <code>lime::interactive_text_explanations()</code>.</p>
<p>First, we need to prepare the data for modeling: we will need to convert the text to a document term matrix (dtm). There are different ways to do this. One is be with the <code>text2vec</code> package.</p>
<blockquote>
<p>“Because of R’s copy-on-modify semantics, it is not easy to iteratively grow a DTM. Thus constructing a DTM, even for a small collections of documents, can be a serious bottleneck for analysts and researchers. It involves reading the whole collection of text documents into RAM and processing it as single vector, which can easily increase memory use by a factor of 2 to 4. The text2vec package solves this problem by providing a better way of constructing a document-term matrix.” <a href="https://cran.r-project.org/web/packages/text2vec/vignettes/text-vectorization.html" class="uri">https://cran.r-project.org/web/packages/text2vec/vignettes/text-vectorization.html</a></p>
</blockquote>
<p>Alternatives to <code>text2vec</code> would be <code>tm</code> + <code>SnowballC</code> or you could work with the <code>tidytext</code> package.</p>
<p>The <code>itoken()</code> function creates vocabularies (here stemmed words), from which we can create the dtm with the <code>create_dtm()</code> function.</p>
<p>All preprocessing steps, starting from the raw text, need to be wrapped in a function that can then be pasted into the <code>lime::lime()</code> function; this is only necessary if you want to use your model with <code>lime</code>.</p>
<pre class="r"><code>get_matrix &lt;- function(text) {
  it &lt;- itoken(text, progressbar = FALSE)
  create_dtm(it, vectorizer = hash_vectorizer())
}</code></pre>
<p>Now, this preprocessing function can be applied to both training and test data.</p>
<pre class="r"><code>dtm_train &lt;- get_matrix(clothing_reviews_train$text)
str(dtm_train)</code></pre>
<pre><code>## Formal class &#39;dgCMatrix&#39; [package &quot;Matrix&quot;] with 6 slots
##   ..@ i       : int [1:889012] 304 764 786 788 793 794 1228 2799 2819 3041 ...
##   ..@ p       : int [1:262145] 0 0 0 0 0 0 0 0 0 0 ...
##   ..@ Dim     : int [1:2] 18789 262144
##   ..@ Dimnames:List of 2
##   .. ..$ : chr [1:18789] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..$ : NULL
##   ..@ x       : num [1:889012] 1 1 2 1 2 1 1 1 1 1 ...
##   ..@ factors : list()</code></pre>
<pre class="r"><code>dtm_test &lt;- get_matrix(clothing_reviews_test$text)
str(dtm_test)</code></pre>
<pre><code>## Formal class &#39;dgCMatrix&#39; [package &quot;Matrix&quot;] with 6 slots
##   ..@ i       : int [1:222314] 2793 400 477 622 2818 2997 3000 4500 3524 2496 ...
##   ..@ p       : int [1:262145] 0 0 0 0 0 0 0 0 0 0 ...
##   ..@ Dim     : int [1:2] 4697 262144
##   ..@ Dimnames:List of 2
##   .. ..$ : chr [1:4697] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..$ : NULL
##   ..@ x       : num [1:222314] 1 1 1 1 1 1 1 1 1 1 ...
##   ..@ factors : list()</code></pre>
<p>And we use it to train a model with the <code>xgboost</code> package (just as in the example of the <code>lime</code> package).</p>
<pre class="r"><code>xgb_model &lt;- xgb.train(list(max_depth = 7, 
                            eta = 0.1, 
                            objective = &quot;binary:logistic&quot;,
                            eval_metric = &quot;error&quot;, nthread = 1),
                       xgb.DMatrix(dtm_train, 
                                   label = clothing_reviews_train$Liked == &quot;1&quot;),
                       nrounds = 50)</code></pre>
<p>Let’s try it on the test data and see how it performs:</p>
<pre class="r"><code>pred &lt;- predict(xgb_model, dtm_test)

confusionMatrix(clothing_reviews_test$Liked,
                as.factor(round(pred, digits = 0)))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 1370  701
##          1  421 2205
##                                           
##                Accuracy : 0.7611          
##                  95% CI : (0.7487, 0.7733)
##     No Information Rate : 0.6187          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.5085          
##  Mcnemar&#39;s Test P-Value : &lt; 2.2e-16       
##                                           
##             Sensitivity : 0.7649          
##             Specificity : 0.7588          
##          Pos Pred Value : 0.6615          
##          Neg Pred Value : 0.8397          
##              Prevalence : 0.3813          
##          Detection Rate : 0.2917          
##    Detection Prevalence : 0.4409          
##       Balanced Accuracy : 0.7619          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p>Okay, not a perfect score but good enough for me - right now, I’m more interested in the explanations of the model’s predictions. For this, we need to run the <code>lime()</code> function and give it</p>
<ul>
<li>the text input that was used to construct the model</li>
<li>the trained model</li>
<li>the preprocessing function</li>
</ul>
<pre class="r"><code>explainer &lt;- lime(clothing_reviews_train$text, 
                  xgb_model, 
                  preprocess = get_matrix)</code></pre>
<p>With this, we could right away call the interactive explainer Shiny app, where we can type any text we want into the field on the left and see the explanation on the right: words that are underlined green support the classification, red words contradict them.</p>
<pre class="r"><code>interactive_text_explanations(explainer)</code></pre>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/text_explanation_2.png" />

</div>
<p>What happens in the background in the app, we can do explicitly by calling the <code>explain()</code> function and give it</p>
<ul>
<li>the test data (here the first four reviews of the test set)</li>
<li>the explainer defined with the <code>lime()</code> function</li>
<li>the number of labels we want to have explanations for (alternatively, you set the label by name)</li>
<li>and the number of features (in this case words) that should be included in the explanations</li>
</ul>
<p>We can plot them either with the <code>plot_text_explanations()</code> function, which gives an output like in the Shiny app or we use the regular <code>plot_features()</code> function.</p>
<pre class="r"><code>explanations &lt;- lime::explain(clothing_reviews_test$text[1:4], explainer, n_labels = 1, n_features = 5)</code></pre>
<pre class="r"><code>plot_text_explanations(explanations)</code></pre>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/text_explanation_0.png" />

</div>
<pre class="r"><code>plot_features(explanations)</code></pre>
<p><img src="/post/2018-07-26_explaining_ml_models_code_text_lime_files/figure-html/lime_text_plot-1.png" width="576" /></p>
<p>As we can see, our explanations contain a lot of stop-words that don’t really make much sense as features in our model. So…</p>
</div>
<div id="lets-try-a-more-complex-example" class="section level2">
<h2>… let’s try a more complex example</h2>
<p>Okay, our model above works but there are still common words and stop words in our model that LIME picks up on. Ideally, we would want to remove them before modeling and keep only relevant words. This we can accomplish by using additional steps and options in our preprocessing function.</p>
<p>Important to know is that whatever preprocessing we do with our text corpus, train and test data has to have the same features (i.e. words)! If we were to incorporate all the steps shown below into one function and call it separately on train and test data, we would end up with different words in our dtm and the <code>predict()</code> function won’t work any more. In the simple example above, it works <a href="https://cran.r-project.org/web/packages/text2vec/vignettes/text-vectorization.html">because we have been using the <code>hash_vectorizer()</code></a>.</p>
<p>Nevertheless, the <code>lime::explain()</code> function expects a preprocessing function that takes a character vector as input.</p>
<p>How do we go about this? First, we will need to create the vocabulary just from the training data. To reduce the number of words to only the most relevant I am performing the following steps:</p>
<ul>
<li>stem all words</li>
<li>remove step-words</li>
<li>prune vocabulary</li>
<li>transform into vector space</li>
</ul>
<pre class="r"><code>stem_tokenizer &lt;- function(x) {
  lapply(word_tokenizer(x), 
         SnowballC::wordStem, 
         language = &quot;en&quot;)
}

stop_words = tm::stopwords(kind = &quot;en&quot;)

# create prunded vocabulary
vocab_train &lt;- itoken(clothing_reviews_train$text, 
                     preprocess_function = tolower, 
                     tokenizer = stem_tokenizer,
                     progressbar = FALSE)
  
v &lt;- create_vocabulary(vocab_train, 
                       stopwords = stop_words)
  
pruned_vocab &lt;- prune_vocabulary(v, 
                                  doc_proportion_max = 0.99, 
                                  doc_proportion_min = 0.01)
  
vectorizer_train &lt;- vocab_vectorizer(pruned_vocab)</code></pre>
<p>This vector space can now be added to the preprocessing function, which we can then apply to both train and test data. Here, I am also transforming the word counts to <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">tfidf</a> values.</p>
<pre class="r"><code># preprocessing function
create_dtm_mat &lt;- function(text, vectorizer = vectorizer_train) {
  
  vocab &lt;- itoken(text, 
               preprocess_function = tolower, 
               tokenizer = stem_tokenizer,
               progressbar = FALSE)
  
  dtm &lt;- create_dtm(vocab, 
             vectorizer = vectorizer)
  
  tfidf = TfIdf$new()
  fit_transform(dtm, tfidf)
}</code></pre>
<pre class="r"><code>dtm_train2 &lt;- create_dtm_mat(clothing_reviews_train$text)
str(dtm_train2)</code></pre>
<pre><code>## Formal class &#39;dgCMatrix&#39; [package &quot;Matrix&quot;] with 6 slots
##   ..@ i       : int [1:415770] 26 74 169 294 588 693 703 708 727 759 ...
##   ..@ p       : int [1:506] 0 189 380 574 765 955 1151 1348 1547 1740 ...
##   ..@ Dim     : int [1:2] 18789 505
##   ..@ Dimnames:List of 2
##   .. ..$ : chr [1:18789] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..$ : chr [1:505] &quot;ad&quot; &quot;sandal&quot; &quot;depend&quot; &quot;often&quot; ...
##   ..@ x       : num [1:415770] 0.177 0.135 0.121 0.17 0.131 ...
##   ..@ factors : list()</code></pre>
<pre class="r"><code>dtm_test2 &lt;- create_dtm_mat(clothing_reviews_test$text)
str(dtm_test2)</code></pre>
<pre><code>## Formal class &#39;dgCMatrix&#39; [package &quot;Matrix&quot;] with 6 slots
##   ..@ i       : int [1:103487] 228 304 360 406 472 518 522 624 732 784 ...
##   ..@ p       : int [1:506] 0 53 113 151 186 216 252 290 323 360 ...
##   ..@ Dim     : int [1:2] 4697 505
##   ..@ Dimnames:List of 2
##   .. ..$ : chr [1:4697] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   .. ..$ : chr [1:505] &quot;ad&quot; &quot;sandal&quot; &quot;depend&quot; &quot;often&quot; ...
##   ..@ x       : num [1:103487] 0.263 0.131 0.135 0.109 0.179 ...
##   ..@ factors : list()</code></pre>
<p>And we will train another gradient boosting model:</p>
<pre class="r"><code>xgb_model2 &lt;- xgb.train(params = list(max_depth = 10, 
                            eta = 0.2, 
                            objective = &quot;binary:logistic&quot;,
                            eval_metric = &quot;error&quot;, nthread = 1),
                       data = xgb.DMatrix(dtm_train2, 
                                   label = clothing_reviews_train$Liked == &quot;1&quot;),
                       nrounds = 500)</code></pre>
<pre class="r"><code>pred2 &lt;- predict(xgb_model2, dtm_test2)

confusionMatrix(clothing_reviews_test$Liked,
                as.factor(round(pred2, digits = 0)))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 1441  630
##          1  426 2200
##                                         
##                Accuracy : 0.7752        
##                  95% CI : (0.763, 0.787)
##     No Information Rate : 0.6025        
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16     
##                                         
##                   Kappa : 0.5392        
##  Mcnemar&#39;s Test P-Value : 4.187e-10     
##                                         
##             Sensitivity : 0.7718        
##             Specificity : 0.7774        
##          Pos Pred Value : 0.6958        
##          Neg Pred Value : 0.8378        
##              Prevalence : 0.3975        
##          Detection Rate : 0.3068        
##    Detection Prevalence : 0.4409        
##       Balanced Accuracy : 0.7746        
##                                         
##        &#39;Positive&#39; Class : 0             
## </code></pre>
<p>Unfortunately, this didn’t really improve the classification accuracy but let’s look at the explanations again:</p>
<pre class="r"><code>explainer2 &lt;- lime(clothing_reviews_train$text, 
                  xgb_model2, 
                  preprocess = create_dtm_mat)</code></pre>
<pre class="r"><code>explanations2 &lt;- lime::explain(clothing_reviews_test$text[1:4], explainer2, n_labels = 1, n_features = 4)
plot_text_explanations(explanations2)</code></pre>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/text_explanation_1.png" />

</div>
<p>The words that get picked up now make much more sense! So, even though making my model more complex didn’t improve “the numbers”, this second model is likely to be much better able to generalize to new reviews because it seems to pick up on words that make intuitive sense.</p>
<p>That’s why I’m sold on the benefits of adding explainer functions to most machine learning workflows - and why I love the <code>lime</code> package in R!</p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] bindrcpp_0.2.2  lime_0.4.0      xgboost_0.71.2  caret_6.0-80   
##  [5] lattice_0.20-35 text2vec_0.5.1  ggthemes_3.5.0  forcats_0.3.0  
##  [9] stringr_1.3.1   dplyr_0.7.6     purrr_0.2.5     readr_1.1.1    
## [13] tidyr_0.8.1     tibble_1.4.2    ggplot2_3.0.0   tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##   [1] colorspace_1.3-2     class_7.3-14         rprojroot_1.3-2     
##   [4] futile.logger_1.4.3  pls_2.6-0            rstudioapi_0.7      
##   [7] DRR_0.0.3            SnowballC_0.5.1      prodlim_2018.04.18  
##  [10] lubridate_1.7.4      xml2_1.2.0           codetools_0.2-15    
##  [13] splines_3.5.1        mnormt_1.5-5         robustbase_0.93-1   
##  [16] knitr_1.20           shinythemes_1.1.1    RcppRoll_0.3.0      
##  [19] mlapi_0.1.0          jsonlite_1.5         broom_0.4.5         
##  [22] ddalpha_1.3.4        kernlab_0.9-26       sfsmisc_1.1-2       
##  [25] shiny_1.1.0          compiler_3.5.1       httr_1.3.1          
##  [28] backports_1.1.2      assertthat_0.2.0     Matrix_1.2-14       
##  [31] lazyeval_0.2.1       cli_1.0.0            later_0.7.3         
##  [34] formatR_1.5          htmltools_0.3.6      tools_3.5.1         
##  [37] NLP_0.1-11           gtable_0.2.0         glue_1.2.0          
##  [40] reshape2_1.4.3       Rcpp_0.12.17         slam_0.1-43         
##  [43] cellranger_1.1.0     nlme_3.1-137         blogdown_0.6        
##  [46] iterators_1.0.9      psych_1.8.4          timeDate_3043.102   
##  [49] gower_0.1.2          xfun_0.3             rvest_0.3.2         
##  [52] mime_0.5             stringdist_0.9.5.1   DEoptimR_1.0-8      
##  [55] MASS_7.3-50          scales_0.5.0         ipred_0.9-6         
##  [58] hms_0.4.2            promises_1.0.1       parallel_3.5.1      
##  [61] lambda.r_1.2.3       yaml_2.1.19          rpart_4.1-13        
##  [64] stringi_1.2.3        foreach_1.4.4        e1071_1.6-8         
##  [67] lava_1.6.2           geometry_0.3-6       rlang_0.2.1         
##  [70] pkgconfig_2.0.1      evaluate_0.10.1      bindr_0.1.1         
##  [73] labeling_0.3         recipes_0.1.3        htmlwidgets_1.2     
##  [76] CVST_0.2-2           tidyselect_0.2.4     plyr_1.8.4          
##  [79] magrittr_1.5         bookdown_0.7         R6_2.2.2            
##  [82] magick_1.9           dimRed_0.1.0         pillar_1.2.3        
##  [85] haven_1.1.2          foreign_0.8-70       withr_2.1.2         
##  [88] survival_2.42-3      abind_1.4-5          nnet_7.3-12         
##  [91] modelr_0.1.2         crayon_1.3.4         futile.options_1.0.1
##  [94] rmarkdown_1.10       grid_3.5.1           readxl_1.1.0        
##  [97] data.table_1.11.4    ModelMetrics_1.1.0   digest_0.6.15       
## [100] tm_0.7-4             xtable_1.8-2         httpuv_1.4.4.2      
## [103] RcppParallel_4.4.0   stats4_3.5.1         munsell_0.5.0       
## [106] glmnet_2.0-16        magic_1.5-8</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Explaining Black-Box Machine Learning Models - Code Part 1: tabular data &#43; caret &#43; iml]]></title>
    <link href="/2018/07/explaining_ml_models_code_caret_iml/"/>
    <id>/2018/07/explaining_ml_models_code_caret_iml/</id>
    <published>2018-07-20T00:00:00+00:00</published>
    <updated>2018-07-20T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>This is code that will encompany an article that will appear in a special edition of a German IT magazine. The article is about explaining black-box machine learning models. In that article I’m showcasing three practical examples:</p>
<p><br></p>
<ol style="list-style-type: decimal">
<li>Explaining supervised classification models built on tabular data using <code>caret</code> and the <code>iml</code> package</li>
<li>Explaining image classification models with <code>keras</code> and <code>lime</code></li>
<li>Explaining text classification models with <code>xgboost</code> and <code>lime</code></li>
</ol>
<p><br></p>
<ul>
<li>The second part has been published <a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits_lime/">here</a>.</li>
<li>The third part has been published <a href="https://shirinsplayground.netlify.com/2018/07/explaining_ml_models_code_text_lime/">here</a>.</li>
</ul>
<p>Below, you will find the code for the first part:</p>
<pre class="r"><code># data wrangling
library(tidyverse)
library(readr)

# ml
library(caret)

# plotting
library(gridExtra)
library(grid)
library(ggridges)
library(ggthemes)
theme_set(theme_minimal())

# explaining models
# https://github.com/christophM/iml
library(iml)

# https://pbiecek.github.io/breakDown/
library(breakDown)

# https://pbiecek.github.io/DALEX/
library(DALEX)</code></pre>
<div id="supervised-classfication-models-with-tabular-data" class="section level2">
<h2>Supervised classfication models with tabular data</h2>
<div id="the-data" class="section level3">
<h3>The data</h3>
<p>The example dataset I am using in this part is the <a href="https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009">wine quality data</a>. Let’s read it in and do some minor housekeeping, like</p>
<ul>
<li>converting the response variable <code>quality</code> into two categories with roughly equal sizes and</li>
<li>replacing the spaces in the column names with “_&quot; to make it easier to handle in the tidyverse</li>
</ul>
<pre class="r"><code>wine_data &lt;- read_csv(&quot;/Users/shiringlander/Documents/Github/ix_lime_etc/winequality-red.csv&quot;) %&gt;%
  mutate(quality = as.factor(ifelse(quality &lt; 6, &quot;qual_low&quot;, &quot;qual_high&quot;)))</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   `fixed acidity` = col_double(),
##   `volatile acidity` = col_double(),
##   `citric acid` = col_double(),
##   `residual sugar` = col_double(),
##   chlorides = col_double(),
##   `free sulfur dioxide` = col_double(),
##   `total sulfur dioxide` = col_double(),
##   density = col_double(),
##   pH = col_double(),
##   sulphates = col_double(),
##   alcohol = col_double(),
##   quality = col_integer()
## )</code></pre>
<pre class="r"><code>colnames(wine_data) &lt;- gsub(&quot; &quot;, &quot;_&quot;, colnames(wine_data))
glimpse(wine_data)</code></pre>
<pre><code>## Observations: 1,599
## Variables: 12
## $ fixed_acidity        &lt;dbl&gt; 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, ...
## $ volatile_acidity     &lt;dbl&gt; 0.700, 0.880, 0.760, 0.280, 0.700, 0.660,...
## $ citric_acid          &lt;dbl&gt; 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06,...
## $ residual_sugar       &lt;dbl&gt; 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2...
## $ chlorides            &lt;dbl&gt; 0.076, 0.098, 0.092, 0.075, 0.076, 0.075,...
## $ free_sulfur_dioxide  &lt;dbl&gt; 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15...
## $ total_sulfur_dioxide &lt;dbl&gt; 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, ...
## $ density              &lt;dbl&gt; 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0...
## $ pH                   &lt;dbl&gt; 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30,...
## $ sulphates            &lt;dbl&gt; 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46,...
## $ alcohol              &lt;dbl&gt; 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, ...
## $ quality              &lt;fct&gt; qual_low, qual_low, qual_low, qual_high, ...</code></pre>
</div>
<div id="eda" class="section level3">
<h3>EDA</h3>
<p>The first step in my machine learning workflows in exploratory data analysis (EDA). This can get pretty extensive, but here I am only looking at the distributions of my features and the class counts of my response variable.</p>
<pre class="r"><code>p1 &lt;- wine_data %&gt;%
  ggplot(aes(x = quality, fill = quality)) +
    geom_bar(alpha = 0.8) +
    scale_fill_tableau() +
    guides(fill = FALSE)</code></pre>
<pre class="r"><code>p2 &lt;- wine_data %&gt;%
  gather(x, y, fixed_acidity:alcohol) %&gt;%
  ggplot(aes(x = y, y = quality, color = quality, fill = quality)) +
    facet_wrap( ~ x, scale = &quot;free&quot;, ncol = 3) +
    scale_fill_tableau() +
    scale_color_tableau() +
    geom_density_ridges(alpha = 0.8) +
    guides(fill = FALSE, color = FALSE)</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, ncol = 2, widths = c(0.3, 0.7))</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/eda_plots-1.png" width="960" /></p>
</div>
<div id="model" class="section level3">
<h3>Model</h3>
<p>For modeling, I am splitting the data into 80% for training and 20% for testing.</p>
<pre class="r"><code>set.seed(42)
idx &lt;- createDataPartition(wine_data$quality, 
                           p = 0.8, 
                           list = FALSE, 
                           times = 1)

wine_train &lt;- wine_data[ idx,]
wine_test  &lt;- wine_data[-idx,]</code></pre>
<p>I am using 5-fold cross-validation, repeated 3x and scale and center the data. The example model I am using here is a Random Forest model.</p>
<pre class="r"><code>fit_control &lt;- trainControl(method = &quot;repeatedcv&quot;,
                           number = 5,
                           repeats = 3)

set.seed(42)
rf_model &lt;- train(quality ~ ., 
                  data = wine_train, 
                  method = &quot;rf&quot;, 
                  preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                  trControl = fit_control,
                  verbose = FALSE)</code></pre>
<pre class="r"><code>rf_model</code></pre>
<pre><code>## Random Forest 
## 
## 1280 samples
##   11 predictor
##    2 classes: &#39;qual_high&#39;, &#39;qual_low&#39; 
## 
## Pre-processing: scaled (11), centered (11) 
## Resampling: Cross-Validated (5 fold, repeated 3 times) 
## Summary of sample sizes: 1023, 1024, 1025, 1024, 1024, 1024, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    2    0.7958240  0.5898607
##    6    0.7893104  0.5766700
##   11    0.7882738  0.5745067
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<p>Now let’s see how good our model is:</p>
<pre class="r"><code>test_predict &lt;- predict(rf_model, wine_test)
confusionMatrix(test_predict, as.factor(wine_test$quality))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  qual_high qual_low
##   qual_high       140       26
##   qual_low         31      122
##                                           
##                Accuracy : 0.8213          
##                  95% CI : (0.7748, 0.8618)
##     No Information Rate : 0.5361          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.6416          
##  Mcnemar&#39;s Test P-Value : 0.5962          
##                                           
##             Sensitivity : 0.8187          
##             Specificity : 0.8243          
##          Pos Pred Value : 0.8434          
##          Neg Pred Value : 0.7974          
##              Prevalence : 0.5361          
##          Detection Rate : 0.4389          
##    Detection Prevalence : 0.5204          
##       Balanced Accuracy : 0.8215          
##                                           
##        &#39;Positive&#39; Class : qual_high       
## </code></pre>
<p>Okay, this model isn’t too accurate but since my focus here is supposed to be on explaining the model, that’s good enough for me at this point.</p>
</div>
<div id="explaininginterpreting-the-model" class="section level3">
<h3>Explaining/interpreting the model</h3>
<p>There are several methods and tools that can be used to explain or interpret machine learning models. You can read more about them in <a href="https://christophm.github.io/interpretable-ml-book/">this ebook</a>. Here, I am going to show a few of them.</p>
<div id="feature-importance" class="section level4">
<h4>Feature importance</h4>
<p>The first metric to look at for Random Forest models (and many other algorithms) is feature importance:</p>
<blockquote>
<p>“Variable importance evaluation functions can be separated into two groups: those that use the model information and those that do not. The advantage of using a model-based approach is that is more closely tied to the model performance and that it may be able to incorporate the correlation structure between the predictors into the importance calculation.” <a href="https://topepo.github.io/caret/variable-importance.html" class="uri">https://topepo.github.io/caret/variable-importance.html</a></p>
</blockquote>
<p>The <code>varImp()</code> function from the <code>caret</code> package can be used to calculate feature importance measures for most methods. For Random Forest classification models such as ours, the prediction error rate is calculated for</p>
<ol style="list-style-type: decimal">
<li>permuted out-of-bag data of each tree and</li>
<li>permutations of every feature</li>
</ol>
<p>These two measures are averaged and normalized as described here:</p>
<blockquote>
<p>“Here are the definitions of the variable importance measures. The first measure is computed from permuting OOB data: For each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression). Then the same is done after permuting each predictor variable. The difference between the two are then averaged over all trees, and normalized by the standard deviation of the differences. If the standard deviation of the differences is equal to 0 for a variable, the division is not done (but the average is almost always equal to 0 in that case). The second measure is the total decrease in node impurities from splitting on the variable, averaged over all trees. For classification, the node impurity is measured by the Gini index. For regression, it is measured by residual sum of squares.” randomForest help function for <code>importance()</code></p>
</blockquote>
<pre class="r"><code>rf_model_imp &lt;- varImp(rf_model, scale = TRUE)
p1 &lt;- rf_model_imp$importance %&gt;%
  as.data.frame() %&gt;%
  rownames_to_column() %&gt;%
  ggplot(aes(x = reorder(rowname, Overall), y = Overall)) +
    geom_bar(stat = &quot;identity&quot;, fill = &quot;#1F77B4&quot;, alpha = 0.8) +
    coord_flip()</code></pre>
<p>We can also use a ROC curve for evaluating feature importance. For this, we have the <code>caret::filterVarImp()</code> function:</p>
<blockquote>
<p>“The importance of each predictor is evaluated individually using a “filter” approach. For classification, ROC curve analysis is conducted on each predictor. For two class problems, a series of cutoffs is applied to the predictor data to predict the class. The sensitivity and specificity are computed for each cutoff and the ROC curve is computed. The trapezoidal rule is used to compute the area under the ROC curve. This area is used as the measure of variable importance. For multi–class outcomes, the problem is decomposed into all pair-wise problems and the area under the curve is calculated for each class pair (i.e class 1 vs. class 2, class 2 vs. class 3 etc.). For a specific class, the maximum area under the curve across the relevant pair–wise AUC’s is used as the variable importance measure. For regression, the relationship between each predictor and the outcome is evaluated. An argument, nonpara, is used to pick the model fitting technique. When nonpara = FALSE, a linear model is fit and the absolute value of the <span class="math inline">\(t\)</span>–value for the slope of the predictor is used. Otherwise, a loess smoother is fit between the outcome and the predictor. The <span class="math inline">\(R^2\)</span> statistic is calculated for this model against the intercept only null model.” caret help for <code>filterVarImp()</code></p>
</blockquote>
<pre class="r"><code>roc_imp &lt;- filterVarImp(x = wine_train[, -ncol(wine_train)], y = wine_train$quality)
p2 &lt;- roc_imp %&gt;%
  as.data.frame() %&gt;%
  rownames_to_column() %&gt;%
  ggplot(aes(x = reorder(rowname, qual_high), y = qual_high)) +
    geom_bar(stat = &quot;identity&quot;, fill = &quot;#1F77B4&quot;, alpha = 0.8) +
    coord_flip()</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, ncol = 2, widths = c(0.5, 0.5))</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/feature_imp_plots-1.png" width="960" /></p>
</div>
<div id="iml" class="section level4">
<h4>iml</h4>
<p>The <code>iml</code> package combines a number of methods for explaining/interpreting machine learning model, like</p>
<ul>
<li>Feature importance</li>
<li>Partial dependence plots</li>
<li>Individual conditional expectation plots (ICE)</li>
<li>Tree surrogate</li>
<li>LocalModel: Local Interpretable Model-agnostic Explanations (similar to <a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits_lime/">lime</a>)</li>
<li>Shapley value for explaining single predictions</li>
</ul>
<p>In order to work with <code>iml</code>, we need to adapt our data a bit by removing the response variable and the creating a new predictor object that holds the model, the data and the class labels.</p>
<blockquote>
<p>“The iml package uses R6 classes: New objects can be created by calling Predictor$new().” <a href="https://github.com/christophM/iml/blob/master/vignettes/intro.Rmd" class="uri">https://github.com/christophM/iml/blob/master/vignettes/intro.Rmd</a></p>
</blockquote>
<pre class="r"><code>X &lt;- wine_train %&gt;%
  select(-quality) %&gt;%
  as.data.frame()

predictor &lt;- Predictor$new(rf_model, data = X, y = wine_train$quality)
str(predictor)</code></pre>
<pre><code>## Classes &#39;Predictor&#39;, &#39;R6&#39; &lt;Predictor&gt;
##   Public:
##     class: NULL
##     clone: function (deep = FALSE) 
##     data: Data, R6
##     initialize: function (model = NULL, data, predict.fun = NULL, y = NULL, class = NULL) 
##     model: train, train.formula
##     predict: function (newdata) 
##     prediction.colnames: NULL
##     prediction.function: function (newdata) 
##     print: function () 
##     task: classification
##   Private:
##     predictionChecked: FALSE</code></pre>
<div id="partial-dependence-individual-conditional-expectation-plots-ice" class="section level5">
<h5>Partial Dependence &amp; Individual Conditional Expectation plots (ICE)</h5>
<p>Now we can explore some of the different methods. Let’s start with <a href="https://christophm.github.io/interpretable-ml-book/pdp.html">partial dependence plots</a> as we had already looked into feature importance.</p>
<blockquote>
<p>“Besides knowing which features were important, we are interested in how the features influence the predicted outcome. The Partial class implements partial dependence plots and individual conditional expectation curves. Each individual line represents the predictions (y-axis) for one data point when we change one of the features (e.g. ‘lstat’ on the x-axis). The highlighted line is the point-wise average of the individual lines and equals the partial dependence plot. The marks on the x-axis indicates the distribution of the ‘lstat’ feature, showing how relevant a region is for interpretation (little or no points mean that we should not over-interpret this region).” <a href="https://github.com/christophM/iml/blob/master/vignettes/intro.Rmd#partial-dependence" class="uri">https://github.com/christophM/iml/blob/master/vignettes/intro.Rmd#partial-dependence</a></p>
</blockquote>
<p>We can look at individual features, like the alcohol or pH and plot the curves:</p>
<pre class="r"><code>pdp_obj &lt;- Partial$new(predictor, feature = &quot;alcohol&quot;)
pdp_obj$center(min(wine_train$alcohol))
glimpse(pdp_obj$results)</code></pre>
<pre><code>## Observations: 51,240
## Variables: 5
## $ alcohol &lt;dbl&gt; 8.400000, 8.742105, 9.084211, 9.426316, 9.768421, 10.1...
## $ .class  &lt;fct&gt; qual_high, qual_high, qual_high, qual_high, qual_high,...
## $ .y.hat  &lt;dbl&gt; 0.00000000, 0.00259375, -0.02496406, -0.03126250, 0.02...
## $ .type   &lt;chr&gt; &quot;pdp&quot;, &quot;pdp&quot;, &quot;pdp&quot;, &quot;pdp&quot;, &quot;pdp&quot;, &quot;pdp&quot;, &quot;pdp&quot;, &quot;pdp&quot;...
## $ .id     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA...</code></pre>
<blockquote>
<p>“The partial dependence plot calculates and plots the dependence of f(X) on a single or two features. It’s the aggregate of all individual conditional expectation curves, that describe how, for a single observation, the prediction changes when the feature changes.” iml help for <code>Partial</code></p>
</blockquote>
<pre class="r"><code>pdp_obj$plot()</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/pd_plots-1.png" width="960" /></p>
<pre class="r"><code>pdp_obj2 &lt;- Partial$new(predictor, feature = c(&quot;sulphates&quot;, &quot;pH&quot;))
pdp_obj2$plot()</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/pd_plots_2-1.png" width="960" /></p>
</div>
<div id="feature-interaction" class="section level5">
<h5>Feature interaction</h5>
<blockquote>
<p>“Interactions between features are measured via the decomposition of the prediction function: If a feature j has no interaction with any other feature, the prediction function can be expressed as the sum of the partial function that depends only on j and the partial function that only depends on features other than j. If the variance of the full function is completely explained by the sum of the partial functions, there is no interaction between feature j and the other features. Any variance that is not explained can be attributed to the interaction and is used as a measure of interaction strength. The interaction strength between two features is the proportion of the variance of the 2-dimensional partial dependence function that is not explained by the sum of the two 1-dimensional partial dependence functions. The interaction measure takes on values between 0 (no interaction) to 1.” iml help for <code>Interaction</code></p>
</blockquote>
<pre class="r"><code>interact &lt;- Interaction$new(predictor, feature = &quot;alcohol&quot;)</code></pre>
<p>All of these methods have a plot argument. However, since I am writing this for an article, I want all my plots to have the same look. That’s why I’m customizing the plots I want to use in my article as shown below.</p>
<pre class="r"><code>#plot(interact)
interact$results %&gt;%
  ggplot(aes(x = reorder(.feature, .interaction), y = .interaction, fill = .class)) +
    facet_wrap(~ .class, ncol = 2) +
    geom_bar(stat = &quot;identity&quot;, alpha = 0.8) +
    scale_fill_tableau() +
    coord_flip() +
    guides(fill = FALSE)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/interaction_plot-1.png" width="672" /></p>
</div>
<div id="tree-surrogate" class="section level5">
<h5>Tree Surrogate</h5>
<p>The tree surrogate method uses decision trees on the predictions.</p>
<blockquote>
<p>“A conditional inference tree is fitted on the predicted  from the machine learning model and the data. The partykit package and function are used to fit the tree. By default a tree of maximum depth of 2 is fitted to improve interpretability.” iml help for <code>TreeSurrogate</code></p>
</blockquote>
<pre class="r"><code>tree &lt;- TreeSurrogate$new(predictor, maxdepth = 5)</code></pre>
<p>The R^2 value gives an estimate of the goodness of fit or how well the decision tree approximates the model.</p>
<pre class="r"><code>tree$r.squared</code></pre>
<pre><code>## [1] 0.4571756 0.4571756</code></pre>
<pre class="r"><code>#plot(tree)
tree$results %&gt;%
  mutate(prediction = colnames(select(., .y.hat.qual_high, .y.hat.qual_low))[max.col(select(., .y.hat.qual_high, .y.hat.qual_low),
                                                                                     ties.method = &quot;first&quot;)],
         prediction = ifelse(prediction == &quot;.y.hat.qual_low&quot;, &quot;qual_low&quot;, &quot;qual_high&quot;)) %&gt;%
  ggplot(aes(x = prediction, fill = prediction)) +
    facet_wrap(~ .path, ncol = 5) +
    geom_bar(alpha = 0.8) +
    scale_fill_tableau() +
    guides(fill = FALSE)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/tree_surr_plot-1.png" width="960" /></p>
</div>
<div id="localmodel-local-interpretable-model-agnostic-explanations" class="section level5">
<h5>LocalModel: Local Interpretable Model-agnostic Explanations</h5>
<p>LocalModel is a implementation of the LIME algorithm from <a href="https://arxiv.org/abs/1602.04938">Ribeiro et al. 2016</a>, similar to <a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits_lime/">lime</a>.</p>
<p>According to the LIME principle, we can look at individual predictions. Here, for example on the first row of the test set:</p>
<pre class="r"><code>X2 &lt;- wine_test[, -12]
i = 1
lime_explain &lt;- LocalModel$new(predictor, x.interest = X2[i, ])
lime_explain$results</code></pre>
<pre><code>##            beta x.recoded      effect x.original              feature
## 1 -0.7653408409       0.7 -0.53573859        0.7     volatile_acidity
## 2 -0.0006292149      34.0 -0.02139331         34 total_sulfur_dioxide
## 3  0.2624431667       9.4  2.46696577        9.4              alcohol
## 4  0.7653408409       0.7  0.53573859        0.7     volatile_acidity
## 5  0.0006292149      34.0  0.02139331         34 total_sulfur_dioxide
## 6 -0.2624431667       9.4 -2.46696577        9.4              alcohol
##             feature.value    .class
## 1    volatile_acidity=0.7 qual_high
## 2 total_sulfur_dioxide=34 qual_high
## 3             alcohol=9.4 qual_high
## 4    volatile_acidity=0.7  qual_low
## 5 total_sulfur_dioxide=34  qual_low
## 6             alcohol=9.4  qual_low</code></pre>
<pre class="r"><code>#plot(lime_explain)
p1 &lt;- lime_explain$results %&gt;%
  ggplot(aes(x = reorder(feature.value, -effect), y = effect, fill = .class)) +
    facet_wrap(~ .class, ncol = 2) +
    geom_bar(stat = &quot;identity&quot;, alpha = 0.8) +
    scale_fill_tableau() +
    coord_flip() +
    labs(title = paste0(&quot;Test case #&quot;, i)) +
    guides(fill = FALSE)</code></pre>
<p>… or for the sixth row:</p>
<pre class="r"><code>i = 6
lime_explain$explain(X2[i, ])
p2 &lt;- lime_explain$results %&gt;%
  ggplot(aes(x = reorder(feature.value, -effect), y = effect, fill = .class)) +
    facet_wrap(~ .class, ncol = 2) +
    geom_bar(stat = &quot;identity&quot;, alpha = 0.8) +
    scale_fill_tableau() +
    coord_flip() +
    labs(title = paste0(&quot;Test case #&quot;, i)) +
    guides(fill = FALSE)</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, ncol = 2)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/lime_plot_1-1.png" width="960" /></p>
</div>
<div id="shapley-value-for-explaining-single-predictions" class="section level5">
<h5>Shapley value for explaining single predictions</h5>
<p>Another way to interpret individual predictions is whit Shapley values.</p>
<blockquote>
<p>“Shapley computes feature contributions for single predictions with the Shapley value, an approach from cooperative game theory. The features values of an instance cooperate to achieve the prediction. The Shapley value fairly distributes the difference of the instance’s prediction and the datasets average prediction among the features.” iml help for <code>Shapley</code></p>
</blockquote>
<p>More information about Shapley values can be found <a href="https://christophm.github.io/interpretable-ml-book/shapley.html">here</a>.</p>
<pre class="r"><code>shapley &lt;- Shapley$new(predictor, x.interest = X2[1, ])</code></pre>
<pre class="r"><code>head(shapley$results)</code></pre>
<pre><code>##               feature     class      phi     phi.var
## 1       fixed_acidity qual_high -0.01100 0.003828485
## 2    volatile_acidity qual_high -0.16356 0.019123037
## 3         citric_acid qual_high -0.02318 0.005886472
## 4      residual_sugar qual_high -0.00950 0.001554939
## 5           chlorides qual_high -0.01580 0.002868889
## 6 free_sulfur_dioxide qual_high  0.00458 0.001250044
##            feature.value
## 1      fixed_acidity=7.4
## 2   volatile_acidity=0.7
## 3          citric_acid=0
## 4     residual_sugar=1.9
## 5        chlorides=0.076
## 6 free_sulfur_dioxide=11</code></pre>
<pre class="r"><code>#shapley$plot()
shapley$results %&gt;%
  ggplot(aes(x = reorder(feature.value, -phi), y = phi, fill = class)) +
    facet_wrap(~ class, ncol = 2) +
    geom_bar(stat = &quot;identity&quot;, alpha = 0.8) +
    scale_fill_tableau() +
    coord_flip() +
    guides(fill = FALSE)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/shapley_plot-1.png" width="672" /></p>
</div>
</div>
<div id="breakdown" class="section level4">
<h4>breakDown</h4>
<p>Another package worth mentioning is <a href="https://cran.r-project.org/web/packages/breakDown/index.html">breakDown</a>. It provides</p>
<blockquote>
<p>“Model agnostic tool for decomposition of predictions from black boxes. Break Down Table shows contributions of every variable to a final prediction. Break Down Plot presents variable contributions in a concise graphical way. This package work for binary classifiers and general regression models.” <a href="https://cran.r-project.org/web/packages/breakDown/index.html" class="uri">https://cran.r-project.org/web/packages/breakDown/index.html</a></p>
</blockquote>
<p>The <code>broken()</code> function decomposes model predictions and outputs the contributions of each feature to the final prediction.</p>
<pre class="r"><code>predict.function &lt;- function(model, new_observation) {
  predict(model, new_observation, type=&quot;prob&quot;)[,2]
}
predict.function(rf_model, X2[1, ])</code></pre>
<pre><code>## [1] 0.966</code></pre>
<pre class="r"><code>br &lt;- broken(model = rf_model, 
             new_observation = X2[1, ], 
             data = X, 
             baseline = &quot;Intercept&quot;, 
             predict.function = predict.function, 
             keep_distributions = TRUE)
br</code></pre>
<pre><code>##                             contribution
## (Intercept)                        0.000
## + alcohol = 9.4                    0.138
## + volatile_acidity = 0.7           0.097
## + sulphates = 0.56                 0.060
## + density = 0.9978                 0.038
## + pH = 3.51                        0.012
## + chlorides = 0.076                0.017
## + citric_acid = 0                  0.026
## + fixed_acidity = 7.4              0.048
## + residual_sugar = 1.9             0.014
## + free_sulfur_dioxide = 11         0.016
## + total_sulfur_dioxide = 34        0.034
## final_prognosis                    0.501
## baseline:  0.4654328</code></pre>
<p>The plot function shows the average predictions and the final prognosis:</p>
<pre class="r"><code>#plot(br)
data.frame(y = br$contribution,
           x = br$variable) %&gt;%
  ggplot(aes(x = reorder(x, y), y = y)) +
    geom_bar(stat = &quot;identity&quot;, fill = &quot;#1F77B4&quot;, alpha = 0.8) +
    coord_flip()</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/breakdown_plot_1-1.png" width="672" /></p>
<p>If we set <code>keep_distributions = TRUE</code>, we can plot these distributions of partial predictions, as well as the average.</p>
<pre class="r"><code>plot(br, plot_distributions = TRUE)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/breakdown_plot_2-1.png" width="672" /></p>
</div>
<div id="dalex-descriptive-machine-learning-explanations" class="section level4">
<h4>DALEX: Descriptive mAchine Learning EXplanations</h4>
<p>The third package I want to showcase is <a href="https://cran.r-project.org/web/packages/DALEX/index.html">DALEX</a>, which stands for Descriptive mAchine Learning EXplanations and contains a collection of functions that help with interpreting/explaining black-box models.</p>
<blockquote>
<p>“Machine Learning (ML) models are widely used and have various applications in classification or regression. Models created with boosting, bagging, stacking or similar techniques are often used due to their high performance, but such black-box models usually lack of interpretability. DALEX package contains various explainers that help to understand the link between input variables and model output. The single_variable() explainer extracts conditional response of a model as a function of a single selected variable. It is a wrapper over packages ‘pdp’ and ‘ALEPlot’. The single_prediction() explainer attributes parts of a model prediction to particular variables used in the model. It is a wrapper over ‘breakDown’ package. The variable_dropout() explainer calculates variable importance scores based on variable shuffling. All these explainers can be plotted with generic plot() function and compared across different models.” <a href="https://cran.r-project.org/web/packages/DALEX/index.html" class="uri">https://cran.r-project.org/web/packages/DALEX/index.html</a></p>
</blockquote>
<p>We first create an explain object, that has the correct structure for use with the <code>DALEX</code> package.</p>
<pre class="r"><code>p_fun &lt;- function(object, newdata){predict(object, newdata = newdata, type = &quot;prob&quot;)[, 2]}
yTest &lt;- as.numeric(wine_test$quality)

explainer_classif_rf &lt;- DALEX::explain(rf_model, label = &quot;rf&quot;,
                                       data = wine_test, y = yTest,
                                       predict_function = p_fun)</code></pre>
<div id="model-performance" class="section level5">
<h5>Model performance</h5>
<p>With DALEX we can do several things, for example analyze model performance as the distribution of residuals.</p>
<pre class="r"><code>mp_classif_rf &lt;- model_performance(explainer_classif_rf)</code></pre>
<pre class="r"><code>plot(mp_classif_rf)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/dalex_perf_plot_1-1.png" width="672" /></p>
<pre class="r"><code>plot(mp_classif_rf, geom = &quot;boxplot&quot;)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/dalex_perf_plot_2-1.png" width="672" /></p>
</div>
<div id="feature-importance-1" class="section level5">
<h5>Feature importance</h5>
<p>Feature importance can be measured with <code>variable_importance()</code> function, which gives the loss from variable dropout.</p>
<pre class="r"><code>vi_classif_rf &lt;- variable_importance(explainer_classif_rf, loss_function = loss_root_mean_square)</code></pre>
<pre class="r"><code>plot(vi_classif_rf)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/dalex_var_imp_plot-1.png" width="672" /></p>
</div>
<div id="variable-response" class="section level5">
<h5>Variable response</h5>
<p>And we can calculate the marginal response for a single variable with the <code>variable_response()</code> function.</p>
<blockquote>
<p>“Calculates the average model response as a function of a single selected variable. Use the ‘type’ parameter to select the type of marginal response to be calculated. Currently for numeric variables we have Partial Dependency and Accumulated Local Effects implemented. Current implementation uses the ‘pdp’ package (Brandon M. Greenwell (2017). pdp: An R Package for Constructing Partial Dependence Plots. The R Journal, 9(1), 421–436.) and ‘ALEPlot’ (Dan Apley (2017). ALEPlot: Accumulated Local Effects Plots and Partial Dependence Plots.)” DALEX help for <code>variable_response</code></p>
</blockquote>
<p>As <code>type</code> we can choose between ‘pdp’ for Partial Dependence Plots and ‘ale’ for Accumulated Local Effects.</p>
<pre class="r"><code>pdp_classif_rf  &lt;- variable_response(explainer_classif_rf, variable = &quot;alcohol&quot;, type = &quot;pdp&quot;)</code></pre>
<pre class="r"><code>plot(pdp_classif_rf)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/dalex_var_resp_plot_1-1.png" width="672" /></p>
<pre class="r"><code>ale_classif_rf  &lt;- variable_response(explainer_classif_rf, variable = &quot;alcohol&quot;, type = &quot;ale&quot;)
plot(ale_classif_rf)</code></pre>
<p><img src="/post/2018-07-20_explaining_ml_models_code_caret_iml_files/figure-html/dalex_var_resp_plot_2-1.png" width="672" /></p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] grid      stats     graphics  grDevices utils     datasets  methods  
## [8] base     
## 
## other attached packages:
##  [1] gower_0.1.2     glmnet_2.0-16   foreach_1.4.4   Matrix_1.2-14  
##  [5] bindrcpp_0.2.2  DALEX_0.2.3     breakDown_0.1.6 iml_0.5.1      
##  [9] ggthemes_3.5.0  ggridges_0.5.0  gridExtra_2.3   caret_6.0-80   
## [13] lattice_0.20-35 forcats_0.3.0   stringr_1.3.1   dplyr_0.7.6    
## [17] purrr_0.2.5     readr_1.1.1     tidyr_0.8.1     tibble_1.4.2   
## [21] ggplot2_3.0.0   tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.1.0        backports_1.1.2     plyr_1.8.4         
##   [4] lazyeval_0.2.1      sp_1.3-1            splines_3.5.1      
##   [7] AlgDesign_1.1-7.3   digest_0.6.15       htmltools_0.3.6    
##  [10] gdata_2.18.0        magrittr_1.5        checkmate_1.8.5    
##  [13] cluster_2.0.7-1     sfsmisc_1.1-2       Metrics_0.1.4      
##  [16] recipes_0.1.3       modelr_0.1.2        dimRed_0.1.0       
##  [19] gmodels_2.18.1      colorspace_1.3-2    rvest_0.3.2        
##  [22] haven_1.1.2         xfun_0.3            crayon_1.3.4       
##  [25] jsonlite_1.5        libcoin_1.0-1       ALEPlot_1.1        
##  [28] bindr_0.1.1         survival_2.42-3     iterators_1.0.9    
##  [31] glue_1.2.0          DRR_0.0.3           gtable_0.2.0       
##  [34] ipred_0.9-6         questionr_0.6.2     kernlab_0.9-26     
##  [37] ddalpha_1.3.4       DEoptimR_1.0-8      abind_1.4-5        
##  [40] scales_0.5.0        mvtnorm_1.0-8       miniUI_0.1.1.1     
##  [43] Rcpp_0.12.17        xtable_1.8-2        spData_0.2.9.0     
##  [46] magic_1.5-8         proxy_0.4-22        foreign_0.8-70     
##  [49] spdep_0.7-7         Formula_1.2-3       stats4_3.5.1       
##  [52] lava_1.6.2          prodlim_2018.04.18  httr_1.3.1         
##  [55] yaImpute_1.0-29     RColorBrewer_1.1-2  pkgconfig_2.0.1    
##  [58] nnet_7.3-12         deldir_0.1-15       labeling_0.3       
##  [61] tidyselect_0.2.4    rlang_0.2.1         reshape2_1.4.3     
##  [64] later_0.7.3         munsell_0.5.0       cellranger_1.1.0   
##  [67] tools_3.5.1         cli_1.0.0           factorMerger_0.3.6 
##  [70] pls_2.6-0           broom_0.4.5         evaluate_0.10.1    
##  [73] geometry_0.3-6      yaml_2.1.19         ModelMetrics_1.1.0 
##  [76] knitr_1.20          robustbase_0.93-1   pdp_0.6.0          
##  [79] randomForest_4.6-14 nlme_3.1-137        mime_0.5           
##  [82] RcppRoll_0.3.0      xml2_1.2.0          compiler_3.5.1     
##  [85] rstudioapi_0.7      e1071_1.6-8         klaR_0.6-14        
##  [88] stringi_1.2.3       highr_0.7           blogdown_0.6       
##  [91] psych_1.8.4         pillar_1.2.3        LearnBayes_2.15.1  
##  [94] combinat_0.0-8      data.table_1.11.4   httpuv_1.4.4.2     
##  [97] agricolae_1.2-8     R6_2.2.2            bookdown_0.7       
## [100] promises_1.0.1      codetools_0.2-15    boot_1.3-20        
## [103] MASS_7.3-50         gtools_3.8.1        assertthat_0.2.0   
## [106] CVST_0.2-2          rprojroot_1.3-2     withr_2.1.2        
## [109] mnormt_1.5-5        expm_0.999-2        parallel_3.5.1     
## [112] hms_0.4.2           rpart_4.1-13        timeDate_3043.102  
## [115] coda_0.19-1         class_7.3-14        rmarkdown_1.10     
## [118] inum_1.0-0          ggpubr_0.1.7        partykit_1.2-2     
## [121] shiny_1.1.0         lubridate_1.7.4</code></pre>
</div>
</div>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[My upcoming conference talks &amp; workshops: M-cubed, ML Summit &amp; data2day]]></title>
    <link href="/2018/07/mcubed_mlsummit_data2day/"/>
    <id>/2018/07/mcubed_mlsummit_data2day/</id>
    <published>2018-07-12T00:00:00+00:00</published>
    <updated>2018-07-12T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I’ll be giving talks and workshops at the following three upcoming conferences; hope to meet some of you there!</p>
<p><br></p>
<ul>
<li>From 15th to 17th October 2018, I’ll be in London for the <a href="https://www.mcubed.london/">M-cubed conference</a>. My talk about <a href="https://www.mcubed.london/sessions/explaining-complex-machine-learning-models-lime/">Explaining complex machine learning models with LIME</a> will take place on October 16</li>
</ul>
<blockquote>
<p>Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex machine learning models are essentially black boxes and too complicated to understand, we need to use approximations.</p>
</blockquote>
<blockquote>
<p>Required audience experience: Basic knowledge of machine learning</p>
</blockquote>
<blockquote>
<p>Objective of the talk: Listeners will get an overview of why understanding machine learning models is important, how it can help us improve models and help gain trust in their decisions. I will explain in detail how one popular approach to explaining complex models – LIME – works and show an example analysis.</p>
</blockquote>
<div class="figure">
<img src="https://www.mcubed.london/wp-content/uploads/2017/12/M3_Image_v1_b-1024x626.jpg" alt="M-cubed banner" />
<p class="caption">M-cubed banner</p>
</div>
<hr />
<p><br></p>
<ul>
<li>At the <a href="www.ml-summit.de">ML Summit</a> held on October 1st and 2nd in Berlin, Germany, I’ll be giving a workshop about <a href="https://ml-summit.de/specialized-topics/bildklassifikation-leicht-gemacht-mit-keras-und-tensorflow/">image classification with Keras</a> (<strong>German language</strong>).</li>
</ul>
<blockquote>
<p>Bildklassifikation leicht gemacht – mit Keras und TensorFlow</p>
</blockquote>
<blockquote>
<p>Tuesday, 2. October 2018 | 10:00 - 13:00 Lange Zeit galt die automatische Erkennung von Objekten, Menschen und Szenen auf Bildern durch Computer als unmöglich. Die Komplexität schien schlicht zu groß, um sie einem Algorithmus programmatisch beibringen zu können. Doch Neuronale Netze haben dies drastisch verändert! Inzwischen ist Bilderkennung ist ein weit verbreitetes Anwendungsgebiet von Maschinellem Lernen. Häufig werden dafür sogenannte “Convolutional Neuronal Networks”, oder “ConvNets” verwendet. In diesem Workshop werde ich zeigen, wie einfach es ist, solch ein Neuronales Netz selber zu bauen. Dafür werden wir Keras und TensorFlow verwenden. Wir werden zunächst ein komplettes Netz selber trainieren: vom Einlesen der Bilder, über das Definieren des Netzes, hin zum Evaluieren auf Testbildern. Anschließend gucken wir uns an, wie man mit Transfer Learning und vortrainierten Netzen auch mit wenigen eigenen Bildern schnell Erfolge sehen kann. Im letzten Teil des Workshops soll es dann darum gehen, wie wir diese Bilderkennungsmodelle besser verstehen können – zum Beispiel indem wir die Knoten in Zwischenschichten visualisieren; so können wir Muster und für die Klassifikation wichtige Bildbereiche finden und die Klassifikation durch das Modell nachvollziehen. Installationshinweise: Wir werden mit Python3 in Google Collaboratory arbeiten.</p>
</blockquote>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/MLS_18_Webbanner_300x250_46396_v1.jpg" alt="ML Summit banner" />
<p class="caption">ML Summit banner</p>
</div>
<hr />
<p><br></p>
<ul>
<li>Together with my colleague Mark, I’ll be giving a workshop about <a href="https://www.data2day.de/veranstaltung-6953-end-2-end-vom-keras-tensorflow-modell-zur-produktion.html?id=6953">“END-2-END VOM KERAS TENSORFLOW-MODELL ZUR PRODUKTION”</a> at the data2day conference, which is being held from September 25th - 27th 2018 in Heidelberg, Germany (<strong>German language</strong>).</li>
</ul>
<blockquote>
<p>Durch das stark wachsende Datenvolumen hat sich das Rollenverständnis von Data Scientists erweitert. Statt Machine-Learning-Modelle für einmalige Analysen zu erstellen, wird häufiger in konkreten Entwicklungsprojekten gearbeitet, in denen Prototypen in produktive Anwendungen überführt werden. Keras ist eine High-Level-Schnittstelle, die ein schnelles, einfaches und flexibles Prototypisieren von Neuronalen Netzwerken mit TensorFlow ermöglicht. Zusammen mit Luigi lassen sich beliebig komplexe Datenverarbeitungs-Workflows in Python erstellen. Das führt dazu, dass auch Nicht-Entwickler den End-2-End-Workflow des Keras-TensorFlow-Modells zur Produktionsreife leicht implementieren können.</p>
</blockquote>
<div class="figure">
<img src="https://www.data2day.de/common/images/konferenzen/data2day2018.svg" alt="data2day banner" />
<p class="caption">data2day banner</p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Addendum: Text-to-Speech with the googleLanguageR package]]></title>
    <link href="/2018/06/googlelanguager/"/>
    <id>/2018/06/googlelanguager/</id>
    <published>2018-06-29T00:00:00+00:00</published>
    <updated>2018-06-29T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>After posting my short blog post about <a href="https://shirinsplayground.netlify.com/2018/06/text_to_speech_r/">Text-to-speech with R</a>, I got two very useful tips. One was to use the <code>googleLanguageR</code> package, which uses the Google Cloud Text-to-Speech API.</p>
<p>And indeed, it was very easy to use and the resulting audio sounded much better than what I tried before!</p>
<p>Here’s a short example of how to use the package for TTS:</p>
<div id="set-up-google-cloud-and-authentification" class="section level2">
<h2>Set up Google Cloud and authentification</h2>
<p>You first need to set up a Google Cloud Account and provide credit card information (the first year is free to use, though). If you haven’t used Google Cloud before, you will need to wait until you activated your account; in my case, I had to wait two days until they sent a small amount of money to my bank account which I then needed to enter in order to verify the information.</p>
<p>Then, you create a project, activate the API(s) you want to use and create the authentication information as a JSON file. More information on how to do this is described <a href="http://code.markedmondson.me/googleLanguageR/index.html">here</a>.</p>
<p>Install and load the library and give the path to the saved authentication JSON file.</p>
<pre class="r"><code>library(googleLanguageR)
gl_auth(&quot;path_to_authentication.json&quot;)</code></pre>
<div id="text-to-speech-with-googlelanguager" class="section level3">
<h3>Text-to-Speech with googleLanguageR</h3>
<p>Now, we can use the <a href="http://code.markedmondson.me/googleLanguageR/articles/text-to-speech.html">Google Cloud Text-to-Speech API</a> from R.</p>
<blockquote>
<p>“Google Cloud Text-to-Speech enables developers to synthesize natural-sounding speech with 30 voices, available in multiple languages and variants. It applies DeepMind’s groundbreaking research in WaveNet and Google’s powerful neural networks to deliver the highest fidelity possible. With this easy-to-use API, you can create lifelike interactions with your users, across many applications and devices.” <a href="http://code.markedmondson.me/googleLanguageR/articles/text-to-speech.html" class="uri">http://code.markedmondson.me/googleLanguageR/articles/text-to-speech.html</a></p>
</blockquote>
<pre class="r"><code>content &lt;- &quot;A common mistake that people make when trying to design something completely foolproof is to underestimate the ingenuity of complete fools.&quot;</code></pre>
<p>We can ask for a list of languages with the <code>gl_talk_languages()</code> function; here, I am looking at all English language options:</p>
<pre class="r"><code>gl_talk_languages(languageCode = &quot;en&quot;)</code></pre>
<pre><code>## # A tibble: 18 x 4
##    languageCodes name             ssmlGender naturalSampleRateHertz
##    &lt;chr&gt;         &lt;chr&gt;            &lt;chr&gt;                       &lt;int&gt;
##  1 en-US         en-US-Wavenet-D  MALE                        24000
##  2 en-US         en-US-Wavenet-A  MALE                        24000
##  3 en-US         en-US-Wavenet-B  MALE                        24000
##  4 en-US         en-US-Wavenet-C  FEMALE                      24000
##  5 en-US         en-US-Wavenet-E  FEMALE                      24000
##  6 en-US         en-US-Wavenet-F  FEMALE                      24000
##  7 en-GB         en-GB-Standard-A FEMALE                      24000
##  8 en-GB         en-GB-Standard-B MALE                        24000
##  9 en-GB         en-GB-Standard-C FEMALE                      24000
## 10 en-GB         en-GB-Standard-D MALE                        24000
## 11 en-US         en-US-Standard-B MALE                        24000
## 12 en-US         en-US-Standard-C FEMALE                      24000
## 13 en-US         en-US-Standard-D MALE                        24000
## 14 en-US         en-US-Standard-E FEMALE                      24000
## 15 en-AU         en-AU-Standard-A FEMALE                      24000
## 16 en-AU         en-AU-Standard-B MALE                        24000
## 17 en-AU         en-AU-Standard-C FEMALE                      24000
## 18 en-AU         en-AU-Standard-D MALE                        24000</code></pre>
<p>Let’s try with three:</p>
<pre class="r"><code>names &lt;- c(&quot;en-US-Wavenet-D&quot;, &quot;en-GB-Standard-C&quot;, &quot;en-AU-Standard-A&quot;)</code></pre>
<pre class="r"><code>for (name in names) {
  gl_talk(content, 
        output = paste0(&quot;/Users/shiringlander/Documents/Github/output_&quot;, name, &quot;.wav&quot;),
        name = name,
        speakingRate = 0.9)
}</code></pre>
<p>The audio files are again <a href="https://soundcloud.com/shirin-glander-729692416/sets/addendum-text-to-speech-with-the-googlelanguager-package/s-oyaAe">saved on SoundCloud</a>.</p>
<p>My verdict: Great package! Great API! :-)</p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.0 (2018-04-23)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.5
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] googleLanguageR_0.2.0
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_0.12.17      knitr_1.20        magrittr_1.5     
##  [4] R6_2.2.2          rlang_0.2.1       stringr_1.3.1    
##  [7] httr_1.3.1        tools_3.5.0       xfun_0.2         
## [10] utf8_1.1.4        cli_1.0.0         googleAuthR_0.6.3
## [13] htmltools_0.3.6   openssl_1.0.1     yaml_2.1.19      
## [16] rprojroot_1.3-2   digest_0.6.15     assertthat_0.2.0 
## [19] tibble_1.4.2      crayon_1.3.4      bookdown_0.7     
## [22] purrr_0.2.5       base64enc_0.1-3   curl_3.2         
## [25] memoise_1.1.0     evaluate_0.10.1   rmarkdown_1.10   
## [28] blogdown_0.6      stringi_1.2.3     pillar_1.2.3     
## [31] compiler_3.5.0    backports_1.1.2   jsonlite_1.5</code></pre>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Code for Workshop: Introduction to Machine Learning with R]]></title>
    <link href="/2018/06/intro_to_ml_workshop_heidelberg/"/>
    <id>/2018/06/intro_to_ml_workshop_heidelberg/</id>
    <published>2018-06-29T00:00:00+00:00</published>
    <updated>2018-06-29T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are the slides from my workshop: Introduction to Machine Learning with R which I gave at the University of Heidelberg, Germany on June 28th 2018. The entire code accompanying the workshop can be found below the video.</p>
<p>The workshop covered the basics of machine learning. With an example dataset I went through a standard machine learning workflow in R with the packages caret and h2o:</p>
<ul>
<li>reading in data</li>
<li>exploratory data analysis</li>
<li>missingness</li>
<li>feature engineering</li>
<li>training and test split</li>
<li>model training with Random Forests, Gradient Boosting, Neural Nets, etc.</li>
<li>hyperparameter tuning</li>
</ul>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/lRX4QJ5TvxgWSv" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen>
</iframe>
<div style="margin-bottom:5px">
<strong> <a href="//www.slideshare.net/ShirinGlander/workshop-introduction-to-machine-learning-with-r" title="Workshop - Introduction to Machine Learning with R" target="_blank">Workshop - Introduction to Machine Learning with R</a> </strong> from <strong><a href="https://www.slideshare.net/ShirinGlander" target="_blank">Shirin Glander</a></strong>
</div>
<p><br></p>
<hr />
<div id="setup" class="section level2">
<h2>Setup</h2>
<p>All analyses are done in R using RStudio. For detailed session information including R version, operating system and package versions, see the <code>sessionInfo()</code> output at the end of this document.</p>
<p>All figures are produced with ggplot2.</p>
<ul>
<li>libraries</li>
</ul>
<pre class="r"><code>library(tidyverse) # for tidy data analysis
library(readr)     # for fast reading of input files
library(mice)      # mice package for Multivariate Imputation by Chained Equations (MICE)</code></pre>
<p><br></p>
</div>
<div id="data-preparation" class="section level2 tabset tabset-fade tabset-pills">
<h2>Data preparation</h2>
<div id="the-dataset" class="section level3">
<h3>The dataset</h3>
<p>The dataset I am using in these example analyses, is the <strong>Breast Cancer Wisconsin (Diagnostic) Dataset</strong>. The data was downloaded from the <a href="http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29">UC Irvine Machine Learning Repository</a>.</p>
<p>The first dataset looks at the predictor classes:</p>
<ul>
<li>malignant or</li>
<li>benign breast mass.</li>
</ul>
<p>The features characterise cell nucleus properties and were generated from image analysis of <a href="https://en.wikipedia.org/wiki/Fine-needle_aspiration">fine needle aspirates (FNA)</a> of breast masses:</p>
<ul>
<li>Sample ID (code number)</li>
<li>Clump thickness</li>
<li>Uniformity of cell size</li>
<li>Uniformity of cell shape</li>
<li>Marginal adhesion</li>
<li>Single epithelial cell size</li>
<li>Number of bare nuclei</li>
<li>Bland chromatin</li>
<li>Number of normal nuclei</li>
<li>Mitosis</li>
<li>Classes, i.e. diagnosis</li>
</ul>
<pre class="r"><code>bc_data &lt;- read_delim(&quot;datasets/breast-cancer-wisconsin.data.txt&quot;,
                      delim = &quot;,&quot;,
                      col_names = c(&quot;sample_code_number&quot;, 
                       &quot;clump_thickness&quot;, 
                       &quot;uniformity_of_cell_size&quot;, 
                       &quot;uniformity_of_cell_shape&quot;, 
                       &quot;marginal_adhesion&quot;, 
                       &quot;single_epithelial_cell_size&quot;, 
                       &quot;bare_nuclei&quot;, 
                       &quot;bland_chromatin&quot;, 
                       &quot;normal_nucleoli&quot;, 
                       &quot;mitosis&quot;, 
                       &quot;classes&quot;)) %&gt;%
  mutate(bare_nuclei = as.numeric(bare_nuclei),
         classes = ifelse(classes == &quot;2&quot;, &quot;benign&quot;,
                          ifelse(classes == &quot;4&quot;, &quot;malignant&quot;, NA)))</code></pre>
<pre class="r"><code>summary(bc_data)</code></pre>
<pre><code>##  sample_code_number clump_thickness  uniformity_of_cell_size
##  Min.   :   61634   Min.   : 1.000   Min.   : 1.000         
##  1st Qu.:  870688   1st Qu.: 2.000   1st Qu.: 1.000         
##  Median : 1171710   Median : 4.000   Median : 1.000         
##  Mean   : 1071704   Mean   : 4.418   Mean   : 3.134         
##  3rd Qu.: 1238298   3rd Qu.: 6.000   3rd Qu.: 5.000         
##  Max.   :13454352   Max.   :10.000   Max.   :10.000         
##                                                             
##  uniformity_of_cell_shape marginal_adhesion single_epithelial_cell_size
##  Min.   : 1.000           Min.   : 1.000    Min.   : 1.000             
##  1st Qu.: 1.000           1st Qu.: 1.000    1st Qu.: 2.000             
##  Median : 1.000           Median : 1.000    Median : 2.000             
##  Mean   : 3.207           Mean   : 2.807    Mean   : 3.216             
##  3rd Qu.: 5.000           3rd Qu.: 4.000    3rd Qu.: 4.000             
##  Max.   :10.000           Max.   :10.000    Max.   :10.000             
##                                                                        
##   bare_nuclei     bland_chromatin  normal_nucleoli     mitosis      
##  Min.   : 1.000   Min.   : 1.000   Min.   : 1.000   Min.   : 1.000  
##  1st Qu.: 1.000   1st Qu.: 2.000   1st Qu.: 1.000   1st Qu.: 1.000  
##  Median : 1.000   Median : 3.000   Median : 1.000   Median : 1.000  
##  Mean   : 3.545   Mean   : 3.438   Mean   : 2.867   Mean   : 1.589  
##  3rd Qu.: 6.000   3rd Qu.: 5.000   3rd Qu.: 4.000   3rd Qu.: 1.000  
##  Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :10.000  
##  NA&#39;s   :16                                                         
##    classes         
##  Length:699        
##  Class :character  
##  Mode  :character  
##                    
##                    
##                    
## </code></pre>
<p><br></p>
</div>
<div id="missing-data" class="section level3">
<h3>Missing data</h3>
<pre class="r"><code># how many NAs are in the data
md.pattern(bc_data, plot = FALSE)</code></pre>
<pre><code>##     sample_code_number clump_thickness uniformity_of_cell_size
## 683                  1               1                       1
## 16                   1               1                       1
##                      0               0                       0
##     uniformity_of_cell_shape marginal_adhesion single_epithelial_cell_size
## 683                        1                 1                           1
## 16                         1                 1                           1
##                            0                 0                           0
##     bland_chromatin normal_nucleoli mitosis classes bare_nuclei   
## 683               1               1       1       1           1  0
## 16                1               1       1       1           0  1
##                   0               0       0       0          16 16</code></pre>
<pre class="r"><code>bc_data &lt;- bc_data %&gt;%
  drop_na() %&gt;%
  select(classes, everything(), -sample_code_number)
head(bc_data)</code></pre>
<pre><code>## # A tibble: 6 x 10
##   classes   clump_thickness uniformity_of_cell_si… uniformity_of_cell_sha…
##   &lt;chr&gt;               &lt;int&gt;                  &lt;int&gt;                   &lt;int&gt;
## 1 benign                  5                      1                       1
## 2 benign                  5                      4                       4
## 3 benign                  3                      1                       1
## 4 benign                  6                      8                       8
## 5 benign                  4                      1                       1
## 6 malignant               8                     10                      10
## # ... with 6 more variables: marginal_adhesion &lt;int&gt;,
## #   single_epithelial_cell_size &lt;int&gt;, bare_nuclei &lt;dbl&gt;,
## #   bland_chromatin &lt;int&gt;, normal_nucleoli &lt;int&gt;, mitosis &lt;int&gt;</code></pre>
<p>Missing values can be imputed with the <em>mice</em> package.</p>
<p>More info and tutorial with code: <a href="https://shirinsplayground.netlify.com/2018/04/flu_prediction/" class="uri">https://shirinsplayground.netlify.com/2018/04/flu_prediction/</a></p>
<p><br></p>
</div>
<div id="data-exploration" class="section level3">
<h3>Data exploration</h3>
<ul>
<li>Response variable for classification</li>
</ul>
<pre class="r"><code>ggplot(bc_data, aes(x = classes, fill = classes)) +
  geom_bar()</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/response_classification-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>More info on dealing with unbalanced classes: <a href="https://shiring.github.io/machine_learning/2017/04/02/unbalanced" class="uri">https://shiring.github.io/machine_learning/2017/04/02/unbalanced</a></p>
<p><br></p>
<ul>
<li>Response variable for regression</li>
</ul>
<pre class="r"><code>ggplot(bc_data, aes(x = clump_thickness)) +
  geom_histogram(bins = 10)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/response_regression-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><br></p>
<ul>
<li>Features</li>
</ul>
<pre class="r"><code>gather(bc_data, x, y, clump_thickness:mitosis) %&gt;%
  ggplot(aes(x = y, color = classes, fill = classes)) +
    geom_density(alpha = 0.3) +
    facet_wrap( ~ x, scales = &quot;free&quot;, ncol = 3)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/features-1.png" width="768" style="display: block; margin: auto;" /></p>
<p><br></p>
<ul>
<li>Correlation graphs</li>
</ul>
<pre class="r"><code>co_mat_benign &lt;- filter(bc_data, classes == &quot;benign&quot;) %&gt;%
  select(-1) %&gt;%
  cor()

co_mat_malignant &lt;- filter(bc_data, classes == &quot;malignant&quot;) %&gt;%
  select(-1) %&gt;%
  cor()

library(igraph)
g_benign &lt;- graph.adjacency(co_mat_benign,
                         weighted = TRUE,
                         diag = FALSE,
                         mode = &quot;upper&quot;)

g_malignant &lt;- graph.adjacency(co_mat_malignant,
                         weighted = TRUE,
                         diag = FALSE,
                         mode = &quot;upper&quot;)


# http://kateto.net/networks-r-igraph

cut.off_b &lt;- mean(E(g_benign)$weight)
cut.off_m &lt;- mean(E(g_malignant)$weight)

g_benign_2 &lt;- delete_edges(g_benign, E(g_benign)[weight &lt; cut.off_b])
g_malignant_2 &lt;- delete_edges(g_malignant, E(g_malignant)[weight &lt; cut.off_m])

c_g_benign_2 &lt;- cluster_fast_greedy(g_benign_2) 
c_g_malignant_2 &lt;- cluster_fast_greedy(g_malignant_2) </code></pre>
<pre class="r"><code>par(mfrow = c(1,2))

plot(c_g_benign_2, g_benign_2,
     vertex.size = colSums(co_mat_benign) * 10,
     vertex.frame.color = NA, 
     vertex.label.color = &quot;black&quot;, 
     vertex.label.cex = 0.8,
     edge.width = E(g_benign_2)$weight * 15,
     layout = layout_with_fr(g_benign_2),
     main = &quot;Benign tumors&quot;)

plot(c_g_malignant_2, g_malignant_2,
     vertex.size = colSums(co_mat_malignant) * 10,
     vertex.frame.color = NA, 
     vertex.label.color = &quot;black&quot;, 
     vertex.label.cex = 0.8,
     edge.width = E(g_malignant_2)$weight * 15,
     layout = layout_with_fr(g_malignant_2),
     main = &quot;Malignant tumors&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/cor_graph-1.png" width="1152" /></p>
<p><br></p>
</div>
<div id="principal-component-analysis" class="section level3">
<h3>Principal Component Analysis</h3>
<pre class="r"><code>library(ellipse)

# perform pca and extract scores
pcaOutput &lt;- prcomp(as.matrix(bc_data[, -1]), scale = TRUE, center = TRUE)
pcaOutput2 &lt;- as.data.frame(pcaOutput$x)
  
# define groups for plotting
pcaOutput2$groups &lt;- bc_data$classes
  
centroids &lt;- aggregate(cbind(PC1, PC2) ~ groups, pcaOutput2, mean)

conf.rgn  &lt;- do.call(rbind, lapply(unique(pcaOutput2$groups), function(t)
  data.frame(groups = as.character(t),
             ellipse(cov(pcaOutput2[pcaOutput2$groups == t, 1:2]),
                   centre = as.matrix(centroids[centroids$groups == t, 2:3]),
                   level = 0.95),
             stringsAsFactors = FALSE)))
    
ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, group = groups, color = groups)) + 
    geom_polygon(data = conf.rgn, aes(fill = groups), alpha = 0.2) +
    geom_point(size = 2, alpha = 0.6) + 
    labs(color = &quot;&quot;,
         fill = &quot;&quot;) </code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/pca-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="multidimensional-scaling" class="section level3">
<h3>Multidimensional Scaling</h3>
<pre class="r"><code>select(bc_data, -1) %&gt;%
  dist() %&gt;%
  cmdscale %&gt;%
  as.data.frame() %&gt;%
  mutate(group = bc_data$classes) %&gt;%
  ggplot(aes(x = V1, y = V2, color = group)) +
    geom_point()</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/mds_plot-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="t-sne-dimensionality-reduction" class="section level3">
<h3>t-SNE dimensionality reduction</h3>
<pre class="r"><code>library(tsne)

select(bc_data, -1) %&gt;%
  dist() %&gt;%
  tsne() %&gt;%
  as.data.frame() %&gt;%
  mutate(group = bc_data$classes) %&gt;%
  ggplot(aes(x = V1, y = V2, color = group)) +
    geom_point()</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/tsne_plot-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
</div>
<div id="machine-learning-packages-for-r" class="section level2 tabset tabset-fade tabset-pills">
<h2>Machine Learning packages for R</h2>
<div id="caret" class="section level3">
<h3><a href="http://topepo.github.io/caret/index.html">caret</a></h3>
<pre class="r"><code># configure multicore
library(doParallel)
cl &lt;- makeCluster(detectCores())
registerDoParallel(cl)

library(caret)</code></pre>
<p><br></p>
<div id="training-validation-and-test-data" class="section level4">
<h4>Training, validation and test data</h4>
<pre class="r"><code>set.seed(42)
index &lt;- createDataPartition(bc_data$classes, p = 0.7, list = FALSE)
train_data &lt;- bc_data[index, ]
test_data  &lt;- bc_data[-index, ]</code></pre>
<pre class="r"><code>bind_rows(data.frame(group = &quot;train&quot;, train_data),
      data.frame(group = &quot;test&quot;, test_data)) %&gt;%
  gather(x, y, clump_thickness:mitosis) %&gt;%
  ggplot(aes(x = y, color = group, fill = group)) +
    geom_density(alpha = 0.3) +
    facet_wrap( ~ x, scales = &quot;free&quot;, ncol = 3)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/distribution-1.png" width="768" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="regression" class="section level4">
<h4>Regression</h4>
<pre class="r"><code>set.seed(42)
model_glm &lt;- caret::train(clump_thickness ~ .,
                          data = train_data,
                          method = &quot;glm&quot;,
                          preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                          trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))</code></pre>
<pre class="r"><code>model_glm</code></pre>
<pre><code>## Generalized Linear Model 
## 
## 479 samples
##   9 predictor
## 
## Pre-processing: scaled (9), centered (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 432, 431, 432, 431, 431, 431, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   1.972314  0.5254215  1.648832</code></pre>
<pre class="r"><code>predictions &lt;- predict(model_glm, test_data)</code></pre>
<pre class="r"><code># model_glm$finalModel$linear.predictors == model_glm$finalModel$fitted.values
data.frame(residuals = resid(model_glm),
           predictors = model_glm$finalModel$linear.predictors) %&gt;%
  ggplot(aes(x = predictors, y = residuals)) +
    geom_jitter() +
    geom_smooth(method = &quot;lm&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/residuals-1.png" width="480" style="display: block; margin: auto;" /></p>
<pre class="r"><code># y == train_data$clump_thickness
data.frame(residuals = resid(model_glm),
           y = model_glm$finalModel$y) %&gt;%
  ggplot(aes(x = y, y = residuals)) +
    geom_jitter() +
    geom_smooth(method = &quot;lm&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/residuals-2.png" width="480" style="display: block; margin: auto;" /></p>
<pre class="r"><code>data.frame(actual = test_data$clump_thickness,
           predicted = predictions) %&gt;%
  ggplot(aes(x = actual, y = predicted)) +
    geom_jitter() +
    geom_smooth(method = &quot;lm&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/regression_result-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="classification" class="section level4">
<h4>Classification</h4>
<div id="decision-trees" class="section level5">
<h5>Decision trees</h5>
<p><a href="https://cran.r-project.org/web/packages/rpart/rpart.pdf">rpart</a></p>
<pre class="r"><code>library(rpart)
library(rpart.plot)

set.seed(42)
fit &lt;- rpart(classes ~ .,
            data = train_data,
            method = &quot;class&quot;,
            control = rpart.control(xval = 10, 
                                    minbucket = 2, 
                                    cp = 0), 
             parms = list(split = &quot;information&quot;))

rpart.plot(fit, extra = 100)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/decision_tree-1.png" width="960" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
</div>
<div id="random-forests" class="section level4">
<h4>Random Forests</h4>
<p><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">Random Forests</a> predictions are based on the generation of multiple classification trees. They can be used for both, classification and regression tasks. Here, I show a classification task.</p>
<pre class="r"><code>set.seed(42)
model_rf &lt;- caret::train(classes ~ .,
                         data = train_data,
                         method = &quot;rf&quot;,
                         preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                         trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 5, 
                                                  repeats = 3, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))</code></pre>
<p>When you specify <code>savePredictions = TRUE</code>, you can access the cross-validation resuls with <code>model_rf$pred</code>.</p>
<pre class="r"><code>model_rf</code></pre>
<pre><code>## Random Forest 
## 
## 479 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: scaled (9), centered (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 432, 431, 431, 431, 431, 431, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##   2     0.9776753  0.9513499
##   5     0.9757957  0.9469999
##   9     0.9714200  0.9370285
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<pre class="r"><code>model_rf$finalModel$confusion</code></pre>
<pre><code>##           benign malignant class.error
## benign       304         7  0.02250804
## malignant      5       163  0.02976190</code></pre>
</div>
<div id="dealing-with-unbalanced-data" class="section level4">
<h4>Dealing with unbalanced data</h4>
<p>Luckily, caret makes it very easy to incorporate over- and under-sampling techniques with cross-validation resampling. We can simply add the sampling option to our trainControl and choose down for under- (also called down-) sampling. The rest stays the same as with our original model.</p>
<pre class="r"><code>set.seed(42)
model_rf_down &lt;- caret::train(classes ~ .,
                         data = train_data,
                         method = &quot;rf&quot;,
                         preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                         trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE,
                                                  sampling = &quot;down&quot;))</code></pre>
<pre class="r"><code>model_rf_down</code></pre>
<pre><code>## Random Forest 
## 
## 479 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: scaled (9), centered (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 432, 431, 431, 431, 431, 431, ... 
## Addtional sampling using down-sampling prior to pre-processing
## 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##   2     0.9797503  0.9563138
##   5     0.9741198  0.9438326
##   9     0.9699578  0.9346310
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<p><br></p>
</div>
<div id="feature-importance" class="section level4">
<h4>Feature Importance</h4>
<pre class="r"><code>imp &lt;- model_rf$finalModel$importance
imp[order(imp, decreasing = TRUE), ]</code></pre>
<pre><code>##     uniformity_of_cell_size    uniformity_of_cell_shape 
##                   43.936945                   39.840595 
##                 bare_nuclei             bland_chromatin 
##                   33.820345                   31.984813 
##             normal_nucleoli single_epithelial_cell_size 
##                   21.686039                   17.761202 
##             clump_thickness           marginal_adhesion 
##                   16.318817                    9.518437 
##                     mitosis 
##                    2.220633</code></pre>
<pre class="r"><code># estimate variable importance
importance &lt;- varImp(model_rf, scale = TRUE)
plot(importance)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/importance_rf-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><br></p>
<ul>
<li>predicting test data</li>
</ul>
<pre class="r"><code>confusionMatrix(predict(model_rf, test_data), as.factor(test_data$classes))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  benign malignant
##   benign       128         4
##   malignant      5        67
##                                           
##                Accuracy : 0.9559          
##                  95% CI : (0.9179, 0.9796)
##     No Information Rate : 0.652           
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.9031          
##  Mcnemar&#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.9624          
##             Specificity : 0.9437          
##          Pos Pred Value : 0.9697          
##          Neg Pred Value : 0.9306          
##              Prevalence : 0.6520          
##          Detection Rate : 0.6275          
##    Detection Prevalence : 0.6471          
##       Balanced Accuracy : 0.9530          
##                                           
##        &#39;Positive&#39; Class : benign          
## </code></pre>
<pre class="r"><code>results &lt;- data.frame(actual = test_data$classes,
                      predict(model_rf, test_data, type = &quot;prob&quot;))

results$prediction &lt;- ifelse(results$benign &gt; 0.5, &quot;benign&quot;,
                             ifelse(results$malignant &gt; 0.5, &quot;malignant&quot;, NA))

results$correct &lt;- ifelse(results$actual == results$prediction, TRUE, FALSE)

ggplot(results, aes(x = prediction, fill = correct)) +
  geom_bar(position = &quot;dodge&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/results_bar_rf-1.png" width="480" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(results, aes(x = prediction, y = benign, color = correct, shape = correct)) +
  geom_jitter(size = 3, alpha = 0.6)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/results_jitter_rf-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="extreme-gradient-boosting-trees" class="section level4">
<h4>Extreme gradient boosting trees</h4>
<p><a href="http://xgboost.readthedocs.io/en/latest/model.html">Extreme gradient boosting (XGBoost)</a> is a faster and improved implementation of <a href="https://en.wikipedia.org/wiki/Gradient_boosting">gradient boosting</a> for supervised learning.</p>
<blockquote>
<p>“XGBoost uses a more regularized model formalization to control over-fitting, which gives it better performance.” Tianqi Chen, developer of xgboost</p>
</blockquote>
<p>XGBoost is a tree ensemble model, which means the sum of predictions from a set of classification and regression trees (CART). In that, XGBoost is similar to Random Forests but it uses a different approach to model training. Can be used for classification and regression tasks. Here, I show a classification task.</p>
<pre class="r"><code>set.seed(42)
model_xgb &lt;- caret::train(classes ~ .,
                          data = train_data,
                          method = &quot;xgbTree&quot;,
                          preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                          trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 5, 
                                                  repeats = 3, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))</code></pre>
<pre class="r"><code>model_xgb</code></pre>
<pre><code>## eXtreme Gradient Boosting 
## 
## 479 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: scaled (9), centered (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 432, 431, 431, 431, 431, 431, ... 
## Resampling results across tuning parameters:
## 
##   eta  max_depth  colsample_bytree  subsample  nrounds  Accuracy 
##   0.3  1          0.6               0.50        50      0.9567788
##   0.3  1          0.6               0.50       100      0.9544912
##   0.3  1          0.6               0.50       150      0.9513572
##   0.3  1          0.6               0.75        50      0.9576164
##   0.3  1          0.6               0.75       100      0.9536448
##   0.3  1          0.6               0.75       150      0.9525987
##   0.3  1          0.6               1.00        50      0.9559409
##   0.3  1          0.6               1.00       100      0.9555242
##   0.3  1          0.6               1.00       150      0.9551031
##   0.3  1          0.8               0.50        50      0.9718588
##   0.3  1          0.8               0.50       100      0.9720583
##   0.3  1          0.8               0.50       150      0.9699879
##   0.3  1          0.8               0.75        50      0.9726964
##   0.3  1          0.8               0.75       100      0.9724664
##   0.3  1          0.8               0.75       150      0.9705868
##   0.3  1          0.8               1.00        50      0.9714202
##   0.3  1          0.8               1.00       100      0.9710035
##   0.3  1          0.8               1.00       150      0.9705866
##   0.3  2          0.6               0.50        50      0.9559448
##   0.3  2          0.6               0.50       100      0.9565397
##   0.3  2          0.6               0.50       150      0.9555063
##   0.3  2          0.6               0.75        50      0.9530150
##   0.3  2          0.6               0.75       100      0.9550985
##   0.3  2          0.6               0.75       150      0.9551070
##   0.3  2          0.6               1.00        50      0.9532320
##   0.3  2          0.6               1.00       100      0.9551072
##   0.3  2          0.6               1.00       150      0.9557237
##   0.3  2          0.8               0.50        50      0.9720583
##   0.3  2          0.8               0.50       100      0.9735166
##   0.3  2          0.8               0.50       150      0.9720540
##   0.3  2          0.8               0.75        50      0.9722494
##   0.3  2          0.8               0.75       100      0.9726703
##   0.3  2          0.8               0.75       150      0.9716374
##   0.3  2          0.8               1.00        50      0.9716327
##   0.3  2          0.8               1.00       100      0.9724622
##   0.3  2          0.8               1.00       150      0.9718416
##   0.3  3          0.6               0.50        50      0.9548905
##   0.3  3          0.6               0.50       100      0.9557237
##   0.3  3          0.6               0.50       150      0.9555198
##   0.3  3          0.6               0.75        50      0.9561404
##   0.3  3          0.6               0.75       100      0.9546820
##   0.3  3          0.6               0.75       150      0.9552982
##   0.3  3          0.6               1.00        50      0.9577983
##   0.3  3          0.6               1.00       100      0.9573819
##   0.3  3          0.6               1.00       150      0.9567655
##   0.3  3          0.8               0.50        50      0.9733131
##   0.3  3          0.8               0.50       100      0.9728829
##   0.3  3          0.8               0.50       150      0.9718499
##   0.3  3          0.8               0.75        50      0.9751879
##   0.3  3          0.8               0.75       100      0.9743546
##   0.3  3          0.8               0.75       150      0.9735212
##   0.3  3          0.8               1.00        50      0.9743372
##   0.3  3          0.8               1.00       100      0.9737122
##   0.3  3          0.8               1.00       150      0.9743461
##   0.4  1          0.6               0.50        50      0.9548861
##   0.4  1          0.6               0.50       100      0.9528290
##   0.4  1          0.6               0.50       150      0.9498772
##   0.4  1          0.6               0.75        50      0.9557239
##   0.4  1          0.6               0.75       100      0.9513529
##   0.4  1          0.6               0.75       150      0.9492779
##   0.4  1          0.6               1.00        50      0.9559365
##   0.4  1          0.6               1.00       100      0.9551031
##   0.4  1          0.6               1.00       150      0.9536361
##   0.4  1          0.8               0.50        50      0.9710164
##   0.4  1          0.8               0.50       100      0.9697577
##   0.4  1          0.8               0.50       150      0.9687074
##   0.4  1          0.8               0.75        50      0.9710122
##   0.4  1          0.8               0.75       100      0.9707996
##   0.4  1          0.8               0.75       150      0.9691455
##   0.4  1          0.8               1.00        50      0.9705911
##   0.4  1          0.8               1.00       100      0.9697446
##   0.4  1          0.8               1.00       150      0.9697576
##   0.4  2          0.6               0.50        50      0.9544866
##   0.4  2          0.6               0.50       100      0.9542694
##   0.4  2          0.6               0.50       150      0.9536357
##   0.4  2          0.6               0.75        50      0.9540611
##   0.4  2          0.6               0.75       100      0.9542694
##   0.4  2          0.6               0.75       150      0.9549033
##   0.4  2          0.6               1.00        50      0.9540653
##   0.4  2          0.6               1.00       100      0.9555239
##   0.4  2          0.6               1.00       150      0.9546818
##   0.4  2          0.8               0.50        50      0.9720670
##   0.4  2          0.8               0.50       100      0.9695629
##   0.4  2          0.8               0.50       150      0.9702006
##   0.4  2          0.8               0.75        50      0.9722627
##   0.4  2          0.8               0.75       100      0.9720500
##   0.4  2          0.8               0.75       150      0.9716289
##   0.4  2          0.8               1.00        50      0.9726705
##   0.4  2          0.8               1.00       100      0.9708042
##   0.4  2          0.8               1.00       150      0.9708129
##   0.4  3          0.6               0.50        50      0.9555150
##   0.4  3          0.6               0.50       100      0.9553021
##   0.4  3          0.6               0.50       150      0.9548943
##   0.4  3          0.6               0.75        50      0.9555281
##   0.4  3          0.6               0.75       100      0.9563662
##   0.4  3          0.6               0.75       150      0.9555324
##   0.4  3          0.6               1.00        50      0.9575900
##   0.4  3          0.6               1.00       100      0.9571735
##   0.4  3          0.6               1.00       150      0.9559104
##   0.4  3          0.8               0.50        50      0.9737255
##   0.4  3          0.8               0.50       100      0.9745501
##   0.4  3          0.8               0.50       150      0.9730874
##   0.4  3          0.8               0.75        50      0.9747539
##   0.4  3          0.8               0.75       100      0.9724664
##   0.4  3          0.8               0.75       150      0.9720498
##   0.4  3          0.8               1.00        50      0.9747539
##   0.4  3          0.8               1.00       100      0.9749624
##   0.4  3          0.8               1.00       150      0.9734996
##   Kappa    
##   0.9050828
##   0.8999999
##   0.8930637
##   0.9067208
##   0.8982284
##   0.8959903
##   0.9028825
##   0.9022543
##   0.9014018
##   0.9382467
##   0.9386326
##   0.9340573
##   0.9400323
##   0.9395968
##   0.9353783
##   0.9372262
##   0.9362148
##   0.9353247
##   0.9032270
##   0.9047203
##   0.9024465
##   0.8968511
##   0.9015282
##   0.9016169
##   0.8971329
##   0.9015111
##   0.9028614
##   0.9387022
##   0.9419143
##   0.9387792
##   0.9391933
##   0.9401872
##   0.9379714
##   0.9377309
##   0.9397601
##   0.9384827
##   0.9008861
##   0.9029797
##   0.9024531
##   0.9037859
##   0.9004226
##   0.9019909
##   0.9074584
##   0.9064701
##   0.9051441
##   0.9414031
##   0.9405025
##   0.9380734
##   0.9456856
##   0.9438986
##   0.9419994
##   0.9438642
##   0.9426000
##   0.9439780
##   0.9007223
##   0.8964381
##   0.8897615
##   0.9027951
##   0.8931520
##   0.8886910
##   0.9030461
##   0.9014362
##   0.8982364
##   0.9363059
##   0.9334254
##   0.9311383
##   0.9361883
##   0.9357131
##   0.9320657
##   0.9353688
##   0.9333607
##   0.9334467
##   0.8999756
##   0.8997888
##   0.8983861
##   0.8991356
##   0.8998960
##   0.9013529
##   0.8990428
##   0.9023340
##   0.9004889
##   0.9387165
##   0.9332663
##   0.9345567
##   0.9393855
##   0.9389455
##   0.9380863
##   0.9401366
##   0.9361847
##   0.9361724
##   0.9021263
##   0.9017938
##   0.9010613
##   0.9025263
##   0.9043436
##   0.9024744
##   0.9069828
##   0.9059579
##   0.9031829
##   0.9424523
##   0.9442537
##   0.9410193
##   0.9447486
##   0.9397683
##   0.9388701
##   0.9449064
##   0.9454375
##   0.9422358
## 
## Tuning parameter &#39;gamma&#39; was held constant at a value of 0
## 
## Tuning parameter &#39;min_child_weight&#39; was held constant at a value of 1
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were nrounds = 50, max_depth = 3,
##  eta = 0.3, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1
##  and subsample = 0.75.</code></pre>
<p><br></p>
<ul>
<li>Feature Importance</li>
</ul>
<pre class="r"><code>importance &lt;- varImp(model_xgb, scale = TRUE)
plot(importance)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/importance_xgb-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><br></p>
<ul>
<li>predicting test data</li>
</ul>
<pre class="r"><code>confusionMatrix(predict(model_xgb, test_data), as.factor(test_data$classes))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  benign malignant
##   benign       128         3
##   malignant      5        68
##                                           
##                Accuracy : 0.9608          
##                  95% CI : (0.9242, 0.9829)
##     No Information Rate : 0.652           
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.9142          
##  Mcnemar&#39;s Test P-Value : 0.7237          
##                                           
##             Sensitivity : 0.9624          
##             Specificity : 0.9577          
##          Pos Pred Value : 0.9771          
##          Neg Pred Value : 0.9315          
##              Prevalence : 0.6520          
##          Detection Rate : 0.6275          
##    Detection Prevalence : 0.6422          
##       Balanced Accuracy : 0.9601          
##                                           
##        &#39;Positive&#39; Class : benign          
## </code></pre>
<pre class="r"><code>results &lt;- data.frame(actual = test_data$classes,
                      predict(model_xgb, test_data, type = &quot;prob&quot;))

results$prediction &lt;- ifelse(results$benign &gt; 0.5, &quot;benign&quot;,
                             ifelse(results$malignant &gt; 0.5, &quot;malignant&quot;, NA))

results$correct &lt;- ifelse(results$actual == results$prediction, TRUE, FALSE)

ggplot(results, aes(x = prediction, fill = correct)) +
  geom_bar(position = &quot;dodge&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/results_bar_xgb-1.png" width="480" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(results, aes(x = prediction, y = benign, color = correct, shape = correct)) +
  geom_jitter(size = 3, alpha = 0.6)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/results_jitter_xgb-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="available-models-in-caret" class="section level2">
<h2>Available models in caret</h2>
<p><a href="https://topepo.github.io/caret/available-models.html" class="uri">https://topepo.github.io/caret/available-models.html</a></p>
<p><br></p>
<div id="feature-selection" class="section level4">
<h4>Feature Selection</h4>
<p>Performing feature selection on the whole dataset would lead to prediction bias, we therefore need to run the whole modeling process on the training data alone!</p>
<ul>
<li>Correlation</li>
</ul>
<p>Correlations between all features are calculated and visualised with the <em>corrplot</em> package. I am then removing all features with a correlation higher than 0.7, keeping the feature with the lower mean.</p>
<pre class="r"><code>library(corrplot)

# calculate correlation matrix
corMatMy &lt;- cor(train_data[, -1])
corrplot(corMatMy, order = &quot;hclust&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/corplot-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#Apply correlation filter at 0.70,
highlyCor &lt;- colnames(train_data[, -1])[findCorrelation(corMatMy, cutoff = 0.7, verbose = TRUE)]</code></pre>
<pre><code>## Compare row 2  and column  3 with corr  0.908 
##   Means:  0.709 vs 0.594 so flagging column 2 
## Compare row 3  and column  7 with corr  0.749 
##   Means:  0.67 vs 0.569 so flagging column 3 
## All correlations &lt;= 0.7</code></pre>
<pre class="r"><code># which variables are flagged for removal?
highlyCor</code></pre>
<pre><code>## [1] &quot;uniformity_of_cell_size&quot;  &quot;uniformity_of_cell_shape&quot;</code></pre>
<pre class="r"><code>#then we remove these variables
train_data_cor &lt;- train_data[, which(!colnames(train_data) %in% highlyCor)]</code></pre>
<p><br></p>
<ul>
<li>Recursive Feature Elimination (RFE)</li>
</ul>
<p>Another way to choose features is with Recursive Feature Elimination. RFE uses a Random Forest algorithm to test combinations of features and rate each with an accuracy score. The combination with the highest score is usually preferential.</p>
<pre class="r"><code>set.seed(7)
results_rfe &lt;- rfe(x = train_data[, -1], 
                   y = as.factor(train_data$classes), 
                   sizes = c(1:9), 
                   rfeControl = rfeControl(functions = rfFuncs, method = &quot;cv&quot;, number = 10))</code></pre>
<pre class="r"><code># chosen features
predictors(results_rfe)</code></pre>
<pre><code>## [1] &quot;bare_nuclei&quot;                 &quot;clump_thickness&quot;            
## [3] &quot;uniformity_of_cell_size&quot;     &quot;uniformity_of_cell_shape&quot;   
## [5] &quot;bland_chromatin&quot;             &quot;normal_nucleoli&quot;            
## [7] &quot;marginal_adhesion&quot;           &quot;single_epithelial_cell_size&quot;</code></pre>
<pre class="r"><code>train_data_rfe &lt;- train_data[, c(1, which(colnames(train_data) %in% predictors(results_rfe)))]</code></pre>
<p><br></p>
<ul>
<li>Genetic Algorithm (GA)</li>
</ul>
<p>The Genetic Algorithm (GA) has been developed based on evolutionary principles of natural selection: It aims to optimize a population of individuals with a given set of genotypes by modeling selection over time. In each generation (i.e. iteration), each individual’s fitness is calculated based on their genotypes. Then, the fittest individuals are chosen to produce the next generation. This subsequent generation of individuals will have genotypes resulting from (re-) combinations of the parental alleles. These new genotypes will again determine each individual’s fitness. This selection process is iterated for a specified number of generations and (ideally) leads to fixation of the fittest alleles in the gene pool.</p>
<p>This concept of optimization can be applied to non-evolutionary models as well, like feature selection processes in machine learning.</p>
<pre class="r"><code>set.seed(27)
model_ga &lt;- gafs(x = train_data[, -1], 
                 y = as.factor(train_data$classes),
                 iters = 10, # generations of algorithm
                 popSize = 10, # population size for each generation
                 levels = c(&quot;malignant&quot;, &quot;benign&quot;),
                 gafsControl = gafsControl(functions = rfGA, # Assess fitness with RF
                                           method = &quot;cv&quot;,    # 10 fold cross validation
                                           genParallel = TRUE, # Use parallel programming
                                           allowParallel = TRUE))</code></pre>
<pre class="r"><code>plot(model_ga) # Plot mean fitness (AUC) by generation</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-38-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>train_data_ga &lt;- train_data[, c(1, which(colnames(train_data) %in% model_ga$ga$final))]</code></pre>
<p><br></p>
</div>
<div id="hyperparameter-tuning-with-caret" class="section level3">
<h3>Hyperparameter tuning with caret</h3>
<ul>
<li><p>Cartesian Grid</p></li>
<li><p>mtry: Number of variables randomly sampled as candidates at each split.</p></li>
</ul>
<pre class="r"><code>set.seed(42)
grid &lt;- expand.grid(mtry = c(1:10))

model_rf_tune_man &lt;- caret::train(classes ~ .,
                         data = train_data,
                         method = &quot;rf&quot;,
                         preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                         trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE),
                         tuneGrid = grid)</code></pre>
<pre class="r"><code>model_rf_tune_man</code></pre>
<pre><code>## Random Forest 
## 
## 479 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: scaled (9), centered (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 432, 431, 431, 431, 431, 431, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    1    0.9785044  0.9532161
##    2    0.9772586  0.9504377
##    3    0.9774625  0.9508246
##    4    0.9766333  0.9488778
##    5    0.9753789  0.9460274
##    6    0.9737078  0.9422613
##    7    0.9730957  0.9408547
##    8    0.9714155  0.9371611
##    9    0.9718280  0.9380578
##   10    0.9718280  0.9380135
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 1.</code></pre>
<pre class="r"><code>plot(model_rf_tune_man)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-42-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
<ul>
<li>Random Search</li>
</ul>
<pre class="r"><code>set.seed(42)
model_rf_tune_auto &lt;- caret::train(classes ~ .,
                         data = train_data,
                         method = &quot;rf&quot;,
                         preProcess = c(&quot;scale&quot;, &quot;center&quot;),
                         trControl = trainControl(method = &quot;repeatedcv&quot;, 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE,
                                                  search = &quot;random&quot;),
                         tuneGrid = grid,
                         tuneLength = 15)</code></pre>
<pre class="r"><code>model_rf_tune_auto</code></pre>
<pre><code>## Random Forest 
## 
## 479 samples
##   9 predictor
##   2 classes: &#39;benign&#39;, &#39;malignant&#39; 
## 
## Pre-processing: scaled (9), centered (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 432, 431, 431, 431, 431, 431, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    1    0.9785044  0.9532161
##    2    0.9772586  0.9504377
##    3    0.9774625  0.9508246
##    4    0.9766333  0.9488778
##    5    0.9753789  0.9460274
##    6    0.9737078  0.9422613
##    7    0.9730957  0.9408547
##    8    0.9714155  0.9371611
##    9    0.9718280  0.9380578
##   10    0.9718280  0.9380135
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 1.</code></pre>
<pre class="r"><code>plot(model_rf_tune_auto)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-46-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="grid-search-with-h2o" class="section level3">
<h3>Grid search with h2o</h3>
<p>The R package h2o provides a convenient interface to <a href="http://www.h2o.ai/h2o/">H2O</a>, which is an open-source machine learning and deep learning platform. H2O distributes a wide range of common machine learning algorithms for classification, regression and deep learning.</p>
<pre class="r"><code>library(h2o)
h2o.init(nthreads = -1)</code></pre>
<pre><code>##  Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         3 days 4 minutes 
##     H2O cluster timezone:       Europe/Berlin 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.20.0.2 
##     H2O cluster version age:    16 days  
##     H2O cluster name:           H2O_started_from_R_shiringlander_jrj894 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   3.27 GB 
##     H2O cluster total cores:    8 
##     H2O cluster allowed cores:  8 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.0 (2018-04-23)</code></pre>
<pre class="r"><code>h2o.no_progress()

bc_data_hf &lt;- as.h2o(bc_data)</code></pre>
<pre class="r"><code>h2o.describe(bc_data_hf) %&gt;%
  gather(x, y, Zeros:Sigma) %&gt;%
  mutate(group = ifelse(x %in% c(&quot;Min&quot;, &quot;Max&quot;, &quot;Mean&quot;), &quot;min, mean, max&quot;, 
                        ifelse(x %in% c(&quot;NegInf&quot;, &quot;PosInf&quot;), &quot;Inf&quot;, &quot;sigma, zeros&quot;))) %&gt;% 
  ggplot(aes(x = Label, y = as.numeric(y), color = x)) +
    geom_point(size = 4, alpha = 0.6) +
    scale_color_brewer(palette = &quot;Set1&quot;) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
    facet_grid(group ~ ., scales = &quot;free&quot;) +
    labs(x = &quot;Feature&quot;,
         y = &quot;Value&quot;,
         color = &quot;&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/h2o_describe-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>library(reshape2) # for melting

bc_data_hf[, 1] &lt;- h2o.asfactor(bc_data_hf[, 1])

cor &lt;- h2o.cor(bc_data_hf)
rownames(cor) &lt;- colnames(cor)

melt(cor) %&gt;%
  mutate(Var2 = rep(rownames(cor), nrow(cor))) %&gt;%
  mutate(Var2 = factor(Var2, levels = colnames(cor))) %&gt;%
  mutate(variable = factor(variable, levels = colnames(cor))) %&gt;%
  ggplot(aes(x = variable, y = Var2, fill = value)) + 
    geom_tile(width = 0.9, height = 0.9) +
    scale_fill_gradient2(low = &quot;white&quot;, high = &quot;red&quot;, name = &quot;Cor.&quot;) +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
    labs(x = &quot;&quot;, 
         y = &quot;&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/corr_plot-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
<div id="training-validation-and-test-data-1" class="section level4">
<h4>Training, validation and test data</h4>
<pre class="r"><code>splits &lt;- h2o.splitFrame(bc_data_hf, 
                         ratios = c(0.7, 0.15), 
                         seed = 1)

train &lt;- splits[[1]]
valid &lt;- splits[[2]]
test &lt;- splits[[3]]

response &lt;- &quot;classes&quot;
features &lt;- setdiff(colnames(train), response)</code></pre>
<pre class="r"><code>summary(as.factor(train$classes), exact_quantiles = TRUE)</code></pre>
<pre><code>##  classes       
##  benign   :313 
##  malignant:167</code></pre>
<pre class="r"><code>summary(as.factor(valid$classes), exact_quantiles = TRUE)</code></pre>
<pre><code>##  classes      
##  benign   :64 
##  malignant:38</code></pre>
<pre class="r"><code>summary(as.factor(test$classes), exact_quantiles = TRUE)</code></pre>
<pre><code>##  classes      
##  benign   :67 
##  malignant:34</code></pre>
<pre class="r"><code>pca &lt;- h2o.prcomp(training_frame = train,
           x = features,
           validation_frame = valid,
           transform = &quot;NORMALIZE&quot;,
           impute_missing = TRUE,
           k = 3,
           seed = 42)

eigenvec &lt;- as.data.frame(pca@model$eigenvectors)
eigenvec$label &lt;- features

library(ggrepel)
ggplot(eigenvec, aes(x = pc1, y = pc2, label = label)) +
  geom_point(color = &quot;navy&quot;, alpha = 0.7) +
  geom_text_repel()</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/pca_features-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
<div id="classification-1" class="section level4">
<h4>Classification</h4>
<div id="random-forest" class="section level5">
<h5>Random Forest</h5>
<pre class="r"><code>hyper_params &lt;- list(
                     ntrees = c(25, 50, 75, 100),
                     max_depth = c(10, 20, 30),
                     min_rows = c(1, 3, 5)
                     )

search_criteria &lt;- list(
                        strategy = &quot;RandomDiscrete&quot;, 
                        max_models = 50,
                        max_runtime_secs = 360,
                        stopping_rounds = 5,          
                        stopping_metric = &quot;AUC&quot;,      
                        stopping_tolerance = 0.0005,
                        seed = 42
                        )</code></pre>
<pre class="r"><code>rf_grid &lt;- h2o.grid(algorithm = &quot;randomForest&quot;, # h2o.randomForest, 
                                                # alternatively h2o.gbm 
                                                # for Gradient boosting trees
                    x = features,
                    y = response,
                    grid_id = &quot;rf_grid&quot;,
                    training_frame = train,
                    validation_frame = valid,
                    nfolds = 25,                           
                    fold_assignment = &quot;Stratified&quot;,
                    hyper_params = hyper_params,
                    search_criteria = search_criteria,
                    seed = 42
                    )</code></pre>
<pre class="r"><code># performance metrics where smaller is better -&gt; order with decreasing = FALSE
sort_options_1 &lt;- c(&quot;mean_per_class_error&quot;, &quot;mse&quot;, &quot;err&quot;, &quot;logloss&quot;)

for (sort_by_1 in sort_options_1) {
  
  grid &lt;- h2o.getGrid(&quot;rf_grid&quot;, sort_by = sort_by_1, decreasing = FALSE)
  
  model_ids &lt;- grid@model_ids
  best_model &lt;- h2o.getModel(model_ids[[1]])
  
  h2o.saveModel(best_model, path=&quot;models&quot;, force = TRUE)
  
}


# performance metrics where bigger is better -&gt; order with decreasing = TRUE
sort_options_2 &lt;- c(&quot;auc&quot;, &quot;precision&quot;, &quot;accuracy&quot;, &quot;recall&quot;, &quot;specificity&quot;)

for (sort_by_2 in sort_options_2) {
  
  grid &lt;- h2o.getGrid(&quot;rf_grid&quot;, sort_by = sort_by_2, decreasing = TRUE)
  
  model_ids &lt;- grid@model_ids
  best_model &lt;- h2o.getModel(model_ids[[1]])
  
  h2o.saveModel(best_model, path = &quot;models&quot;, force = TRUE)
  
}</code></pre>
<pre class="r"><code>files &lt;- list.files(path = &quot;models&quot;)</code></pre>
<pre class="r"><code>rf_models &lt;- files[grep(&quot;rf_grid_model&quot;, files)]

for (model_id in rf_models) {
  
  path &lt;- paste0(getwd(), &quot;/models/&quot;, model_id)
  best_model &lt;- h2o.loadModel(path)
  mse_auc_test &lt;- data.frame(model_id = model_id, 
                             mse = h2o.mse(h2o.performance(best_model, test)),
                             auc = h2o.auc(h2o.performance(best_model, test)))
  
  if (model_id == rf_models[[1]]) {
    
    mse_auc_test_comb &lt;- mse_auc_test
    
  } else {
    
    mse_auc_test_comb &lt;- rbind(mse_auc_test_comb, mse_auc_test)
    
  }
}</code></pre>
<pre class="r"><code>mse_auc_test_comb %&gt;%
  gather(x, y, mse:auc) %&gt;%
  ggplot(aes(x = model_id, y = y, fill = model_id)) +
    facet_grid(x ~ ., scales = &quot;free&quot;) +
    geom_bar(stat = &quot;identity&quot;, alpha = 0.8, position = &quot;dodge&quot;) +
    scale_fill_brewer(palette = &quot;Set1&quot;) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
          plot.margin = unit(c(0.5, 0, 0, 1.5), &quot;cm&quot;)) +
    labs(x = &quot;&quot;, y = &quot;value&quot;, fill = &quot;&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/auc_mse-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>for (model_id in rf_models) {
  
  best_model &lt;- h2o.getModel(model_id)
  
  finalRf_predictions &lt;- data.frame(model_id = rep(best_model@model_id, 
                                                   nrow(test)),
                                    actual = as.vector(test$classes), 
                                    as.data.frame(h2o.predict(object = best_model, 
                                                              newdata = test)))
  
  finalRf_predictions$accurate &lt;- ifelse(finalRf_predictions$actual == 
                                           finalRf_predictions$predict, 
                                         &quot;yes&quot;, &quot;no&quot;)
  
  finalRf_predictions$predict_stringent &lt;- ifelse(finalRf_predictions$benign &gt; 0.8, 
                                                  &quot;benign&quot;, 
                                                  ifelse(finalRf_predictions$malignant 
                                                         &gt; 0.8, &quot;malignant&quot;, &quot;uncertain&quot;))
  
  finalRf_predictions$accurate_stringent &lt;- ifelse(finalRf_predictions$actual == 
                                                     finalRf_predictions$predict_stringent, &quot;yes&quot;, 
                                         ifelse(finalRf_predictions$predict_stringent == 
                                                  &quot;uncertain&quot;, &quot;na&quot;, &quot;no&quot;))
  
  if (model_id == rf_models[[1]]) {
    
    finalRf_predictions_comb &lt;- finalRf_predictions
    
  } else {
    
    finalRf_predictions_comb &lt;- rbind(finalRf_predictions_comb, finalRf_predictions)
    
  }
}</code></pre>
<pre class="r"><code>finalRf_predictions_comb %&gt;%
  ggplot(aes(x = actual, fill = accurate)) +
    geom_bar(position = &quot;dodge&quot;) +
    scale_fill_brewer(palette = &quot;Set1&quot;) +
    facet_wrap(~ model_id, ncol = 2) +
    labs(fill = &quot;Were\npredictions\naccurate?&quot;,
         title = &quot;Default predictions&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/final_predictions_rf-1.png" width="864" style="display: block; margin: auto;" /></p>
<pre class="r"><code>finalRf_predictions_comb %&gt;%
  subset(accurate_stringent != &quot;na&quot;) %&gt;%
  ggplot(aes(x = actual, fill = accurate_stringent)) +
    geom_bar(position = &quot;dodge&quot;) +
    scale_fill_brewer(palette = &quot;Set1&quot;) +
    facet_wrap(~ model_id, ncol = 2) +
    labs(fill = &quot;Were\npredictions\naccurate?&quot;,
         title = &quot;Stringent predictions&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/final_predictions_rf-2.png" width="864" style="display: block; margin: auto;" /></p>
<pre class="r"><code>rf_model &lt;- h2o.loadModel(&quot;models/rf_grid_model_0&quot;)</code></pre>
<pre class="r"><code>h2o.varimp_plot(rf_model)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-60-1.png" width="672" /></p>
<pre class="r"><code>#h2o.varimp(rf_model)</code></pre>
<pre class="r"><code>h2o.mean_per_class_error(rf_model, train = TRUE, valid = TRUE, xval = TRUE)</code></pre>
<pre><code>##      train      valid       xval 
## 0.02196246 0.02343750 0.02515735</code></pre>
<pre class="r"><code>h2o.confusionMatrix(rf_model, valid = TRUE)</code></pre>
<pre><code>## Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.533333333333333:
##           benign malignant    Error    Rate
## benign        61         3 0.046875   =3/64
## malignant      0        38 0.000000   =0/38
## Totals        61        41 0.029412  =3/102</code></pre>
<pre class="r"><code>plot(rf_model,
     timestep = &quot;number_of_trees&quot;,
     metric = &quot;classification_error&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-63-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plot(rf_model,
     timestep = &quot;number_of_trees&quot;,
     metric = &quot;logloss&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-64-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plot(rf_model,
     timestep = &quot;number_of_trees&quot;,
     metric = &quot;AUC&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-65-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plot(rf_model,
     timestep = &quot;number_of_trees&quot;,
     metric = &quot;rmse&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-66-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>h2o.auc(rf_model, train = TRUE)</code></pre>
<pre><code>## [1] 0.9907214</code></pre>
<pre class="r"><code>h2o.auc(rf_model, valid = TRUE)</code></pre>
<pre><code>## [1] 0.9829359</code></pre>
<pre class="r"><code>h2o.auc(rf_model, xval = TRUE)</code></pre>
<pre><code>## [1] 0.9903005</code></pre>
<pre class="r"><code>perf &lt;- h2o.performance(rf_model, test)
perf</code></pre>
<pre><code>## H2OBinomialMetrics: drf
## 
## MSE:  0.03258482
## RMSE:  0.1805127
## LogLoss:  0.1072519
## Mean Per-Class Error:  0.02985075
## AUC:  0.9916594
## Gini:  0.9833187
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##           benign malignant    Error    Rate
## benign        63         4 0.059701   =4/67
## malignant      0        34 0.000000   =0/34
## Totals        63        38 0.039604  =4/101
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.306667 0.944444  18
## 2                       max f2  0.306667 0.977011  18
## 3                 max f0point5  0.720000 0.933735  13
## 4                 max accuracy  0.533333 0.960396  16
## 5                max precision  1.000000 1.000000   0
## 6                   max recall  0.306667 1.000000  18
## 7              max specificity  1.000000 1.000000   0
## 8             max absolute_mcc  0.306667 0.917235  18
## 9   max min_per_class_accuracy  0.533333 0.955224  16
## 10 max mean_per_class_accuracy  0.306667 0.970149  18
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`</code></pre>
<pre class="r"><code>plot(perf)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/auc_curve-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>perf@metrics$thresholds_and_metric_scores %&gt;%
  ggplot(aes(x = fpr, y = tpr)) +
    geom_point() +
    geom_line() +
    geom_abline(slope = 1, intercept = 0) +
    labs(x = &quot;False Positive Rate&quot;,
         y = &quot;True Positive Rate&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/unnamed-chunk-69-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>h2o.logloss(perf)</code></pre>
<pre><code>## [1] 0.1072519</code></pre>
<pre class="r"><code>h2o.mse(perf)</code></pre>
<pre><code>## [1] 0.03258482</code></pre>
<pre class="r"><code>h2o.auc(perf)</code></pre>
<pre><code>## [1] 0.9916594</code></pre>
<pre class="r"><code>head(h2o.metric(perf))</code></pre>
<pre><code>## Metrics for Thresholds: Binomial metrics as a function of classification thresholds
##   threshold       f1       f2 f0point5 accuracy precision   recall
## 1  1.000000 0.583333 0.466667 0.777778 0.801980  1.000000 0.411765
## 2  0.986667 0.666667 0.555556 0.833333 0.831683  1.000000 0.500000
## 3  0.973333 0.716981 0.612903 0.863636 0.851485  1.000000 0.558824
## 4  0.960000 0.740741 0.641026 0.877193 0.861386  1.000000 0.588235
## 5  0.946667 0.763636 0.668790 0.889831 0.871287  1.000000 0.617647
## 6  0.920000 0.807018 0.723270 0.912698 0.891089  1.000000 0.676471
##   specificity absolute_mcc min_per_class_accuracy mean_per_class_accuracy
## 1    1.000000     0.563122               0.411765                0.705882
## 2    1.000000     0.631514               0.500000                0.750000
## 3    1.000000     0.675722               0.558824                0.779412
## 4    1.000000     0.697542               0.588235                0.794118
## 5    1.000000     0.719221               0.617647                0.808824
## 6    1.000000     0.762280               0.676471                0.838235
##   tns fns fps tps      tnr      fnr      fpr      tpr idx
## 1  67  20   0  14 1.000000 0.588235 0.000000 0.411765   0
## 2  67  17   0  17 1.000000 0.500000 0.000000 0.500000   1
## 3  67  15   0  19 1.000000 0.441176 0.000000 0.558824   2
## 4  67  14   0  20 1.000000 0.411765 0.000000 0.588235   3
## 5  67  13   0  21 1.000000 0.382353 0.000000 0.617647   4
## 6  67  11   0  23 1.000000 0.323529 0.000000 0.676471   5</code></pre>
<pre class="r"><code>finalRf_predictions &lt;- data.frame(actual = as.vector(test$classes), 
                                  as.data.frame(h2o.predict(object = rf_model, 
                                                            newdata = test)))

finalRf_predictions$accurate &lt;- ifelse(finalRf_predictions$actual == 
                                         finalRf_predictions$predict, &quot;yes&quot;, &quot;no&quot;)

finalRf_predictions$predict_stringent &lt;- ifelse(finalRf_predictions$benign &gt; 0.8, &quot;benign&quot;, 
                                                ifelse(finalRf_predictions$malignant 
                                                       &gt; 0.8, &quot;malignant&quot;, &quot;uncertain&quot;))
finalRf_predictions$accurate_stringent &lt;- ifelse(finalRf_predictions$actual == 
                                                   finalRf_predictions$predict_stringent, &quot;yes&quot;, 
                                       ifelse(finalRf_predictions$predict_stringent == 
                                                &quot;uncertain&quot;, &quot;na&quot;, &quot;no&quot;))

finalRf_predictions %&gt;%
  group_by(actual, predict) %&gt;%
  dplyr::summarise(n = n())</code></pre>
<pre><code>## # A tibble: 4 x 3
## # Groups:   actual [?]
##   actual    predict       n
##   &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;
## 1 benign    benign       64
## 2 benign    malignant     3
## 3 malignant benign        1
## 4 malignant malignant    33</code></pre>
<pre class="r"><code>finalRf_predictions %&gt;%
  group_by(actual, predict_stringent) %&gt;%
  dplyr::summarise(n = n())</code></pre>
<pre><code>## # A tibble: 5 x 3
## # Groups:   actual [?]
##   actual    predict_stringent     n
##   &lt;fct&gt;     &lt;chr&gt;             &lt;int&gt;
## 1 benign    benign               62
## 2 benign    malignant             2
## 3 benign    uncertain             3
## 4 malignant malignant            29
## 5 malignant uncertain             5</code></pre>
<pre class="r"><code>finalRf_predictions %&gt;%
  ggplot(aes(x = actual, fill = accurate)) +
    geom_bar(position = &quot;dodge&quot;) +
    scale_fill_brewer(palette = &quot;Set1&quot;) +
    labs(fill = &quot;Were\npredictions\naccurate?&quot;,
         title = &quot;Default predictions&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/default_vs_stringent-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>finalRf_predictions %&gt;%
  subset(accurate_stringent != &quot;na&quot;) %&gt;%
  ggplot(aes(x = actual, fill = accurate_stringent)) +
    geom_bar(position = &quot;dodge&quot;) +
    scale_fill_brewer(palette = &quot;Set1&quot;) +
    labs(fill = &quot;Were\npredictions\naccurate?&quot;,
         title = &quot;Stringent predictions&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/default_vs_stringent-2.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>df &lt;- finalRf_predictions[, c(1, 3, 4)]

thresholds &lt;- seq(from = 0, to = 1, by = 0.1)

prop_table &lt;- data.frame(threshold = thresholds, prop_true_b = NA, prop_true_m = NA)

for (threshold in thresholds) {
  pred &lt;- ifelse(df$benign &gt; threshold, &quot;benign&quot;, &quot;malignant&quot;)
  pred_t &lt;- ifelse(pred == df$actual, TRUE, FALSE)
  
  group &lt;- data.frame(df, &quot;pred&quot; = pred_t) %&gt;%
  group_by(actual, pred) %&gt;%
  dplyr::summarise(n = n())
  
  group_b &lt;- filter(group, actual == &quot;benign&quot;)
  
  prop_b &lt;- sum(filter(group_b, pred == TRUE)$n) / sum(group_b$n)
  prop_table[prop_table$threshold == threshold, &quot;prop_true_b&quot;] &lt;- prop_b
  
  group_m &lt;- filter(group, actual == &quot;malignant&quot;)
  
  prop_m &lt;- sum(filter(group_m, pred == TRUE)$n) / sum(group_m$n)
  prop_table[prop_table$threshold == threshold, &quot;prop_true_m&quot;] &lt;- prop_m
}

prop_table %&gt;%
  gather(x, y, prop_true_b:prop_true_m) %&gt;%
  ggplot(aes(x = threshold, y = y, color = x)) +
    geom_point() +
    geom_line() +
    scale_color_brewer(palette = &quot;Set1&quot;) +
    labs(y = &quot;proportion of true predictions&quot;,
         color = &quot;b: benign cases\nm: malignant cases&quot;)</code></pre>
<p><img src="/post/2018-06-29_intro_to_ml_workshop_heidelberg_files/figure-html/prop_table-1.png" width="576" style="display: block; margin: auto;" /></p>
<hr />
<p>If you are interested in more machine learning posts, check out the category listing for <strong>machine_learning</strong> on my blog - <a href="https://shirinsplayground.netlify.com/categories/#posts-list-machine-learning" class="uri">https://shirinsplayground.netlify.com/categories/#posts-list-machine-learning</a> - <a href="https://shiring.github.io/categories.html#machine_learning-ref" class="uri">https://shiring.github.io/categories.html#machine_learning-ref</a></p>
<hr />
<p><br></p>
<pre class="r"><code>stopCluster(cl)
h2o.shutdown()</code></pre>
<pre><code>## Are you sure you want to shutdown the H2O instance running at http://localhost:54321/ (Y/N)?</code></pre>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.0 (2018-04-23)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.5
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods  
## [8] base     
## 
## other attached packages:
##  [1] ggrepel_0.8.0     reshape2_1.4.3    h2o_3.20.0.2     
##  [4] corrplot_0.84     caret_6.0-80      doParallel_1.0.11
##  [7] iterators_1.0.9   foreach_1.4.4     ellipse_0.4.1    
## [10] igraph_1.2.1      bindrcpp_0.2.2    mice_3.1.0       
## [13] lattice_0.20-35   forcats_0.3.0     stringr_1.3.1    
## [16] dplyr_0.7.5       purrr_0.2.5       readr_1.1.1      
## [19] tidyr_0.8.1       tibble_1.4.2      ggplot2_2.2.1    
## [22] tidyverse_1.2.1  
## 
## loaded via a namespace (and not attached):
##  [1] minqa_1.2.4         colorspace_1.3-2    class_7.3-14       
##  [4] rprojroot_1.3-2     pls_2.6-0           rstudioapi_0.7     
##  [7] DRR_0.0.3           prodlim_2018.04.18  lubridate_1.7.4    
## [10] xml2_1.2.0          codetools_0.2-15    splines_3.5.0      
## [13] mnormt_1.5-5        robustbase_0.93-1   knitr_1.20         
## [16] RcppRoll_0.3.0      jsonlite_1.5        nloptr_1.0.4       
## [19] broom_0.4.4         ddalpha_1.3.4       kernlab_0.9-26     
## [22] sfsmisc_1.1-2       compiler_3.5.0      httr_1.3.1         
## [25] backports_1.1.2     assertthat_0.2.0    Matrix_1.2-14      
## [28] lazyeval_0.2.1      cli_1.0.0           htmltools_0.3.6    
## [31] tools_3.5.0         gtable_0.2.0        glue_1.2.0         
## [34] Rcpp_0.12.17        cellranger_1.1.0    nlme_3.1-137       
## [37] blogdown_0.6        psych_1.8.4         timeDate_3043.102  
## [40] xfun_0.2            gower_0.1.2         lme4_1.1-17        
## [43] rvest_0.3.2         pan_1.4             DEoptimR_1.0-8     
## [46] MASS_7.3-50         scales_0.5.0        ipred_0.9-6        
## [49] hms_0.4.2           RColorBrewer_1.1-2  yaml_2.1.19        
## [52] rpart_4.1-13        stringi_1.2.3       randomForest_4.6-14
## [55] e1071_1.6-8         lava_1.6.1          geometry_0.3-6     
## [58] bitops_1.0-6        rlang_0.2.1         pkgconfig_2.0.1    
## [61] evaluate_0.10.1     bindr_0.1.1         recipes_0.1.3      
## [64] labeling_0.3        CVST_0.2-2          tidyselect_0.2.4   
## [67] plyr_1.8.4          magrittr_1.5        bookdown_0.7       
## [70] R6_2.2.2            mitml_0.3-5         dimRed_0.1.0       
## [73] pillar_1.2.3        haven_1.1.1         foreign_0.8-70     
## [76] withr_2.1.2         RCurl_1.95-4.10     survival_2.42-3    
## [79] abind_1.4-5         nnet_7.3-12         modelr_0.1.2       
## [82] crayon_1.3.4        jomo_2.6-2          xgboost_0.71.2     
## [85] utf8_1.1.4          rmarkdown_1.10      grid_3.5.0         
## [88] readxl_1.1.0        data.table_1.11.4   ModelMetrics_1.1.0 
## [91] digest_0.6.15       stats4_3.5.0        munsell_0.5.0      
## [94] magic_1.5-8</code></pre>
</div>
</div>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Text-to-speech with R]]></title>
    <link href="/2018/06/text_to_speech_r/"/>
    <id>/2018/06/text_to_speech_r/</id>
    <published>2018-06-27T00:00:00+00:00</published>
    <updated>2018-06-27T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Computers started talking to us! They do this with so called Text-to-Speech (TTS) systems. With neural nets, deep learning and lots of training data, these systems have gotten a whole lot better in recent years. In some cases, they are so good that you can’t distinguish between human and machine voice.</p>
<p>In one of our recent <a href="https://blog.codecentric.de/2018/04/kuenstliche-intelligenz-codecentric_ai/">codecentric.AI</a> <a href="https://youtu.be/2EEMSsVBE8w">videos</a>, we compared <a href="https://youtu.be/2EEMSsVBE8w">different Text-to-Speech systems</a> (the video is in German, though - but the text snippets and their voice recordings we show in the video are a mix of German and English). In this video, we had a small contest between Polly, Alexa, Siri And Co to find out who best speaks different tongue twisters.</p>
<p><br> <iframe width="560" height="315" src="https://www.youtube.com/embed/2EEMSsVBE8w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
<p>Here, I want to find out what’s possible with R and Text-to-Speech packages.</p>
<p><br></p>
<p>PS: In a second post I also tried the <a href="https://shirinsplayground.netlify.com/2018/06/googlelanguager">googleLanguageR</a> package - with much better results!</p>
<div id="how-does-tts-work" class="section level2">
<h2>How does TTS work?</h2>
<p>Challenges for good TTS systems are the complexity of the human language: we intone words differently, depending on where they are in a sentence, what we want to convey with that sentence, how our mood is, and so on. AI-based TTS systems can take phonemes and intonation into account.</p>
<p>There are different ways to artificially produce speech. A very important method is Unit Selection synthesis. With this method, text is first normalized and divided into smaller entities that represent sentences, syllables, words, phonemes, etc. The structure (e.g. the pronunciation) of these entities is then learned in context. We call this part Natural Language Processing (NLP). Usually, these learned segments are stored in a database (either as human voice recordings or synthetically generated) that can be searched to find suitable speech parts (Unit Selection). This search is often done with decision trees, neural nets or Hidden-Markov-Models.</p>
<p>If the speech has been generated by a computer, this is called formant synthesis. It offers more flexibility because the collection of words isn’t limited to what has been pre-recorded by a human. Even imaginary or new words can easily be produced and the voices can be readily exchanged. Until recently, this synthetic voice did not sound anything like a human recorded voice; you could definitely hear that it was “fake”. Most of the TTS systems today still suffer from this, but this is in the process of changing: there are already a few artificial TTS systems that do sound very human.</p>
</div>
<div id="what-tts-systems-are-there" class="section level2">
<h2>What TTS systems are there?</h2>
<p>We already find TTS systems in many digital devices, like computers, smart phones, etc. Most of the “big players” offer TTS-as-a-service, but there are also many “smaller” and free programs for TTS. Many can be downloaded as software or used from a web browser or as an API. Here is an incomplete list:</p>
<ul>
<li>Microsoft/Windows: includes <a href="https://support.microsoft.com/de-de/help/22798/windows-10-narrator-get-started"><strong>Narrator</strong></a> and <a href="https://www.microsoft.com/en-us/download/details.aspx?id=10121"><strong>Microsoft Speech API</strong></a></li>
<li>Mac: <a href="https://www.apple.com/de/accessibility/mac/vision/"><strong>VoiceOver</strong></a></li>
<li>Linux: different software can be installed, e.g. <a href="https://sourceforge.net/projects/espeak/files/espeak/"><strong>eSpeak</strong></a></li>
<li><a href="https://www.ibm.com/watson/services/text-to-speech/">IBM Watson</a></li>
<li><a href="https://cloud.google.com/text-to-speech/">Google Cloud</a></li>
<li><a href="https://azure.microsoft.com/de-de/services/cognitive-services/speech/">Microsoft Azure</a></li>
<li><a href="https://developer.amazon.com/de/alexa">Amazon Alexa</a></li>
<li><a href="https://9to5mac.com/2017/03/10/how-to-get-siri-to-read-articles-to-you-on-ios-macos/">Siri on iPhone</a></li>
<li><a href="https://aws.amazon.com/de/polly/">Polly on Amazon AWS</a></li>
<li><a href="https://support.microsoft.com/de-de/help/17214/windows-10-what-is">Microsoft Cortana</a></li>
<li><a href="https://freetts.sourceforge.io/docs/">FreeTTS</a></li>
<li><a href="https://www.ispeech.org/text.to.speech">iSpeech</a></li>
<li><a href="https://www.naturalreaders.com/online/">Natural Readers</a></li>
<li><a href="http://www.cross-plus-a.com/balabolka.htm">Balabolka</a></li>
<li><a href="https://www.panopreter.com/en/products/pb/download.php">Panopreter</a></li>
<li><a href="https://www.text2speech.org/">text2speech.org</a></li>
<li><a href="http://text-to-speech-translator.paralink.com/">text-to-speech-translator.paralink.com/</a></li>
</ul>
</div>
<div id="text-to-speech-in-r" class="section level2">
<h2>Text-to-Speech in R</h2>
<p>The only package for TTS I found was <code>Rtts</code>. It doesn’t seem very comprehensive but it does the job of converting text to speech. The only API that works right now is **ITRI (<a href="http://tts.itri.org.tw)**" class="uri">http://tts.itri.org.tw)**</a>. And it only supports English and Chinese.</p>
<p>Let’s try it out!</p>
<pre class="r"><code>library(Rtts)</code></pre>
<pre><code>## Lade nötiges Paket: RCurl</code></pre>
<pre><code>## Lade nötiges Paket: bitops</code></pre>
<p>Here, I’ll be using a quote from <strong>DOUGLAS ADAMS’ THE HITCHHIKER’S GUIDE TO THE GALAXY</strong>:</p>
<pre class="r"><code>content &lt;- &quot;A common mistake that people make when trying to design something completely foolproof is to underestimate the ingenuity of complete fools.&quot;</code></pre>
<p>The main TTS function is <code>tts_ITRI()</code> and I’m going to loop over the different voice options.</p>
<pre class="r"><code>speakers = c(&quot;Bruce&quot;, &quot;Theresa&quot;, &quot;Angela&quot;, &quot;MCHEN_Bruce&quot;, &quot;MCHEN_Joddess&quot;, &quot;ENG_Bob&quot;, &quot;ENG_Alice&quot;, &quot;ENG_Tracy&quot;)
lapply(speakers, function(x) tts_ITRI(content, speaker = x,
         destfile = paste0(&quot;audio_tts_&quot;, x, &quot;.mp3&quot;)))</code></pre>
<p>I uploaded the results to Soundcloud for you to hear: - <a href="https://soundcloud.com/shirin-glander-729692416/audio-tts-bruce/s-iZC6u?in=shirin-glander-729692416/sets/text-to-speech-blogpost/s-lXYea">audio-tts-bruce</a> - <a href="https://soundcloud.com/shirin-glander-729692416/audio-tts-theresa/s-lYt0R?in=shirin-glander-729692416/sets/text-to-speech-blogpost/s-lXYea">audio-tts-theresa</a> - <a href="https://soundcloud.com/shirin-glander-729692416/audio-tts-angela/s-KVUMS?in=shirin-glander-729692416/sets/text-to-speech-blogpost/s-lXYea">audio-tts-angela</a> - <a href="https://soundcloud.com/shirin-glander-729692416/audio-tts-mchen-bruce/s-KVDeb?in=shirin-glander-729692416/sets/text-to-speech-blogpost/s-lXYea">audio-tts-mchen-bruce</a> - <a href="https://soundcloud.com/shirin-glander-729692416/audio-tts-mchen-joddess/s-mDdik?in=shirin-glander-729692416/sets/text-to-speech-blogpost/s-lXYea">audio-tts-mchen-joddess</a> - <a href="https://soundcloud.com/shirin-glander-729692416/audio-tts-eng-bob/s-520Y2?in=shirin-glander-729692416/sets/text-to-speech-blogpost/s-lXYea">audio-tts-eng-bob</a> - <a href="https://soundcloud.com/shirin-glander-729692416/audio-tts-eng-alice/s-BKTpj?in=shirin-glander-729692416/sets/text-to-speech-blogpost/s-lXYea">audio-tts-eng-alice</a> - <a href="https://soundcloud.com/shirin-glander-729692416/audio-tts-eng-tracy/s-SKVDm?in=shirin-glander-729692416/sets/text-to-speech-blogpost/s-lXYea">audio-tts-eng-tracy</a></p>
<p>As you can hear, it sounds quite wonky. There are many better alternatives out there, but most of them aren’t free and/or can’t be used (as easily) from R. Noam Ross tried <a href="https://rpubs.com/noamross/153216">IBM Watson’s TTS API in this post</a>, which would be a very good solution. Or you could access the <a href="https://cloud.google.com/text-to-speech/">Google Cloud</a> API from within R.</p>
<p>The most convenient solution for me was to use <a href="https://sourceforge.net/projects/espeak/files/espeak/"><strong>eSpeak</strong></a> from the command line. The output sounds relatively good, it is free and offers many languages and voices with lots of parameters to tweak. This is how you would produce audio from text with eSpeak:</p>
<ul>
<li>English US</li>
</ul>
<pre><code>espeak -v english-us -s 150 -w &#39;/Users/shiringlander/Documents/Github/audio_tts_espeak_en_us.wav&#39; &quot;A common mistake that people make when trying to design something completely foolproof is to underestimate the ingenuity of complete fools.&quot;</code></pre>
<ul>
<li>just for fun: English Scottish</li>
</ul>
<pre><code>espeak -v en-scottish -s 150 -w &#39;/Users/shiringlander/Documents/Github/audio_tts_espeak_en-scottish.wav&#39; &quot;A common mistake that people make when trying to design something completely foolproof is to underestimate the ingenuity of complete fools.&quot;</code></pre>
<ul>
<li>even funnier: German</li>
</ul>
<pre><code>espeak -v german -s 150 -w &#39;/Users/shiringlander/Documents/Github/audio_tts_espeak_german.wav&#39; &quot;A common mistake that people make when trying to design something completely foolproof is to underestimate the ingenuity of complete fools.&quot;</code></pre>
<p>The <a href="https://soundcloud.com/shirin-glander-729692416/sets/text-to-speech-blogpost/s-lXYea">playlist</a> contains all audio files I generated in this post.</p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.0 (2018-04-23)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.5
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] Rtts_0.3.3      RCurl_1.95-4.10 bitops_1.0-6   
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_0.12.17    bookdown_0.7    digest_0.6.15   rprojroot_1.3-2
##  [5] backports_1.1.2 magrittr_1.5    evaluate_0.10.1 blogdown_0.6   
##  [9] stringi_1.2.3   rmarkdown_1.10  tools_3.5.0     stringr_1.3.1  
## [13] xfun_0.2        yaml_2.1.19     compiler_3.5.0  htmltools_0.3.6
## [17] knitr_1.20</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Explaining Keras image classification models with lime]]></title>
    <link href="/2018/06/keras_fruits_lime/"/>
    <id>/2018/06/keras_fruits_lime/</id>
    <published>2018-06-21T00:00:00+00:00</published>
    <updated>2018-06-21T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p><a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits/">Last week I published a blog post about how easy it is to train image classification models with Keras</a>.</p>
<p>What I did not show in that post was how to use the model for making predictions. This, I will do here. But predictions alone are boring, so I’m adding explanations for the predictions using the <code>lime</code> package.</p>
<p>I have already written a few blog posts (<a href="https://shirinsplayground.netlify.com/2018/01/looking_beyond_accuracy_to_improve_trust_in_ml/">here</a>, <a href="https://shiring.github.io/machine_learning/2017/04/23/lime">here</a> and <a href="https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/">here</a>) about LIME and have given talks (<a href="https://shirinsplayground.netlify.com/2018/02/m3_2018/">here</a> and <a href="https://shirinsplayground.netlify.com/2018/04/hh_datascience_meetup_2018_slides/">here</a>) about it, too.</p>
<p>Neither of them applies LIME to image classification models, though. And with the new(ish) release from March of <a href="https://cran.r-project.org/web/packages/lime/index.html">Thomas Lin Pedersen’s <code>lime</code> package</a>, <code>lime</code> is now not only on CRAN but it natively supports Keras and image classification models.</p>
<p>Thomas wrote a very nice <a href="https://www.data-imaginist.com/2018/lime-v0-4-the-kitten-picture-edition/">article about how to use <code>keras</code> and <code>lime</code> in R</a>! Here, I am following this article to use Imagenet (VGG16) to make and explain predictions of fruit images and then I am extending the analysis to <a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits/">last week’s model</a> and compare it with the pretrained net.</p>
<div id="loading-libraries-and-models" class="section level2">
<h2>Loading libraries and models</h2>
<pre class="r"><code>library(keras)   # for working with neural nets
library(lime)    # for explaining models
library(magick)  # for preprocessing images
library(ggplot2) # for additional plotting</code></pre>
<ul>
<li>Loading the pretrained Imagenet model</li>
</ul>
<pre class="r"><code>model &lt;- application_vgg16(weights = &quot;imagenet&quot;, include_top = TRUE)
model</code></pre>
<pre><code>## Model
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## input_1 (InputLayer)             (None, 224, 224, 3)           0           
## ___________________________________________________________________________
## block1_conv1 (Conv2D)            (None, 224, 224, 64)          1792        
## ___________________________________________________________________________
## block1_conv2 (Conv2D)            (None, 224, 224, 64)          36928       
## ___________________________________________________________________________
## block1_pool (MaxPooling2D)       (None, 112, 112, 64)          0           
## ___________________________________________________________________________
## block2_conv1 (Conv2D)            (None, 112, 112, 128)         73856       
## ___________________________________________________________________________
## block2_conv2 (Conv2D)            (None, 112, 112, 128)         147584      
## ___________________________________________________________________________
## block2_pool (MaxPooling2D)       (None, 56, 56, 128)           0           
## ___________________________________________________________________________
## block3_conv1 (Conv2D)            (None, 56, 56, 256)           295168      
## ___________________________________________________________________________
## block3_conv2 (Conv2D)            (None, 56, 56, 256)           590080      
## ___________________________________________________________________________
## block3_conv3 (Conv2D)            (None, 56, 56, 256)           590080      
## ___________________________________________________________________________
## block3_pool (MaxPooling2D)       (None, 28, 28, 256)           0           
## ___________________________________________________________________________
## block4_conv1 (Conv2D)            (None, 28, 28, 512)           1180160     
## ___________________________________________________________________________
## block4_conv2 (Conv2D)            (None, 28, 28, 512)           2359808     
## ___________________________________________________________________________
## block4_conv3 (Conv2D)            (None, 28, 28, 512)           2359808     
## ___________________________________________________________________________
## block4_pool (MaxPooling2D)       (None, 14, 14, 512)           0           
## ___________________________________________________________________________
## block5_conv1 (Conv2D)            (None, 14, 14, 512)           2359808     
## ___________________________________________________________________________
## block5_conv2 (Conv2D)            (None, 14, 14, 512)           2359808     
## ___________________________________________________________________________
## block5_conv3 (Conv2D)            (None, 14, 14, 512)           2359808     
## ___________________________________________________________________________
## block5_pool (MaxPooling2D)       (None, 7, 7, 512)             0           
## ___________________________________________________________________________
## flatten (Flatten)                (None, 25088)                 0           
## ___________________________________________________________________________
## fc1 (Dense)                      (None, 4096)                  102764544   
## ___________________________________________________________________________
## fc2 (Dense)                      (None, 4096)                  16781312    
## ___________________________________________________________________________
## predictions (Dense)              (None, 1000)                  4097000     
## ===========================================================================
## Total params: 138,357,544
## Trainable params: 138,357,544
## Non-trainable params: 0
## ___________________________________________________________________________</code></pre>
<ul>
<li>loading my own model from <a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits/">last week’s post</a></li>
</ul>
<pre class="r"><code>model2 &lt;- load_model_hdf5(filepath = &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/keras/fruits_checkpoints.h5&quot;)
model2</code></pre>
<pre><code>## Model
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## conv2d_1 (Conv2D)                (None, 20, 20, 32)            896         
## ___________________________________________________________________________
## activation_1 (Activation)        (None, 20, 20, 32)            0           
## ___________________________________________________________________________
## conv2d_2 (Conv2D)                (None, 20, 20, 16)            4624        
## ___________________________________________________________________________
## leaky_re_lu_1 (LeakyReLU)        (None, 20, 20, 16)            0           
## ___________________________________________________________________________
## batch_normalization_1 (BatchNorm (None, 20, 20, 16)            64          
## ___________________________________________________________________________
## max_pooling2d_1 (MaxPooling2D)   (None, 10, 10, 16)            0           
## ___________________________________________________________________________
## dropout_1 (Dropout)              (None, 10, 10, 16)            0           
## ___________________________________________________________________________
## flatten_1 (Flatten)              (None, 1600)                  0           
## ___________________________________________________________________________
## dense_1 (Dense)                  (None, 100)                   160100      
## ___________________________________________________________________________
## activation_2 (Activation)        (None, 100)                   0           
## ___________________________________________________________________________
## dropout_2 (Dropout)              (None, 100)                   0           
## ___________________________________________________________________________
## dense_2 (Dense)                  (None, 16)                    1616        
## ___________________________________________________________________________
## activation_3 (Activation)        (None, 16)                    0           
## ===========================================================================
## Total params: 167,300
## Trainable params: 167,268
## Non-trainable params: 32
## ___________________________________________________________________________</code></pre>
</div>
<div id="load-and-prepare-images" class="section level2">
<h2>Load and prepare images</h2>
<p>Here, I am loading and preprocessing two images of fruits (and yes, I am cheating a bit because I am choosing images where I expect my model to work as they are similar to the training images…).</p>
<ul>
<li>Banana</li>
</ul>
<pre class="r"><code>test_image_files_path &lt;- &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Test&quot;

img &lt;- image_read(&#39;https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Banana-Single.jpg/272px-Banana-Single.jpg&#39;)
img_path &lt;- file.path(test_image_files_path, &quot;Banana&quot;, &#39;banana.jpg&#39;)
image_write(img, img_path)
#plot(as.raster(img))</code></pre>
<ul>
<li>Clementine</li>
</ul>
<pre class="r"><code>img2 &lt;- image_read(&#39;https://cdn.pixabay.com/photo/2010/12/13/09/51/clementine-1792_1280.jpg&#39;)
img_path2 &lt;- file.path(test_image_files_path, &quot;Clementine&quot;, &#39;clementine.jpg&#39;)
image_write(img2, img_path2)
#plot(as.raster(img2))</code></pre>
<div id="superpixels" class="section level3">
<h3>Superpixels</h3>
<blockquote>
<p>The segmentation of an image into superpixels are an important step in generating explanations for image models. It is both important that the segmentation is correct and follows meaningful patterns in the picture, but also that the size/number of superpixels are appropriate. If the important features in the image are chopped into too many segments the permutations will probably damage the picture beyond recognition in almost all cases leading to a poor or failing explanation model. As the size of the object of interest is varying it is impossible to set up hard rules for the number of superpixels to segment into - the larger the object is relative to the size of the image, the fewer superpixels should be generated. Using plot_superpixels it is possible to evaluate the superpixel parameters before starting the time consuming explanation function. (help(plot_superpixels))</p>
</blockquote>
<pre class="r"><code>plot_superpixels(img_path, n_superpixels = 35, weight = 10)</code></pre>
<p><img src="/post/2018-06-21_keras_fruits_lime_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>plot_superpixels(img_path2, n_superpixels = 50, weight = 20)</code></pre>
<p><img src="/post/2018-06-21_keras_fruits_lime_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>From the superpixel plots we can see that the clementine image has a higher resolution than the banana image.</p>
</div>
</div>
<div id="prepare-images-for-imagenet" class="section level2">
<h2>Prepare images for Imagenet</h2>
<pre class="r"><code>image_prep &lt;- function(x) {
  arrays &lt;- lapply(x, function(path) {
    img &lt;- image_load(path, target_size = c(224,224))
    x &lt;- image_to_array(img)
    x &lt;- array_reshape(x, c(1, dim(x)))
    x &lt;- imagenet_preprocess_input(x)
  })
  do.call(abind::abind, c(arrays, list(along = 1)))
}</code></pre>
<ul>
<li>test predictions</li>
</ul>
<pre class="r"><code>res &lt;- predict(model, image_prep(c(img_path, img_path2)))
imagenet_decode_predictions(res)</code></pre>
<pre><code>## [[1]]
##   class_name class_description        score
## 1  n07753592            banana 0.9929747581
## 2  n03532672              hook 0.0013420776
## 3  n07747607            orange 0.0010816186
## 4  n07749582             lemon 0.0010625814
## 5  n07716906  spaghetti_squash 0.0009176208
## 
## [[2]]
##   class_name class_description      score
## 1  n07747607            orange 0.78233224
## 2  n07753592            banana 0.04653566
## 3  n07749582             lemon 0.03868873
## 4  n03134739      croquet_ball 0.03350329
## 5  n07745940        strawberry 0.01862431</code></pre>
<ul>
<li>load labels and train explainer</li>
</ul>
<pre class="r"><code>model_labels &lt;- readRDS(system.file(&#39;extdata&#39;, &#39;imagenet_labels.rds&#39;, package = &#39;lime&#39;))
explainer &lt;- lime(c(img_path, img_path2), as_classifier(model, model_labels), image_prep)</code></pre>
<p>Training the explainer (<code>explain()</code> function) can take pretty long. It will be much faster with the smaller images in my own model but with the bigger Imagenet it takes a few minutes to run.</p>
<pre class="r"><code>explanation &lt;- explain(c(img_path, img_path2), explainer, 
                       n_labels = 2, n_features = 35,
                       n_superpixels = 35, weight = 10,
                       background = &quot;white&quot;)</code></pre>
<ul>
<li><code>plot_image_explanation()</code> only supports showing one case at a time</li>
</ul>
<pre class="r"><code>plot_image_explanation(explanation)</code></pre>
<p><img src="/post/2018-06-21_keras_fruits_lime_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code>clementine &lt;- explanation[explanation$case == &quot;clementine.jpg&quot;,]
plot_image_explanation(clementine)</code></pre>
<p><img src="/post/2018-06-21_keras_fruits_lime_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="prepare-images-for-my-own-model" class="section level2">
<h2>Prepare images for my own model</h2>
<ul>
<li>test predictions (analogous to training and validation images)</li>
</ul>
<pre class="r"><code>test_datagen &lt;- image_data_generator(rescale = 1/255)

test_generator &lt;- flow_images_from_directory(
        test_image_files_path,
        test_datagen,
        target_size = c(20, 20),
        class_mode = &#39;categorical&#39;)

predictions &lt;- as.data.frame(predict_generator(model2, test_generator, steps = 1))

load(&quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/fruits_classes_indices.RData&quot;)
fruits_classes_indices_df &lt;- data.frame(indices = unlist(fruits_classes_indices))
fruits_classes_indices_df &lt;- fruits_classes_indices_df[order(fruits_classes_indices_df$indices), , drop = FALSE]
colnames(predictions) &lt;- rownames(fruits_classes_indices_df)

t(round(predictions, digits = 2))</code></pre>
<pre><code>##             [,1] [,2]
## Kiwi           0    0
## Banana         0    1
## Apricot        0    0
## Avocado        0    0
## Cocos          0    0
## Clementine     1    0
## Mandarine      0    0
## Orange         0    0
## Limes          0    0
## Lemon          0    0
## Peach          0    0
## Plum           0    0
## Raspberry      0    0
## Strawberry     0    0
## Pineapple      0    0
## Pomegranate    0    0</code></pre>
<pre class="r"><code>for (i in 1:nrow(predictions)) {
  cat(i, &quot;:&quot;)
  print(unlist(which.max(predictions[i, ])))
}</code></pre>
<pre><code>## 1 :Clementine 
##          6 
## 2 :Banana 
##      2</code></pre>
<p>This seems to be incompatible with lime, though (or if someone knows how it works, please let me know) - so I prepared the images similarly to the Imagenet images.</p>
<pre class="r"><code>image_prep2 &lt;- function(x) {
  arrays &lt;- lapply(x, function(path) {
    img &lt;- image_load(path, target_size = c(20, 20))
    x &lt;- image_to_array(img)
    x &lt;- reticulate::array_reshape(x, c(1, dim(x)))
    x &lt;- x / 255
  })
  do.call(abind::abind, c(arrays, list(along = 1)))
}</code></pre>
<ul>
<li>prepare labels</li>
</ul>
<pre class="r"><code>fruits_classes_indices_l &lt;- rownames(fruits_classes_indices_df)
names(fruits_classes_indices_l) &lt;- unlist(fruits_classes_indices)
fruits_classes_indices_l</code></pre>
<pre><code>##             9            10             8             2            11 
##        &quot;Kiwi&quot;      &quot;Banana&quot;     &quot;Apricot&quot;     &quot;Avocado&quot;       &quot;Cocos&quot; 
##             3            13            14             7             6 
##  &quot;Clementine&quot;   &quot;Mandarine&quot;      &quot;Orange&quot;       &quot;Limes&quot;       &quot;Lemon&quot; 
##             1             5             0             4            15 
##       &quot;Peach&quot;        &quot;Plum&quot;   &quot;Raspberry&quot;  &quot;Strawberry&quot;   &quot;Pineapple&quot; 
##            12 
## &quot;Pomegranate&quot;</code></pre>
<ul>
<li>train explainer</li>
</ul>
<pre class="r"><code>explainer2 &lt;- lime(c(img_path, img_path2), as_classifier(model2, fruits_classes_indices_l), image_prep2)
explanation2 &lt;- explain(c(img_path, img_path2), explainer2, 
                        n_labels = 1, n_features = 20,
                        n_superpixels = 35, weight = 10,
                        background = &quot;white&quot;)</code></pre>
<ul>
<li>plot feature weights to find a good threshold for plotting <code>block</code> (see below)</li>
</ul>
<pre class="r"><code>explanation2 %&gt;%
  ggplot(aes(x = feature_weight)) +
    facet_wrap(~ case, scales = &quot;free&quot;) +
    geom_density()</code></pre>
<p><img src="/post/2018-06-21_keras_fruits_lime_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<ul>
<li>plot predictions</li>
</ul>
<pre class="r"><code>plot_image_explanation(explanation2, display = &#39;block&#39;, threshold = 5e-07)</code></pre>
<p><img src="/post/2018-06-21_keras_fruits_lime_files/figure-html/banana_explanation-1.png" width="672" /></p>
<pre class="r"><code>clementine2 &lt;- explanation2[explanation2$case == &quot;clementine.jpg&quot;,]
plot_image_explanation(clementine2, display = &#39;block&#39;, threshold = 0.16)</code></pre>
<p><img src="/post/2018-06-21_keras_fruits_lime_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] ggplot2_3.0.0 magick_1.9    lime_0.4.0    keras_2.1.6  
## 
## loaded via a namespace (and not attached):
##  [1] stringdist_0.9.5.1    reticulate_1.8.0.9003 tidyselect_0.2.4     
##  [4] xfun_0.3              purrr_0.2.5           lattice_0.20-35      
##  [7] colorspace_1.3-2      htmltools_0.3.6       yaml_2.1.19          
## [10] base64enc_0.1-3       rlang_0.2.1           pillar_1.2.3         
## [13] later_0.7.3           withr_2.1.2           glue_1.2.0           
## [16] bindrcpp_0.2.2        foreach_1.4.4         plyr_1.8.4           
## [19] bindr_0.1.1           tensorflow_1.8        stringr_1.3.1        
## [22] munsell_0.5.0         blogdown_0.6          gtable_0.2.0         
## [25] htmlwidgets_1.2       codetools_0.2-15      evaluate_0.10.1      
## [28] labeling_0.3          knitr_1.20            httpuv_1.4.4.2       
## [31] tfruns_1.3            curl_3.2              parallel_3.5.1       
## [34] Rcpp_0.12.17          xtable_1.8-2          scales_0.5.0         
## [37] backports_1.1.2       promises_1.0.1        jsonlite_1.5         
## [40] abind_1.4-5           mime_0.5              digest_0.6.15        
## [43] stringi_1.2.3         bookdown_0.7          dplyr_0.7.6          
## [46] shiny_1.1.0           grid_3.5.1            rprojroot_1.3-2      
## [49] tools_3.5.1           magrittr_1.5          shinythemes_1.1.1    
## [52] lazyeval_0.2.1        glmnet_2.0-16         tibble_1.4.2         
## [55] whisker_0.3-2         pkgconfig_2.0.1       zeallot_0.1.0        
## [58] Matrix_1.2-14         gower_0.1.2           assertthat_0.2.0     
## [61] rmarkdown_1.10        iterators_1.0.9       R6_2.2.2             
## [64] compiler_3.5.1</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI: Practical Deep Learning with Rachel Thomas]]></title>
    <link href="/2018/06/twimlai138/"/>
    <id>/2018/06/twimlai138/</id>
    <published>2018-06-18T00:00:00+00:00</published>
    <updated>2018-06-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Practical Deep Learning with Rachel Thomas</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai138.jpg" alt="Sketchnotes from TWiMLAI talk: Practical Deep Learning with Rachel Thomas" />
<p class="caption">Sketchnotes from TWiMLAI talk: Practical Deep Learning with Rachel Thomas</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-138-practical-deep-learning-with-rachel-thomas/">here</a>.</p>
<blockquote>
<p>In this episode, i’m joined by Rachel Thomas, founder and researcher at Fast AI. If you’re not familiar with Fast AI, the company offers a series of courses including Practical Deep Learning for Coders, Cutting Edge Deep Learning for Coders and Rachel’s Computational Linear Algebra course. The courses are designed to make deep learning more accessible to those without the extensive math backgrounds some other courses assume. Rachel and I cover a lot of ground in this conversation, starting with the philosophy and goals behind the Fast AI courses. We also cover Fast AI’s recent decision to switch to their courses from Tensorflow to Pytorch, the reasons for this, and the lessons they’ve learned in the process. We discuss the role of the Fast AI deep learning library as well, and how it was recently used to held their team achieve top results on a popular industry benchmark of training time and training cost by a factor of more than ten. <a href="https://twimlai.com/twiml-talk-138-practical-deep-learning-with-rachel-thomas/" class="uri">https://twimlai.com/twiml-talk-138-practical-deep-learning-with-rachel-thomas/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[It&#39;s that easy! Image classification with keras in roughly 100 lines of code.]]></title>
    <link href="/2018/06/keras_fruits/"/>
    <id>/2018/06/keras_fruits/</id>
    <published>2018-06-15T00:00:00+00:00</published>
    <updated>2018-06-15T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I’ve been using keras and TensorFlow for a while now - and love its simplicity and straight-forward way to modeling. As part of the latest update to my <a href="https://shirinsplayground.netlify.com/2018/05/deep_learning_keras_tensorflow_18_07/">Workshop about deep learning with R and keras</a> I’ve added a new example analysis:</p>
<p><strong>Building an image classifier to differentiate different types of fruits</strong></p>
<p>And I was (again) suprised how fast and easy it was to build the model; it took not even half an hour and only around 100 lines of code (counting only the main code; for this post I added comments and line breaks to make it easier to read)!</p>
<iframe src="https://giphy.com/embed/5p2wQFyu8GsFO" width="480" height="271" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>
<a href="https://giphy.com/gifs/5p2wQFyu8GsFO">via GIPHY</a>
</p>
<p>That’s why I wanted to share it here and spread the <code>keras</code> love. &lt;3</p>
<div id="the-code" class="section level2">
<h2>The code</h2>
<p>If you haven’t installed keras before, follow the instructions of <a href="https://keras.rstudio.com/">RStudio’s keras site</a></p>
<pre class="r"><code>library(keras)</code></pre>
<p>The dataset is the <a href="https://www.kaggle.com/moltean/fruits/data">fruit images dataset from Kaggle</a>. I downloaded it to my computer and unpacked it. Because I don’t want to build a model for all the different fruits, I define a list of fruits (corresponding to the folder names) that I want to include in the model.</p>
<p>I also define a few other parameters in the beginning to make adapting as easy as possible.</p>
<pre class="r"><code># list of fruits to modle
fruit_list &lt;- c(&quot;Kiwi&quot;, &quot;Banana&quot;, &quot;Apricot&quot;, &quot;Avocado&quot;, &quot;Cocos&quot;, &quot;Clementine&quot;, &quot;Mandarine&quot;, &quot;Orange&quot;,
                &quot;Limes&quot;, &quot;Lemon&quot;, &quot;Peach&quot;, &quot;Plum&quot;, &quot;Raspberry&quot;, &quot;Strawberry&quot;, &quot;Pineapple&quot;, &quot;Pomegranate&quot;)

# number of output classes (i.e. fruits)
output_n &lt;- length(fruit_list)

# image size to scale down to (original images are 100 x 100 px)
img_width &lt;- 20
img_height &lt;- 20
target_size &lt;- c(img_width, img_height)

# RGB = 3 channels
channels &lt;- 3

# path to image folders
train_image_files_path &lt;- &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Training/&quot;
valid_image_files_path &lt;- &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/Validation/&quot;</code></pre>
<div id="loading-images" class="section level3">
<h3>Loading images</h3>
<p>The handy <code>image_data_generator()</code> and <code>flow_images_from_directory()</code> functions can be used to load images from a directory. If you want to use data augmentation, you can directly define how and in what way you want to augment your images with <code>image_data_generator</code>. Here I am not augmenting the data, I only scale the pixel values to fall between 0 and 1.</p>
<pre class="r"><code># optional data augmentation
train_data_gen = image_data_generator(
  rescale = 1/255 #,
  #rotation_range = 40,
  #width_shift_range = 0.2,
  #height_shift_range = 0.2,
  #shear_range = 0.2,
  #zoom_range = 0.2,
  #horizontal_flip = TRUE,
  #fill_mode = &quot;nearest&quot;
)

# Validation data shouldn&#39;t be augmented! But it should also be scaled.
valid_data_gen &lt;- image_data_generator(
  rescale = 1/255
  )  </code></pre>
<p>Now we load the images into memory and resize them.</p>
<pre class="r"><code># training images
train_image_array_gen &lt;- flow_images_from_directory(train_image_files_path, 
                                          train_data_gen,
                                          target_size = target_size,
                                          class_mode = &quot;categorical&quot;,
                                          classes = fruit_list,
                                          seed = 42)

# validation images
valid_image_array_gen &lt;- flow_images_from_directory(valid_image_files_path, 
                                          valid_data_gen,
                                          target_size = target_size,
                                          class_mode = &quot;categorical&quot;,
                                          classes = fruit_list,
                                          seed = 42)</code></pre>
<pre class="r"><code>cat(&quot;Number of images per class:&quot;)</code></pre>
<pre><code>## Number of images per class:</code></pre>
<pre class="r"><code>table(factor(train_image_array_gen$classes))</code></pre>
<pre><code>## 
##   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15 
## 466 490 492 427 490 490 490 479 490 492 492 447 490 492 490 492</code></pre>
<pre class="r"><code>cat(&quot;\nClass label vs index mapping:\n&quot;)</code></pre>
<pre><code>## 
## Class label vs index mapping:</code></pre>
<pre class="r"><code>train_image_array_gen$class_indices</code></pre>
<pre><code>## $Lemon
## [1] 9
## 
## $Peach
## [1] 10
## 
## $Limes
## [1] 8
## 
## $Apricot
## [1] 2
## 
## $Plum
## [1] 11
## 
## $Avocado
## [1] 3
## 
## $Strawberry
## [1] 13
## 
## $Pineapple
## [1] 14
## 
## $Orange
## [1] 7
## 
## $Mandarine
## [1] 6
## 
## $Banana
## [1] 1
## 
## $Clementine
## [1] 5
## 
## $Kiwi
## [1] 0
## 
## $Cocos
## [1] 4
## 
## $Pomegranate
## [1] 15
## 
## $Raspberry
## [1] 12</code></pre>
<pre class="r"><code>fruits_classes_indices &lt;- train_image_array_gen$class_indices
save(fruits_classes_indices, file = &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/fruits_classes_indices.RData&quot;)</code></pre>
</div>
<div id="define-model" class="section level3">
<h3>Define model</h3>
<p>Next, we define the <code>keras</code> model.</p>
<pre class="r"><code># number of training samples
train_samples &lt;- train_image_array_gen$n
# number of validation samples
valid_samples &lt;- valid_image_array_gen$n

# define batch size and number of epochs
batch_size &lt;- 32
epochs &lt;- 10</code></pre>
<p>The model I am using here is a very simple sequential convolutional neural net with the following hidden layers: 2 convolutional layers, one pooling layer and one dense layer.</p>
<pre class="r"><code># initialise model
model &lt;- keras_model_sequential()

# add layers
model %&gt;%
  layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = &quot;same&quot;, input_shape = c(img_width, img_height, channels)) %&gt;%
  layer_activation(&quot;relu&quot;) %&gt;%
  
  # Second hidden layer
  layer_conv_2d(filter = 16, kernel_size = c(3,3), padding = &quot;same&quot;) %&gt;%
  layer_activation_leaky_relu(0.5) %&gt;%
  layer_batch_normalization() %&gt;%

  # Use max pooling
  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;%
  layer_dropout(0.25) %&gt;%
  
  # Flatten max filtered output into feature vector 
  # and feed into dense layer
  layer_flatten() %&gt;%
  layer_dense(100) %&gt;%
  layer_activation(&quot;relu&quot;) %&gt;%
  layer_dropout(0.5) %&gt;%

  # Outputs from dense layer are projected onto output layer
  layer_dense(output_n) %&gt;% 
  layer_activation(&quot;softmax&quot;)

# compile
model %&gt;% compile(
  loss = &quot;categorical_crossentropy&quot;,
  optimizer = optimizer_rmsprop(lr = 0.0001, decay = 1e-6),
  metrics = &quot;accuracy&quot;
)</code></pre>
<p>Fit the model; because I used <code>image_data_generator()</code> and <code>flow_images_from_directory()</code> I am now also using the <code>fit_generator()</code> to run the training.</p>
<pre class="r"><code># fit
hist &lt;- model %&gt;% fit_generator(
  # training data
  train_image_array_gen,
  
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = epochs, 
  
  # validation data
  validation_data = valid_image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size),
  
  # print progress
  verbose = 2,
  callbacks = list(
    # save best model after every epoch
    callback_model_checkpoint(&quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/keras/fruits_checkpoints.h5&quot;, save_best_only = TRUE),
    # only needed for visualising with TensorBoard
    callback_tensorboard(log_dir = &quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/keras/logs&quot;)
  )
)</code></pre>
<p>In RStudio we are seeing the output as an interactive plot in the “Viewer” pane but we can also plot it:</p>
<pre class="r"><code>plot(hist)</code></pre>
<p><img src="/post/2018-06-15_keras_fruits_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>As we can see, the model is quite accurate on the validation data. However, we need to keep in mind that our images are very uniform, they all have the same white background and show the fruits centered and without anything else in the images. Thus, our model will not work with images that don’t look similar as the ones we trained on (that’s also why we can achieve such good results with such a small neural net).</p>
<p>Finally, I want to have a look at the TensorFlow graph with TensorBoard.</p>
<pre class="r"><code>tensorboard(&quot;/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/keras/logs&quot;)</code></pre>
<div class="figure">
<img src="/img/tensorboard.png" />

</div>
<p>That’s all there is to it!</p>
<p>Of course, you could now save your model and/or the weights, visualize the hidden layers, run predictions on test data, etc. For now, I’ll leave it at that, though. :-)</p>
<hr />
<p>There now is a <a href="https://shirinsplayground.netlify.com/2018/06/keras_fruits_lime/">second part: Explaining Keras image classification models with lime</a></p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.0 (2018-04-23)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.5
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] keras_2.1.6
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_0.12.17     compiler_3.5.0   pillar_1.2.3     plyr_1.8.4      
##  [5] base64enc_0.1-3  tools_3.5.0      zeallot_0.1.0    digest_0.6.15   
##  [9] jsonlite_1.5     evaluate_0.10.1  tibble_1.4.2     gtable_0.2.0    
## [13] lattice_0.20-35  rlang_0.2.1      Matrix_1.2-14    yaml_2.1.19     
## [17] blogdown_0.6     xfun_0.2         stringr_1.3.1    knitr_1.20      
## [21] rprojroot_1.3-2  grid_3.5.0       reticulate_1.8   R6_2.2.2        
## [25] rmarkdown_1.10   bookdown_0.7     ggplot2_2.2.1    reshape2_1.4.3  
## [29] magrittr_1.5     whisker_0.3-2    backports_1.1.2  scales_0.5.0    
## [33] tfruns_1.3       htmltools_0.3.6  colorspace_1.3-2 labeling_0.3    
## [37] tensorflow_1.8   stringi_1.2.3    lazyeval_0.2.1   munsell_0.5.0</code></pre>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[rOpenSci unconference 2018 &#43; introduction to TensorFlow Probability &amp; the &#39;greta&#39; package]]></title>
    <link href="/2018/05/ropensci_unconf18/"/>
    <id>/2018/05/ropensci_unconf18/</id>
    <published>2018-05-30T00:00:00+00:00</published>
    <updated>2018-05-30T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/viz/viz.js"></script>
<link href="/rmarkdown-libs/DiagrammeR-styles/styles.css" rel="stylesheet" />
<script src="/rmarkdown-libs/grViz-binding/grViz.js"></script>


<p>On May 21st and 22nd, I had the honor of having been chosen to attend the rOpenSci unconference 2018 in Seattle. It was a great event and I got to meet many amazing people!</p>
<div id="ropensci" class="section level2">
<h2>rOpenSci</h2>
<p><a href="https://ropensci.org/">rOpenSci</a> is a non-profit organisation that maintains a number of widely used R <a href="https://ropensci.org/packages/">packages</a> and is very active in promoting a <a href="https://ropensci.org/community/">community</a> spirit around the R-world. Their core values are to have open and reproducible research, shared data and easy-to-use tools and to make all this accessible to a large number of people.</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/rOpenSci/IMG_3637.jpg" />

</div>
</div>
<div id="ropensci-unconference" class="section level2">
<h2>rOpenSci unconference</h2>
<p>Part of creating a welcoming community infrastructure is their yearly unconference. At the unconference, about 60 invited R users from around the world get together to work on small projects that are relevant to the R community at the time. Project ideas are collected and discussed in Github issues during the weeks before the unconference but the final decision which projects will be worked on is made by the participants on the first morning of the unconference.</p>
<p><a href="http://unconf18.ropensci.org/">This year’s rOpenSci unconference</a> was held at the Microsoft Reactor in Seattle.</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/rOpenSci/IMG_3716.jpg" />

</div>
<p>The whole organizing team - most and foremost Stefanie Butland - did a wonderful job hosting this event. Everybody made sure that the spirit of the unconference was inclusive and very welcoming to everybody, from long-established fixtures in the R-world to newbies and anyone in between.</p>
<p>We were a pretty diverse group of social scientists, bioinformaticians, ecologists, historians, data scientists, developers, people working with Google, Microsoft or RStudio and R enthusiasts from many other fields. Some people already knew a few others, many knew each other from Twitter, R-Ladies, or other online communities but most of us (including me) had never met in person.</p>
<p>Therefore, the official part of the unconference was started on Monday morning with a few “ice breakers”: Stefanie would ask a question or make a statement and we would position ourselves in the room according to our answer and discuss with the people close to us. Starting with “Are you a dog or a cat person?” and finishing with “I know my place in the R community”, we all quickly had a lot to talk about! It was a great way to meet many of the people we would spend the next two days with.</p>
<p>It was a great experience working with so many talented and motivated people who share my passion for the R language - particularly because in my line of work as a data scientist R is often considered inferior to Python and the majority of the active R community is situated in the Pacific Northwest and California. It was a whole new experience to work together with other people on an R project and I absolutely loved it!</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/rOpenSci/IMG_3717.jpg" />

</div>
</div>
<div id="working-on-greta" class="section level2">
<h2>Working on <code>greta</code></h2>
<p>During the 2 days of the unconference, people worked on many interesting, useful and cool projects <a href="https://github.com/ropenscilabs/runconf18-projects/blob/master/index.Rmd">(click here for a complete list with links to the Github repos for every project!)</a>!</p>
<p>The group I joined originally wanted to bring <strong>TensorFlow Probability</strong> to R.</p>
<blockquote>
<p>TensorFlow Probability is a library for probabilistic reasoning and statistical analysis in TensorFlow. As part of the TensorFlow ecosystem, TensorFlow Probability provides integration of probabilistic methods with deep networks, gradient-based inference via automatic differentiation, and scalability to large datasets and models via hardware acceleration (e.g., GPUs) and distributed computation. <a href="https://github.com/tensorflow/probability" class="uri">https://github.com/tensorflow/probability</a></p>
</blockquote>
<p>In the end, we - that is <a href="https://github.com/michaelquinn32">Michael Quinn</a>, <a href="https://github.com/revodavid">David Smith</a>, <a href="https://github.com/TiphaineCMartin">Tiphaine Martin</a>, <a href="https://github.com/mmulvahill">Matt Mulvahill</a> and I - ended up working with the R package <code>greta</code>, which has similar functionalities as TensorFlow Probability. We recreated some of the examples from the TensorFlow Probability package tutorials in <code>greta</code> and we also added a few additional examples that show how you can use <code>greta</code>.</p>
<p>Check out <a href="https://github.com/ropenscilabs/greta/tree/unconf/README.md">the README repo for an overview and links to everything we’ve contributed</a>; it is a forked repo from the <a href="https://github.com/greta-dev/greta">original package repo of <code>greta</code></a> and the vignettes will hopefully get included in the main repo at some point in the near future.</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/rOpenSci/IMG_3715.jpg" />

</div>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/rOpenSci/IMG_3724.jpg" />

</div>
<div id="what-is-greta" class="section level3">
<h3>What is <code>greta</code>?</h3>
<p><a href="https://greta-dev.github.io/greta"><code>greta</code></a> is an R package that has been created by <a href="https://github.com/goldingn">Nick Golding</a> for implementing Markov-Chain Monte-Carlo (MCMC) models, e.g. a Hamiltonian Monte Carlo (HMC) method. It offers a number of functions that make it easy to define these models, particularly for Bayesian statistics (similar to Stan).</p>
<blockquote>
<p>greta lets us build statistical models interactively in R, and then sample from them by MCMC. <a href="https://greta-dev.github.io/greta/get_started.html#how_greta_works" class="uri">https://greta-dev.github.io/greta/get_started.html#how_greta_works</a></p>
</blockquote>
<p>Google’s TensorFlow is used as a backend to compute the defined models. Because TensorFlow has been optimized for large-scale computing, multi-core and GPU calculations are supported as well, <code>greta</code> is particularly efficient and useful for working with complex models. As TensorFlow is not natively an R package, <code>greta</code> makes use of RStudio’s <a href="https://rstudio.github.io/reticulate/articles/python_packages.html">reticulate</a> and <a href="https://tensorflow.rstudio.com/">tensorflow</a> packages to connect with the TensorFlow backend. This way, we can work with all the TensorFlow functions directly from within R.</p>
</div>
<div id="how-does-greta-work" class="section level3">
<h3>How does <code>greta</code> work?</h3>
<blockquote>
<p>There are three layers to how greta defines a model: users manipulate greta arrays, these define nodes, and nodes then define Tensors. <a href="https://greta-dev.github.io/greta/technical_details.html" class="uri">https://greta-dev.github.io/greta/technical_details.html</a></p>
</blockquote>
<p>This is the minimum working example of the linear mixed model that we developed in <code>greta</code> based on an example from a TensorFlow Probability Jupyter notebook. The full example with explanations can be found <a href="https://github.com/ropenscilabs/greta/blob/unconf/vignettes/8_schools_example_model.Rmd">here</a>.</p>
<pre class="r"><code>library(greta)</code></pre>
<pre class="text"><code># data
N &lt;- letters[1:8]
treatment_effects &lt;- c(28.39, 7.94, -2.75 , 6.82, -0.64, 0.63, 18.01, 12.16)
treatment_stddevs &lt;- c(14.9, 10.2, 16.3, 11.0, 9.4, 11.4, 10.4, 17.6)</code></pre>
<pre class="r"><code># variables and priors
avg_effect &lt;- normal(mean = 0, sd = 10)
avg_stddev &lt;- normal(5, 1)
school_effects_standard &lt;- normal(0, 1, dim = length(N))
school_effects &lt;- avg_effect + exp(avg_stddev) * school_effects_standard

# likelihood
distribution(treatment_effects) &lt;- normal(school_effects, treatment_stddevs)

# defining the hierarchical model
m &lt;- model(avg_effect, avg_stddev, school_effects_standard)
m</code></pre>
<pre><code>## greta model</code></pre>
<pre class="r"><code>plot(m)</code></pre>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"diagram":"digraph {\n\ngraph [layout = \"dot\",\n       outputorder = \"edgesfirst\",\n       bgcolor = \"white\",\n       rankdir = \"LR\"]\n\nnode [fontname = \"Helvetica\",\n      fontsize = \"10\",\n      shape = \"circle\",\n      fixedsize = \"true\",\n      width = \"0.5\",\n      style = \"filled\",\n      fillcolor = \"aliceblue\",\n      color = \"gray70\",\n      fontcolor = \"gray50\"]\n\nedge [fontname = \"Helvetica\",\n     fontsize = \"8\",\n     len = \"1.5\",\n     color = \"gray80\",\n     arrowsize = \"0.5\"]\n\n  \"1\" [label = \"avg_effect\n\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"circle\", color = \"#E0D2EE\", width = \"0.6\", height = \"0.48\", fillcolor = \"#F4F0F9\"] \n  \"2\" [label = \"normal\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"diamond\", color = \"#B797D7\", width = \"1\", height = \"0.8\", fillcolor = \"#E0D2EE\"] \n  \"3\" [label = \"0\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"square\", color = \"#E0D2EE\", width = \"0.5\", height = \"0.4\", fillcolor = \"#FFFFFF\"] \n  \"4\" [label = \"10\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"square\", color = \"#E0D2EE\", width = \"0.5\", height = \"0.4\", fillcolor = \"#FFFFFF\"] \n  \"5\" [label = \"school_effects\n\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"circle\", color = \"lightgray\", width = \"0.2\", height = \"0.16\", fillcolor = \"#D3D3D3\"] \n  \"6\" [label = \"normal\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"diamond\", color = \"#B797D7\", width = \"1\", height = \"0.8\", fillcolor = \"#E0D2EE\"] \n  \"7\" [label = \"\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"square\", color = \"#E0D2EE\", width = \"0.5\", height = \"0.4\", fillcolor = \"#FFFFFF\"] \n  \"8\" [label = \"\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"square\", color = \"#E0D2EE\", width = \"0.5\", height = \"0.4\", fillcolor = \"#FFFFFF\"] \n  \"9\" [label = \"\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"circle\", color = \"lightgray\", width = \"0.2\", height = \"0.16\", fillcolor = \"#D3D3D3\"] \n  \"10\" [label = \"\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"circle\", color = \"lightgray\", width = \"0.2\", height = \"0.16\", fillcolor = \"#D3D3D3\"] \n  \"11\" [label = \"avg_stddev\n\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"circle\", color = \"#E0D2EE\", width = \"0.6\", height = \"0.48\", fillcolor = \"#F4F0F9\"] \n  \"12\" [label = \"normal\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"diamond\", color = \"#B797D7\", width = \"1\", height = \"0.8\", fillcolor = \"#E0D2EE\"] \n  \"13\" [label = \"5\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"square\", color = \"#E0D2EE\", width = \"0.5\", height = \"0.4\", fillcolor = \"#FFFFFF\"] \n  \"14\" [label = \"1\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"square\", color = \"#E0D2EE\", width = \"0.5\", height = \"0.4\", fillcolor = \"#FFFFFF\"] \n  \"15\" [label = \"school_effects_standard\n\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"circle\", color = \"#E0D2EE\", width = \"0.6\", height = \"0.48\", fillcolor = \"#F4F0F9\"] \n  \"16\" [label = \"normal\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"diamond\", color = \"#B797D7\", width = \"1\", height = \"0.8\", fillcolor = \"#E0D2EE\"] \n  \"17\" [label = \"0\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"square\", color = \"#E0D2EE\", width = \"0.5\", height = \"0.4\", fillcolor = \"#FFFFFF\"] \n  \"18\" [label = \"1\", fontcolor = \"#8960B3\", fontsize = \"12\", penwidth = \"2\", shape = \"square\", color = \"#E0D2EE\", width = \"0.5\", height = \"0.4\", fillcolor = \"#FFFFFF\"] \n\"1\"->\"5\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"add\", style = \"solid\"] \n\"2\"->\"1\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", penwidth = \"3\", style = \"dashed\"] \n\"3\"->\"2\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"mean\", style = \"solid\"] \n\"4\"->\"2\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"sd\", style = \"solid\"] \n\"5\"->\"6\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"mean\", style = \"solid\"] \n\"6\"->\"8\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", penwidth = \"3\", style = \"dashed\"] \n\"7\"->\"6\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"sd\", style = \"solid\"] \n\"9\"->\"5\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"add\", style = \"solid\"] \n\"10\"->\"9\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"multiply\", style = \"solid\"] \n\"11\"->\"10\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"exp\", style = \"solid\"] \n\"12\"->\"11\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", penwidth = \"3\", style = \"dashed\"] \n\"13\"->\"12\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"mean\", style = \"solid\"] \n\"14\"->\"12\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"sd\", style = \"solid\"] \n\"15\"->\"9\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"multiply\", style = \"solid\"] \n\"16\"->\"15\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", penwidth = \"3\", style = \"dashed\"] \n\"17\"->\"16\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"mean\", style = \"solid\"] \n\"18\"->\"16\" [color = \"Gainsboro\", fontname = \"Helvetica\", fontcolor = \"gray\", fontsize = \"11\", penwidth = \"3\", label = \"sd\", style = \"solid\"] \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<pre class="r"><code># sampling
draws &lt;- greta::mcmc(m)
plot(draws)</code></pre>
<p>The main type of object you’ll be using in <code>greta</code> is the <strong>greta array</strong>. You can <a href="https://greta-dev.github.io/greta/structures.html">create <code>greta</code> arrays</a> or <a href="https://greta-dev.github.io/greta/as_data.html">convert R objects, like data frames into <code>greta</code> arrays</a>. <code>greta</code> arrays are basically a list with one element: an R6 class object with <code>node</code> + <code>data</code>, <code>operation</code> or <code>variable</code> property. This way, <code>greta</code> makes use of the graph-based organisation of modeling. Every node in our model graph is from a <code>greta</code> array node and thus connects variables, data and operations to create a directed acyclic graph (DAG) that defines the model when the <code>model()</code> function is called.</p>
</div>
<div id="tensorflow-probability" class="section level3">
<h3>TensorFlow Probability</h3>
<p>While <code>greta</code> makes it super easy to build similar models as with TensorFlow Probability, I also tried migrating the example code directly into R using the <code>reticulate</code> package. It’s still a work in progress but for everyone who might want to try as well (and achieve what I couldn’t up until now), here is how I started out.</p>
<p>TensorFlow Probability isn’t part of the core TensorFlow package, so we won’t have it loaded with <code>library(tensorflow)</code>. But we can use the <code>reticulate</code> package instead to import any Python module (aka library) into R and use it there. This way, we could use the original functions from the <code>tensorflow_probability</code> Python package in R.</p>
<p>We could, for example, work with the Edward2 functionalities from TensorFlow probabilities.</p>
<blockquote>
<p>Edward is a Python library for probabilistic modeling, inference, and criticism. It is a testbed for fast experimentation and research with probabilistic models, ranging from classical hierarchical models on small data sets to complex deep probabilistic models on large data sets. Edward fuses three fields: Bayesian statistics and machine learning, deep learning, and probabilistic programming. […] Edward is built on TensorFlow. It enables features such as computational graphs, distributed training, CPU/GPU integration, automatic differentiation, and visualization with TensorBoard. <a href="http://edwardlib.org/" class="uri">http://edwardlib.org/</a></p>
</blockquote>
<pre class="r"><code>library(reticulate)
tf &lt;- import(&quot;tensorflow&quot;)
tfp &lt;- import(&quot;tensorflow_probability&quot;)
ed &lt;- tfp$edward2</code></pre>
</div>
<div id="note-on-installing-a-working-version-of-tensorflow-probability-for-r" class="section level3">
<h3>Note on installing a working version of TensorFlow Probability for R</h3>
<p>As TensorFlow Probability isn’t part of the core TensorFlow package, we need to install the nightly bleeding edge version. However, we had a few problems installing a working version of TensorFlow Probability that had all the necessary submodules we wanted to use (like <code>edward2</code>). So, this is the version that worked in the end (as of today):</p>
<ul>
<li>TensorFlow Probability version 0.0.1.dev20180515</li>
<li>TensorFlow version 1.9.0.dev20180515</li>
</ul>
<p>For full disclosure: I worked from within the R virtualenv <strong>r-tensorflow</strong> that was created when I ran <code>install_tensorflow()</code> from within R. In this environment I installed:</p>
<pre><code>pip install tfp-nightly==0.0.1.dev20180515
pip install tf-nightly==1.9.0.dev20180515</code></pre>
<p>I used Python 3.6 on a Mac OS High Sierra version 10.13.4.</p>
</div>
</div>
<div id="thanks" class="section level2">
<h2>Thanks</h2>
<p>Huge thanks go out to my amazing <code>greta</code> team and to rOpenSci - particularly Stefanie Butland - for organizing such a wonderful event!</p>
<p>Thank you also to all <a href="http://unconf18.ropensci.org/#sponsors">sponsors</a>, who made it possible for me to fly all the way over to the Pacific Northwest and attend the unconf!</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="en" dir="ltr">
A sincere thank you to all participants in <a href="https://twitter.com/hashtag/runconf18?src=hash&amp;ref_src=twsrc%5Etfw">#runconf18</a> <br><br>This thread👇includes links to all project repos: <a href="https://t.co/2PhAz4zSuK">https://t.co/2PhAz4zSuK</a><a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> <a href="https://t.co/8SICcWkQ0v">pic.twitter.com/8SICcWkQ0v</a>
</p>
— rOpenSci (<span class="citation">@rOpenSci</span>) <a href="https://twitter.com/rOpenSci/status/1000024996876468224?ref_src=twsrc%5Etfw">May 25, 2018</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
<div id="session-information" class="section level2">
<h2>Session Information</h2>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.0 (2018-04-23)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.4
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] bindrcpp_0.2.2 greta_0.2.4   
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_0.12.17       lattice_0.20-35    tidyr_0.8.1       
##  [4] visNetwork_2.0.3   prettyunits_1.0.2  assertthat_0.2.0  
##  [7] rprojroot_1.3-2    digest_0.6.15      R6_2.2.2          
## [10] plyr_1.8.4         backports_1.1.2    evaluate_0.10.1   
## [13] coda_0.19-1        ggplot2_2.2.1      blogdown_0.6      
## [16] pillar_1.2.3       tfruns_1.3         rlang_0.2.1       
## [19] progress_1.1.2     lazyeval_0.2.1     rstudioapi_0.7    
## [22] whisker_0.3-2      Matrix_1.2-14      reticulate_1.7    
## [25] rmarkdown_1.9      DiagrammeR_1.0.0   downloader_0.4    
## [28] readr_1.1.1        stringr_1.3.1      htmlwidgets_1.2   
## [31] igraph_1.2.1       munsell_0.4.3      compiler_3.5.0    
## [34] influenceR_0.1.0   rgexf_0.15.3       xfun_0.1          
## [37] pkgconfig_2.0.1    base64enc_0.1-3    tensorflow_1.5    
## [40] htmltools_0.3.6    tidyselect_0.2.4   tibble_1.4.2      
## [43] gridExtra_2.3      bookdown_0.7       XML_3.98-1.11     
## [46] viridisLite_0.3.0  dplyr_0.7.5        grid_3.5.0        
## [49] jsonlite_1.5       gtable_0.2.0       magrittr_1.5      
## [52] scales_0.5.0       stringi_1.2.2      viridis_0.5.1     
## [55] brew_1.0-6         RColorBrewer_1.1-2 tools_3.5.0       
## [58] glue_1.2.0         purrr_0.2.5        hms_0.4.2         
## [61] Rook_1.1-1         parallel_3.5.0     yaml_2.1.19       
## [64] colorspace_1.3-2   knitr_1.20         bindr_0.1.1</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[July 5th &amp; 6th in Münster: Workshop on Deep Learning with Keras and TensorFlow in R]]></title>
    <link href="/2018/05/deep_learning_keras_tensorflow_18_07/"/>
    <id>/2018/05/deep_learning_keras_tensorflow_18_07/</id>
    <published>2018-05-22T00:00:00+00:00</published>
    <updated>2018-05-22T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Registration is now open for my 1.5-day <a href="https://www.eventbrite.de/e/workshop-deep-learning-mit-keras-und-tensorflow-tickets-42710095044?utm-medium=discovery&amp;utm-campaign=social&amp;utm-content=attendeeshare&amp;aff=escb&amp;utm-source=cp&amp;utm-term=listing">workshop on deep learning with Keras and TensorFlow using R</a>.</p>
<p>It will take place on <strong>July 5th &amp; 6th</strong> in <strong>Münster, Germany</strong>.</p>
<p><br></p>
<div class="figure">
<img src="https://blog.keras.io/img/keras-tensorflow-logo.jpg" />

</div>
<p><br></p>
<p><a href="https://blog.codecentric.de/en/2018/02/deep-learning-workshop-bei-der-codecentric-ag-solingen/">You can read about one participant’s experience in my last workshop:</a></p>
<blockquote>
<p>Big Data – a buzz word you can find everywhere these days, from nerdy blogs to scientific research papers and even in the news. But how does Big Data Analysis work, exactly? In order to find that out, I attended the workshop on “Deep Learning with Keras and TensorFlow”. On a stormy Thursday afternoon, we arrived at the modern and light-flooded codecentric AG headquarters. There, we met performance expert Dieter Dirkes and Data Scientist Dr. Shirin Glander. In the following two days, Shirin gave us a hands-on introduction into the secrets of Deep Learning and helped us to program our first Neural Net. After a short round of introduction of the participants, it became clear that many different areas and domains are interested in Deep Learning: geologists want to classify (satellite) images, energy providers want to analyse time-series, insurers want to predict numbers and I – a humanities major – want to classify text. And codecentric employees were also interested in getting to know the possibilities of Deep Learning, so that a third of the participants were employees from the company itself.</p>
</blockquote>
<p><a href="https://blog.codecentric.de/en/2018/02/deep-learning-workshop-bei-der-codecentric-ag-solingen/">Continue reading…</a></p>
<p>In my workshop, you will learn</p>
<ul>
<li>the basics of deep learning</li>
<li>what cross-entropy and loss is</li>
<li>about activation functions</li>
<li>how to optimize weights and biases with backpropagation and gradient descent</li>
<li>how to build (deep) neural networks with Keras and TensorFlow</li>
<li>how to save and load models and model weights</li>
<li>how to visualize models with TensorBoard</li>
<li>how to make predictions on test data</li>
</ul>
<p>The workshop will be held in German but my slides and all material is in English. :-)</p>
<p><a href="https://www.eventbrite.de/e/workshop-deep-learning-mit-keras-und-tensorflow-tickets-46223151691">Tickets can be booked via eventbrite</a>.</p>
<p><br></p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/keras_workshop_april18.png" />

</div>
<p>Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. Keras is very convenient for fast and easy prototyping of neural networks. It is highly modular and very flexible, so that you can build basically any type of neural network you want. It supports convolutional neural networks and recurrent neural networks, as well as combinations of both. Due to its layer structure, it is highly extensible and can run on CPU or GPU.</p>
<p>The <code>keras</code> R package provides an interface to the Python library of Keras, just as the tensorflow package provides an interface to TensorFlow. Basically, R creates a conda instance and runs Keras it it, while you can still use all the functionalities of R for plotting, etc. Almost all function names are the same, so models can easily be recreated in Python for deployment.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI: Adversarial Attacks Against Reinforcement Learning Agents with Ian Goodfellow &amp; Sandy Huang]]></title>
    <link href="/2018/05/twimlai_adversarial_attacks/"/>
    <id>/2018/05/twimlai_adversarial_attacks/</id>
    <published>2018-05-14T00:00:00+00:00</published>
    <updated>2018-05-14T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Adversarial Attacks Against Reinforcement Learning Agents with Ian Goodfellow &amp; Sandy Huang</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai_adversarial_attacks.jpg" alt="Sketchnotes from TWiMLAI talk: Adversarial Attacks Against Reinforcement Learning Agents with Ian Goodfellow &amp; Sandy Huang" />
<p class="caption">Sketchnotes from TWiMLAI talk: Adversarial Attacks Against Reinforcement Learning Agents with Ian Goodfellow &amp; Sandy Huang</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-119-adversarial-attacks-reinforcement-learning-agents-ian-goodfellow-sandy-huang/">here</a>.</p>
<blockquote>
<p>In this episode, I’m joined by Ian Goodfellow, Staff Research Scientist at Google Brain and Sandy Huang, Phd Student in the EECS department at UC Berkeley, to discuss their work on the paper Adversarial Attacks on Neural Network Policies. If you’re a regular listener here you’ve probably heard of adversarial attacks, and have seen examples of deep learning based object detectors that can be fooled into thinking that, for example, a giraffe is actually a school bus, by injecting some imperceptible noise into the image. Well, Sandy and Ian’s paper sits at the intersection of adversarial attacks and reinforcement learning, another area we’ve discussed quite a bit on the podcast. In their paper, they describe how adversarial attacks can also be effective at targeting neural network policies in reinforcement learning. Sandy gives us an overview of the paper, including how changing a single pixel value can throw off performance of a model trained to play Atari games. We also cover a lot of interesting topics relating to adversarial attacks and RL individually, and some related areas such as hierarchical reward functions and transfer learning. This was a great conversation that I’m really excited to bring to you! <a href="https://twimlai.com/twiml-talk-119-adversarial-attacks-reinforcement-learning-agents-ian-goodfellow-sandy-huang/" class="uri">https://twimlai.com/twiml-talk-119-adversarial-attacks-reinforcement-learning-agents-ian-goodfellow-sandy-huang/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Comparing dependencies of popular machine learning packages with `pkgnet`]]></title>
    <link href="/2018/04/pkgnet/"/>
    <id>/2018/04/pkgnet/</id>
    <published>2018-04-30T00:00:00+00:00</published>
    <updated>2018-04-30T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>When looking through the CRAN list of packages, I stumbled upon <a href="https://cran.r-project.org/web/packages/pkgnet/vignettes/pkgnet-intro.html">this little gem</a>:</p>
<blockquote>
<p>pkgnet is an R library designed for the analysis of R libraries! The goal of the package is to build a graph representation of a package and its dependencies.</p>
</blockquote>
<p>And I thought it would be fun to play around with it. The little analysis I ended up doing was to compare dependencies of popular machine learning packages.</p>
<div id="update-an-alternative-package-to-use-would-be-cranly." class="section level2">
<h2><strong>Update:</strong> An alternative package to use would be <a href="https://cran.r-project.org/web/packages/cranly/vignettes/cranly.html"><code>cranly</code></a>.</h2>
<ul>
<li>I first loaded the packages:</li>
</ul>
<pre class="r"><code>library(pkgnet)
library(tidygraph)</code></pre>
<pre><code>## 
## Attache Paket: &#39;tidygraph&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     filter</code></pre>
<pre class="r"><code>library(ggraph)</code></pre>
<pre><code>## Lade nötiges Paket: ggplot2</code></pre>
<ul>
<li>I then created a function that will</li>
</ul>
<ol style="list-style-type: decimal">
<li>create the package report with <code>pkgnet::CreatePackageReport</code></li>
<li>convert the edge (<code>report$DependencyReporter$edges</code>) and node (<code>report$DependencyReporter$nodes</code>) data into a graph object with <code>tidygraph::as_tbl_graph</code></li>
</ol>
<pre class="r"><code>create_pkg_graph &lt;- function(package_name, DependencyReporter = TRUE) {
  
  report &lt;- CreatePackageReport(pkg_name = package_name)
  
  if (DependencyReporter) {
    graph &lt;- as_tbl_graph(report$DependencyReporter$edges,
                      directed = TRUE,
                      nodes = as.data.frame(report$DependencyReporter$nodes))
  } else {
    graph &lt;- as_tbl_graph(report$FunctionReporter$edges,
                      directed = TRUE,
                      nodes = as.data.frame(report$FunctionReporter$nodes))
  }
  
  return(graph)
}</code></pre>
<ul>
<li><p>To create a vector of machine learning packages from R I looked at <a href="https://cran.r-project.org/web/views/MachineLearning.html">CRAN’s machine learning task view</a></p></li>
<li><p>These are the packages I ended up including:</p></li>
</ul>
<pre class="r"><code>pkg_list &lt;- c(&quot;caret&quot;, &quot;h2o&quot;, &quot;e1071&quot;, &quot;mlr&quot;)</code></pre>
<p><em>Note</em>: I wanted to include other packages, like <code>tensorflow</code>, <code>randomFores</code>, <code>gbm</code>, etc. but for those, <code>pkgnet</code> threw an error:</p>
<blockquote>
<p>Error in data.table::data.table(node = names(igraph::V(self$pkg_graph)), : column or argument 1 is NULL</p>
</blockquote>
<ul>
<li>Next, I ran them through my function from before and assigned them each a unique name.</li>
</ul>
<pre class="r"><code>for (pkg in pkg_list) {
  graph &lt;- create_pkg_graph(pkg)
  assign(paste0(&quot;graph_&quot;, pkg), graph)
}</code></pre>
<ul>
<li>These individual objects I combined with <a href="https://cran.r-project.org/web/packages/tidygraph/index.html"><code>tidygraph</code></a> and calculated node centrality as the number of outgoing edges.</li>
</ul>
<pre class="r"><code>graph &lt;- graph_caret %&gt;% 
  graph_join(graph_h2o, by = &quot;name&quot;) %&gt;%
  graph_join(graph_e1071, by = &quot;name&quot;) %&gt;%
  graph_join(graph_mlr, by = &quot;name&quot;) %&gt;%
  mutate(color = ifelse(name %in% pkg_list, &quot;a&quot;, &quot;b&quot;),
         centrality = centrality_degree(mode = &quot;out&quot;))</code></pre>
<ul>
<li>Finally, I plotted the dependency network with <a href="https://github.com/thomasp85/ggraph"><code>ggraph</code></a>:</li>
</ul>
<p>The bigger the node labels (package names), the higher their centrality. Seems like the more basic utilitarian packages have the highest centrality (not really a surprise…).</p>
<pre class="r"><code>graph %&gt;%
  ggraph(layout = &#39;nicely&#39;) + 
    geom_edge_link(arrow = arrow()) + 
    geom_node_point() +
    geom_node_label(aes(label = name, fill = color, size = centrality), show.legend = FALSE, repel = TRUE) +
    theme_graph() +
    scale_fill_brewer(palette = &quot;Set1&quot;)</code></pre>
<p><img src="/post/2018-04-30_pkgnet_files/figure-html/graph-1.png" width="960" /></p>
<ul>
<li>Because the complete network is a bit hard to make sense of, I plotted it again with only the packages I wanted to analyze plus dependencies that had at least 1 outgoing edge; now it is easier to see shared dependencies.</li>
</ul>
<p>For example, <code>methods</code> and <code>stats</code> are dependencies of <code>caret</code>, <code>mlr</code> and <code>e1071</code> but not <code>h2o</code>, while <code>utils</code> is a dependency of all four.</p>
<pre class="r"><code>graph %&gt;%
  filter(centrality &gt; 1 | color == &quot;a&quot;) %&gt;%
  ggraph(layout = &#39;nicely&#39;) + 
    geom_edge_link(arrow = arrow()) + 
    geom_node_point() +
    geom_node_label(aes(label = name, fill = color, size = centrality), show.legend = FALSE, repel = TRUE) +
    theme_graph() +
    scale_fill_brewer(palette = &quot;Set1&quot;)</code></pre>
<p><img src="/post/2018-04-30_pkgnet_files/figure-html/graph_subset-1.png" width="576" /></p>
<p>It would of course be interesting to analyse a bigger network with more packages. Maybe someone knows how to get these other packages to work with <code>pkgnet</code>?</p>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.0 (2018-04-23)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.4
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] bindrcpp_0.2.2  ggraph_1.0.1    ggplot2_2.2.1   tidygraph_1.1.0
## [5] pkgnet_0.2.0   
## 
## loaded via a namespace (and not attached):
##  [1] Rcpp_0.12.16         RColorBrewer_1.1-2   plyr_1.8.4          
##  [4] compiler_3.5.0       pillar_1.2.2         formatR_1.5         
##  [7] futile.logger_1.4.3  bindr_0.1.1          viridis_0.5.1       
## [10] futile.options_1.0.1 tools_3.5.0          digest_0.6.15       
## [13] viridisLite_0.3.0    gtable_0.2.0         jsonlite_1.5        
## [16] evaluate_0.10.1      tibble_1.4.2         pkgconfig_2.0.1     
## [19] rlang_0.2.0          igraph_1.2.1         ggrepel_0.7.0       
## [22] yaml_2.1.18          blogdown_0.6         xfun_0.1            
## [25] gridExtra_2.3        stringr_1.3.0        dplyr_0.7.4         
## [28] knitr_1.20           htmlwidgets_1.2      grid_3.5.0          
## [31] rprojroot_1.3-2      glue_1.2.0           data.table_1.10.4-3 
## [34] R6_2.2.2             rmarkdown_1.9        bookdown_0.7        
## [37] udunits2_0.13        tweenr_0.1.5         tidyr_0.8.0         
## [40] purrr_0.2.4          lambda.r_1.2.2       magrittr_1.5        
## [43] units_0.5-1          MASS_7.3-49          scales_0.5.0        
## [46] backports_1.1.2      mvbutils_2.7.4.1     htmltools_0.3.6     
## [49] assertthat_0.2.0     ggforce_0.1.1        colorspace_1.3-2    
## [52] labeling_0.3         stringi_1.1.7        visNetwork_2.0.3    
## [55] lazyeval_0.2.1       munsell_0.4.3</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Slides from my JAX 2018 talk: Deep Learning - a Primer]]></title>
    <link href="/2018/04/jax2018_slides/"/>
    <id>/2018/04/jax2018_slides/</id>
    <published>2018-04-27T00:00:00+00:00</published>
    <updated>2018-04-27T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Here I am sharing the slides for a talk that my colleague Uwe Friedrichsen and I gave about <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/">Deep Learning - a Primer</a> at the JAX conference on Tuesday, April 24th 2018 in Mainz, Germany.</p>
<p>Slides can be found here: <a href="https://www.slideshare.net/ShirinGlander/deep-learning-a-primer-95197733" class="uri">https://www.slideshare.net/ShirinGlander/deep-learning-a-primer-95197733</a></p>
<blockquote>
<p>Deep Learning is one of the “hot” topics in the AI area – a lot of hype, a lot of inflated expectation, but also quite some impressive success stories. As some AI experts already predict that Deep Learning will become “Software 2.0”, it might be a good time to have a closer look at the topic. In this session I will try to give a comprehensive overview of Deep Learning. We will start with a bit of history and some theoretical foundations that we will use to create a little Deep Learning taxonomy. Then we will have a look at current and upcoming application areas: Where can we apply Deep Learning successfully and what does it differentiate from other approaches? Afterwards we will examine the ecosystem: Which tools and libraries are available? What are their strengths and weaknesses? And to complete the session, we will look into some practical code examples and the typical pitfalls of Deep Learning. After this session you will have a much better idea of the why, what and how of Deep Learning, including if and how you might want to apply it to your own work. <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/" class="uri">https://jax.de/big-data-machine-learning/deep-learning-a-primer/</a></p>
</blockquote>
<div class="figure">
<img src="https://pbs.twimg.com/media/DUt3SXyUQAE3TOv.jpg" alt="https://twitter.com/jaxcon/status/957990506331557890" />
<p class="caption"><a href="https://twitter.com/jaxcon/status/957990506331557890" class="uri">https://twitter.com/jaxcon/status/957990506331557890</a></p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #121: Reproducibility and the Philosophy of Data with Clare Gollnick]]></title>
    <link href="/2018/04/twimlai121/"/>
    <id>/2018/04/twimlai121/</id>
    <published>2018-04-22T00:00:00+00:00</published>
    <updated>2018-04-22T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Reproducibility and the Philosophy of Data with Clare Gollnick</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai121.jpg" alt="Sketchnotes from TWiMLAI talk #121: Reproducibility and the Philosophy of Data with Clare Gollnick" />
<p class="caption">Sketchnotes from TWiMLAI talk #121: Reproducibility and the Philosophy of Data with Clare Gollnick</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-121-reproducibility-philosophy-data-clare-gollnick/">here</a>.</p>
<blockquote>
<p>In this episode, i’m joined by Clare Gollnick, CTO of Terbium Labs, to discuss her thoughts on the “reproducibility crisis” currently haunting the scientific landscape. For a little background, a “Nature” survey in 2016 showed that more than 70% of researchers have tried and failed to reproduce another scientist’s experiments, and more than half have failed to reproduce their own experiments. Clare gives us her take on the situation, and how it applies to data science, along with some great nuggets about the philosophy of data and a few interesting use cases as well. We also cover her thoughts on Bayesian vs Frequentist techniques and while we’re at it, the Vim vs Emacs debate. No, actually I’m just kidding on that last one. But this was indeed a very fun conversation that I think you’ll enjoy! <a href="https://twimlai.com/twiml-talk-121-reproducibility-philosophy-data-clare-gollnick/" class="uri">https://twimlai.com/twiml-talk-121-reproducibility-philosophy-data-clare-gollnick/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Update: Can we predict flu outcome with Machine Learning in R?]]></title>
    <link href="/2018/04/flu_prediction/"/>
    <id>/2018/04/flu_prediction/</id>
    <published>2018-04-22T00:00:00+00:00</published>
    <updated>2018-04-22T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Since I migrated my blog from <a href="shiring.github.io">Github Pages</a> to <a href="www.shirin-glander.de">blogdown and Netlify</a>, I wanted to start migrating (most of) my old posts too - and use that opportunity to update them and make sure the code still works.</p>
<p>Here I am updating my very first machine learning post from 27 Nov 2016: <a href="https://shiring.github.io/machine_learning/2016/11/27/flu_outcome_ML_post">Can we predict flu deaths with Machine Learning and R?</a>. Changes are marked as <strong>bold</strong> comments.</p>
<p>The main changes I made are:</p>
<ul>
<li>using the <code>tidyverse</code> more consistently throughout the analysis</li>
<li>focusing on comparing multiple imputations from the <code>mice</code> package, rather than comparing different algorithms</li>
<li>using <code>purrr</code>, <code>map()</code>, <code>nest()</code> and <code>unnest()</code> to model and predict the machine learning algorithm over the different imputed datasets</li>
</ul>
<hr />
<p>Among the many nice R packages containing data collections is the <a href="https://mran.microsoft.com/web/packages/outbreaks/outbreaks.pdf">outbreaks</a> package. It contains a dataset on epidemics and among them is data from the 2013 outbreak of <a href="http://www.who.int/influenza/human_animal_interface/faq_H7N9/en/">influenza A H7N9</a> in <a href="http://www.who.int/influenza/human_animal_interface/influenza_h7n9/ChinaH7N9JointMissionReport2013u.pdf?ua=1">China</a> as analysed by Kucharski et al. (2014):</p>
<blockquote>
<p>A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. PLOS Currents Outbreaks. Mar 7, edition 1. doi: 10.1371/currents.outbreaks.e1473d9bfc99d080ca242139a06c455f.</p>
</blockquote>
<blockquote>
<p>A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Data from: Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. Dryad Digital Repository. <a href="http://dx.doi.org/10.5061/dryad.2g43n" class="uri">http://dx.doi.org/10.5061/dryad.2g43n</a>.</p>
</blockquote>
<p>I will be using their data as an example to show how to use Machine Learning algorithms for predicting disease outcome.</p>
<p><br></p>
<pre class="r"><code>library(outbreaks)
library(tidyverse)
library(plyr)
library(mice)
library(caret)
library(purrr)</code></pre>
<div id="the-data" class="section level1">
<h1>The data</h1>
<p>The dataset contains case ID, date of onset, date of hospitalization, date of outcome, gender, age, province and of course outcome: Death or Recovery.</p>
<div id="pre-processing" class="section level2">
<h2>Pre-processing</h2>
<p><strong>Change: variable names (i.e. column names) have been renamed, dots have been replaced with underscores, letters are all lower case now.</strong></p>
<p><strong>Change: I am using the tidyverse notation more consistently.</strong></p>
<p>First, I’m doing some preprocessing, including:</p>
<ul>
<li>renaming missing data as NA</li>
<li>adding an ID column</li>
<li>setting column types</li>
<li>gathering date columns</li>
<li>changing factor names of dates (to make them look nicer in plots) and of <code>province</code> (to combine provinces with few cases)</li>
</ul>
<pre class="r"><code>fluH7N9_china_2013$age[which(fluH7N9_china_2013$age == &quot;?&quot;)] &lt;- NA
fluH7N9_china_2013_gather &lt;- fluH7N9_china_2013 %&gt;%
  mutate(case_id = paste(&quot;case&quot;, case_id, sep = &quot;_&quot;),
         age = as.numeric(age)) %&gt;%
  gather(Group, Date, date_of_onset:date_of_outcome) %&gt;%
  mutate(Group = as.factor(mapvalues(Group, from = c(&quot;date_of_onset&quot;, &quot;date_of_hospitalisation&quot;, &quot;date_of_outcome&quot;), 
          to = c(&quot;date of onset&quot;, &quot;date of hospitalisation&quot;, &quot;date of outcome&quot;))),
         province = mapvalues(province, from = c(&quot;Anhui&quot;, &quot;Beijing&quot;, &quot;Fujian&quot;, &quot;Guangdong&quot;, &quot;Hebei&quot;, &quot;Henan&quot;, &quot;Hunan&quot;, &quot;Jiangxi&quot;, &quot;Shandong&quot;, &quot;Taiwan&quot;), to = rep(&quot;Other&quot;, 10)))</code></pre>
<p>I’m also</p>
<ul>
<li>adding a third gender level for unknown gender</li>
</ul>
<pre class="r"><code>levels(fluH7N9_china_2013_gather$gender) &lt;- c(levels(fluH7N9_china_2013_gather$gender), &quot;unknown&quot;)
fluH7N9_china_2013_gather$gender[is.na(fluH7N9_china_2013_gather$gender)] &lt;- &quot;unknown&quot;
head(fluH7N9_china_2013_gather)</code></pre>
<pre><code>##   case_id outcome gender age province         Group       Date
## 1  case_1   Death      m  58 Shanghai date of onset 2013-02-19
## 2  case_2   Death      m   7 Shanghai date of onset 2013-02-27
## 3  case_3   Death      f  11    Other date of onset 2013-03-09
## 4  case_4    &lt;NA&gt;      f  18  Jiangsu date of onset 2013-03-19
## 5  case_5 Recover      f  20  Jiangsu date of onset 2013-03-19
## 6  case_6   Death      f   9  Jiangsu date of onset 2013-03-21</code></pre>
<p>For plotting, I am defining a custom <code>ggplot2</code> theme:</p>
<pre class="r"><code>my_theme &lt;- function(base_size = 12, base_family = &quot;sans&quot;){
  theme_minimal(base_size = base_size, base_family = base_family) +
  theme(
    axis.text = element_text(size = 12),
    axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5),
    axis.title = element_text(size = 14),
    panel.grid.major = element_line(color = &quot;grey&quot;),
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = &quot;aliceblue&quot;),
    strip.background = element_rect(fill = &quot;lightgrey&quot;, color = &quot;grey&quot;, size = 1),
    strip.text = element_text(face = &quot;bold&quot;, size = 12, color = &quot;black&quot;),
    legend.position = &quot;bottom&quot;,
    legend.justification = &quot;top&quot;, 
    legend.box = &quot;horizontal&quot;,
    legend.box.background = element_rect(colour = &quot;grey50&quot;),
    legend.background = element_blank(),
    panel.border = element_rect(color = &quot;grey&quot;, fill = NA, size = 0.5)
  )
}</code></pre>
<p>And use that theme to visualize the data:</p>
<pre class="r"><code>ggplot(data = fluH7N9_china_2013_gather, aes(x = Date, y = age, fill = outcome)) +
  stat_density2d(aes(alpha = ..level..), geom = &quot;polygon&quot;) +
  geom_jitter(aes(color = outcome, shape = gender), size = 1.5) +
  geom_rug(aes(color = outcome)) +
  scale_y_continuous(limits = c(0, 90)) +
  labs(
    fill = &quot;Outcome&quot;,
    color = &quot;Outcome&quot;,
    alpha = &quot;Level&quot;,
    shape = &quot;Gender&quot;,
    x = &quot;Date in 2013&quot;,
    y = &quot;Age&quot;,
    title = &quot;2013 Influenza A H7N9 cases in China&quot;,
    subtitle = &quot;Dataset from &#39;outbreaks&#39; package (Kucharski et al. 2014)&quot;,
    caption = &quot;&quot;
  ) +
  facet_grid(Group ~ province) +
  my_theme() +
  scale_shape_manual(values = c(15, 16, 17)) +
  scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) +
  scale_fill_brewer(palette=&quot;Set1&quot;)</code></pre>
<p><img src="/post/2018-04-22_flu_outcome_ML_post_files/figure-html/main-1.png" width="1152" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(data = fluH7N9_china_2013_gather, aes(x = Date, y = age, color = outcome)) +
  geom_point(aes(color = outcome, shape = gender), size = 1.5, alpha = 0.6) +
  geom_path(aes(group = case_id)) +
  facet_wrap( ~ province, ncol = 2) +
  my_theme() +
  scale_shape_manual(values = c(15, 16, 17)) +
  scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) +
  scale_fill_brewer(palette=&quot;Set1&quot;) +
  labs(
    color = &quot;Outcome&quot;,
    shape = &quot;Gender&quot;,
    x = &quot;Date in 2013&quot;,
    y = &quot;Age&quot;,
    title = &quot;2013 Influenza A H7N9 cases in China&quot;,
    subtitle = &quot;Dataset from &#39;outbreaks&#39; package (Kucharski et al. 2014)&quot;,
    caption = &quot;\nTime from onset of flu to outcome.&quot;
  )</code></pre>
<p><img src="/post/2018-04-22_flu_outcome_ML_post_files/figure-html/unnamed-chunk-5-1.png" width="768" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
</div>
<div id="features" class="section level1">
<h1>Features</h1>
<p>In machine learning-speak features are what we call the variables used for model training. Using the right features dramatically influences the accuracy and success of your model.</p>
<p>For this example, I am keeping age, but I am also generating new features from the date information and converting gender and province into numerical values.</p>
<pre class="r"><code>dataset &lt;- fluH7N9_china_2013 %&gt;%
  mutate(hospital = as.factor(ifelse(is.na(date_of_hospitalisation), 0, 1)),
         gender_f = as.factor(ifelse(gender == &quot;f&quot;, 1, 0)),
         province_Jiangsu = as.factor(ifelse(province == &quot;Jiangsu&quot;, 1, 0)),
         province_Shanghai = as.factor(ifelse(province == &quot;Shanghai&quot;, 1, 0)),
         province_Zhejiang = as.factor(ifelse(province == &quot;Zhejiang&quot;, 1, 0)),
         province_other = as.factor(ifelse(province == &quot;Zhejiang&quot; | province == &quot;Jiangsu&quot; | province == &quot;Shanghai&quot;, 0, 1)),
         days_onset_to_outcome = as.numeric(as.character(gsub(&quot; days&quot;, &quot;&quot;,
                                      as.Date(as.character(date_of_outcome), format = &quot;%Y-%m-%d&quot;) - 
                                        as.Date(as.character(date_of_onset), format = &quot;%Y-%m-%d&quot;)))),
         days_onset_to_hospital = as.numeric(as.character(gsub(&quot; days&quot;, &quot;&quot;,
                                      as.Date(as.character(date_of_hospitalisation), format = &quot;%Y-%m-%d&quot;) - 
                                        as.Date(as.character(date_of_onset), format = &quot;%Y-%m-%d&quot;)))),
         age = age,
         early_onset = as.factor(ifelse(date_of_onset &lt; summary(fluH7N9_china_2013$date_of_onset)[[3]], 1, 0)),
         early_outcome = as.factor(ifelse(date_of_outcome &lt; summary(fluH7N9_china_2013$date_of_outcome)[[3]], 1, 0))) %&gt;%
  subset(select = -c(2:4, 6, 8))
rownames(dataset) &lt;- dataset$case_id
dataset[, -2] &lt;- as.numeric(as.matrix(dataset[, -2]))
head(dataset)</code></pre>
<pre><code>##   case_id outcome age hospital gender_f province_Jiangsu province_Shanghai
## 1       1   Death  87        0        0                0                 1
## 2       2   Death  27        1        0                0                 1
## 3       3   Death  35        1        1                0                 0
## 4       4    &lt;NA&gt;  45        1        1                1                 0
## 5       5 Recover  48        1        1                1                 0
## 6       6   Death  32        1        1                1                 0
##   province_Zhejiang province_other days_onset_to_outcome
## 1                 0              0                    13
## 2                 0              0                    11
## 3                 0              1                    31
## 4                 0              0                    NA
## 5                 0              0                    57
## 6                 0              0                    36
##   days_onset_to_hospital early_onset early_outcome
## 1                     NA           1             1
## 2                      4           1             1
## 3                     10           1             1
## 4                      8           1            NA
## 5                     11           1             0
## 6                      7           1             1</code></pre>
<pre class="r"><code>summary(dataset$outcome)</code></pre>
<pre><code>##   Death Recover    NA&#39;s 
##      32      47      57</code></pre>
<p><br></p>
<div id="imputing-missing-values" class="section level2">
<h2>Imputing missing values</h2>
<p>I am using the <a href="https://gerkovink.github.io/miceVignettes/Ad_hoc_and_mice/Ad_hoc_methods.html"><code>mice</code> package for imputing missing values</a></p>
<p><strong>Note: Since publishing this blogpost I learned that the idea behind using <code>mice</code> is to compare different imputations to see how stable they are, instead of picking one imputed set as fixed for the remainder of the analysis. Therefore, I changed the focus of this post a little bit: in the old post I compared many different algorithms and their outcome; in this updated version I am only showing the Random Forest algorithm and focus on comparing the different imputed datasets. I am ignoring feature importance and feature plots because nothing changed compared to the old post.</strong></p>
<p><br></p>
<ul>
<li><code>md.pattern()</code> shows the pattern of missingness in the data:</li>
</ul>
<pre class="r"><code>md.pattern(dataset)</code></pre>
<pre><code>##    case_id hospital province_Jiangsu province_Shanghai province_Zhejiang
## 42       1        1                1                 1                 1
## 27       1        1                1                 1                 1
##  2       1        1                1                 1                 1
##  2       1        1                1                 1                 1
## 18       1        1                1                 1                 1
##  1       1        1                1                 1                 1
## 36       1        1                1                 1                 1
##  3       1        1                1                 1                 1
##  3       1        1                1                 1                 1
##  2       1        1                1                 1                 1
##          0        0                0                 0                 0
##    province_other age gender_f early_onset outcome early_outcome
## 42              1   1        1           1       1             1
## 27              1   1        1           1       1             1
##  2              1   1        1           1       1             0
##  2              1   1        1           0       1             1
## 18              1   1        1           1       0             0
##  1              1   1        1           1       1             0
## 36              1   1        1           1       0             0
##  3              1   1        1           0       1             0
##  3              1   1        1           0       0             0
##  2              1   0        0           0       1             0
##                 0   2        2          10      57            65
##    days_onset_to_outcome days_onset_to_hospital    
## 42                     1                      1   0
## 27                     1                      0   1
##  2                     0                      1   2
##  2                     0                      0   3
## 18                     0                      1   3
##  1                     0                      0   3
## 36                     0                      0   4
##  3                     0                      0   4
##  3                     0                      0   5
##  2                     0                      0   6
##                       67                     74 277</code></pre>
<ul>
<li><code>mice()</code> generates the imputations</li>
</ul>
<pre class="r"><code>dataset_impute &lt;- mice(data = dataset[, -2],  print = FALSE)</code></pre>
<ul>
<li>by default, <code>mice()</code> calculates five (m = 5) imputed data sets</li>
<li>we can combine them all in one output with the <code>complete(&quot;long&quot;)</code> function</li>
<li>I did not want to impute missing values in the <code>outcome</code> column, so I have to merge it back in with the imputed data</li>
</ul>
<pre class="r"><code>datasets_complete &lt;- right_join(dataset[, c(1, 2)], 
                           complete(dataset_impute, &quot;long&quot;),
                           by = &quot;case_id&quot;) %&gt;%
  select(-.id)</code></pre>
<pre class="r"><code>head(datasets_complete)</code></pre>
<pre><code>##   case_id outcome .imp age hospital gender_f province_Jiangsu
## 1       1   Death    1  87        0        0                0
## 2       2   Death    1  27        1        0                0
## 3       3   Death    1  35        1        1                0
## 4       4    &lt;NA&gt;    1  45        1        1                1
## 5       5 Recover    1  48        1        1                1
## 6       6   Death    1  32        1        1                1
##   province_Shanghai province_Zhejiang province_other days_onset_to_outcome
## 1                 1                 0              0                    13
## 2                 1                 0              0                    11
## 3                 0                 0              1                    31
## 4                 0                 0              0                    28
## 5                 0                 0              0                    57
## 6                 0                 0              0                    36
##   days_onset_to_hospital early_onset early_outcome
## 1                      5           1             1
## 2                      4           1             1
## 3                     10           1             1
## 4                      8           1             1
## 5                     11           1             0
## 6                      7           1             1</code></pre>
<p>Let’s compare the distributions of the five different imputed datasets:</p>
<pre class="r"><code>datasets_complete %&gt;%
  gather(x, y, age:early_outcome) %&gt;%
  ggplot(aes(x = y, fill = .imp, color = .imp)) +
    facet_wrap(~ x, ncol = 3, scales = &quot;free&quot;) +
    geom_density(alpha = 0.4) +
    scale_fill_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) +
    scale_color_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) +
    my_theme()</code></pre>
<p><img src="/post/2018-04-22_flu_outcome_ML_post_files/figure-html/unnamed-chunk-12-1.png" width="768" style="display: block; margin: auto;" /></p>
<p><br></p>
</div>
</div>
<div id="test-train-and-validation-data-sets" class="section level1">
<h1>Test, train and validation data sets</h1>
<p>Now, we can go ahead with machine learning!</p>
<p>The dataset contains a few missing values in the <code>outcome</code> column; those will be the test set used for final predictions (see the old blog post for this).</p>
<pre class="r"><code>train_index &lt;- which(is.na(datasets_complete$outcome))
train_data &lt;- datasets_complete[-train_index, ]
test_data  &lt;- datasets_complete[train_index, -2]</code></pre>
<p>The remainder of the data will be used for modeling. Here, I am splitting the data into 70% training and 30% test data.</p>
<p>Because I want to model each imputed dataset separately, I am using the <code>nest()</code> and <code>map()</code> functions.</p>
<pre class="r"><code>set.seed(42)
val_data &lt;- train_data %&gt;%
  group_by(.imp) %&gt;%
  nest() %&gt;%
  mutate(val_index = map(data, ~ createDataPartition(.$outcome, p = 0.7, list = FALSE)),
         val_train_data = map2(data, val_index, ~ .x[.y, ]),
         val_test_data = map2(data, val_index, ~ .x[-.y, ]))</code></pre>
<p><br></p>
</div>
<div id="machine-learning-algorithms" class="section level1">
<h1>Machine Learning algorithms</h1>
<div id="random-forest" class="section level2">
<h2>Random Forest</h2>
<p>To make the code tidier, I am first defining the modeling function with the parameters I want.</p>
<pre class="r"><code>model_function &lt;- function(df) {
  caret::train(outcome ~ .,
               data = df,
               method = &quot;rf&quot;,
               preProcess = c(&quot;scale&quot;, &quot;center&quot;),
               trControl = trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 3, verboseIter = FALSE))
}</code></pre>
<p>Next, I am using the nested tibble from before to <code>map()</code> the model function, predict the outcome and calculate confusion matrices.</p>
<pre class="r"><code>set.seed(42)
val_data_model &lt;- val_data %&gt;%
  mutate(model = map(val_train_data, ~ model_function(.x)),
         predict = map2(model, val_test_data, ~ data.frame(prediction = predict(.x, .y[, -2]))),
         predict_prob = map2(model, val_test_data, ~ data.frame(outcome = .y[, 2],
                                                                prediction = predict(.x, .y[, -2], type = &quot;prob&quot;))),
         confusion_matrix = map2(val_test_data, predict, ~ confusionMatrix(.x$outcome, .y$prediction)),
         confusion_matrix_tbl = map(confusion_matrix, ~ as.tibble(.x$table)))</code></pre>
<p><br></p>
</div>
<div id="comparing-accuracy-of-models" class="section level2">
<h2>Comparing accuracy of models</h2>
<p>To compare how the different imputations did, I am plotting</p>
<ul>
<li>the confusion matrices:</li>
</ul>
<pre class="r"><code>val_data_model %&gt;%
  unnest(confusion_matrix_tbl) %&gt;%
  ggplot(aes(x = Prediction, y = Reference, fill = n)) +
    facet_wrap(~ .imp, ncol = 5, scales = &quot;free&quot;) +
    geom_tile() +
    my_theme()</code></pre>
<p><img src="/post/2018-04-22_flu_outcome_ML_post_files/figure-html/unnamed-chunk-17-1.png" width="1440" style="display: block; margin: auto;" /></p>
<ul>
<li>and the prediction probabilities for correct and wrong predictions:</li>
</ul>
<pre class="r"><code>val_data_model %&gt;%
  unnest(predict_prob) %&gt;%
  gather(x, y, prediction.Death:prediction.Recover) %&gt;%
  ggplot(aes(x = x, y = y, fill = outcome)) +
    facet_wrap(~ .imp, ncol = 5, scales = &quot;free&quot;) +
    geom_boxplot() +
    scale_fill_brewer(palette=&quot;Set1&quot;, na.value = &quot;grey50&quot;) +
    my_theme()</code></pre>
<p><img src="/post/2018-04-22_flu_outcome_ML_post_files/figure-html/unnamed-chunk-18-1.png" width="1440" style="display: block; margin: auto;" /></p>
<p>Hope, you found that example interesting and helpful!</p>
<p><br></p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.0 (2018-04-23)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.4
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] bindrcpp_0.2.2  caret_6.0-79    mice_2.46.0     lattice_0.20-35
##  [5] plyr_1.8.4      forcats_0.3.0   stringr_1.3.1   dplyr_0.7.4    
##  [9] purrr_0.2.4     readr_1.1.1     tidyr_0.8.0     tibble_1.4.2   
## [13] ggplot2_2.2.1   tidyverse_1.2.1 outbreaks_1.3.0
## 
## loaded via a namespace (and not attached):
##  [1] nlme_3.1-137        lubridate_1.7.4     dimRed_0.1.0       
##  [4] RColorBrewer_1.1-2  httr_1.3.1          rprojroot_1.3-2    
##  [7] tools_3.5.0         backports_1.1.2     R6_2.2.2           
## [10] rpart_4.1-13        lazyeval_0.2.1      colorspace_1.3-2   
## [13] nnet_7.3-12         withr_2.1.2         tidyselect_0.2.4   
## [16] mnormt_1.5-5        compiler_3.5.0      cli_1.0.0          
## [19] rvest_0.3.2         xml2_1.2.0          labeling_0.3       
## [22] bookdown_0.7        scales_0.5.0        sfsmisc_1.1-2      
## [25] DEoptimR_1.0-8      psych_1.8.4         robustbase_0.93-0  
## [28] randomForest_4.6-14 digest_0.6.15       foreign_0.8-70     
## [31] rmarkdown_1.9       pkgconfig_2.0.1     htmltools_0.3.6    
## [34] rlang_0.2.0         readxl_1.1.0        ddalpha_1.3.3      
## [37] rstudioapi_0.7      bindr_0.1.1         jsonlite_1.5       
## [40] ModelMetrics_1.1.0  magrittr_1.5        Matrix_1.2-14      
## [43] Rcpp_0.12.17        munsell_0.4.3       abind_1.4-5        
## [46] stringi_1.2.2       yaml_2.1.19         MASS_7.3-50        
## [49] recipes_0.1.2       grid_3.5.0          parallel_3.5.0     
## [52] crayon_1.3.4        haven_1.1.1         splines_3.5.0      
## [55] hms_0.4.2           knitr_1.20          pillar_1.2.2       
## [58] reshape2_1.4.3      codetools_0.2-15    stats4_3.5.0       
## [61] CVST_0.2-1          magic_1.5-8         glue_1.2.0         
## [64] evaluate_0.10.1     blogdown_0.6        modelr_0.1.2       
## [67] foreach_1.4.4       cellranger_1.1.0    gtable_0.2.0       
## [70] kernlab_0.9-26      assertthat_0.2.0    DRR_0.0.3          
## [73] xfun_0.1            gower_0.1.2         prodlim_2018.04.18 
## [76] broom_0.4.4         e1071_1.6-8         class_7.3-14       
## [79] survival_2.42-3     geometry_0.3-6      timeDate_3043.102  
## [82] RcppRoll_0.2.2      iterators_1.0.9     lava_1.6.1         
## [85] ipred_0.9-6</code></pre>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Look, something shiny: How to use R Shiny to make Münster traffic data accessible. Join MünsteR for our next meetup!]]></title>
    <link href="/2018/04/meetup_june18/"/>
    <id>/2018/04/meetup_june18/</id>
    <published>2018-04-19T00:00:00+00:00</published>
    <updated>2018-04-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://shiring.github.io/r_users_group/2017/05/20/map.png" />

</div>
<p>In our <a href="http://meetu.ps/e/DVFrp/w54bW/f">next MünsteR R-user group meetup</a> on <strong>Monday, June 11th, 2018</strong> Thomas Kluth and Thorben Jensen will give a talk titled <strong>Look, something shiny: How to use R Shiny to make Münster traffic data accessible</strong>. You can RSVP here: <a href="http://meetu.ps/e/F7zDN/w54bW/f" class="uri">http://meetu.ps/e/F7zDN/w54bW/f</a></p>
<blockquote>
<p>About a year ago, we stumbled upon rich datasets on <em>traffic dynamics</em> of Münster: count data of bikes, cars, and bus passengers of high resolution. Since that day we have been crunching, modeling, and visualizing it. To involve local stakeholders and NGOs (e.g., the <a href="http://fahrradstadt.ms">IG Fahrradstadt Münster</a>), we found the R Shiny framework to be very useful.</p>
</blockquote>
<blockquote>
<p>Shiny is probably the fastest way to take your R projects online. According to <a href="https://shiny.rstudio.com/">RStudio</a>, it “is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage or embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions.”</p>
</blockquote>
<blockquote>
<p>We would like to introduce Shiny to you using the following topics:</p>
</blockquote>
<ol style="list-style-type: decimal">
<li>Why we love Shiny, and why you should, too</li>
<li>How Shiny works, from code to browser</li>
<li>How to deploy your R Shiny project with Docker</li>
<li>Our <a href="https://traffics.codeformuenster.org">Shiny project traffic dynamics</a></li>
<li>Plotting results of Bayesian and frequentist models within Shiny</li>
<li>Group discussion: what else to present with Shiny?</li>
</ol>
<blockquote>
<p>You will not have to bring much previous knowledge to our talk. A basic understanding of how R code works will take you far. The part about statistical modeling will be as intuitive as possible. Overall, we will try to <em>keep it simple and shiny</em>.</p>
</blockquote>
<blockquote>
<p>All parts of our talk will be connected to traffic data for Münster. We look forward to your feedback and ideas for more analyses. You find our <em>traffic dynamics</em> projects on <a href="https://github.com/codeformuenster">Code for Münster’s github page</a>.</p>
</blockquote>
<div id="about-the-speakers" class="section level2">
<h2>About the speakers</h2>
<p>The speakers Thomas Kluth and Thorben Jensen are members of <a href="http://codeformuenster.org">Code for Münster</a>. We meet each Tuesday at 18:30 at the Dreiklang bar to make our city a better place by coding. New coders are always welcome!</p>
<p>Thomas Kluth has studied Computer Science in Münster and Bremen. During his currently ongoing (close-to-be-finished) Linguistics PhD in Bielefeld, he models human cognitive behavior. Using computational cognitive models, he aims to link spatial language use with perceptual mechanisms such as visual attention. The statistical analysis of empirical data convinced him to use R and Bayesian modeling to explain almost everything. He is looking forward to applying his skill set to real-world domains for creating a sustainable future.</p>
<p>Thorben Jensen has studied, designed, and implemented predictive models since more than 10 years. After studying modeling and computer science in 5 countries, he graduated from the PhD program at Delft University of Technology. His PhD thesis and other publications propose increased use of optimization methods and automation when building simulations with software agents. When consulting clients on Data Science, he enjoys making predictions intuitive with R Shiny.</p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[HH Data Science Meetup slides: Explaining complex machine learning models with LIME]]></title>
    <link href="/2018/04/hh_datascience_meetup_2018_slides/"/>
    <id>/2018/04/hh_datascience_meetup_2018_slides/</id>
    <published>2018-04-18T00:00:00+00:00</published>
    <updated>2018-04-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>On April 12th, 2018 I gave a talk about <strong>Explaining complex machine learning models with LIME</strong> at the <a href="https://www.meetup.com/Hamburg-Data-Science-Meetup/events/krrqhpyxfbkc/">Hamburg Data Science Meetup</a> - so if you’re intersted: the slides can be found here: <a href="https://www.slideshare.net/ShirinGlander/hh-data-science-meetup-explaining-complex-machine-learning-models-with-lime-94218890" class="uri">https://www.slideshare.net/ShirinGlander/hh-data-science-meetup-explaining-complex-machine-learning-models-with-lime-94218890</a></p>
<blockquote>
<p>Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex machine learning models are essentially black boxes and too complicated to understand, we need to use approximations to get a better sense of how they work. One such approach is LIME, which stands for Local Interpretable Model-agnostic Explanations and is a tool that helps understand and explain the decisions made by complex machine learning models.</p>
</blockquote>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/3CI6SYAltdniw" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; display: block; margin: auto; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen>
</iframe>
<div style="margin-bottom:5px">
<strong> <a href="//www.slideshare.net/ShirinGlander/hh-data-science-meetup-explaining-complex-machine-learning-models-with-lime-94218890" title="HH Data Science Meetup: Explaining complex machine learning models with LIME" target="_blank">HH Data Science Meetup: Explaining complex machine learning models with LIME</a> </strong> from <strong><a href="https://www.slideshare.net/ShirinGlander" target="_blank">Shirin Glander</a></strong>
</div>
<p>– slide deck was produced with <a href="www.beautiful.ai">beautiful.ai</a> –</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #124: Systems and Software for Machine Learning at Scale with Jeff Dean]]></title>
    <link href="/2018/04/twimlai124/"/>
    <id>/2018/04/twimlai124/</id>
    <published>2018-04-18T00:00:00+00:00</published>
    <updated>2018-04-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Systems and Software for Machine Learning at Scale with Jeff Dean</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai124.jpg" alt="Sketchnotes from TWiMLAI talk #124: Systems and Software for Machine Learning at Scale with Jeff Dean" />
<p class="caption">Sketchnotes from TWiMLAI talk #124: Systems and Software for Machine Learning at Scale with Jeff Dean</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-124-systems-software-machine-learning-scale-jeff-dean/">here</a>.</p>
<blockquote>
<p>In this episode I’m joined by Jeff Dean, Google Senior Fellow and head of the company’s deep learning research team Google Brain, who I had a chance to sit down with last week at the Googleplex in Mountain View. As you’ll hear, I was very excited for this interview, because so many of Jeff’s contributions since he started at Google in ‘99 have touched my life and work. In our conversation, Jeff and I dig into a bunch of the core machine learning innovations we’ve seen from Google. Of course we discuss TensorFlow, and its origins and evolution at Google. We also explore AI acceleration hardware, including TPU v1, v2 and future directions from Google and the broader market in this area. We talk through the machine learning toolchain, including some things that Googlers might take for granted, and where the recently announced Cloud AutoML fits in. We also discuss Google’s process for mapping problems across a variety of domains to deep learning, and much, much more. This was definitely one of my favorite conversations, and I’m pumped to be able to share it with you. <a href="https://twimlai.com/twiml-talk-124-systems-software-machine-learning-scale-jeff-dean/" class="uri">https://twimlai.com/twiml-talk-124-systems-software-machine-learning-scale-jeff-dean/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Meetup slides: Introducing Deep Learning with Keras]]></title>
    <link href="/2018/04/ruhrpy_meetup_2018_slides/"/>
    <id>/2018/04/ruhrpy_meetup_2018_slides/</id>
    <published>2018-04-11T00:00:00+00:00</published>
    <updated>2018-04-11T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>On April 4th, 2018 I gave a talk about <strong>Deep Learning with Keras</strong> at the Ruhr.Py Meetup in Essen, Germany. The talk was not specific to Python, though - so if you’re intersted: the slides can be found here: <a href="https://www.slideshare.net/ShirinGlander/ruhrpy-introducing-deep-learning-with-keras-and-python" class="uri">https://www.slideshare.net/ShirinGlander/ruhrpy-introducing-deep-learning-with-keras-and-python</a></p>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/cZz1j6qtlwm62l" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; display: block; margin: auto; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen>
</iframe>
<div style="margin-bottom:5px">
<strong> <a href="//www.slideshare.net/ShirinGlander/ruhrpy-introducing-deep-learning-with-keras-and-python" title="Ruhr.PY - Introducing Deep Learning with Keras and Python" target="_blank">Ruhr.PY - Introducing Deep Learning with Keras and Python</a> </strong> von <strong><a href="//www.slideshare.net/ShirinGlander" target="_blank">Shirin Glander</a></strong>
</div>
<p>There is also a video recording of my talk, which you can see here: <a href="https://youtu.be/Q8hVXnpEPmc" class="uri">https://youtu.be/Q8hVXnpEPmc</a></p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Q8hVXnpEPmc?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
</iframe>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Join MünsteR for our next meetup on deep learning with Keras and R]]></title>
    <link href="/2018/03/meetup_april18/"/>
    <id>/2018/03/meetup_april18/</id>
    <published>2018-03-28T00:00:00+00:00</published>
    <updated>2018-03-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://shiring.github.io/r_users_group/2017/05/20/map.png" />

</div>
<p>In our <a href="http://meetu.ps/e/DVFrp/w54bW/f">next MünsteR R-user group meetup</a> on <strong>Tuesday, April 17th, 2018</strong> Kai Lichtenberg will talk about deep learning with Keras. You can RSVP here: <a href="http://meetu.ps/e/DDY1B/w54bW/f" class="uri">http://meetu.ps/e/DDY1B/w54bW/f</a></p>
<blockquote>
<p>Although neural networks have been around for quite a while now, deep learning really just took of a few years ago. It pretty much all started when Alex Krizhevsky and Geoffrey Hinton utterly crushed classic image recognition in the 2012 ImageNet Large Scale Visual Recognition Challenge by implementing a deep neural network with CUDA on graphics cards. A lot has changed since that time: The toolchain to do deep learning has rapidly evolved into API’s with a very high level of abstraction. Nowadays everyone can train complex neural networks with billions of free parameters. Just last year RStudio announced the Keras for R package. Keras is a high level neural network API that makes it really easy to define the architecture of a neural network. In this talk we will rush through an explanation of convolutional neural networks for image recognition, learn how easy it has become to do production ready deep learning with the use of docker and why R’s syntax is even better suited to define a neural network than python’s. (Hint: Probably this is not a pipe ;-)</p>
</blockquote>
<p>Kai Lichtenberg is a PhD student in the Bosch PhD Program working on models to predict the reliability of components in drive trains by leveraging the ever more available high dimensional data in the era of the Internet of Things. Coming from the field of mechanical engineering and technical reliability (which has a lot to do with stochastic processes) he found his destination in data science. As a long time computer enthusiast he is always keen to use the newest technologies. To bear the pain of getting a toolchain up and running is probably his super power.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[My upcoming meetup talks about Deep Learning with Keras and explaining complex Machine Learning Models with LIME]]></title>
    <link href="/2018/03/meetup_talk_ruhrpy_april_18/"/>
    <id>/2018/03/meetup_talk_ruhrpy_april_18/</id>
    <published>2018-03-28T00:00:00+00:00</published>
    <updated>2018-03-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I’ll be talking about Deep Learning with Keras in R and Python at the following upcoming meetup:</p>
<ul>
<li><a href="https://www.meetup.com/Ruhr-py/events/248093628/">Ruhr.Py 2018</a> on Wednesday, April 4th</li>
</ul>
<blockquote>
<p>Introducing Deep Learning with Keras and Python Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. In this talk we build, train and visualize a Model using Python and Keras - all interactive with Jupyter Notebooks!</p>
</blockquote>
<hr />
<p>And I’ll be talking about explaining complex Machine Learning Models with LIME at this upcoming meetup:</p>
<ul>
<li><a href="https://www.meetup.com/Hamburg-Data-Science-Meetup/events/244145443/">Data Science Meetup Hamburg</a> on Thursday, April 12th</li>
</ul>
<blockquote>
<p>Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex machine learning models are essentially black boxes and too complicated to understand, we need to use approximations to get a better sense of how they work. One such approach is LIME, which stands for Local Interpretable Model-agnostic Explanations and is a tool that helps understand and explain the decisions made by complex machine learning models.</p>
</blockquote>
<blockquote>
<p>Dr. Shirin Glander is Data Scientist at codecentric AG. She has received a PhD in Bioinformatics and applies methods of analysis and visualization from different areas - for instance, machine learning, classical statistics, text mining, etc. -to extract and leverage information from data.</p>
</blockquote>
<div class="figure">
<img src="https://secure.meetupstatic.com/s/img/5455565085016210254/logo/svg/logo--script.svg" />

</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #115: Scaling Machine Learning at Uber with Mike Del Balso]]></title>
    <link href="/2018/03/twimlai115/"/>
    <id>/2018/03/twimlai115/</id>
    <published>2018-03-07T00:00:00+00:00</published>
    <updated>2018-03-07T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Scaling Machine Learning at Uber with Mike Del Balso</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai115.jpg" alt="Sketchnotes from TWiMLAI talk #115: Scaling Machine Learning at Uber with Mike Del Balso" />
<p class="caption">Sketchnotes from TWiMLAI talk #115: Scaling Machine Learning at Uber with Mike Del Balso</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-115-scaling-machine-learning-uber-mike-del-balso/">here</a>.</p>
<blockquote>
<p>In this episode, I speak with Mike Del Balso, Product Manager for Machine Learning Platforms at Uber. Mike and I sat down last fall at the Georgian Partners Portfolio conference to discuss his presentation “Finding success with machine learning in your company.” In our discussion, Mike shares some great advice for organizations looking to get value out of machine learning. He also details some of the pitfalls companies run into, such as not have proper infrastructure in place for maintenance and monitoring, not managing their expectations, and not putting the right tools in place for data science and development teams. On this last point, we touch on the Michelangelo platform, which Uber uses internally to build, deploy and maintain ML systems at scale, and the open source distributed TensorFlow system they’ve created, Horovod. This was a very insightful interview, so get your notepad ready! <a href="https://twimlai.com/twiml-talk-115-scaling-machine-learning-uber-mike-del-balso/" class="uri">https://twimlai.com/twiml-talk-115-scaling-machine-learning-uber-mike-del-balso/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Another Game of Thrones network analysis - this time with tidygraph and ggraph]]></title>
    <link href="/2018/03/got_network/"/>
    <id>/2018/03/got_network/</id>
    <published>2018-03-04T00:00:00+00:00</published>
    <updated>2018-03-04T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>A while back, I did <a href="https://shiring.github.io/networks/2017/05/15/got_final">an analysis of the family network of major characters from the A Song of Ice and Fire books and the Game of Thrones TV show</a>. In that analysis I found out that House Stark (specifically Ned and Sansa) and House Lannister (especially Tyrion) are the most important family connections in Game of Thrones; they also connect many of the story lines and are central parts of the narrative.</p>
<p>In that old post, I used <code>igraph</code> for plotting and calculating network metrics.</p>
<p>But there are two packages that integrate network analysis much more nicely with the <code>tidyverse</code>: <code>tidygraph</code> and <code>ggraph</code>. These, I am going to show how to use for analyzing yet another network of characters from <strong>A Song of Ice and Fire</strong> / <strong>Game of Thrones</strong> (to be correct, this new network here is strictly based on the <strong>A Song of Ice and Fire</strong> books, NOT on the TV show).</p>
<div id="what-can-network-analysis-tell-us" class="section level2">
<h2>What can network analysis tell us?</h2>
<p>Network analysis can e.g. be used to explore relationships in social or professional networks. In such cases, we would typically ask questions like:</p>
<ul>
<li>How many connections does each person have?</li>
<li>Who is the most connected (i.e. influential or “important”) person?</li>
<li>Are there clusters of tightly connected people?</li>
<li>Are there a few key players that connect clusters of people?</li>
<li>etc.</li>
</ul>
<p>These answers can give us a lot of information about the patterns of how people interact.</p>
<p>So, how do we find out who the most important characters are in this network? We consider a character “important” if he has connections to many other characters. There are a few network properties, that tell us more about this, like node centrality and which characters are key-players in the books.</p>
<p><strong>A word of caution before you read on: BEWARE of SPOILERS for all books!</strong></p>
<div class="figure">
<img src="https://shirinsplayground.netlify.com/post/2018-03-03_got_network_files/figure-html/unnamed-chunk-18-1.png" alt="A Song of Ice and Fire character network across all five books; find out how I made it by following the code below…" />
<p class="caption">A Song of Ice and Fire character network across all five books; find out how I made it by following the code below…</p>
</div>
<pre class="r"><code>library(readr)     # fast reading of csv files
library(tidyverse) # tidy data analysis
library(tidygraph) # tidy graph analysis
library(ggraph)    # for plotting</code></pre>
</div>
<div id="the-data" class="section level2">
<h2>The Data</h2>
<p>I obtained the following data by cloning <a href="https://github.com/mathbeveridge/asoiaf">this Github repository</a> from Andrew Beveridge:</p>
<blockquote>
<p>Character Interaction Networks for George R. R. Martin’s “A Song of Ice and Fire” saga These networks were created by connecting two characters whenever their names (or nicknames) appeared within 15 words of one another in one of the books in “A Song of Ice and Fire.” The edge weight corresponds to the number of interactions. You can use this data to explore the dynamics of the Seven Kingdoms using network science techniques. For example, community detection finds coherent plotlines. Centrality measures uncover the multiple ways in which characters play important roles in the saga.</p>
</blockquote>
<p>Andrew already did a great job analyzing these character networks and you can read all his conclusions on his site <a href="https://networkofthrones.wordpress.com" class="uri">https://networkofthrones.wordpress.com</a>. Here, I don’t aim to replicate his analyses but I want to show how you could do this or similar analyses with <code>tidygraph</code> and <code>ggraph</code>. Thus, I am also not going to use all of his node and edge files.</p>
<pre class="r"><code>path &lt;- &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data/&quot;
files &lt;- list.files(path = path, full.names = TRUE)
files</code></pre>
<pre><code>##  [1] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-all-edges.csv&quot;   
##  [2] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-all-nodes.csv&quot;   
##  [3] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book1-edges.csv&quot; 
##  [4] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book1-nodes.csv&quot; 
##  [5] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book2-edges.csv&quot; 
##  [6] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book2-nodes.csv&quot; 
##  [7] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book3-edges.csv&quot; 
##  [8] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book3-nodes.csv&quot; 
##  [9] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book4-edges.csv&quot; 
## [10] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book4-nodes.csv&quot; 
## [11] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book45-edges.csv&quot;
## [12] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book45-nodes.csv&quot;
## [13] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book5-edges.csv&quot; 
## [14] &quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book5-nodes.csv&quot;</code></pre>
</div>
<div id="characters-across-all-books" class="section level2">
<h2>Characters across all books</h2>
<p>The first data set I am going to use are the character interactions in all five books. I am not using the node files here, because I find the edge names sufficient for this demonstration. If you wanted to have nice name labels, you could use the node files.</p>
<pre class="r"><code>cooc_all_edges &lt;- read_csv(files[1])</code></pre>
<p>Because there are so many characters in the books, many of them minor, I am subsetting the data to the 100 characters with the most interactions across all books. The edges are undirected, therefore there are no redundant Source-Target combinations; because of this, I gathered Source and Target data before summing up the weights.</p>
<pre class="r"><code>main_ch &lt;- cooc_all_edges %&gt;%
  select(-Type) %&gt;%
  gather(x, name, Source:Target) %&gt;%
  group_by(name) %&gt;%
  summarise(sum_weight = sum(weight)) %&gt;%
  ungroup()

main_ch_l &lt;- main_ch %&gt;%
  arrange(desc(sum_weight)) %&gt;%
  top_n(100, sum_weight)
main_ch_l</code></pre>
<pre><code>## # A tibble: 100 x 2
##    name               sum_weight
##    &lt;chr&gt;                   &lt;int&gt;
##  1 Tyrion-Lannister         2873
##  2 Jon-Snow                 2757
##  3 Cersei-Lannister         2232
##  4 Joffrey-Baratheon        1762
##  5 Eddard-Stark             1649
##  6 Daenerys-Targaryen       1608
##  7 Jaime-Lannister          1569
##  8 Sansa-Stark              1547
##  9 Bran-Stark               1508
## 10 Robert-Baratheon         1488
## # ... with 90 more rows</code></pre>
<pre class="r"><code>cooc_all_f &lt;- cooc_all_edges %&gt;%
  filter(Source %in% main_ch_l$name &amp; Target %in% main_ch_l$name)</code></pre>
</div>
<div id="tidygraph-and-ggraph" class="section level2">
<h2>tidygraph and ggraph</h2>
<p>Both <code>tidygraph</code> and <code>ggraph</code> have been developed by <a href="https://www.data-imaginist.com">Thomas Lin Pedersen</a>:</p>
<blockquote>
<p>With tidygraph I set out to make it easier to get your data into a graph and perform common transformations on it, but the aim has expanded since its inception. The goal of tidygraph is to empower the user to formulate complex questions regarding relational data as simple steps, thus enabling them to retrieve insights directly from the data itself. The central idea this all boils down to is this: you don’t have to plot a network to understand it. While I absolutely love the field of network visualisation, it is in many ways overused in data science — especially when it comes to extracting knowledge from a network. Just as you don’t need a plot to tell you which car in a dataset is the fastest, you don’t need a plot to tell you which pair of friends are the closest. What you do need, instead of a plot, is a tool that allow you to formulate your question into a logic sequence of operations. For many people in the world of rectangular data, this tool is increasingly dplyr (and friends), and I do hope that tidygraph can take on the same role in the world of relational data. <a href="https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/" class="uri">https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/</a></p>
</blockquote>
<p>The first step is to convert our edge table into a <code>tbl_graph</code> object structure. Here, we use the <code>as_tbl_graph()</code> function from <code>tidygraph</code>; it can take many different types of input data, like <code>data.frame</code>, <code>matrix</code>, <code>dendrogram</code>, <code>igraph</code>, etc.</p>
<blockquote>
<p>Underneath the hood of tidygraph lies the well-oiled machinery of igraph, ensuring efficient graph manipulation. Rather than keeping the node and edge data in a list and creating igraph objects on the fly when needed, tidygraph subclasses igraph with the tbl_graph class and simply exposes it in a tidy manner. This ensures that all your beloved algorithms that expects igraph objects still works with tbl_graph objects. Further, tidygraph is very careful not to override any of igraphs exports so the two packages can coexist quite happily. <a href="https://www.data-imaginist.com/2017/introducing-tidygraph/" class="uri">https://www.data-imaginist.com/2017/introducing-tidygraph/</a></p>
</blockquote>
<p>A central aspect of <code>tidygraph</code> is that you can directly manipulate node and edge data from this <code>tbl_graph</code> object by <strong>activating</strong> nodes or edges. When we first create a <code>tbl_graph</code> object, the nodes will be activated. We can then directly calculate node or edge metrics, like centrality, using <code>tidyverse</code> functions.</p>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE)</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Node Data: 100 x 1 (active)
##   name                           
##   &lt;chr&gt;                          
## 1 Aemon-Targaryen-(Maester-Aemon)
## 2 Aeron-Greyjoy                  
## 3 Aerys-II-Targaryen             
## 4 Alliser-Thorne                 
## 5 Arianne-Martell                
## 6 Arya-Stark                     
## # ... with 94 more rows
## #
## # Edge Data: 798 x 5
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1     1     4 Undirected    43      7
## 2     1    13 Undirected    44      4
## 3     1    28 Undirected    52      3
## # ... with 795 more rows</code></pre>
<p>We can change that with the <code>activate()</code> function. We can now, for example, remove multiple edges. When you are using RStudio, start typing <code>?edge_is_</code> and wait for the autocomplete function to show you what else is possible (or go to the <code>tidygraph</code> manual).</p>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(edges) %&gt;%
  filter(!edge_is_multiple())</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Edge Data: 798 x 5 (active)
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1     1     4 Undirected    43      7
## 2     1    13 Undirected    44      4
## 3     1    28 Undirected    52      3
## 4     1    32 Undirected    53     20
## 5     1    34 Undirected    54      5
## 6     1    41 Undirected    56      5
## # ... with 792 more rows
## #
## # Node Data: 100 x 1
##   name                           
##   &lt;chr&gt;                          
## 1 Aemon-Targaryen-(Maester-Aemon)
## 2 Aeron-Greyjoy                  
## 3 Aerys-II-Targaryen             
## # ... with 97 more rows</code></pre>
<div id="node-ranking" class="section level3">
<h3>Node ranking</h3>
<blockquote>
<p>Often, especially when visualising networks with certain layouts, the order in which the nodes appear will have a huge influence on the insight you can get out (e.g. matrix plots and arc diagrams). The node_rank_*() family of algorithms have been introduced to provide different ways of sorting nodes so that closely related nodes are positionally close. As there is often not a single correct answer to this endeavor, there’s a lot of different algorithms that may provide different insights into your network. Many of them are based on the seriation package, and the vignette provided therein serves as a nice introduction to the different algorithms. <a href="https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/" class="uri">https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/</a></p>
</blockquote>
<p>There are many options for node ranking (go to <code>?node_rank</code> for a full list); let’s try out <strong>Minimize hamiltonian path length using a travelling salesperson solver</strong>.</p>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(nodes) %&gt;% 
  mutate(n_rank_trv = node_rank_traveller()) %&gt;%
  arrange(n_rank_trv)</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Node Data: 100 x 2 (active)
##   name             n_rank_trv
##   &lt;chr&gt;                 &lt;int&gt;
## 1 Gendry                    1
## 2 Hot-Pie                   2
## 3 Lem                       3
## 4 Beric-Dondarrion          4
## 5 Eddard-Stark              5
## 6 Ramsay-Snow               6
## # ... with 94 more rows
## #
## # Edge Data: 798 x 5
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1    44    46 Undirected    43      7
## 2    44    45 Undirected    44      4
## 3    44    92 Undirected    52      3
## # ... with 795 more rows</code></pre>
</div>
<div id="centrality" class="section level3">
<h3>Centrality</h3>
<p>Centrality describes the number of edges that are in- or outgoing to/from nodes. High centrality networks have few nodes with many connections, low centrality networks have many nodes with similar numbers of edges. The centrality of a node measures the importance of it in the network.</p>
<blockquote>
<p>This version adds 19(!) new ways to define the notion of centrality along with a manual version where you can mix and match different distance measures and summation strategies opening up the world to even more centrality scores. All of this wealth of centrality comes from the netrankr package that provides a framework for defining and calculating centrality scores. If you use centrality measures somewhere in your analysis I cannot recommend the vignettes provided by netrankr enough as they provide a fundamental intuition about the nature of such measures and how they can/should be used. <a href="https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/" class="uri">https://www.data-imaginist.com/2018/tidygraph-1-1-a-tidy-hope/</a></p>
</blockquote>
<p>Again, type <code>?centrality</code> for an overview about all possible centrality measures you can use. Let’s try out <code>centrality_degree()</code>.</p>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(nodes) %&gt;% 
  mutate(neighbors = centrality_degree()) %&gt;%
  arrange(-neighbors)</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Node Data: 100 x 2 (active)
##   name              neighbors
##   &lt;chr&gt;                 &lt;dbl&gt;
## 1 Tyrion-Lannister        54.
## 2 Cersei-Lannister        49.
## 3 Joffrey-Baratheon       49.
## 4 Robert-Baratheon        47.
## 5 Jaime-Lannister         45.
## 6 Sansa-Stark             44.
## # ... with 94 more rows
## #
## # Edge Data: 798 x 5
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1    41    42 Undirected    43      7
## 2    41    60 Undirected    44      4
## 3    41    63 Undirected    52      3
## # ... with 795 more rows</code></pre>
</div>
<div id="grouping-and-clustering" class="section level3">
<h3>Grouping and clustering</h3>
<blockquote>
<p>Another common operation is to group nodes based on the graph topology, sometimes referred to as community detection based on its commonality in social network analysis. All clustering algorithms from igraph is available in tidygraph using the group_* prefix. All of these functions return an integer vector with nodes (or edges) sharing the same integer being grouped together. <a href="https://www.data-imaginist.com/2017/introducing-tidygraph/" class="uri">https://www.data-imaginist.com/2017/introducing-tidygraph/</a></p>
</blockquote>
<p>We can use <code>?group_graph</code> for an overview about all possible ways to cluster and group nodes. Here I am using <code>group_infomap()</code>: <strong>Group nodes by minimizing description length using</strong>.</p>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(nodes) %&gt;% 
  mutate(group = group_infomap()) %&gt;%
  arrange(-group)</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Node Data: 100 x 2 (active)
##   name              group
##   &lt;chr&gt;             &lt;int&gt;
## 1 Arianne-Martell       7
## 2 Doran-Martell         7
## 3 Davos-Seaworth        6
## 4 Melisandre            6
## 5 Selyse-Florent        6
## 6 Stannis-Baratheon     6
## # ... with 94 more rows
## #
## # Edge Data: 798 x 5
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1    32    33 Undirected    43      7
## 2    32    34 Undirected    44      4
## 3    32    36 Undirected    52      3
## # ... with 795 more rows</code></pre>
</div>
<div id="querying-node-types" class="section level3">
<h3>Querying node types</h3>
<p>We can also query different node types (<code>?node_types</code> gives us a list of options):</p>
<blockquote>
<p>These functions all lets the user query whether each node is of a certain type. All of the functions returns a logical vector indicating whether the node is of the type in question. Do note that the types are not mutually exclusive and that nodes can thus be of multiple types.</p>
</blockquote>
<p>Here, I am trying out <code>node_is_center()</code> (does the node have the minimal eccentricity in the graph) and <code>node_is_keyplayer()</code> to identify the top 10 key-players in the network. You can read more about the <code>node_is_keyplayer()</code> function in the manual for the <code>influenceR</code> package:</p>
<blockquote>
<p>The “Key Player” family of node importance algorithms (Borgatti 2006) involves the selection of a metric of node importance and a combinatorial optimization strategy to choose the set S of vertices of size k that maximize that metric. This function implements KPP-Pos, a metric intended to identify k nodes which optimize resource diffusion through the net … <a href="https://cran.r-project.org/web/packages/influenceR/" class="uri">https://cran.r-project.org/web/packages/influenceR/</a></p>
</blockquote>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(nodes) %&gt;% 
  mutate(center = node_is_center(),
         keyplayer = node_is_keyplayer(k = 10))</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Node Data: 100 x 3 (active)
##   name                            center keyplayer
##   &lt;chr&gt;                           &lt;lgl&gt;  &lt;lgl&gt;    
## 1 Aemon-Targaryen-(Maester-Aemon) FALSE  FALSE    
## 2 Aeron-Greyjoy                   FALSE  FALSE    
## 3 Aerys-II-Targaryen              FALSE  FALSE    
## 4 Alliser-Thorne                  FALSE  FALSE    
## 5 Arianne-Martell                 FALSE  FALSE    
## 6 Arya-Stark                      FALSE  FALSE    
## # ... with 94 more rows
## #
## # Edge Data: 798 x 5
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1     1     4 Undirected    43      7
## 2     1    13 Undirected    44      4
## 3     1    28 Undirected    52      3
## # ... with 795 more rows</code></pre>
</div>
</div>
<div id="node-pairs" class="section level2">
<h2>Node pairs</h2>
<blockquote>
<p>Some statistics are a measure between two nodes, such as distance or similarity between nodes. In a tidy context one of the ends must always be the node defined by the row, while the other can be any other node. All of the node pair functions are prefixed with node_* and ends with _from/_to if the measure is not symmetric and _with if it is; e.g. there’s both a node_max_flow_to() and node_max_flow_from() function while only a single node_cocitation_with() function. The other part of the node pair can be specified as an integer vector that will get recycled if needed, or a logical vector which will get recycled and converted to indexes with which(). This means that output from node type functions can be used directly in the calls. <a href="https://www.data-imaginist.com/2017/introducing-tidygraph/" class="uri">https://www.data-imaginist.com/2017/introducing-tidygraph/</a></p>
</blockquote>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(nodes) %&gt;% 
  mutate(dist_to_center = node_distance_to(node_is_center()))</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Node Data: 100 x 2 (active)
##   name                            dist_to_center
##   &lt;chr&gt;                                    &lt;dbl&gt;
## 1 Aemon-Targaryen-(Maester-Aemon)             1.
## 2 Aeron-Greyjoy                               2.
## 3 Aerys-II-Targaryen                          1.
## 4 Alliser-Thorne                              1.
## 5 Arianne-Martell                             2.
## 6 Arya-Stark                                  1.
## # ... with 94 more rows
## #
## # Edge Data: 798 x 5
##    from    to Type          id weight
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;
## 1     1     4 Undirected    43      7
## 2     1    13 Undirected    44      4
## 3     1    28 Undirected    52      3
## # ... with 795 more rows</code></pre>
<div id="edge-betweenness" class="section level3">
<h3>Edge betweenness</h3>
<p>Similarly to node metrics, we can calculate all kinds of edge metrics. Betweenness, for example, describes the shortest paths between nodes. More about what you can do with edges can be found with <code>?edge_types</code> and in the <a href="https://cran.r-project.org/web/packages/tidygraph/tidygraph.pdf">tidygraph manual</a>.</p>
<pre class="r"><code>as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  activate(edges) %&gt;% 
  mutate(centrality_e = centrality_edge_betweenness())</code></pre>
<pre><code>## # A tbl_graph: 100 nodes and 798 edges
## #
## # An undirected simple graph with 1 component
## #
## # Edge Data: 798 x 6 (active)
##    from    to Type          id weight centrality_e
##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;        &lt;dbl&gt;
## 1     1     4 Undirected    43      7         1.00
## 2     1    13 Undirected    44      4        30.2 
## 3     1    28 Undirected    52      3        42.1 
## 4     1    32 Undirected    53     20         0.  
## 5     1    34 Undirected    54      5        35.2 
## 6     1    41 Undirected    56      5        18.9 
## # ... with 792 more rows
## #
## # Node Data: 100 x 1
##   name                           
##   &lt;chr&gt;                          
## 1 Aemon-Targaryen-(Maester-Aemon)
## 2 Aeron-Greyjoy                  
## 3 Aerys-II-Targaryen             
## # ... with 97 more rows</code></pre>
</div>
</div>
<div id="the-complete-code" class="section level2">
<h2>The complete code</h2>
<p>Now let’s combine what we’ve done above in true tidyverse fashion:</p>
<pre class="r"><code>cooc_all_f_graph &lt;- as_tbl_graph(cooc_all_f, directed = FALSE) %&gt;%
  mutate(n_rank_trv = node_rank_traveller(),
         neighbors = centrality_degree(),
         group = group_infomap(),
         center = node_is_center(),
         dist_to_center = node_distance_to(node_is_center()),
         keyplayer = node_is_keyplayer(k = 10)) %&gt;%
  activate(edges) %&gt;% 
  filter(!edge_is_multiple()) %&gt;%
  mutate(centrality_e = centrality_edge_betweenness())</code></pre>
<p>We can also convert our active node or edge table back to a <code>tibble</code>:</p>
<pre class="r"><code>cooc_all_f_graph %&gt;%
  activate(nodes) %&gt;% # %N&gt;%
  as.tibble()</code></pre>
<pre><code>## # A tibble: 100 x 7
##    name         n_rank_trv neighbors group center dist_to_center keyplayer
##    &lt;chr&gt;             &lt;int&gt;     &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt;           &lt;dbl&gt; &lt;lgl&gt;    
##  1 Aemon-Targa…          7       13.     2 FALSE              1. FALSE    
##  2 Aeron-Greyj…         85        5.     5 FALSE              2. FALSE    
##  3 Aerys-II-Ta…         59       12.     1 FALSE              1. FALSE    
##  4 Alliser-Tho…          6       13.     2 FALSE              1. FALSE    
##  5 Arianne-Mar…         95        4.     7 FALSE              2. FALSE    
##  6 Arya-Stark           34       37.     1 FALSE              1. FALSE    
##  7 Asha-Greyjoy         87        7.     5 FALSE              1. FALSE    
##  8 Balon-Greyj…         86       11.     5 FALSE              2. FALSE    
##  9 Barristan-S…         91       23.     3 FALSE              1. FALSE    
## 10 Belwas               42        6.     3 FALSE              2. FALSE    
## # ... with 90 more rows</code></pre>
<pre class="r"><code>cooc_all_f_graph %&gt;%
  activate(edges) %&gt;% # %E&gt;%
  as.tibble()</code></pre>
<pre><code>## # A tibble: 798 x 6
##     from    to Type          id weight centrality_e
##    &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;int&gt;  &lt;int&gt;        &lt;dbl&gt;
##  1     1     4 Undirected    43      7         1.00
##  2     1    13 Undirected    44      4        30.2 
##  3     1    28 Undirected    52      3        42.1 
##  4     1    32 Undirected    53     20         0.  
##  5     1    34 Undirected    54      5        35.2 
##  6     1    41 Undirected    56      5        18.9 
##  7     1    42 Undirected    57     25         0.  
##  8     1    48 Undirected    58    110         0.  
##  9     1    58 Undirected    60      5        24.5 
## 10     1    71 Undirected    62      5        17.0 
## # ... with 788 more rows</code></pre>
<div id="plotting-with-ggraph" class="section level3">
<h3>Plotting with ggraph</h3>
<p>For plotting our graph object, we can make good use of the <code>ggraph</code> package:</p>
<blockquote>
<p>ggraph is an extension of ggplot2 aimed at supporting relational data structures such as networks, graphs, and trees. While it builds upon the foundation of ggplot2 and its API it comes with its own self-contained set of geoms, facets, etc., as well as adding the concept of layouts to the grammar. <a href="https://github.com/thomasp85/ggraph" class="uri">https://github.com/thomasp85/ggraph</a></p>
</blockquote>
<p>First, I am going to define a layout. There are lots of <a href="https://www.data-imaginist.com/2017/ggraph-introduction-layouts/">options for layouts</a>, here I am using a <a href="http://igraph.org/r/doc/layout_with_fr.html">Fruchterman-Reingold</a> algorithm.</p>
<pre class="r"><code>layout &lt;- create_layout(cooc_all_f_graph, 
                        layout = &quot;fr&quot;)</code></pre>
<p>The rest works like any <code>ggplot2</code> function call, just that we use special geoms for our network, like <code>geom_edge_density()</code> to draw a shadow where the edge density is higher, <code>geom_edge_link()</code> to connect edges with a straight line, <code>geom_node_point()</code> to draw node points and <code>geom_node_text()</code> to draw the labels. More options can be found <a href="https://github.com/thomasp85/ggraph">here</a>.</p>
<p>Here are three options of plotting the network with the metrics we just calculated:</p>
<pre class="r"><code>ggraph(layout) + 
    geom_edge_density(aes(fill = weight)) +
    geom_edge_link(aes(width = weight), alpha = 0.2) + 
    geom_node_point(aes(color = factor(group)), size = 10) +
    geom_node_text(aes(label = name), size = 8, repel = TRUE) +
    scale_color_brewer(palette = &quot;Set1&quot;) +
    theme_graph() +
    labs(title = &quot;A Song of Ice and Fire character network&quot;,
         subtitle = &quot;Nodes are colored by group&quot;)</code></pre>
<p><img src="/post/2018-03-03_got_network_files/figure-html/unnamed-chunk-18-1.png" width="2880" /></p>
<p>Interestingly, many of the groups reflect the narrative perfectly: the men from the Night’s Watch are grouped together with the Wildlings, Stannis, Davos, Selyse and Melisandre form another group, the Greyjoys, Bran’s group in Winterfell before they left for the North, Dany and her squad and the Martells (except for Quentyn, who “belongs” to Dany - just like in the books ;-)). The big group around the remaining characters is the only one that’s not split up very well.</p>
<p>For the next graphs, I want specific colors form the <code>RColorBrewer</code> palette “Set1”:</p>
<pre class="r"><code>cols &lt;- RColorBrewer::brewer.pal(3, &quot;Set1&quot;)</code></pre>
<pre class="r"><code>ggraph(layout) + 
    geom_edge_density(aes(fill = weight)) +
    geom_edge_link(aes(width = weight), alpha = 0.2) + 
    geom_node_point(aes(color = factor(center), size = dist_to_center)) +
    geom_node_text(aes(label = name), size = 8, repel = TRUE) +
    scale_colour_manual(values = c(cols[2], cols[1])) +
    theme_graph() +
    labs(title = &quot;A Song of Ice and Fire character network&quot;,
         subtitle = &quot;Nodes are colored by centeredness&quot;)</code></pre>
<p><img src="/post/2018-03-03_got_network_files/figure-html/unnamed-chunk-20-1.png" width="2880" /></p>
<p>In the next graph I plotted the center-most characters in red and the distance to center as node size. The two center characters across all books are Robert Baratheon and Tyrion Lannister. I had not expected Robert, since he dies pretty much right away but I guess he and his rebellion following Lyanna’s “abduction” is the main trigger for most of what happens in the books, so why not… And that Tyrion is the best character (and George RR Martin’s favorite) is a given, anyways! ;-)</p>
</div>
</div>
<div id="characters-devided-by-books" class="section level2">
<h2>Characters devided by books</h2>
<p>The second data set I am going to use is a comparison of character interactions in the five books.</p>
<p><strong>A little node on the side:</strong> My original plan was to loop over the separate edge files for each book, concatenate them together with the information from which book they are and then plot them via faceting. This turned out to be a bad solution because I wanted to show the different key-players in each of the five books. So, instead of using one joined graph, I created separate graphs for every book and used the <code>bind_graphs()</code> and <code>facet_nodes()</code> functions to plot them together.</p>
<pre class="r"><code>for (i in 1:5) {
  cooc &lt;- read_csv(paste0(&quot;/Users/shiringlander/Documents/Github/Data/asoiaf/data//asoiaf-book&quot;, i, &quot;-edges.csv&quot;)) %&gt;%
    mutate(book = paste0(&quot;book_&quot;, i)) %&gt;%
    filter(Source %in% main_ch_l$name &amp; Target %in% main_ch_l$name)
  
  assign(paste0(&quot;coocs_book_&quot;, i), cooc)
}</code></pre>
<p>The concepts are the same as above, here I want to know the key-players in each book:</p>
<pre class="r"><code>cooc_books_1_graph &lt;- as_tbl_graph(coocs_book_1, directed = FALSE) %&gt;%
  mutate(book = &quot;Book 1: A Game of Thrones&quot;,
         keyplayer = node_is_keyplayer(k = 10))

cooc_books_2_graph &lt;- as_tbl_graph(coocs_book_2, directed = FALSE) %&gt;%
  mutate(book = &quot;Book 2: A Clash of Kings&quot;,
         keyplayer = node_is_keyplayer(k = 10))

cooc_books_3_graph &lt;- as_tbl_graph(coocs_book_3, directed = FALSE) %&gt;%
  mutate(book = &quot;Book 3: A Storm of Swords&quot;,
         keyplayer = node_is_keyplayer(k = 10))

cooc_books_4_graph &lt;- as_tbl_graph(coocs_book_4, directed = FALSE) %&gt;%
  mutate(book = &quot;Book 4: A Feast for Crows&quot;,
         keyplayer = node_is_keyplayer(k = 10))

cooc_books_5_graph &lt;- as_tbl_graph(coocs_book_5, directed = FALSE) %&gt;%
  mutate(book = &quot;Book 5: A Dance with Dragons&quot;,
         keyplayer = node_is_keyplayer(k = 10))</code></pre>
<p>And let’s combine and plot the key-players:</p>
<pre class="r"><code>cooc_books_1_graph %&gt;% 
  bind_graphs(cooc_books_2_graph)  %&gt;%
  bind_graphs(cooc_books_3_graph)  %&gt;%
  bind_graphs(cooc_books_4_graph)  %&gt;%
  bind_graphs(cooc_books_5_graph)  %&gt;%
  ggraph(layout = &quot;fr&quot;) + 
    facet_nodes( ~ book, scales = &quot;free&quot;, ncol = 1) +
    geom_edge_density(aes(fill = weight)) +
    geom_edge_link(aes(edge_width = weight), alpha = 0.2) + 
    geom_node_point(aes(color = factor(keyplayer)), size = 3) +
    geom_node_text(aes(label = name), color = &quot;black&quot;, size = 3, repel = TRUE) +
    theme_graph() +
    scale_colour_manual(values = c(cols[2], cols[1]))</code></pre>
<p><img src="/post/2018-03-03_got_network_files/figure-html/unnamed-chunk-23-1.png" width="960" /></p>
<p>The networks and key-players of the five different books also offer a few surprises but also a lot that reflects the narrative quite well. I’m not going to go into details here as that would go a bit too far for an R-related blog - but if you are interested in in-depth discussions about the books, email me… ;-)</p>
</div>
<div id="more-info" class="section level2">
<h2>More info</h2>
<p>You can find more info about</p>
<ul>
<li><code>tidygraph</code> <a href="https://cran.r-project.org/web/packages/tidygraph">here</a></li>
<li><code>ggraph</code> <a href="https://cran.r-project.org/web/packages/ggraph">here</a></li>
<li><code>influenceR</code> <a href="https://cran.r-project.org/web/packages/influenceR">here</a></li>
<li>and DataCamp has a Python project for the same data set <a href="https://www.datacamp.com/projects/76?utm_campaign=broadcast&amp;utm_medium=broadcast_8&amp;utm_source=main">here</a></li>
</ul>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.4
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] methods   stats     graphics  grDevices utils     datasets  base     
## 
## other attached packages:
##  [1] bindrcpp_0.2       ggraph_1.0.1       tidygraph_1.1.0   
##  [4] forcats_0.3.0      stringr_1.3.0      dplyr_0.7.4       
##  [7] purrr_0.2.4        tidyr_0.8.0        tibble_1.4.2      
## [10] ggplot2_2.2.1.9000 tidyverse_1.2.1    readr_1.1.1       
## 
## loaded via a namespace (and not attached):
##  [1] nlme_3.1-131.1     bitops_1.0-6       lubridate_1.7.3   
##  [4] RColorBrewer_1.1-2 httr_1.3.1         prabclus_2.2-6    
##  [7] rprojroot_1.3-2    tools_3.4.3        backports_1.1.2   
## [10] utf8_1.1.3         R6_2.2.2           KernSmooth_2.23-15
## [13] lazyeval_0.2.1     colorspace_1.3-2   trimcluster_0.1-2 
## [16] nnet_7.3-12        withr_2.1.1.9000   tidyselect_0.2.4  
## [19] gridExtra_2.3      mnormt_1.5-5       compiler_3.4.3    
## [22] cli_1.0.0          rvest_0.3.2        TSP_1.1-5         
## [25] influenceR_0.1.0   xml2_1.2.0         labeling_0.3      
## [28] bookdown_0.7       diptest_0.75-7     caTools_1.17.1    
## [31] scales_0.5.0.9000  DEoptimR_1.0-8     robustbase_0.92-8 
## [34] mvtnorm_1.0-7      psych_1.7.8        digest_0.6.15     
## [37] foreign_0.8-69     rmarkdown_1.8      pkgconfig_2.0.1   
## [40] htmltools_0.3.6    rlang_0.2.0.9000   readxl_1.0.0      
## [43] rstudioapi_0.7     bindr_0.1          jsonlite_1.5      
## [46] mclust_5.4         gtools_3.5.0       dendextend_1.7.0  
## [49] magrittr_1.5       modeltools_0.2-21  Rcpp_0.12.15      
## [52] munsell_0.4.3      viridis_0.5.0      stringi_1.1.6     
## [55] whisker_0.3-2      yaml_2.1.17        MASS_7.3-49       
## [58] flexmix_2.3-14     gplots_3.0.1       plyr_1.8.4        
## [61] grid_3.4.3         parallel_3.4.3     gdata_2.18.0      
## [64] ggrepel_0.7.0      crayon_1.3.4       udunits2_0.13     
## [67] lattice_0.20-35    haven_1.1.1        hms_0.4.1         
## [70] knitr_1.20         pillar_1.2.1       igraph_1.1.2      
## [73] fpc_2.1-11         stats4_3.4.3       reshape2_1.4.3    
## [76] codetools_0.2-15   glue_1.2.0         gclus_1.3.1       
## [79] evaluate_0.10.1    blogdown_0.5       modelr_0.1.1      
## [82] tweenr_0.1.5       foreach_1.4.4      cellranger_1.1.0  
## [85] gtable_0.2.0       kernlab_0.9-25     assertthat_0.2.0  
## [88] xfun_0.1           ggforce_0.1.1      broom_0.4.3       
## [91] class_7.3-14       viridisLite_0.3.0  seriation_1.2-3   
## [94] iterators_1.0.9    registry_0.5       units_0.5-1       
## [97] cluster_2.0.6</code></pre>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #111: Learning “Common Sense” and Physical Concepts with Roland Memisevic]]></title>
    <link href="/2018/02/twimlai111/"/>
    <id>/2018/02/twimlai111/</id>
    <published>2018-02-19T00:00:00+00:00</published>
    <updated>2018-02-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Learning “Common Sense” and Physical Concepts with Roland Memisevic</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai111.jpg" alt="Sketchnotes from TWiMLAI talk #111: Learning Common Sense and Physical Concepts with Roland Memisevic" />
<p class="caption">Sketchnotes from TWiMLAI talk #111: Learning “Common Sense” and Physical Concepts with Roland Memisevic</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-111-learning-common-sense-physical-concepts-roland-memisevic/">here</a>.</p>
<blockquote>
<p>In today’s episode, I’m joined by Roland Memisevic, co-founder, CEO, and chief scientist at Twenty Billion Neurons. Roland joined me at the RE•WORK Deep Learning Summit in Montreal to discuss the work his company is doing to train deep neural networks to understand physical actions. In our conversation, we dig into video analysis and understanding, including how data-rich video can help us develop what Roland calls comparative understanding, or AI “common sense”. We briefly touch on the implications of AI/ML systems having comparative understanding, and how Roland and his team are addressing problems like getting properly labeled training data. <a href="https://twimlai.com/twiml-talk-111-learning-common-sense-physical-concepts-roland-memisevic/" class="uri">https://twimlai.com/twiml-talk-111-learning-common-sense-physical-concepts-roland-memisevic/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[April 12th &amp; 13th in Hamburg: Workshop on Deep Learning with Keras and TensorFlow in R]]></title>
    <link href="/2018/02/deep_learning_keras_tensorflow_18_04/"/>
    <id>/2018/02/deep_learning_keras_tensorflow_18_04/</id>
    <published>2018-02-06T00:00:00+00:00</published>
    <updated>2018-02-06T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Registration is now open for my 1.5-day <a href="https://www.eventbrite.de/e/workshop-deep-learning-mit-keras-und-tensorflow-tickets-42710095044?utm-medium=discovery&amp;utm-campaign=social&amp;utm-content=attendeeshare&amp;aff=escb&amp;utm-source=cp&amp;utm-term=listing">workshop on deep learning with Keras and TensorFlow using R</a>.</p>
<p>It will take place on <strong>April 12th and 13th</strong> in <strong>Hamburg, Germany</strong>.</p>
<p><a href="https://blog.codecentric.de/en/2018/02/deep-learning-workshop-bei-der-codecentric-ag-solingen/">You can read about one participant’s experience in my last workshop:</a></p>
<blockquote>
<p>Big Data – a buzz word you can find everywhere these days, from nerdy blogs to scientific research papers and even in the news. But how does Big Data Analysis work, exactly? In order to find that out, I attended the workshop on “Deep Learning with Keras and TensorFlow”. On a stormy Thursday afternoon, we arrived at the modern and light-flooded codecentric AG headquarters. There, we met performance expert Dieter Dirkes and Data Scientist Dr. Shirin Glander. In the following two days, Shirin gave us a hands-on introduction into the secrets of Deep Learning and helped us to program our first Neural Net. After a short round of introduction of the participants, it became clear that many different areas and domains are interested in Deep Learning: geologists want to classify (satellite) images, energy providers want to analyse time-series, insurers want to predict numbers and I – a humanities major – want to classify text. And codecentric employees were also interested in getting to know the possibilities of Deep Learning, so that a third of the participants were employees from the company itself.</p>
</blockquote>
<p><a href="https://blog.codecentric.de/en/2018/02/deep-learning-workshop-bei-der-codecentric-ag-solingen/">Continue reading…</a></p>
<p>In my workshop, you will learn</p>
<ul>
<li>the basics of deep learning</li>
<li>what cross-entropy and loss is</li>
<li>about activation functions</li>
<li>how to optimize weights and biases with backpropagation and gradient descent</li>
<li>how to build (deep) neural networks with Keras and TensorFlow</li>
<li>how to save and load models and model weights</li>
<li>how to visualize models with TensorBoard</li>
<li>how to make predictions on test data</li>
</ul>
<p>My slides and all material is in English, so I’m flexible with the language. If it’s all German participants it’ll be in German, if some prefer English it’ll be English or a mix of German and English so that everybody understands. :-)</p>
<p><a href="https://www.eventbrite.de/e/workshop-deep-learning-mit-keras-und-tensorflow-tickets-42710095044?utm-medium=discovery&amp;utm-campaign=social&amp;utm-content=attendeeshare&amp;aff=escb&amp;utm-source=cp&amp;utm-term=listing">Tickets can be booked via eventbrite</a>.</p>
<p><br></p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/keras_workshop_april18.png" />

</div>
<p>Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. Keras is very convenient for fast and easy prototyping of neural networks. It is highly modular and very flexible, so that you can build basically any type of neural network you want. It supports convolutional neural networks and recurrent neural networks, as well as combinations of both. Due to its layer structure, it is highly extensible and can run on CPU or GPU.</p>
<p>The <code>keras</code> R package provides an interface to the Python library of Keras, just as the tensorflow package provides an interface to TensorFlow. Basically, R creates a conda instance and runs Keras it it, while you can still use all the functionalities of R for plotting, etc. Almost all function names are the same, so models can easily be recreated in Python for deployment.</p>
<p><br></p>
<div class="figure">
<img src="https://blog.keras.io/img/keras-tensorflow-logo.jpg" />

</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Conferences, webinars, podcasts, workshops, books, articles and the likes]]></title>
    <link href="/page/conferences_podcasts_webinars/"/>
    <id>/page/conferences_podcasts_webinars/</id>
    <published>2018-02-01T16:06:06+02:00</published>
    <updated>2018-02-01T16:06:06+02:00</updated>
    <content type="html"><![CDATA[

<p>Here, you can find a list of all the talks I gave at conferences, webinars, podcasts, workshops, and all the other places you can and could hear me talk. You will also find a section with magazine articles and books I&rsquo;ve written. :-)</p>

<p><img src="https://secure.meetupstatic.com/s/img/5455565085016210254/logo/svg/logo--script.svg" alt="" /></p>

<h2 id="workshops-i-am-giving">Workshops I am giving</h2>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2017/11/deep_learning_keras_tensorflow/">Workshop on Deep Learning with Keras and TensorFlow in R</a></li>
</ul>

<blockquote>
<p>I offer a workshop on deep learning with Keras and TensorFlow using R.
Date and place depend on who and how many people are interested, so please contact me either directly or via the workshop page: <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/">https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/</a> (the description is in German but I also offer to give the workshop in English).</p>
</blockquote>

<p><a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/">The next dates for the workshop are: tbd</a></p>

<p><br></p>

<ul>
<li><a href="https://www.codecentric.de/schulung/end-2-end-vom-keras-tensorflow-modell-zur-produktion/">Workshop: End-2-End vom Keras TensorFlow-Modell zur Produktion</a></li>
</ul>

<blockquote>
<p>Durch das stark wachsende Datenvolumen hat sich das Rollenverständnis von Data Scientists erweitert. Statt Machine-Learning-Modelle für einmalige Analysen zu erstellen,  wird häufiger in konkreten Entwicklungsprojekten gearbeitet, in denen Prototypen in produktive Anwendungen überführt werden.
Keras ist eine High-Level-Schnittstelle, die ein schnelles, einfaches und flexibles Prototypisieren von Neuronalen Netzwerken mit TensorFlow ermöglicht. Zusammen mit Luigi lassen sich beliebig komplexe Datenverarbeitungs-Workflows in Python erstellen. Das führt dazu, dass auch Nicht-Entwickler den End-2-End-Workflow des Keras-TensorFlow-Modells zur Produktionsreife leicht
implementieren können.</p>
</blockquote>

<p><a href="https://www.codecentric.de/schulung/end-2-end-vom-keras-tensorflow-modell-zur-produktion/">Next dates: tbd</a></p>

<p><br></p>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/06/intro_to_ml_workshop_heidelberg/">Machine Learning Basics in R</a></li>
</ul>

<hr />

<p><br></p>

<h2 id="where-you-find-my-written-word">Where you find my written word</h2>

<ul>
<li><p><a href="https://shop.heise.de/katalog/blick-in-die-blackbox">An article about about <strong>Explaining Black-Box Machine Learning Models</strong> in the <strong>iX Developer Machine Learning</strong></a> (German)</p></li>

<li><p><a href="https://shop.heise.de/katalog/ix-12-2018">A short version of this article about <strong>Explaining Black-Box Machine Learning Models</strong> in the <strong>iX <sup>12</sup>&frasl;<sub>2018</sub></strong></a> (German)</p></li>
</ul>

<p><img src="https://shiring.github.io/netlify_images/ix_article_01.jpeg" alt="" /></p>

<ul>
<li>Book chapter Predictive Analytics (German)</li>
</ul>

<hr />

<p><br></p>

<h2 id="upcoming-talks-webinars-podcasts-etc">Upcoming talks, webinars, podcasts, etc.</h2>

<ul>
<li>tbd</li>
</ul>

<hr />

<p><br></p>

<h2 id="past-talks-webinars-podcasts-etc">Past talks, webinars, podcasts, etc.</h2>

<ul>
<li>At the <a href="https://shirinsplayground.netlify.com/2018/11/twimlai_meetup/">This week in machine learning and AI European online Meetup on December 5th, 2018</a>, I presented and led a discussion about the Anchors paper, the next generation of machine learning interpretability tools. You can find the slides <a href="https://shirinsplayground.netlify.com/2018/12/trust_in_ml_slides_ix/">here</a>.</li>
</ul>

<p><br></p>

<ul>
<li>On November 7th, Uwe Friedrichsen and I gave our talk from the <a href="https://shirinsplayground.netlify.com/2018/01/jax2018/">JAX conference 2018: Deep Learning - a Primer</a> again at the <a href="https://shirinsplayground.netlify.com/2018/01/slides_demystifying_dl/">W-JAX</a> in Munich.</li>
</ul>

<p><br></p>

<ul>
<li>While in London for the <a href="https://shirinsplayground.netlify.com/2018/10/mcubed_slides/">M-cubed conference</a>, I also gave a talk at the R-Ladies London Meetup about <a href="https://shirinsplayground.netlify.com/2018/09/rladieslondontalk/">Interpretable Deep Learning with R, Keras and LIME</a>.</li>
</ul>

<p><br></p>

<ul>
<li>From 15th to 17th October 2018, I was in London for the <a href="https://www.mcubed.london/">M-cubed conference</a> with my talk about <a href="https://shirinsplayground.netlify.com/2018/10/mcubed_slides/">Explaining complex machine learning models with LIME</a>.</li>
</ul>

<p><br></p>

<ul>
<li>Together with a colleague from codecentric, I gave a workshop about <a href="https://www.data2day.de/veranstaltung-6953-end-2-end-vom-keras-tensorflow-modell-zur-produktion.html?id=6953">&ldquo;END-2-END VOM KERAS TENSORFLOW-MODELL ZUR PRODUKTION&rdquo;</a> at the data2day conference, which was being held from September 25th - 27th 2018 in Heidelberg, Germany (German language):
<a href="https://www.data2day.de/veranstaltung-6953-end-2-end-vom-keras-tensorflow-modell-zur-produktion.html?id=6953">https://www.data2day.de/veranstaltung-6953-end-2-end-vom-keras-tensorflow-modell-zur-produktion.html?id=6953</a></li>
</ul>

<p><br></p>

<ul>
<li>On Wednesday, October 26th, I was talking about <a href="https://shirinsplayground.netlify.com/2018/09/ffm_datascience_meetup/">&lsquo;Decoding The Black Box&rsquo; at the Frankfurt Data Sciene Meetup</a>.
Slides can be found <a href="https://shirinsplayground.netlify.com/2018/09/ffm_datascience_meetup_slides/">here</a>.</li>
</ul>

<p><br></p>

<ul>
<li>At the <a href="https://www.ml-summit.de">ML Summit</a> held on October 1st and 2nd in Berlin, Germany, I gave a workshop about <a href="https://ml-summit.de/specialized-topics/bildklassifikation-leicht-gemacht-mit-keras-und-tensorflow/">image classification with Keras</a>: <a href="https://ml-summit.de/specialized-topics/bildklassifikation-leicht-gemacht-mit-keras-und-tensorflow/">https://ml-summit.de/specialized-topics/bildklassifikation-leicht-gemacht-mit-keras-und-tensorflow/</a> (German language)</li>
</ul>

<p><br></p>

<ul>
<li>In August 2018 I gave a webinar for SAP about <a href="https://shirinsplayground.netlify.com/2018/08/sap_webinar_slides/">Explaining Keras Image Classification Models with LIME</a>.</li>
</ul>

<p><br></p>

<ul>
<li>In June 2018 I gave a 3-hour workshop about the basics of machine learning with R at the University of Heidelberg, Germany. <a href="https://shirinsplayground.netlify.com/2018/06/intro_to_ml_workshop_heidelberg/">Slides and workshop code can be found here</a>.</li>
</ul>

<p><br></p>

<ul>
<li>In May 2018 I was at the <a href="http://unconf18.ropensci.org/">ROpenSci unconference in Seattle, WA</a>
You can read about my experience and the project I worked on <a href="https://shirinsplayground.netlify.com/2018/05/ropensci_unconf18/">here</a>.</li>
</ul>

<p><br></p>

<ul>
<li>At the <a href="https://aws.amazon.com/de/events/web-days-2018/">Amazon AWS AI &amp; Machine Learning Web Day on May 8th</a>, I gave a presentation on how to get started with Amazon SageMaker. The recording can be found on <a href="https://youtu.be/lQmcXAN7GJ4">YouTube</a>; slides are on <a href="https://www.slideshare.net/AWSAktuell/ai-machine-learning-web-day-wie-gelingt-der-einstieg-in-amazon-sagemaker-prsentiert-von-codecentric">Slideshare</a></li>
</ul>

<p><br></p>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/02/m3_2018/">I talked about explaining complex machine learning models at Minds Mastering Machines Conference</a> on Wednesday, April 25th 2018 in Colone
The presentation was in German but the slides were similar to those: <a href="https://shirinsplayground.netlify.com/2018/04/hh_datascience_meetup_2018_slides/">https://shirinsplayground.netlify.com/2018/04/hh_datascience_meetup_2018_slides/</a></li>
</ul>

<p><br></p>

<ul>
<li>My colleague Uwe Friedrichsen and I gave a talk at the <a href="https://shirinsplayground.netlify.com/2018/01/jax2018/">JAX conference 2018: Deep Learning - a Primer</a> on April 24th 2018 in Mainz.
Slides can be found here: <a href="https://www.slideshare.net/ShirinGlander/deep-learning-a-primer-95197733">https://www.slideshare.net/ShirinGlander/deep-learning-a-primer-95197733</a></li>
</ul>

<blockquote>
<p>Deep Learning is one of the &ldquo;hot&rdquo; topics in the AI area – a lot of hype, a lot of inflated expectation, but also quite some impressive success stories. As some AI experts already predict that Deep Learning will become &ldquo;Software 2.0&rdquo;, it might be a good time to have a closer look at the topic. In this session I will try to give a comprehensive overview of Deep Learning. We will start with a bit of history and some theoretical foundations that we will use to create a little Deep Learning taxonomy. Then we will have a look at current and upcoming application areas: Where can we apply Deep Learning successfully and what does it differentiate from other approaches? Afterwards we will examine the ecosystem: Which tools and libraries are available? What are their strengths and weaknesses? And to complete the session, we will look into some practical code examples and the typical pitfalls of Deep Learning. After this session you will have a much better idea of the why, what and how of Deep Learning, including if and how you might want to apply it to your own work. <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/">https://jax.de/big-data-machine-learning/deep-learning-a-primer/</a></p>
</blockquote>

<p><br></p>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/03/meetup_talk_ruhrpy_april_18/">I talked about explaining complex Machine Learning Models with LIME</a> at this meetup: <a href="https://www.meetup.com/Hamburg-Data-Science-Meetup/events/244145443/">Data Science Meetup Hamburg</a> on Thursday, April 12th 2018.
Slides can be found here: <a href="https://shirinsplayground.netlify.com/2018/04/hh_datascience_meetup_2018_slides/">https://shirinsplayground.netlify.com/2018/04/hh_datascience_meetup_2018_slides/</a></li>
</ul>

<blockquote>
<p>Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex machine learning models are essentially black boxes and too complicated to understand, we need to use approximations to get a better sense of how they work. One such approach is LIME, which stands for Local Interpretable Model-agnostic Explanations and is a tool that helps understand and explain the decisions made by complex machine learning models.
Dr. Shirin Glander is Data Scientist at codecentric AG. She has received a PhD in Bioinformatics and applies methods of analysis and visualization from different areas - for instance, machine learning, classical statistics, text mining, etc. -to extract and leverage information from data.</p>
</blockquote>

<p><br></p>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/03/meetup_talk_ruhrpy_april_18/">I talked about Deep Learning with Keras in R and Python</a> at this meetup: <a href="https://www.meetup.com/Ruhr-py/events/248093628/">Ruhr.Py 2018</a> on Wednesday, April 4th 2018.
Slides can be found here: <a href="https://shirinsplayground.netlify.com/2018/04/ruhrpy_meetup_2018_slides/">https://shirinsplayground.netlify.com/2018/04/ruhrpy_meetup_2018_slides/</a></li>
</ul>

<blockquote>
<p>Introducing Deep Learning with Keras and Python
Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. In this talk we build, train and visualize a Model using Python and Keras - all interactive with Jupyter Notebooks!</p>
</blockquote>

<p><br></p>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2018/02/herr_mies_wills_wissen/">I talked about machine learning with Daniel Mies (Podcast in German, though)</a></li>
</ul>

<blockquote>
<p>In January 2018 I was interviewed for a tech podcast where I talked about machine learning, neural nets, why I love R and Rstudio and how I became a Data Scientist.</p>
</blockquote>

<p><br></p>

<ul>
<li><a href="https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/">Explaining Predictions of Machine Learning Models with LIME - Münster Data Science Meetup</a></li>
</ul>

<blockquote>
<p>In December 2017 I talked about Explaining Predictions of Machine Learning Models with LIME at the Münster Data Science Meetup.</p>
</blockquote>

<p><br></p>

<ul>
<li><a href="https://shiring.github.io/blogging/2017/09/20/webinar_biology_to_data_science">From Biology to Industry. A Blogger’s Journey to Data Science</a></li>
</ul>

<blockquote>
<p>In September 2017 I gave a webinar for the Applied Epidemiology Didactic of the University of Wisconsin - Madison titled “From Biology to Industry. A Blogger’s Journey to Data Science.”
I talked about how blogging about R and Data Science helped me become a Data Scientist. I also gave a short introduction to Machine Learning, Big Data and Neural Networks.</p>
</blockquote>

<p><br></p>

<ul>
<li><a href="https://shiring.github.io/machine_learning/2017/03/31/webinar_code">Building meaningful machine learning models for disease prediction</a></li>
</ul>

<blockquote>
<p>In March 2017 I gave a webinar for the ISDS R Group about my work on building machine-learning models to predict the course of different diseases. I went over building a model, evaluating its performance, and answering or addressing different disease related questions using machine learning. My talk covered the theory of machine learning as it is applied using R.</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Announcing my talk about explainability of machine learning models at Minds Mastering Machines conference]]></title>
    <link href="/2018/02/m3_2018/"/>
    <id>/2018/02/m3_2018/</id>
    <published>2018-02-01T00:00:00+00:00</published>
    <updated>2018-02-01T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>On Wednesday, April 25th 2018 I am going to <a href="https://www.m3-konferenz.de/veranstaltung-6274-erkl%C3%A4rbarkeit-von-machine-learning%3A-wie-k%C3%B6nnen-wir-vertrauen-in-komplexe-modelle-schaffen.html?id=6274">talk about explainability of machine learning models at the Minds Mastering Machines conference in Cologne</a>. The conference will be in German, though.</p>
<div class="figure">
<img src="https://www.m3-konferenz.de/common/images/konferenzen/m3.png" />

</div>
<blockquote>
<p>ERKLÄRBARKEIT VON MACHINE LEARNING: WIE KÖNNEN WIR VERTRAUEN IN KOMPLEXE MODELLE SCHAFFEN?</p>
</blockquote>
<blockquote>
<p>Mit Machine-Learning getroffene Entscheidungen sind inhärent schwierig – wenn nicht gar unmöglich – nachzuvollziehen. Die Komplexität einiger der besten Modelle, wie Neuronale Netzwerke, ist genau das, was sie so erfolgreich macht. Aber es macht sie gleichzeitig zu einer Black Box. Das kann problematisch sein, denn Geschäftsführer oder Vorstände werden weniger geneigt sein einer Entscheidung zu vertrauen und nach ihr zu handeln, wenn sie sie nicht verstehen.</p>
</blockquote>
<blockquote>
<p>Local Interpretable Model-Agnostic Explanations (LIME) ist ein Versuch, diese komplexen Modelle zumindest teilweise nachvollziehbar zu machen. In diesem Vortrag erkläre ich das Prinzip und zeige Anwendungsbeispiele von LIME.</p>
</blockquote>
<blockquote>
<p>Vorkenntnisse Grundkenntnisse Machine Learning &amp; Statistik</p>
</blockquote>
<blockquote>
<p>Lernziele * Einblick in Möglichkeit, die komplexe Modelle erklärbar machen * Vertrauen in Entscheidungen durch Machine Learning schaffen</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[I talk about machine learning with Daniel Mies (Podcast in German, though)]]></title>
    <link href="/2018/02/herr_mies_wills_wissen/"/>
    <id>/2018/02/herr_mies_wills_wissen/</id>
    <published>2018-02-01T00:00:00+00:00</published>
    <updated>2018-02-01T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>For those of you out there who speak German:</p>
<p>I was interviewed for a tech podcast where I talked about machine learning, neural nets, why I love R and Rstudio and how I became a Data Scientist.</p>
<p>You can download and listen to the podcast here: <a href="https://mies.me/2018/01/31/hmww17-machine-learning-mit-dr-shirin-glander/" class="uri">https://mies.me/2018/01/31/hmww17-machine-learning-mit-dr-shirin-glander/</a></p>
<div class="figure">
<img src="https://mies.me/wp-content/cache/podlove/09/cad1c2bcc3b506410d277c27cc12fb/herr-mies-wills-wissen_500x500.png" />

</div>
<blockquote>
<p>In der aktuellen Episode gibt Dr. Shirin Glander (Twitter, Homepage) uns ein paar Einblicke in das Thema Machine Learning. Wir klären zunächst, was Machine Learning ist und welche Möglichkeiten es bietet bevor wir etwas mehr in die Tiefe gehen. Wir beginnen mit Neuronalen Netzen und Entscheidungsbäumen und wie sich diese unterschieden. Hier kommen wir natürlich auch nicht an Supervised Learning, Unsupervised Learning und Reinforcement Learning vorbei. Wichtig bei der Arbeit mit Machine Learning sind die verwendeten Daten: Hier beginnt man mit Testdaten und Trainingsdaten, welche man mit Hilfe von Feature Engineering für die jeweilige Aufgabe optimieren kann. Shirin erzählt, wie sie mit Daten arbeitet und wie sie die richtigen Algorithmen findet. Eine wichtige Rolle spielen hier R und R Studio, welches sich besonders für statistische Analysen eignet. Gerade die Visualisierung der Daten ist hier hilfreich um selbige besser zu verstehen. Aber auch die Möglichkeiten Reports zu erzeugen und beispielsweise als PDF zu exportieren überzeugen. Wenn ihr R für Machine Learning einsetzen wollt, solltet ihr Euch auch caret ansehen. Shirin organisiert übrigens auch MünsteR, die R Users group in Münster. Wenn ihr Euch näher mit Machine Learning beschäftigen wollt, solltet ihr Euch Datacamp oder Coursera ansehen. Wenn ihr Euch für R interessiert schaut Euch die R Bloggers an Am Ende sprechen wir auch noch kurz über Deep Dreaming. Den passenden Generator hierfür, findet ihr unter deepdreamgenerator.com.</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[JAX 2018 talk announcement: Deep Learning - a Primer]]></title>
    <link href="/2018/01/jax2018/"/>
    <id>/2018/01/jax2018/</id>
    <published>2018-01-30T00:00:00+00:00</published>
    <updated>2018-01-30T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I am happy to announce that on Tuesday, April 24th 2018 Uwe Friedrichsen and I will give a talk about <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/">Deep Learning - a Primer</a> at the JAX conference in Mainz, Germany.</p>
<blockquote>
<p>Deep Learning is one of the “hot” topics in the AI area – a lot of hype, a lot of inflated expectation, but also quite some impressive success stories. As some AI experts already predict that Deep Learning will become “Software 2.0”, it might be a good time to have a closer look at the topic. In this session I will try to give a comprehensive overview of Deep Learning. We will start with a bit of history and some theoretical foundations that we will use to create a little Deep Learning taxonomy. Then we will have a look at current and upcoming application areas: Where can we apply Deep Learning successfully and what does it differentiate from other approaches? Afterwards we will examine the ecosystem: Which tools and libraries are available? What are their strengths and weaknesses? And to complete the session, we will look into some practical code examples and the typical pitfalls of Deep Learning. After this session you will have a much better idea of the why, what and how of Deep Learning, including if and how you might want to apply it to your own work. <a href="https://jax.de/big-data-machine-learning/deep-learning-a-primer/" class="uri">https://jax.de/big-data-machine-learning/deep-learning-a-primer/</a></p>
</blockquote>
<div class="figure">
<img src="https://pbs.twimg.com/media/DUt3SXyUQAE3TOv.jpg" alt="https://twitter.com/jaxcon/status/957990506331557890" />
<p class="caption"><a href="https://twitter.com/jaxcon/status/957990506331557890" class="uri">https://twitter.com/jaxcon/status/957990506331557890</a></p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #94: Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley]]></title>
    <link href="/2018/01/twimlai94/"/>
    <id>/2018/01/twimlai94/</id>
    <published>2018-01-28T00:00:00+00:00</published>
    <updated>2018-01-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley</em>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai94.jpg" alt="Sketchnotes from TWiMLAI talk #94: Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley" />
<p class="caption">Sketchnotes from TWiMLAI talk #94: Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-94-neuroevolution-evolving-novel-neural-network-architectures-kenneth-stanley/">here</a>.</p>
<blockquote>
<p>Kenneth studied under TWiML Talk #47 guest Risto Miikkulainen at UT Austin, and joined Uber AI Labs after Geometric Intelligence , the company he co-founded with Gary Marcus and others, was acquired in late 2016. Kenneth’s research focus is what he calls Neuroevolution, applies the idea of genetic algorithms to the challenge of evolving neural network architectures. In this conversation, we discuss the Neuroevolution of Augmenting Topologies (or NEAT) paper that Kenneth authored along with Risto, which won the 2017 International Society for Artificial Life’s Award for Outstanding Paper of the Decade 2002 – 2012. We also cover some of the extensions to that approach he’s created since, including, HyperNEAT, which can efficiently evolve very large networks with connectivity patterns that look more like those of the human and that are generally much larger than what prior approaches to neural learning could produce, and novelty search, an approach which unlike most evolutionary algorithms has no defined objective, but rather simply searches for novel behaviors. We also cover concepts like “Complexification” and “Deception”, biology vs computation including differences and similarities, and some of his other work including his book, and NERO, a video game complete with Real-time Neuroevolution. This is a meaty “Nerd Alert” interview that I think you’ll really enjoy. <a href="https://twimlai.com/twiml-talk-94-neuroevolution-evolving-novel-neural-network-architectures-kenneth-stanley/" class="uri">https://twimlai.com/twiml-talk-94-neuroevolution-evolving-novel-neural-network-architectures-kenneth-stanley/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Join MünsteR for our next meetup on obtaining functional implications of gene expression data with R]]></title>
    <link href="/2018/01/meetup_march18/"/>
    <id>/2018/01/meetup_march18/</id>
    <published>2018-01-24T00:00:00+00:00</published>
    <updated>2018-01-24T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://shiring.github.io/r_users_group/2017/05/20/map.png" />

</div>
<p>In our <a href="http://meetu.ps/e/DDY1B/w54bW/f">next MünsteR R-user group meetup</a> on <strong>March 5th, 2018</strong> Frank Rühle will talk about bioinformatics and how to analyse genome data.</p>
<p>You can RSVP here: <a href="http://meetu.ps/e/DDY1B/w54bW/f" class="uri">http://meetu.ps/e/DDY1B/w54bW/f</a></p>
<blockquote>
<p>Next-Generation sequencing and array-based technologies provided a plethora of gene expression data in the public genomics databases. But how to get meaningful information and functional implications out of this vast amount of data? Various R-packages have been published by the Bioconductor user community for distinct kinds of analysis strategies. Here, several approaches will be presented for functional gene annotation, gene enrichment analysis and co-expression network analysis. A collection of wrapper functions for streamlined analysis of expression data can be found at: <a href="https://github.com/frankRuehle/systemsbio" class="uri">https://github.com/frankRuehle/systemsbio</a>.</p>
</blockquote>
<p>Dr. Frank Rühle is a post-doctoral research fellow in the group of genetic epidemiology at the Institute of Human Genetics at the University of Münster. As biologist with focus on computational biology he aims at identifying genomic biomarker for complex cardiovascular diseases by analyzing multiomics data with respect to a systems biology view. Further research interests include the functions of long non-coding RNAs and their impact on gene regulation in heart-related phenotypes.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #92: Learning State Representations with Yael Niv]]></title>
    <link href="/2018/01/twimlai92/"/>
    <id>/2018/01/twimlai92/</id>
    <published>2018-01-19T00:00:00+00:00</published>
    <updated>2018-01-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about Learning State Representations with Yael Niv: <a href="https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/" class="uri">https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/</a></p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai92.jpg" alt="Sketchnotes from TWiMLAI talk #92: Learning State Representations with Yael Niv" />
<p class="caption">Sketchnotes from TWiMLAI talk #92: Learning State Representations with Yael Niv</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/">here</a>.</p>
<blockquote>
<p>In this interview Yael and I explore the relationship between neuroscience and machine learning. In particular, we discusses the importance of state representations in human learning, some of her experimental results in this area, and how a better understanding of representation learning can lead to insights into machine learning problems such as reinforcement and transfer learning. Did I mention this was a nerd alert show? I really enjoyed this interview and I know you will too. Be sure to send over any thoughts or feedback via the show notes page. <a href="https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/" class="uri">https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[How to make your machine learning model available as an API with the plumber package]]></title>
    <link href="/2018/01/plumber/"/>
    <id>/2018/01/plumber/</id>
    <published>2018-01-16T00:00:00+00:00</published>
    <updated>2018-01-16T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://d33wubrfki0l68.cloudfront.net/03f3d9156c39a8043be42caeee1507d43d832472/8d968/components/images/plumber.png" />

</div>
<p>The <strong>plumber</strong> package for R makes it easy to expose existing R code as a webservice via an API (<a href="https://www.rplumber.io/" class="uri">https://www.rplumber.io/</a>, Trestle Technology, LLC 2017).</p>
<p>You take an existing R script and make it accessible with <code>plumber</code> by simply adding a few lines of comments. If you have worked with Roxygen before, e.g. when building a package, you will already be familiar with the core concepts. If not, here are the most important things to know:</p>
<ul>
<li>you define the output or endpoint</li>
<li>you can add additional annotation to customize your input, output and other functionalities of your API</li>
<li>you can define every input parameter that will go into your function</li>
<li>every such annotation will begin with either <code>#'</code> or <code>#*</code></li>
</ul>
<p>With this setup, we can take a trained machine learning model and make it available as an API. With this API, other programs can access it and use it to make predictions.</p>
<div id="what-are-apis-and-webservices" class="section level2">
<h2>What are APIs and webservices?</h2>
<p>With <code>plumber</code>, we can build so called <strong>HTTP APIs</strong>. HTTP stands for Hypertext Transfer Protocol and is used to transmit information on the web; API stands for Application Programming Interface and governs the connection between some software and underlying applications. Software can then communicate via HTTP APIs. This way, our R script can be called from other software, even if the other program is not written in R and we have built a tool for machine-to-machine communication, i.e. a webservice.</p>
</div>
<div id="how-to-convert-your-r-script-into-an-api-with-plumber" class="section level2">
<h2>How to convert your R script into an API with plumber</h2>
<div id="training-and-saving-a-model" class="section level3">
<h3>Training and saving a model</h3>
<p>Let’s say we have trained a machine learning model as in <a href="https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/">this post about LIME</a>. I loaded a data set on chronic kidney disease, did some preprocessing (converting categorical features into dummy variables, scaling and centering), split it into training and test data and trained a Random Forest model with <code>caret</code>. We can use this trained model to make predictions for one test case with the following code:</p>
<pre class="r"><code>library(tidyverse)

# load test and train data
load(&quot;../../data/test_data.RData&quot;)
load(&quot;../../data/train_data.RData&quot;)

# load model
load(&quot;../../data/model_rf.RData&quot;)

# take first test case for prediction
input_data &lt;- test_data[1, ] %&gt;%
  select(-class)

# predict test case using model
pred &lt;- predict(model_rf, input_data)
cat(&quot;----------------\nTest case predicted to be&quot;, as.character(pred), &quot;\n----------------&quot;)</code></pre>
<pre><code>## ----------------
## Test case predicted to be ckd 
## ----------------</code></pre>
</div>
<div id="the-input" class="section level3">
<h3>The input</h3>
<p>For our API to work, we need to define the input, in our case the features of the test data. When we look at the model object, we see that it expects the following parameters:</p>
<pre class="r"><code>var_names &lt;- model_rf$finalModel$xNames
var_names</code></pre>
<pre><code>##  [1] &quot;age&quot;            &quot;bp&quot;             &quot;sg_1.005&quot;       &quot;sg_1.010&quot;      
##  [5] &quot;sg_1.015&quot;       &quot;sg_1.020&quot;       &quot;sg_1.025&quot;       &quot;al_0&quot;          
##  [9] &quot;al_1&quot;           &quot;al_2&quot;           &quot;al_3&quot;           &quot;al_4&quot;          
## [13] &quot;al_5&quot;           &quot;su_0&quot;           &quot;su_1&quot;           &quot;su_2&quot;          
## [17] &quot;su_3&quot;           &quot;su_4&quot;           &quot;su_5&quot;           &quot;rbc_normal&quot;    
## [21] &quot;rbc_abnormal&quot;   &quot;pc_normal&quot;      &quot;pc_abnormal&quot;    &quot;pcc_present&quot;   
## [25] &quot;pcc_notpresent&quot; &quot;ba_present&quot;     &quot;ba_notpresent&quot;  &quot;bgr&quot;           
## [29] &quot;bu&quot;             &quot;sc&quot;             &quot;sod&quot;            &quot;pot&quot;           
## [33] &quot;hemo&quot;           &quot;pcv&quot;            &quot;wbcc&quot;           &quot;rbcc&quot;          
## [37] &quot;htn_yes&quot;        &quot;htn_no&quot;         &quot;dm_yes&quot;         &quot;dm_no&quot;         
## [41] &quot;cad_yes&quot;        &quot;cad_no&quot;         &quot;appet_good&quot;     &quot;appet_poor&quot;    
## [45] &quot;pe_yes&quot;         &quot;pe_no&quot;          &quot;ane_yes&quot;        &quot;ane_no&quot;</code></pre>
<p>Good practice is to write the input parameter definition into you <a href="https://swagger.io/swagger-ui/">API Swagger UI</a>, but the code would work without these annotations. We define the parameters by annotating them with name and description in our R-script using <code>@parameter</code>. For this purpose, I want to know the type and min/max values for each of my variables in the training data. Because categorical data has been converted to dummy variables and then scaled and centered, these values will all be numeric and between 0 and 1 in this example. If I would build this script for a real case, I’d use the raw data as input and add a preprocessing function to my script, though!</p>
<pre class="r"><code># show parameter definition for the first three features
for (i in 1:3) {
# if you wanted to see it for all features, use
#for (i in 1:length(var_names)) {
  var &lt;- var_names[i]
  train_data_subs &lt;- train_data[, which(colnames(train_data) == var)]
  type &lt;- class(train_data_subs)
  
  if (type == &quot;numeric&quot;) {
    min &lt;- min(train_data_subs)
    max &lt;- max(train_data_subs)
  }
  
  cat(&quot;Variable:&quot;, var, &quot;is of type:&quot;, type, &quot;\n&quot;,
      &quot;Min value in training data =&quot;, min, &quot;\n&quot;,
      &quot;Max value in training data =&quot;, max, &quot;\n----------\n&quot;)

}</code></pre>
<pre><code>## Variable: age is of type: numeric 
##  Min value in training data = 0 
##  Max value in training data = 0.9777778 
## ----------
## Variable: bp is of type: numeric 
##  Min value in training data = 0 
##  Max value in training data = 0.7222222 
## ----------
## Variable: sg_1.005 is of type: numeric 
##  Min value in training data = 0 
##  Max value in training data = 1 
## ----------</code></pre>
<blockquote>
<p>Unless otherwise instructed, all parameters passed into plumber endpoints from query strings or dynamic paths will be character strings. <a href="https://www.rplumber.io/docs/routing-and-input.html#typed-dynamic-routes" class="uri">https://www.rplumber.io/docs/routing-and-input.html#typed-dynamic-routes</a></p>
</blockquote>
<p>This means that we need to convert numeric values before we process them further. Or we can define the parameter type explicitly, e.g. by writing <code>variable_1:numeric</code> if we want to specifiy that <em>variable_1</em> is supposed to be numeric.</p>
<p>To make sure that the model will perform as expected, it is also advisable to add a few validation functions. Here, I will validate</p>
<ul>
<li>whether every parameter is numeric/integer by checking for NAs (which would have resulted from <code>as.numeric()</code>/<code>as.integer()</code> applied to data of character type)</li>
<li>whether every parameter is between 0 and 1</li>
</ul>
<p>In order for <code>plumber</code> to work with our input, it needs to be part of the HTTP request, which can then be routed to our R function. The <a href="https://www.rplumber.io/docs/routing-and-input.html#query-strings">plumber documentation</a> describes how to use query strings as inputs. But in our case, manually writing query strings is not practical because we have so many parameters. Of course, there are programs that let us generate query strings but the easiest way to format the input from a line of data I found is to use JSON.</p>
<p>The <code>toJSON()</code> function from the <code>rjson</code> package converts our input line to JSON format:</p>
<pre class="r"><code>library(rjson)
test_case_json &lt;- toJSON(input_data)
cat(test_case_json)</code></pre>
<pre><code>## {&quot;age&quot;:0.511111111111111,&quot;bp&quot;:0.111111111111111,&quot;sg_1.005&quot;:1,&quot;sg_1.010&quot;:0,&quot;sg_1.015&quot;:0,&quot;sg_1.020&quot;:0,&quot;sg_1.025&quot;:0,&quot;al_0&quot;:0,&quot;al_1&quot;:0,&quot;al_2&quot;:0,&quot;al_3&quot;:0,&quot;al_4&quot;:1,&quot;al_5&quot;:0,&quot;su_0&quot;:1,&quot;su_1&quot;:0,&quot;su_2&quot;:0,&quot;su_3&quot;:0,&quot;su_4&quot;:0,&quot;su_5&quot;:0,&quot;rbc_normal&quot;:1,&quot;rbc_abnormal&quot;:0,&quot;pc_normal&quot;:0,&quot;pc_abnormal&quot;:1,&quot;pcc_present&quot;:1,&quot;pcc_notpresent&quot;:0,&quot;ba_present&quot;:0,&quot;ba_notpresent&quot;:1,&quot;bgr&quot;:0.193877551020408,&quot;bu&quot;:0.139386189258312,&quot;sc&quot;:0.0447368421052632,&quot;sod&quot;:0.653374233128834,&quot;pot&quot;:0,&quot;hemo&quot;:0.455056179775281,&quot;pcv&quot;:0.425925925925926,&quot;wbcc&quot;:0.170454545454545,&quot;rbcc&quot;:0.225,&quot;htn_yes&quot;:1,&quot;htn_no&quot;:0,&quot;dm_yes&quot;:0,&quot;dm_no&quot;:1,&quot;cad_yes&quot;:0,&quot;cad_no&quot;:1,&quot;appet_good&quot;:0,&quot;appet_poor&quot;:1,&quot;pe_yes&quot;:1,&quot;pe_no&quot;:0,&quot;ane_yes&quot;:1,&quot;ane_no&quot;:0}</code></pre>
</div>
<div id="defining-the-endpoint-and-output" class="section level3">
<h3>Defining the endpoint and output</h3>
<p>In order to convert this very simple script into an API, we need to define the endpoint(s). Endpoints will return an output, in our case it will return the output of the <code>predict()</code> function pasted into a line of text (e.g. “Test case predicted to be ckd”). Here, we want to have the predictions returned, so we annotate the entire function with <code>@get</code>. This endpoint in the API will get a custom name, so that we can call it later; here we call it <code>predict</code> and therefore write <code>#' @get /predict</code>.</p>
<blockquote>
<p>According to the design of the HTTP specification, GET (along with HEAD) requests are used only to read data and not change it. Therefore, when used this way, they are considered safe. That is, they can be called without risk of data modification or corruption — calling it once has the same effect as calling it 10 times, or none at all. Additionally, GET (and HEAD) is idempotent, which means that making multiple identical requests ends up having the same result as a single request. <a href="http://www.restapitutorial.com/lessons/httpmethods.html" class="uri">http://www.restapitutorial.com/lessons/httpmethods.html</a></p>
</blockquote>
<p>In this case, we could also consider using <code>@post</code> to avoid caching issues, but for this example I’ll leave it as <code>@get</code>.</p>
<blockquote>
<p>The POST verb is most-often utilized to <strong>create</strong> new resources. In particular, it’s used to create subordinate resources. That is, subordinate to some other (e.g. parent) resource. In other words, when creating a new resource, POST to the parent and the service takes care of associating the new resource with the parent, assigning an ID (new resource URI), etc. On successful creation, return HTTP status 201, returning a Location header with a link to the newly-created resource with the 201 HTTP status. POST is neither safe nor idempotent. It is therefore recommended for non-idempotent resource requests. Making two identical POST requests will most-likely result in two resources containing the same information. <a href="http://www.restapitutorial.com/lessons/httpmethods.html" class="uri">http://www.restapitutorial.com/lessons/httpmethods.html</a></p>
</blockquote>
<p>We can also customize the output. Keep in mind though, that the output should be <a href="https://www.rplumber.io/docs/rendering-and-output.html#serializers">“serialized”</a>. By default, the output will be in JSON format. Here, I want to have a text output, so I’ll specify <code>@html</code> without html formatting specifications, although I could add them if I wanted to display the text on a website. If we were to <a href="https://www.rplumber.io/docs/runtime.html#external-data-store">store the data in a database</a>, however, this would not be a good idea. In that case, it would be better to output the result as a JSON object.</p>
</div>
<div id="logging-with-filters" class="section level3">
<h3>Logging with filters</h3>
<p>It is also useful to provide some sort of logging for your API. Here, I am using the simple example from the <a href="https://www.rplumber.io/docs/routing-and-input.html#filters">plumber documentation</a> that uses filters and output the logs to the console or your API server logs. You could also <a href="https://www.rplumber.io/docs/runtime.html#file-system">write your logging output to a file</a>. In production, it would be better to use a real logging setup that stores information about each request, e.g. the time stamp, whether any errors or warnings occurred, etc. The <code>forward()</code> part of the logging function passes control on to the next handler in the pipeline, here our predict function.</p>
</div>
<div id="running-the-plumber-script" class="section level3">
<h3>Running the plumber script</h3>
<p>We need to save the entire script with annotations as an <em>.R</em> file as seen below. The regular comments <code>#</code> describe what each section does.</p>
<pre><code># script name:
# plumber.R

# set API title and description to show up in http://localhost:8000/__swagger__/

#&#39; @apiTitle Run predictions for Chronic Kidney Disease with Random Forest Model
#&#39; @apiDescription This API takes as patient data on Chronic Kidney Disease and returns a prediction whether the lab values
#&#39; indicate Chronic Kidney Disease (ckd) or not (notckd).
#&#39; For details on how the model is built, see https://shirinsplayground.netlify.com/2017/12/lime_sketchnotes/
#&#39; For further explanations of this plumber function, see https://shirinsplayground.netlify.com/2018/01/plumber/

# load model
# this path would have to be adapted if you would deploy this
load(&quot;/Users/shiringlander/Documents/Github/shirinsplayground/data/model_rf.RData&quot;)

#&#39; Log system time, request method and HTTP user agent of the incoming request
#&#39; @filter logger
function(req){
  cat(&quot;System time:&quot;, as.character(Sys.time()), &quot;\n&quot;,
      &quot;Request method:&quot;, req$REQUEST_METHOD, req$PATH_INFO, &quot;\n&quot;,
      &quot;HTTP user agent:&quot;, req$HTTP_USER_AGENT, &quot;@&quot;, req$REMOTE_ADDR, &quot;\n&quot;)
  plumber::forward()
}

# core function follows below:
# define parameters with type and description
# name endpoint
# return output as html/text
# specify 200 (okay) return

#&#39; predict Chronic Kidney Disease of test case with Random Forest model
#&#39; @param age:numeric The age of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param bp:numeric The blood pressure of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param sg_1.005:int The urinary specific gravity of the patient, integer (1: sg = 1.005, otherwise 0)
#&#39; @param sg_1.010:int The urinary specific gravity of the patient, integer (1: sg = 1.010, otherwise 0)
#&#39; @param sg_1.015:int The urinary specific gravity of the patient, integer (1: sg = 1.015, otherwise 0)
#&#39; @param sg_1.020:int The urinary specific gravity of the patient, integer (1: sg = 1.020, otherwise 0)
#&#39; @param sg_1.025:int The urinary specific gravity of the patient, integer (1: sg = 1.025, otherwise 0)
#&#39; @param al_0:int The urine albumin level of the patient, integer (1: al = 0, otherwise 0)
#&#39; @param al_1:int The urine albumin level of the patient, integer (1: al = 1, otherwise 0)
#&#39; @param al_2:int The urine albumin level of the patient, integer (1: al = 2, otherwise 0)
#&#39; @param al_3:int The urine albumin level of the patient, integer (1: al = 3, otherwise 0)
#&#39; @param al_4:int The urine albumin level of the patient, integer (1: al = 4, otherwise 0)
#&#39; @param al_5:int The urine albumin level of the patient, integer (1: al = 5, otherwise 0)
#&#39; @param su_0:int The sugar level of the patient, integer (1: su = 0, otherwise 0)
#&#39; @param su_1:int The sugar level of the patient, integer (1: su = 1, otherwise 0)
#&#39; @param su_2:int The sugar level of the patient, integer (1: su = 2, otherwise 0)
#&#39; @param su_3:int The sugar level of the patient, integer (1: su = 3, otherwise 0)
#&#39; @param su_4:int The sugar level of the patient, integer (1: su = 4, otherwise 0)
#&#39; @param su_5:int The sugar level of the patient, integer (1: su = 5, otherwise 0)
#&#39; @param rbc_normal:int The red blood cell count of the patient, integer (1: rbc = normal, otherwise 0)
#&#39; @param rbc_abnormal:int The red blood cell count of the patient, integer (1: rbc = abnormal, otherwise 0)
#&#39; @param pc_normal:int The pus cell level of the patient, integer (1: pc = normal, otherwise 0)
#&#39; @param pc_abnormal:int The pus cell level of the patient, integer (1: pc = abnormal, otherwise 0)
#&#39; @param pcc_present:int The puc cell clumps status of the patient, integer (1: pcc = present, otherwise 0)
#&#39; @param pcc_notpresent:int The puc cell clumps status of the patient, integer (1: pcc = notpresent, otherwise 0)
#&#39; @param ba_present:int The bacteria status of the patient, integer (1: ba = present, otherwise 0)
#&#39; @param ba_notpresent:int The bacteria status of the patient, integer (1: ba = notpresent, otherwise 0)
#&#39; @param bgr:numeric The blood glucose random level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param bu:numeric The blood urea level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param sc:numeric The serum creatinine level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param sod:numeric The sodium level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param pot:numeric The potassium level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param hemo:numeric The hemoglobin level of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param pcv:numeric The packed cell volume of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param wbcc:numeric The white blood cell count of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param rbcc:numeric The red blood cell count of the patient, numeric (scaled and centered to be btw 0 and 1)
#&#39; @param htn_yes:int The hypertension status of the patient, integer (1: htn = yes, otherwise 0)
#&#39; @param htn_no:int The hypertension status of the patient, integer (1: htn = no, otherwise 0)
#&#39; @param dm_yes:int The diabetes mellitus status of the patient, integer (1: dm = yes, otherwise 0)
#&#39; @param dm_no:int The diabetes mellitus status of the patient, integer (1: dm = no, otherwise 0)
#&#39; @param cad_yes:int The coronary artery disease status of the patient, integer (1: cad = yes, otherwise 0)
#&#39; @param cad_no:int The coronary artery disease status of the patient, integer (1: cad = no, otherwise 0)
#&#39; @param appet_good:int The appetite of the patient, integer (1: appet = good, otherwise 0)
#&#39; @param appet_poor:int The appetite of the patient, integer (1: appet = poor, otherwise 0)
#&#39; @param pe_yes:int The pedal edema status of the patient, integer (1: pe = yes, otherwise 0)
#&#39; @param pe_no:int The pedal edema status of the patient, integer (1: pe = no, otherwise 0)
#&#39; @param ane_yes:int The anemia status of the patient, integer (1: ane = yes, otherwise 0)
#&#39; @param ane_no:int The anemia status of the patient, integer (1: ane = no, otherwise 0)
#&#39; @get /predict
#&#39; @html
#&#39; @response 200 Returns the class (ckd or notckd) prediction from the Random Forest model; ckd = Chronic Kidney Disease
calculate_prediction &lt;- function(age, bp, sg_1.005, sg_1.010, sg_1.015, sg_1.020, sg_1.025, al_0, al_1, al_2, 
                                al_3, al_4, al_5, su_0, su_1, su_2, su_3, su_4, su_5, rbc_normal, rbc_abnormal, pc_normal, pc_abnormal,
                                pcc_present, pcc_notpresent, ba_present, ba_notpresent, bgr, bu, sc, sod, pot, hemo, pcv, 
                                wbcc, rbcc, htn_yes, htn_no, dm_yes, dm_no, cad_yes, cad_no, appet_good, appet_poor, pe_yes, pe_no, 
                                ane_yes, ane_no) {
  
  # make data frame from numeric parameters
  input_data_num &lt;&lt;- data.frame(age, bp, bgr, bu, sc, sod, pot, hemo, pcv, wbcc, rbcc,
                     stringsAsFactors = FALSE)
  # and make sure they really are numeric
  input_data_num &lt;&lt;- as.data.frame(t(sapply(input_data_num, as.numeric)))
  
  # make data frame from (binary) integer parameters
  input_data_int &lt;&lt;- data.frame(sg_1.005, sg_1.010, sg_1.015, sg_1.020, sg_1.025, al_0, al_1, al_2, 
                                al_3, al_4, al_5, su_0, su_1, su_2, su_3, su_4, su_5, rbc_normal, rbc_abnormal, pc_normal, pc_abnormal,
                                pcc_present, pcc_notpresent, ba_present, ba_notpresent, htn_yes, htn_no, dm_yes, dm_no, 
                                cad_yes, cad_no, appet_good, appet_poor, pe_yes, pe_no, ane_yes, ane_no,
                                stringsAsFactors = FALSE)
  # and make sure they really are numeric
  input_data_int &lt;&lt;- as.data.frame(t(sapply(input_data_int, as.integer)))
  # combine into one data frame
  input_data &lt;&lt;- as.data.frame(cbind(input_data_num, input_data_int))
  
  # validation for parameter
  if (any(is.na(input_data))) {
    res$status &lt;- 400
    res$body &lt;- &quot;Parameters have to be numeric or integers&quot;
  }
  
  if (any(input_data &lt; 0) || any(input_data &gt; 1)) {
    res$status &lt;- 400
    res$body &lt;- &quot;Parameters have to be between 0 and 1&quot;
  }

  # predict and return result
  pred_rf &lt;&lt;- predict(model_rf, input_data)
  paste(&quot;----------------\nTest case predicted to be&quot;, as.character(pred_rf), &quot;\n----------------\n&quot;)
}
</code></pre>
<p>Note that I am using the “double-assignment” operator <code>&lt;&lt;-</code> in my function, because I want to make sure that objects are overwritten at the top level (i.e. globally). This would have been relevant had I set a global parameter, but to show it the example, I decided to use it here as well.</p>
<p>We can now call our script with the <code>plumb()</code> function, run it with <code>run()</code> and open it on port 800. Calling <code>plumb()</code> creates an environment in which all our functions are evaluated.</p>
<pre class="r"><code>library(plumber)</code></pre>
<pre class="r"><code>r &lt;- plumb(&quot;/Users/shiringlander/Documents/Github/shirinsplayground/static/scripts/plumber.R&quot;)
r$run(port = 8000)</code></pre>
<p>We will now see the following message in our R console:</p>
<pre><code>Starting server to listen on port 8000
Running the swagger UI at http://127.0.0.1:8000/__swagger__/</code></pre>
<p>If you go to *<a href="http://localhost:8000/__swagger__/*" class="uri">http://localhost:8000/__swagger__/*</a>, you could now try out the function by manually choosing values for all the parameters we defined in the script.</p>
<p><img src="https://shiring.github.io/netlify_images/swagger1.png" alt="http://localhost:8000/__swagger__/" /> … <img src="https://shiring.github.io/netlify_images/swagger2.png" alt="http://localhost:8000/__swagger__/ continued" /></p>
<p>Because we annotated the <code>calculate_prediction()</code> function in our script with <code>#' @get /predict</code> we can access it via *<a href="http://localhost:8000/predict*" class="uri">http://localhost:8000/predict*</a>. But because we have no input specified as of yet, we will only see an error on this site. So, we still need to put our JSON formatted input into the function. To do this, we can use <a href="https://en.wikipedia.org/wiki/CURL"><em>curl</em></a> from the terminal and feed in the JSON string from above. If you are using RStudio in the latest version, you have a handy terminal window open in your working directory. You find it right next to the Console.</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/terminal_rstudio.png" alt="Terminal in RStudio" />
<p class="caption">Terminal in RStudio</p>
</div>
<pre><code>curl -H &quot;Content-Type: application/json&quot; -X GET -d &#39;{&quot;age&quot;:0.511111111111111,&quot;bp&quot;:0.111111111111111,&quot;sg_1.005&quot;:1,&quot;sg_1.010&quot;:0,&quot;sg_1.015&quot;:0,&quot;sg_1.020&quot;:0,&quot;sg_1.025&quot;:0,&quot;al_0&quot;:0,&quot;al_1&quot;:0,&quot;al_2&quot;:0,&quot;al_3&quot;:0,&quot;al_4&quot;:1,&quot;al_5&quot;:0,&quot;su_0&quot;:1,&quot;su_1&quot;:0,&quot;su_2&quot;:0,&quot;su_3&quot;:0,&quot;su_4&quot;:0,&quot;su_5&quot;:0,&quot;rbc_normal&quot;:1,&quot;rbc_abnormal&quot;:0,&quot;pc_normal&quot;:0,&quot;pc_abnormal&quot;:1,&quot;pcc_present&quot;:1,&quot;pcc_notpresent&quot;:0,&quot;ba_present&quot;:0,&quot;ba_notpresent&quot;:1,&quot;bgr&quot;:0.193877551020408,&quot;bu&quot;:0.139386189258312,&quot;sc&quot;:0.0447368421052632,&quot;sod&quot;:0.653374233128834,&quot;pot&quot;:0,&quot;hemo&quot;:0.455056179775281,&quot;pcv&quot;:0.425925925925926,&quot;wbcc&quot;:0.170454545454545,&quot;rbcc&quot;:0.225,&quot;htn_yes&quot;:1,&quot;htn_no&quot;:0,&quot;dm_yes&quot;:0,&quot;dm_no&quot;:1,&quot;cad_yes&quot;:0,&quot;cad_no&quot;:1,&quot;appet_good&quot;:0,&quot;appet_poor&quot;:1,&quot;pe_yes&quot;:1,&quot;pe_no&quot;:0,&quot;ane_yes&quot;:1,&quot;ane_no&quot;:0}&#39; &quot;http://localhost:8000/predict&quot;</code></pre>
<blockquote>
<p><strong>-H</strong> defines an extra header to include in the request when sending HTTP to a server (<a href="https://curl.haxx.se/docs/manpage.html#-H" class="uri">https://curl.haxx.se/docs/manpage.html#-H</a>).</p>
</blockquote>
<blockquote>
<p><strong>-X</strong> pecifies a custom request method to use when communicating with the HTTP server (<a href="https://curl.haxx.se/docs/manpage.html#-X" class="uri">https://curl.haxx.se/docs/manpage.html#-X</a>).</p>
</blockquote>
<blockquote>
<p><strong>-d</strong> sends the specified data in a request to the HTTP server, in the same way that a browser does when a user has filled in an HTML form and presses the submit button. This will cause curl to pass the data to the server using the content-type application/x-www-form-urlencoded (<a href="https://curl.haxx.se/docs/manpage.html#-d" class="uri">https://curl.haxx.se/docs/manpage.html#-d</a>).</p>
</blockquote>
<p>This will return the following output:</p>
<ul>
<li><code>cat()</code> outputs to the R console if you use R interactively; if you use R on a server, it will be included in the server logs.</li>
</ul>
<pre><code>System time: 2018-01-15 13:34:32 
 Request method: GET /predict 
 HTTP user agent: curl/7.54.0 @ 127.0.0.1 </code></pre>
<ul>
<li><code>paste</code> outputs to the terminal</li>
</ul>
<pre><code>----------------
Test case predicted to be ckd 
----------------</code></pre>
</div>
<div id="security" class="section level3">
<h3>Security</h3>
<p>This example shows a pretty simply R-script API. But if you plan on deploying your API to production, you should consider the <a href="https://www.rplumber.io/docs/security.html">security section of the plumber documentation</a>. It give additional information about how you can make your code (more) secure.</p>
</div>
<div id="finalize" class="section level3">
<h3>Finalize</h3>
<p>If you wanted to deploy this API you would need to <a href="https://www.rplumber.io/docs/hosting.html">host</a> it, i.e. provide the model and run an R environment with plumber, ideally on a server. A good way to do this, would be to package everything in a <a href="https://www.rplumber.io/docs/hosting.html#docker">Docker</a> container and run this. Docker will ensure that you have a working snapshot of the system settings, R and package versions that won’t change. For more information on dockerizing your API, check out <a href="https://hub.docker.com/r/trestletech/plumber/" class="uri">https://hub.docker.com/r/trestletech/plumber/</a>.</p>
<hr />
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.6
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] plumber_0.4.6   rjson_0.2.20    forcats_0.3.0   stringr_1.3.1  
##  [5] dplyr_0.7.6     purrr_0.2.5     readr_1.1.1     tidyr_0.8.1    
##  [9] tibble_1.4.2    ggplot2_3.0.0   tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##  [1] nlme_3.1-137        lubridate_1.7.4     dimRed_0.1.0       
##  [4] httr_1.3.1          rprojroot_1.3-2     tools_3.5.1        
##  [7] backports_1.1.2     R6_2.2.2            rpart_4.1-13       
## [10] lazyeval_0.2.1      colorspace_1.3-2    nnet_7.3-12        
## [13] withr_2.1.2         tidyselect_0.2.4    compiler_3.5.1     
## [16] cli_1.0.0           rvest_0.3.2         xml2_1.2.0         
## [19] bookdown_0.7        scales_0.5.0        sfsmisc_1.1-2      
## [22] DEoptimR_1.0-8      robustbase_0.93-2   randomForest_4.6-14
## [25] digest_0.6.15       rmarkdown_1.10      pkgconfig_2.0.1    
## [28] htmltools_0.3.6     rlang_0.2.1         readxl_1.1.0       
## [31] ddalpha_1.3.4       rstudioapi_0.7      bindr_0.1.1        
## [34] jsonlite_1.5        ModelMetrics_1.1.0  magrittr_1.5       
## [37] Matrix_1.2-14       Rcpp_0.12.18        munsell_0.5.0      
## [40] abind_1.4-5         stringi_1.2.4       yaml_2.2.0         
## [43] MASS_7.3-50         plyr_1.8.4          recipes_0.1.3      
## [46] grid_3.5.1          promises_1.0.1      pls_2.6-0          
## [49] crayon_1.3.4        lattice_0.20-35     haven_1.1.2        
## [52] splines_3.5.1       hms_0.4.2           knitr_1.20         
## [55] pillar_1.3.0        reshape2_1.4.3      codetools_0.2-15   
## [58] stats4_3.5.1        CVST_0.2-2          magic_1.5-8        
## [61] glue_1.3.0          evaluate_0.11       blogdown_0.8       
## [64] modelr_0.1.2        httpuv_1.4.5        foreach_1.4.4      
## [67] cellranger_1.1.0    gtable_0.2.0        kernlab_0.9-26     
## [70] assertthat_0.2.0    DRR_0.0.3           xfun_0.3           
## [73] gower_0.1.2         prodlim_2018.04.18  broom_0.5.0        
## [76] later_0.7.3         class_7.3-14        survival_2.42-6    
## [79] geometry_0.3-6      timeDate_3043.102   RcppRoll_0.3.0     
## [82] iterators_1.0.10    bindrcpp_0.2.2      lava_1.6.2         
## [85] caret_6.0-80        ipred_0.9-6</code></pre>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Sketchnotes from TWiML&amp;AI #91: Philosophy of Intelligence with Matthew Crosby]]></title>
    <link href="/2018/01/twimlai91/"/>
    <id>/2018/01/twimlai91/</id>
    <published>2018-01-14T00:00:00+00:00</published>
    <updated>2018-01-14T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about Philosophy of Intelligence with Matthew Crosby: <a href="https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/" class="uri">https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/</a></p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai91.jpg" alt="Sketchnotes from TWiMLAI talk #92: Philosophy of Intelligence with Matthew Crosby" />
<p class="caption">Sketchnotes from TWiMLAI talk #92: Philosophy of Intelligence with Matthew Crosby</p>
</div>
<p>You can listen to the podcast <a href="https://twimlai.com/twiml-talk-91-philosophy-intelligence-matthew-crosby/">here</a>.</p>
<blockquote>
<p>This week on the podcast we’re featuring a series of conversations from the NIPs conference in Long Beach, California. I attended a bunch of talks and learned a ton, organized an impromptu roundtable on Building AI Products, and met a bunch of great people, including some former TWiML Talk guests.This time around i’m joined by Matthew Crosby, a researcher at Imperial College London, working on the Kinds of Intelligence Project. Matthew joined me after the NIPS Symposium of the same name, an event that brought researchers from a variety of disciplines together towards three aims: a broader perspective of the possible types of intelligence beyond human intelligence, better measurements of intelligence, and a more purposeful analysis of where progress should be made in AI to best benefit society. Matthew’s research explores intelligence from a philosophical perspective, exploring ideas like predictive processing and controlled hallucination, and how these theories of intelligence impact the way we approach creating artificial intelligence. This was a very interesting conversation, i’m sure you’ll enjoy. <a href="https://twimlai.com/twiml-talk-91-philosophy-intelligence-matthew-crosby/" class="uri">https://twimlai.com/twiml-talk-91-philosophy-intelligence-matthew-crosby/</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Looking beyond accuracy to improve trust in machine learning]]></title>
    <link href="/2018/01/looking_beyond_accuracy_to_improve_trust_in_ml/"/>
    <id>/2018/01/looking_beyond_accuracy_to_improve_trust_in_ml/</id>
    <published>2018-01-10T00:00:00+00:00</published>
    <updated>2018-01-10T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I have written another blogpost about <a href="https://blog.codecentric.de/en/2018/01/look-beyond-accuracy-improve-trust-machine-learning/">Looking beyond accuracy to improve trust in machine learning</a> at my company <a href="https://blog.codecentric.de/en/">codecentric</a>&rsquo;s blog:</p>

<blockquote>
<p>Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected. But being able to understand and interpret such models can be immensely important for improving model quality, increasing trust and transparency and for reducing bias. Because complex machine learning models are essentially black boxes and too complicated to understand, we need to use approximations to get a better sense of how they work. One such approach is LIME, which stands for Local Interpretable Model-agnostic Explanations and is a tool that helps understand and explain the decisions made by complex machine learning models.</p>
</blockquote>

<p>Continue reading at <a href="https://blog.codecentric.de/en/2018/01/look-beyond-accuracy-improve-trust-machine-learning/">https://blog.codecentric.de/en/2018/01/look-beyond-accuracy-improve-trust-machine-learning/</a>&hellip;</p>

<p>Links to the entire example code and more info are given at the end of the blog post.</p>

<p>The blog post is <a href="https://blog.codecentric.de/2018/01/vertrauen-und-vorurteile-maschinellem-lernen/">also available in German</a>.</p>

<p><img src="https://blog.codecentric.de/files/2018/01/lime_output_figure.png" alt="" /></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[TWiMLAI talk 88 sketchnotes: Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru]]></title>
    <link href="/2018/01/twimlai88_sketchnotes/"/>
    <id>/2018/01/twimlai88_sketchnotes/</id>
    <published>2018-01-10T00:00:00+00:00</published>
    <updated>2018-01-10T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>These are my sketchnotes taken from the <a href="https://twimlai.com/twiml-talk-88-using-deep-learning-google-street-view-estimate-demographics-timnit-gebru/">“This week in Machine Learning &amp; AI” podcast number 88 about Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru</a>:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/twimlai88_sketchnotes_vhjzac.jpg" alt="Sketchnotes from TWiMLAI talk #88: Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru" />
<p class="caption">Sketchnotes from TWiMLAI talk #88: Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru</p>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Registration now open for workshop on Deep Learning with Keras and TensorFlow using R]]></title>
    <link href="/2017/12/keras_sketchnotes/"/>
    <id>/2017/12/keras_sketchnotes/</id>
    <published>2017-12-20T00:00:00+00:00</published>
    <updated>2017-12-20T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Recently, I announced <a href="https://shirinsplayground.netlify.com/2017/11/deep_learning_keras_tensorflow/">my workshop on Deep Learning with Keras and TensorFlow</a>.</p>
<p>The next dates for it are <strong>January 18th and 19th</strong> in Solingen, Germany.</p>
<p>You can register now by following this link: <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow" class="uri">https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow</a></p>
<p>If any non-German-speaking people want to attend, I’m happy to give the course in English!</p>
<p><a href="mailto:shirin.glander@codecentric.de">Contact me if you have further questions.</a></p>
<hr />
<p>As a little bonus, I am also sharing my sketch notes from a Podcast I listened to when first getting into Keras:</p>
<ul>
<li><a href="https://softwareengineeringdaily.com/2016/01/29/deep-learning-and-keras-with-francois-chollet/">Software Engineering Daily with Francois Chollet</a></li>
</ul>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/keras_sketchnotes_lgrnvo.jpg" alt="Sketchnotes: Software Engineering Daily - Podcast from Jan 29th 2016" />
<p class="caption">Sketchnotes: Software Engineering Daily - Podcast from Jan 29th 2016</p>
</div>
<p>Links from the notes:</p>
<ul>
<li><a href="https://keras.io/">Keras for Python</a></li>
<li><a href="https://keras.rstudio.com/">Keras for R</a></li>
<li><a href="https://github.com/maxpumperla/elephas">elephas</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Explaining Predictions of Machine Learning Models with LIME - Münster Data Science Meetup]]></title>
    <link href="/2017/12/lime_sketchnotes/"/>
    <id>/2017/12/lime_sketchnotes/</id>
    <published>2017-12-12T00:00:00+00:00</published>
    <updated>2017-12-12T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div id="slides-from-munster-data-science-meetup" class="section level2">
<h2>Slides from Münster Data Science Meetup</h2>
<p><a href="https://shiring.github.io/netlify_images/lime_meetup_slides_wvsh6s.pdf">These are my slides</a> from the <a href="https://www.meetup.com/Data-Science-Meetup-Muenster/events/244173239/">Münster Data Science Meetup on December 12th, 2017</a>.</p>
<pre class="r"><code>knitr::include_url(&quot;https://shiring.github.io/netlify_images/lime_meetup_slides_wvsh6s.pdf&quot;)</code></pre>
<iframe src="https://shiring.github.io/netlify_images/lime_meetup_slides_wvsh6s.pdf" width="672" height="400px">
</iframe>
<p><br></p>
<p>My sketchnotes were collected from these two podcasts:</p>
<ul>
<li><a href="https://twimlai.com/twiml-talk-7-carlos-guestrin-explaining-predictions-machine-learning-models/" class="uri">https://twimlai.com/twiml-talk-7-carlos-guestrin-explaining-predictions-machine-learning-models/</a></li>
<li><a href="https://dataskeptic.com/blog/episodes/2016/trusting-machine-learning-models-with-lime" class="uri">https://dataskeptic.com/blog/episodes/2016/trusting-machine-learning-models-with-lime</a></li>
</ul>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/lime_sketchnotes_guq6u5.jpg" alt="Sketchnotes: TWiML Talk #7 with Carlos Guestrin – Explaining the Predictions of Machine Learning Models &amp; Data Skeptic Podcast - Trusting Machine Learning Models with Lime" />
<p class="caption">Sketchnotes: TWiML Talk #7 with Carlos Guestrin – Explaining the Predictions of Machine Learning Models &amp; Data Skeptic Podcast - Trusting Machine Learning Models with Lime</p>
</div>
<hr />
</div>
<div id="example-code" class="section level2">
<h2>Example Code</h2>
<ul>
<li>the following libraries were loaded:</li>
</ul>
<pre class="r"><code>library(tidyverse)  # for tidy data analysis
library(farff)      # for reading arff file
library(missForest) # for imputing missing values
library(dummies)    # for creating dummy variables
library(caret)      # for modeling
library(lime)       # for explaining predictions</code></pre>
<div id="data" class="section level3">
<h3>Data</h3>
<p>The Chronic Kidney Disease dataset was downloaded from UC Irvine’s Machine Learning repository: <a href="http://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease" class="uri">http://archive.ics.uci.edu/ml/datasets/Chronic_Kidney_Disease</a></p>
<pre class="r"><code>data_file &lt;- file.path(&quot;path/to/chronic_kidney_disease_full.arff&quot;)</code></pre>
<ul>
<li>load data with the <code>farff</code> package</li>
</ul>
<pre class="r"><code>data &lt;- readARFF(data_file)</code></pre>
<div id="features" class="section level4">
<h4>Features</h4>
<ul>
<li>age - age</li>
<li>bp - blood pressure</li>
<li>sg - specific gravity</li>
<li>al - albumin</li>
<li>su - sugar</li>
<li>rbc - red blood cells</li>
<li>pc - pus cell</li>
<li>pcc - pus cell clumps</li>
<li>ba - bacteria</li>
<li>bgr - blood glucose random</li>
<li>bu - blood urea</li>
<li>sc - serum creatinine</li>
<li>sod - sodium</li>
<li>pot - potassium</li>
<li>hemo - hemoglobin</li>
<li>pcv - packed cell volume</li>
<li>wc - white blood cell count</li>
<li>rc - red blood cell count</li>
<li>htn - hypertension</li>
<li>dm - diabetes mellitus</li>
<li>cad - coronary artery disease</li>
<li>appet - appetite</li>
<li>pe - pedal edema</li>
<li>ane - anemia</li>
<li>class - class</li>
</ul>
</div>
</div>
<div id="missing-data" class="section level3">
<h3>Missing data</h3>
<ul>
<li>impute missing data with Nonparametric Missing Value Imputation using Random Forest (<code>missForest</code> package)</li>
</ul>
<pre class="r"><code>data_imp &lt;- missForest(data)</code></pre>
</div>
<div id="one-hot-encoding" class="section level3">
<h3>One-hot encoding</h3>
<ul>
<li>create dummy variables (<code>caret::dummy.data.frame()</code>)</li>
<li>scale and center</li>
</ul>
<pre class="r"><code>data_imp_final &lt;- data_imp$ximp
data_dummy &lt;- dummy.data.frame(dplyr::select(data_imp_final, -class), sep = &quot;_&quot;)
data &lt;- cbind(dplyr::select(data_imp_final, class), scale(data_dummy, 
                                                   center = apply(data_dummy, 2, min),
                                                   scale = apply(data_dummy, 2, max)))</code></pre>
</div>
<div id="modeling" class="section level3">
<h3>Modeling</h3>
<pre class="r"><code># training and test set
set.seed(42)
index &lt;- createDataPartition(data$class, p = 0.9, list = FALSE)
train_data &lt;- data[index, ]
test_data  &lt;- data[-index, ]

# modeling
model_rf &lt;- caret::train(class ~ .,
  data = train_data,
  method = &quot;rf&quot;, # random forest
  trControl = trainControl(method = &quot;repeatedcv&quot;, 
       number = 10, 
       repeats = 5, 
       verboseIter = FALSE))</code></pre>
<pre class="r"><code>model_rf</code></pre>
<pre><code>## Random Forest 
## 
## 360 samples
##  48 predictor
##   2 classes: &#39;ckd&#39;, &#39;notckd&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 324, 324, 324, 324, 325, 324, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    2    0.9922647  0.9838466
##   25    0.9917392  0.9826070
##   48    0.9872930  0.9729881
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<pre class="r"><code># predictions
pred &lt;- data.frame(sample_id = 1:nrow(test_data), predict(model_rf, test_data, type = &quot;prob&quot;), actual = test_data$class) %&gt;%
  mutate(prediction = colnames(.)[2:3][apply(.[, 2:3], 1, which.max)], correct = ifelse(actual == prediction, &quot;correct&quot;, &quot;wrong&quot;))

confusionMatrix(pred$actual, pred$prediction)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction ckd notckd
##     ckd     23      2
##     notckd   0     15
##                                           
##                Accuracy : 0.95            
##                  95% CI : (0.8308, 0.9939)
##     No Information Rate : 0.575           
##     P-Value [Acc &gt; NIR] : 1.113e-07       
##                                           
##                   Kappa : 0.8961          
##  Mcnemar&#39;s Test P-Value : 0.4795          
##                                           
##             Sensitivity : 1.0000          
##             Specificity : 0.8824          
##          Pos Pred Value : 0.9200          
##          Neg Pred Value : 1.0000          
##              Prevalence : 0.5750          
##          Detection Rate : 0.5750          
##    Detection Prevalence : 0.6250          
##       Balanced Accuracy : 0.9412          
##                                           
##        &#39;Positive&#39; Class : ckd             
## </code></pre>
</div>
<div id="lime" class="section level3">
<h3>LIME</h3>
<ul>
<li>LIME needs data without response variable</li>
</ul>
<pre class="r"><code>train_x &lt;- dplyr::select(train_data, -class)
test_x &lt;- dplyr::select(test_data, -class)

train_y &lt;- dplyr::select(train_data, class)
test_y &lt;- dplyr::select(test_data, class)</code></pre>
<ul>
<li>build explainer</li>
</ul>
<pre class="r"><code>explainer &lt;- lime(train_x, model_rf, n_bins = 5, quantile_bins = TRUE)</code></pre>
<ul>
<li>run <code>explain()</code> function</li>
</ul>
<pre class="r"><code>explanation_df &lt;- lime::explain(test_x, explainer, n_labels = 1, n_features = 8, n_permutations = 1000, feature_select = &quot;forward_selection&quot;)</code></pre>
<ul>
<li>model reliability</li>
</ul>
<pre class="r"><code>explanation_df %&gt;%
  ggplot(aes(x = model_r2, fill = label)) +
    geom_density(alpha = 0.5)</code></pre>
<p><img src="/post/2017-12-12_lime_sketchnotes_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<ul>
<li>plot explanations</li>
</ul>
<pre class="r"><code>plot_features(explanation_df[1:24, ], ncol = 1)</code></pre>
<p><img src="/post/2017-12-12_lime_sketchnotes_files/figure-html/unnamed-chunk-15-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="session-info" class="section level3">
<h3>Session Info</h3>
<pre><code>## Session info -------------------------------------------------------------</code></pre>
<pre><code>##  setting  value                       
##  version  R version 3.4.3 (2017-11-30)
##  system   x86_64, darwin15.6.0        
##  ui       X11                         
##  language (EN)                        
##  collate  de_DE.UTF-8                 
##  tz       Europe/Berlin               
##  date     2018-04-22</code></pre>
<pre><code>## Packages -----------------------------------------------------------------</code></pre>
<pre><code>##  package      * version    date       source                            
##  assertthat     0.2.0      2017-04-11 CRAN (R 3.4.0)                    
##  backports      1.1.2      2017-12-13 CRAN (R 3.4.3)                    
##  base         * 3.4.3      2017-12-07 local                             
##  BBmisc         1.11       2017-03-10 CRAN (R 3.4.0)                    
##  bindr          0.1        2016-11-13 CRAN (R 3.4.0)                    
##  bindrcpp     * 0.2        2017-06-17 CRAN (R 3.4.0)                    
##  blogdown       0.5        2018-01-24 CRAN (R 3.4.3)                    
##  bookdown       0.7        2018-02-18 CRAN (R 3.4.3)                    
##  broom          0.4.3      2017-11-20 CRAN (R 3.4.2)                    
##  caret        * 6.0-78     2017-12-10 CRAN (R 3.4.3)                    
##  cellranger     1.1.0      2016-07-27 CRAN (R 3.4.0)                    
##  checkmate      1.8.5      2017-10-24 CRAN (R 3.4.2)                    
##  class          7.3-14     2015-08-30 CRAN (R 3.4.3)                    
##  cli            1.0.0      2017-11-05 CRAN (R 3.4.2)                    
##  codetools      0.2-15     2016-10-05 CRAN (R 3.4.3)                    
##  colorspace     1.3-2      2016-12-14 CRAN (R 3.4.0)                    
##  compiler       3.4.3      2017-12-07 local                             
##  crayon         1.3.4      2017-09-16 CRAN (R 3.4.1)                    
##  CVST           0.2-1      2013-12-10 CRAN (R 3.4.0)                    
##  datasets     * 3.4.3      2017-12-07 local                             
##  ddalpha        1.3.1.1    2018-02-02 CRAN (R 3.4.3)                    
##  DEoptimR       1.0-8      2016-11-19 CRAN (R 3.4.0)                    
##  devtools       1.13.5     2018-02-18 CRAN (R 3.4.3)                    
##  digest         0.6.15     2018-01-28 CRAN (R 3.4.3)                    
##  dimRed         0.1.0      2017-05-04 CRAN (R 3.4.0)                    
##  dplyr        * 0.7.4      2017-09-28 CRAN (R 3.4.2)                    
##  DRR            0.0.3      2018-01-06 CRAN (R 3.4.3)                    
##  dummies      * 1.5.6      2012-06-14 CRAN (R 3.4.0)                    
##  e1071          1.6-8      2017-02-02 CRAN (R 3.4.0)                    
##  evaluate       0.10.1     2017-06-24 CRAN (R 3.4.1)                    
##  farff        * 1.0        2016-09-11 CRAN (R 3.4.0)                    
##  forcats      * 0.3.0      2018-02-19 CRAN (R 3.4.3)                    
##  foreach      * 1.4.4      2017-12-12 CRAN (R 3.4.3)                    
##  foreign        0.8-69     2017-06-22 CRAN (R 3.4.3)                    
##  ggplot2      * 2.2.1.9000 2018-02-28 Github (thomasp85/ggplot2@7859a29)
##  glmnet         2.0-13     2017-09-22 CRAN (R 3.4.2)                    
##  glue           1.2.0      2017-10-29 CRAN (R 3.4.2)                    
##  gower          0.1.2      2017-02-23 CRAN (R 3.4.0)                    
##  graphics     * 3.4.3      2017-12-07 local                             
##  grDevices    * 3.4.3      2017-12-07 local                             
##  grid           3.4.3      2017-12-07 local                             
##  gtable         0.2.0      2016-02-26 CRAN (R 3.4.0)                    
##  haven          1.1.1      2018-01-18 CRAN (R 3.4.3)                    
##  highr          0.6        2016-05-09 CRAN (R 3.4.0)                    
##  hms            0.4.1      2018-01-24 CRAN (R 3.4.3)                    
##  htmltools      0.3.6      2017-04-28 CRAN (R 3.4.0)                    
##  htmlwidgets    1.0        2018-01-20 CRAN (R 3.4.3)                    
##  httpuv         1.3.6.1    2018-02-28 CRAN (R 3.4.3)                    
##  httr           1.3.1      2017-08-20 CRAN (R 3.4.1)                    
##  ipred          0.9-6      2017-03-01 CRAN (R 3.4.0)                    
##  iterators    * 1.0.9      2017-12-12 CRAN (R 3.4.3)                    
##  itertools    * 0.1-3      2014-03-12 CRAN (R 3.4.0)                    
##  jsonlite       1.5        2017-06-01 CRAN (R 3.4.0)                    
##  kernlab        0.9-25     2016-10-03 CRAN (R 3.4.0)                    
##  knitr          1.20       2018-02-20 CRAN (R 3.4.3)                    
##  labeling       0.3        2014-08-23 CRAN (R 3.4.0)                    
##  lattice      * 0.20-35    2017-03-25 CRAN (R 3.4.3)                    
##  lava           1.6        2018-01-13 CRAN (R 3.4.3)                    
##  lazyeval       0.2.1      2017-10-29 CRAN (R 3.4.2)                    
##  lime         * 0.3.1      2017-11-24 CRAN (R 3.4.3)                    
##  lubridate      1.7.3      2018-02-27 CRAN (R 3.4.3)                    
##  magrittr       1.5        2014-11-22 CRAN (R 3.4.0)                    
##  MASS           7.3-49     2018-02-23 CRAN (R 3.4.3)                    
##  Matrix         1.2-12     2017-11-20 CRAN (R 3.4.3)                    
##  memoise        1.1.0      2017-04-21 CRAN (R 3.4.0)                    
##  methods      * 3.4.3      2017-12-07 local                             
##  mime           0.5        2016-07-07 CRAN (R 3.4.0)                    
##  missForest   * 1.4        2013-12-31 CRAN (R 3.4.0)                    
##  mnormt         1.5-5      2016-10-15 CRAN (R 3.4.0)                    
##  ModelMetrics   1.1.0      2016-08-26 CRAN (R 3.4.0)                    
##  modelr         0.1.1      2017-07-24 CRAN (R 3.4.1)                    
##  munsell        0.4.3      2016-02-13 CRAN (R 3.4.0)                    
##  nlme           3.1-131.1  2018-02-16 CRAN (R 3.4.3)                    
##  nnet           7.3-12     2016-02-02 CRAN (R 3.4.3)                    
##  parallel       3.4.3      2017-12-07 local                             
##  pillar         1.2.1      2018-02-27 CRAN (R 3.4.3)                    
##  pkgconfig      2.0.1      2017-03-21 CRAN (R 3.4.0)                    
##  plyr           1.8.4      2016-06-08 CRAN (R 3.4.0)                    
##  prodlim        1.6.1      2017-03-06 CRAN (R 3.4.0)                    
##  psych          1.7.8      2017-09-09 CRAN (R 3.4.1)                    
##  purrr        * 0.2.4      2017-10-18 CRAN (R 3.4.2)                    
##  R6             2.2.2      2017-06-17 CRAN (R 3.4.0)                    
##  randomForest * 4.6-12     2015-10-07 CRAN (R 3.4.0)                    
##  Rcpp           0.12.15    2018-01-20 CRAN (R 3.4.3)                    
##  RcppRoll       0.2.2      2015-04-05 CRAN (R 3.4.0)                    
##  readr        * 1.1.1      2017-05-16 CRAN (R 3.4.0)                    
##  readxl         1.0.0      2017-04-18 CRAN (R 3.4.0)                    
##  recipes        0.1.2      2018-01-11 CRAN (R 3.4.3)                    
##  reshape2       1.4.3      2017-12-11 CRAN (R 3.4.3)                    
##  rlang          0.2.0.9000 2018-02-28 Github (tidyverse/rlang@9ea33dd)  
##  rmarkdown      1.8        2017-11-17 CRAN (R 3.4.2)                    
##  robustbase     0.92-8     2017-11-01 CRAN (R 3.4.2)                    
##  rpart          4.1-13     2018-02-23 CRAN (R 3.4.3)                    
##  rprojroot      1.3-2      2018-01-03 CRAN (R 3.4.3)                    
##  rstudioapi     0.7        2017-09-07 CRAN (R 3.4.1)                    
##  rvest          0.3.2      2016-06-17 CRAN (R 3.4.0)                    
##  scales         0.5.0.9000 2018-02-28 Github (hadley/scales@d767915)    
##  sfsmisc        1.1-1      2017-06-08 CRAN (R 3.4.0)                    
##  shiny          1.0.5      2017-08-23 CRAN (R 3.4.1)                    
##  shinythemes    1.1.1      2016-10-12 CRAN (R 3.4.0)                    
##  splines        3.4.3      2017-12-07 local                             
##  stats        * 3.4.3      2017-12-07 local                             
##  stats4         3.4.3      2017-12-07 local                             
##  stringdist     0.9.4.6    2017-07-31 CRAN (R 3.4.1)                    
##  stringi        1.1.6      2017-11-17 CRAN (R 3.4.2)                    
##  stringr      * 1.3.0      2018-02-19 CRAN (R 3.4.3)                    
##  survival       2.41-3     2017-04-04 CRAN (R 3.4.3)                    
##  tibble       * 1.4.2      2018-01-22 CRAN (R 3.4.3)                    
##  tidyr        * 0.8.0      2018-01-29 CRAN (R 3.4.3)                    
##  tidyselect     0.2.4      2018-02-26 CRAN (R 3.4.3)                    
##  tidyverse    * 1.2.1      2017-11-14 CRAN (R 3.4.2)                    
##  timeDate       3043.102   2018-02-21 CRAN (R 3.4.3)                    
##  tools          3.4.3      2017-12-07 local                             
##  utils        * 3.4.3      2017-12-07 local                             
##  withr          2.1.1.9000 2018-02-28 Github (jimhester/withr@5d05571)  
##  xfun           0.1        2018-01-22 CRAN (R 3.4.3)                    
##  xml2           1.2.0      2018-01-24 CRAN (R 3.4.3)                    
##  xtable         1.8-2      2016-02-05 CRAN (R 3.4.0)                    
##  yaml           2.1.17     2018-02-27 CRAN (R 3.4.3)</code></pre>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[MICE (Multiple Imputation by Chained Equations) in R - sketchnotes from MünsteR Meetup]]></title>
    <link href="/2017/11/mice_sketchnotes/"/>
    <id>/2017/11/mice_sketchnotes/</id>
    <published>2017-11-28T00:00:00+00:00</published>
    <updated>2017-11-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Last night, the <a href="http://meetu.ps/c/3ffGL/w54bW/f">MünsteR R user-group</a> had <a href="https://www.meetup.com/Munster-R-Users-Group/events/243388360/">another great meetup</a>:</p>
<p>Karin Groothuis-Oudshoorn, Assistant Professor at the University of Twente, presented her R package <code>mice</code> about Multivariate Imputation by Chained Equations.</p>
<p>It was a very interesting talk and here are my sketchnotes that I took during it:</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/mice_sketchnote_gxjsgc.jpg" alt="MICE talk sketchnotes" />
<p class="caption">MICE talk sketchnotes</p>
</div>
<p>Here is the link to the paper referenced in my notes: <a href="https://www.jstatsoft.org/article/view/v045i03" class="uri">https://www.jstatsoft.org/article/view/v045i03</a></p>
<blockquote>
<p>“The mice package implements a method to deal with missing data. The package creates multiple imputations (replacement values) for multivariate missing data. The method is based on Fully Conditional Specification, where each incomplete variable is imputed by a separate model. The MICE algorithm can impute mixes of continuous, binary, unordered categorical and ordered categorical data. In addition, MICE can impute continuous two-level data, and maintain consistency between imputations by means of passive imputation. Many diagnostic plots are implemented to inspect the quality of the imputations.”&quot; (<a href="https://cran.r-project.org/web/packages/mice/README.html" class="uri">https://cran.r-project.org/web/packages/mice/README.html</a>)</p>
</blockquote>
<p>For more information on the package go to <a href="http://stefvanbuuren.github.io/mice/" class="uri">http://stefvanbuuren.github.io/mice/</a>.</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Workshop on Deep Learning with Keras and TensorFlow in R]]></title>
    <link href="/2017/11/deep_learning_keras_tensorflow/"/>
    <id>/2017/11/deep_learning_keras_tensorflow/</id>
    <published>2017-11-20T00:00:00+00:00</published>
    <updated>2017-11-20T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>You can now book me and my 1-day workshop on <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/">deep learning with Keras and TensorFlow using R</a>.</p>
<p>In my workshop, you will learn</p>
<ul>
<li>the basics of deep learning</li>
<li>what cross-entropy and loss is</li>
<li>about activation functions</li>
<li>how to optimize weights and biases with backpropagation and gradient descent</li>
<li>how to build (deep) neural networks with Keras and TensorFlow</li>
<li>how to save and load models and model weights</li>
<li>how to visualize models with TensorBoard</li>
<li>how to make predictions on test data</li>
</ul>
<p>Date and place depend on who and how many people are interested, so please contact me either directly or via the workshop page: <a href="https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/" class="uri">https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow/</a> (the description is in German but I also offer to give the workshop in English).</p>
<p><br></p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/mlp_r7pv7z.jpg" alt="Neural Network with three densely connected hidden layers" />
<p class="caption">Neural Network with three densely connected hidden layers</p>
</div>
<p>Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. Keras is very convenient for fast and easy prototyping of neural networks. It is highly modular and very flexible, so that you can build basically any type of neural network you want. It supports convolutional neural networks and recurrent neural networks, as well as combinations of both. Due to its layer structure, it is highly extensible and can run on CPU or GPU.</p>
<p>The <code>keras</code> R package provides an interface to the Python library of Keras, just as the tensorflow package provides an interface to TensorFlow. Basically, R creates a conda instance and runs Keras it it, while you can still use all the functionalities of R for plotting, etc. Almost all function names are the same, so models can easily be recreated in Python for deployment.</p>
<p><br></p>
<div class="figure">
<img src="https://blog.keras.io/img/keras-tensorflow-logo.jpg" />

</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[How to combine point and boxplots in timeline charts with ggplot2 facets]]></title>
    <link href="/2017/11/combine_point_boxplot_ggplot/"/>
    <id>/2017/11/combine_point_boxplot_ggplot/</id>
    <published>2017-11-18T00:00:00+00:00</published>
    <updated>2017-11-18T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>In a recent project, I was looking to plot data from different variables along the same time axis. The difficulty was, that some of these variables I wanted to have as point plots, while others I wanted as box-plots.</p>
<p>Because I work with the tidyverse, I wanted to produce these plots with ggplot2. Faceting was the obvious first step but it took me quite a while to figure out how to best combine facets with point plots (where I have one value per time point) with and box-plots (where I have multiple values per time point).</p>
<p>The reason why this isn’t trivial is that box plots require groups or factors on the x-axis, while points can be plotted over a continuous range of x-values. If your alarm bells are ringing right now, you are absolutely right: before you try to combine plots with different x-axis properties, you should think long and hard whether this is an accurate representation of the data and if its a good idea to do so! Here, I had multiple values per time point for one variable and I wanted to make the median + variation explicitly clear, while also showing the continuous changes of other variables over the same range of time.</p>
<p>So, I am writing this short tutorial here in hopes that it saves the next person trying to do something similar from spending an entire morning on stackoverflow. ;-)</p>
<p>For this demonstration, I am creating some fake data:</p>
<pre class="r"><code>library(tidyverse)
dates &lt;- seq(as.POSIXct(&quot;2017-10-01 07:00&quot;), as.POSIXct(&quot;2017-10-01 10:30&quot;), by = 180) # 180 seconds == 3 minutes
fake_data &lt;- data.frame(time = dates,
                        var1_1 = runif(length(dates)),
                        var1_2 = runif(length(dates)),
                        var1_3 = runif(length(dates)),
                        var2 = runif(length(dates))) %&gt;%
  sample_frac(size = 0.33)
head(fake_data)</code></pre>
<pre><code>##                   time    var1_1    var1_2     var1_3      var2
## 51 2017-10-01 09:30:00 0.4534363 0.9947001 0.07223936 0.8891859
## 35 2017-10-01 08:42:00 0.4260230 0.5613454 0.77475368 0.5780837
## 3  2017-10-01 07:06:00 0.0871770 0.2824280 0.97726978 0.4705974
## 59 2017-10-01 09:54:00 0.6824320 0.9735636 0.67654248 0.4235517
## 5  2017-10-01 07:12:00 0.7979666 0.5857256 0.03911439 0.6918448
## 52 2017-10-01 09:33:00 0.7537796 0.3054030 0.61354248 0.5045606</code></pre>
<p>Here, variable 1 (<code>var1</code>) has three measurements per time point, while variable 2 (<code>var2</code>) has one.</p>
<p>First, for plotting with ggplot2 we want our data in a tidy long format. I also add another column for faceting that groups the variables from <code>var1</code> together.</p>
<pre class="r"><code>fake_data_long &lt;- fake_data %&gt;%
  gather(x, y, var1_1:var2) %&gt;%
  mutate(facet = ifelse(x %in% c(&quot;var1_1&quot;, &quot;var1_2&quot;, &quot;var1_3&quot;), &quot;var1&quot;, x))
head(fake_data_long)</code></pre>
<pre><code>##                  time      x         y facet
## 1 2017-10-01 09:30:00 var1_1 0.4534363  var1
## 2 2017-10-01 08:42:00 var1_1 0.4260230  var1
## 3 2017-10-01 07:06:00 var1_1 0.0871770  var1
## 4 2017-10-01 09:54:00 var1_1 0.6824320  var1
## 5 2017-10-01 07:12:00 var1_1 0.7979666  var1
## 6 2017-10-01 09:33:00 var1_1 0.7537796  var1</code></pre>
<p>Now, we can plot this the following way:</p>
<ul>
<li>facet by variable</li>
<li>subset data to facets for point plots and give aesthetics in <code>geom_point()</code></li>
<li>subset data to facets for box plots and give aesthetics in <code>geom_boxplot()</code>. Here we also need to set the <code>group</code> aesthetic; if we don’t specifically give that, we will get a plot with one big box, instead of a box for every time point.</li>
</ul>
<pre class="r"><code>fake_data_long %&gt;%
  ggplot() +
    facet_grid(facet ~ ., scales = &quot;free&quot;) +
    geom_point(data = subset(fake_data_long, facet == &quot;var2&quot;), 
               aes(x = time, y = y),
               size = 1) +
    geom_line(data = subset(fake_data_long, facet == &quot;var2&quot;), 
               aes(x = time, y = y)) +
    geom_boxplot(data = subset(fake_data_long, facet == &quot;var1&quot;), 
               aes(x = time, y = y, group = time))</code></pre>
<p><img src="/post/2017-11-18-combine_point_boxplot_ggplot_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.4.3 (2017-11-30)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.4
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8
## 
## attached base packages:
## [1] methods   stats     graphics  grDevices utils     datasets  base     
## 
## other attached packages:
##  [1] bindrcpp_0.2       forcats_0.3.0      stringr_1.3.0     
##  [4] dplyr_0.7.4        purrr_0.2.4        readr_1.1.1       
##  [7] tidyr_0.8.0        tibble_1.4.2       ggplot2_2.2.1.9000
## [10] tidyverse_1.2.1   
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_0.2.4  xfun_0.1          reshape2_1.4.3   
##  [4] haven_1.1.1       lattice_0.20-35   colorspace_1.3-2 
##  [7] htmltools_0.3.6   yaml_2.1.17       rlang_0.2.0.9000 
## [10] pillar_1.2.1      withr_2.1.1.9000  foreign_0.8-69   
## [13] glue_1.2.0        modelr_0.1.1      readxl_1.0.0     
## [16] bindr_0.1         plyr_1.8.4        munsell_0.4.3    
## [19] blogdown_0.5      gtable_0.2.0      cellranger_1.1.0 
## [22] rvest_0.3.2       psych_1.7.8       evaluate_0.10.1  
## [25] labeling_0.3      knitr_1.20        parallel_3.4.3   
## [28] broom_0.4.3       Rcpp_0.12.15      backports_1.1.2  
## [31] scales_0.5.0.9000 jsonlite_1.5      mnormt_1.5-5     
## [34] hms_0.4.1         digest_0.6.15     stringi_1.1.6    
## [37] bookdown_0.7      grid_3.4.3        rprojroot_1.3-2  
## [40] cli_1.0.0         tools_3.4.3       magrittr_1.5     
## [43] lazyeval_0.2.1    crayon_1.3.4      pkgconfig_2.0.1  
## [46] xml2_1.2.0        lubridate_1.7.3   assertthat_0.2.0 
## [49] rmarkdown_1.8     httr_1.3.1        rstudioapi_0.7   
## [52] R6_2.2.2          nlme_3.1-131.1    compiler_3.4.3</code></pre>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Explore Predictive Maintenance with flexdashboard]]></title>
    <link href="/2017/11/predictive_maintenance_dashboard/"/>
    <id>/2017/11/predictive_maintenance_dashboard/</id>
    <published>2017-11-02T00:00:00+00:00</published>
    <updated>2017-11-02T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I have written the following post about <a href="https://blog.codecentric.de/en/2017/11/explore-predictive-maintenance-flexdashboard/">Predictive Maintenance and flexdashboard</a> at my company <a href="https://blog.codecentric.de/en/">codecentric</a>’s blog:</p>
<blockquote>
<p>Predictive Maintenance is an increasingly popular strategy associated with Industry 4.0; it uses advanced analytics and machine learning to optimize machine costs and output (see Google Trends plot below). A common use-case for Predictive Maintenance is to proactively monitor machines, so as to predict when a check-up is needed to reduce failure and maximize performance. In contrast to traditional maintenance, where each machine has to undergo regular routine check-ups, Predictive Maintenance can save costs and reduce downtime. A machine learning approach to such a problem would be to analyze machine failure over time to train a supervised classification model that predicts failure. Data from sensors and weather information is often used as features in modeling.</p>
</blockquote>
<blockquote>
<p>…</p>
</blockquote>
<blockquote>
<p>With flexdashboard RStudio provides a great way to create interactive dashboards with R. It is an easy and very fast way to present analyses or create story maps. Here, I have used it to demonstrate different analysis techniques for Predictive Maintenance. It uses Shiny run-time to create interactive content.</p>
</blockquote>
<blockquote>
<p>…</p>
</blockquote>
<p>Continue reading at <a href="https://blog.codecentric.de/en/2017/11/explore-predictive-maintenance-flexdashboard/" class="uri">https://blog.codecentric.de/en/2017/11/explore-predictive-maintenance-flexdashboard/</a></p>
<div class="figure">
<img src="https://blog.codecentric.de/files/2017/10/dashboard_screenshot.png" />

</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Blockchain &amp; distributed ML - my report from the data2day conference]]></title>
    <link href="/2017/09/data2day/"/>
    <id>/2017/09/data2day/</id>
    <published>2017-09-28T00:00:00+00:00</published>
    <updated>2017-09-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<div class="figure">
<img src="https://www.data2day.de/common/images/konferenzen/data2day2017.svg" />

</div>
<p>Yesterday and today I attended the <a href="www.data2day.de">data2day</a>, a conference about Big Data, Machine Learning and Data Science in Heidelberg, Germany. Topics and workshops covered a range of topics surrounding (big) data analysis and Machine Learning, like Deep Learning, Reinforcement Learning, TensorFlow applications, etc. Distributed systems and scalability were a major part of a lot of the talks as well, reflecting the growing desire to build bigger and more complex models that can’t (or would take too long to) run on a single computer. Most of the application examples were built in Python but one talk by Andreas Prawitt was specifically titled “Using R for Predictive Maintenance: an example from the TRUMPF Laser GmbH”. I also saw quite a few graphs that were obviously made with ggplot!</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="de" dir="ltr">
Guten Morgen auf der <a href="https://twitter.com/data2day"><span class="citation">@data2day</span></a> Kommt uns doch mal am Stand besuchen :-) <a href="https://t.co/YK46ACdNj9">pic.twitter.com/YK46ACdNj9</a>
</p>
— codecentric AG (<span class="citation">@codecentric</span>) <a href="https://twitter.com/codecentric/status/912928993279606784">September 27, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><br></p>
<p>The keynote lecture on Wednesday about <strong>Blockchains for AI</strong> was given by Trent McConaghy. <a href="https://www.sitepen.com/blog/2017/09/21/blockchain-basics/">Blockchain technology</a> is based on a decentralized system of storing and validating data and changes in data. It experiences a huge hype at the moment but it is only starting to gain track in Data Science and Machine Learning as well. I therefore found it a very fitting topic for the keynote lecture! Trent and his colleagues at <a href="www.bigchaindb.com">BigchainDB</a> are implementing an “internet-scale blockchain database for the world” - the Interplanetary Database (IPDB).</p>
<blockquote>
<p>“IPDB is a blockchain database that offers decentralized control, immutability and the creation and trading of digital assets. […] As a database for the world, IPDB offers decentralized control, strong governance and universal accessibility. IPDB relies on “caretaker” organizations around the world, who share responsibility for managing the network and governing the IPDB Foundation. Anyone in the world will be able to use IPDB. […]” <a href="https://blog.bigchaindb.com/ipdb-announced-as-public-planetary-scale-blockchain-database-7a363824fc14" class="uri">https://blog.bigchaindb.com/ipdb-announced-as-public-planetary-scale-blockchain-database-7a363824fc14</a></p>
</blockquote>
<p>He presented a number of examples where blockchain technology for decentralized data storage/access can be beneficial to Machine Learning and AI, like exchanging data from self-driving cars, of online market places and for generating art with computers. You can learn more about him <a href="https://blog.oceanprotocol.com/from-ai-to-blockchain-to-data-meet-ocean-f210ff460465">here</a>:</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="en" dir="ltr">
It's always been about the data.<br>Announcing Ocean.<a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a> <a href="https://twitter.com/hashtag/Blockchain?src=hash&amp;ref_src=twsrc%5Etfw">#Blockchain</a> <a href="https://twitter.com/oceanprotocol?ref_src=twsrc%5Etfw"><span class="citation">@OceanProtocol</span></a><a href="https://t.co/Do4XNn3ucN">https://t.co/Do4XNn3ucN</a>
</p>
— Trent McConaghy (<span class="citation">@trentmc0</span>) <a href="https://twitter.com/trentmc0/status/909793166416662528?ref_src=twsrc%5Etfw">September 18, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><br></p>
<p>The other talks were a mix of high- and low-level topics: from introductions to machine learning, Apache Spark and data analysis with Python to (distributed) data streaming with Kappa architecture or Apache Kafka, containerization with Docker and Kubernetes, data archiving with Apache Cassandra, relevance tuning with Solr and much more. While I spent most of the time at my company’s conference stand, I did hear three of the talks. I summarize each of them below…</p>
<p><br></p>
<ol style="list-style-type: decimal">
<li><strong>Scalable Machine Learning with Apache Spark for Fraud Detection</strong></li>
</ol>
<p>In this first talk I heard, Dr. Patrick Baier and Dr. Stanimir Dragiev presented their work at <a href="www.zalando.de/">Zalando</a>. They built a scalable machine learning framework with Apache Spark, Scala and AWS to assess and predict fraud in online transactions. <a href="www.zalando.de/">Zalando</a> is a German online store that sells clothes, shoes and accessories. Normally, they allow registered customers to buy via invoice, i.e. they receive their ordered items before they pay them. This leaves them vulnerable to fraud where item are not paid for. The goal of their data science team is to use customer and basket data to obtain a probability score for how likely a transaction is going to be fraudulent. High-risk payment options, like invoice, can then be disabled in transactions with high fraud probability. To build and run such machine learning models, the Zalando data science team uses a combination of Spark, Scala, R, AWS, SQL, Python, Docker, etc. In their workflow, they use a combination of static and dynamic features, imputing missing values and building a decision model. In order to scale their modeling workflow to process more requests, use more data in training, update models more frequently and/or run more models, they described a workflow that uses Spark, Scala and Amazon Web Services (AWS). Spark’s machine learning library can be used for modeling and scaled horizontally by increasing the number of clusters on which to run the models. Scala provides multi-threading functionality and AWS is used for storing data in S3 and extending computation power depending on changing needs. Finally, they include a model inspector into their workflow to assure comparability of training and test data and check that the model is behaving as expected. Problems that they are dealing with include highly unbalanced data (which is getting even worse the better their models work as they keep reducing the number of fraud cases), delayed labeling due to the long process of the transactions, seasonality in data.</p>
<p><br></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Sparse Data: Don’t Mind the Gap!</strong></li>
</ol>
<p>In this talk, my colleagues from <a href="www.codecentric.de">codecentric</a> Dr. Daniel Pape and Dr. Michael Plümacher showed an example from ad targeting of how to deal with sparse data. Sparse data occurs in many areas, e.g. as rare events over a long period of time or in areas where there are many items and few occurrences per item, like in recommender systems or in natural language processing (NLP). In ad targeting, the measure of success is the rate of the click-through rate (CRT): this is the number of clicks on a given advertisement displayed to a user on a website divided by the total number of advertisements, or impressions. Because financial revenue comes from a high CTR, advertisements should be placed in a way that maximizes their chance of being clicked, i.e. we want to recommend advertisements for specific users that match their interests or are of actual relevance. Sparsity come into play with ad targeting because the number of clicks is very low compared to two metrics: a) from all the potential ads that a user could see, only a small proportion is actually shown to her/him and b) of the ads that a user sees, she/he only clicks on very few. This means that, a CTR matrix of advertisements x targets will have very few combinations that have been clicked (the mean CTR is 0.003) and contain many missing values. The approach they took was to impute the missing values and predict for each target/user the most similar ads from the imputed CTR matrix. This approach worked well for a reasonably large data set but it didn’t perform so well with smaller (and therefore even sparser) data. They then talked about alternative approaches, like grouping users and/or ads into groups in order to reduce the sparsity of the data. Their take-home messages were that 1) there is no one-size-fits-all solution, what works depends on the context and 2) if the underlying data is of bad quality, the results will be sub-optimal - no matter how sophisticated the model.</p>
<p><br></p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Distributed TensorFlow with Kubernetes</strong></li>
</ol>
<p>In the third talk, another colleague of mine from <a href="www.codecentric.de">codecentric</a>, Jakob Karalus, explained in detail how to set up a distributed machine learning modelling set-up with <a href="https://www.tensorflow.org/">TensorFlow</a> and <a href="https://kubernetes.io/">Kubernetes</a>. TensorFlow is used to build neural networks in a graph-based manner. Distributed and parallel machine learning can be necessary when training big neural networks with a lot of training data, very deep neural networks, with complex parameters, grid search for hyper-parameter tuning, etc. A good way to build neural networks in a controlled and stable environment is to use <a href="https://www.docker.com/">Docker</a> containers. Kubernetes is a container orchestration tool that can set up distribution of nodes from our TensorFlow modeling container. Setting up this distributed system is quite complex, though and Jakob recommended to try to stay on one CPU/GPU as long as possible.</p>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="de" dir="ltr">
Tag 2 auf der <a href="https://twitter.com/data2day?ref_src=twsrc%5Etfw"><span class="citation">@data2day</span></a> kommt am Stand vorbei, wir haben noch ein paar T-Shirts und Softwerker für euch :-) <a href="https://t.co/xyG8Leg3lF">pic.twitter.com/xyG8Leg3lF</a>
</p>
— codecentric AG (<span class="citation">@codecentric</span>) <a href="https://twitter.com/codecentric/status/913301091755941888?ref_src=twsrc%5Etfw">September 28, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet" data-lang="en">
<p lang="de" dir="ltr">
Verteiltes Deep Learning mit TensorFlow und Kubernetes - <a href="https://twitter.com/krallistic"><span class="citation">@krallistic</span></a> auf der <a href="https://twitter.com/data2day"><span class="citation">@data2day</span></a> <a href="https://t.co/5AGJdhL5U1">pic.twitter.com/5AGJdhL5U1</a>
</p>
— codecentric AG (<span class="citation">@codecentric</span>) <a href="https://twitter.com/codecentric/status/913041395128111105">September 27, 2017</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[From Biology to Industry. A Blogger’s Journey to Data Science.]]></title>
    <link href="/2017/09/from-biology-to-industry.-a-bloggers-journey-to-data-science./"/>
    <id>/2017/09/from-biology-to-industry.-a-bloggers-journey-to-data-science./</id>
    <published>2017-09-20T00:00:00+00:00</published>
    <updated>2017-09-20T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Today, I have given a webinar for the Applied Epidemiology Didactic of the University of Wisconsin - Madison titled “From Biology to Industry. A Blogger’s Journey to Data Science.”</p>
<p>I talked about how blogging about R and Data Science helped me become a Data Scientist. I also gave a short introduction to Machine Learning, Big Data and Neural Networks.</p>
<p>My slides can be found here: <a href="https://www.slideshare.net/ShirinGlander/from-biology-to-industry-a-bloggers-journey-to-data-science" class="uri">https://www.slideshare.net/ShirinGlander/from-biology-to-industry-a-bloggers-journey-to-data-science</a></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Why I use R for Data Science - An Ode to R]]></title>
    <link href="/2017/09/ode_to_r/"/>
    <id>/2017/09/ode_to_r/</id>
    <published>2017-09-19T00:00:00+00:00</published>
    <updated>2017-09-19T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>Working in Data Science, I often feel like I have to justify using R over Python. And while I do use Python for running scripts in production, I am much more comfortable with the R environment. Basically, whenever I can, I use R for prototyping, testing, visualizing and teaching. But because personal gut-feeling preference isn’t a very good reason to give to (scientifically minded) people, I’ve thought a lot about the pros and cons of using R. This is what I came up with why I still prefer R…</p>
<p><em>Disclaimer:</em> I have “grown up” with R and I’m much more familiar with it, so I admit that I am quite biased in my assessment. If you think I’m not doing other languages justice, I’ll be happy to hear your pros and cons!</p>
<p><br></p>
<ol style="list-style-type: decimal">
<li><p>First of, <a href="https://www.r-project.org/">R</a> is an <a href="https://cran.r-project.org/">open-source, cross-platform</a> language, so it’s free to use by any- and everybody. This in itself doesn’t make it special, though, because so are other languages, like Python.</p></li>
<li><p>It is an established language, so that there are lots and lots of packages for basically every type of analysis you can think of. You find packages for <a href="https://www.analyticsvidhya.com/blog/2015/08/list-r-packages-data-analysis/">data analysis</a>, <a href="http://www.kdnuggets.com/2017/02/top-r-packages-machine-learning.html">machine learning</a>, <a href="https://support.rstudio.com/hc/en-us/articles/201057987-Quick-list-of-useful-R-packages">visualization</a>, <a href="https://www.computerworld.com/article/2921176/business-intelligence/great-r-packages-for-data-import-wrangling-visualization.html">data wrangling</a>, <a href="https://cran.r-project.org/web/views/Spatial.html">spatial analysis</a>, <a href="https://www.bioconductor.org/">bioinformatics</a> and much more. But, same as with Python, this plethora of packages can sometimes make things a bit confusing: you would often need to test and compare several similar packages in order to find the best one.</p></li>
<li><p>Most of the packages are of very high quality. And when a package is on <a href="https://cran.r-project.org/web/packages/available_packages_by_name.html">CRAN</a> or <a href="https://www.bioconductor.org/">Bioconductor</a> (as most are), you can be sure that it has been checked, that you will get proper documentation and that you won’t have problems with installation, dependencies, etc. In my experience, R package and function documentation generally tends to be better than, say, of Python packages.</p></li>
<li><p>R’s graphics capabilities are superior to any other I know. Especially <a href="http://ggplot2.org/">ggplot2</a> with all its <a href="http://www.ggplot2-exts.org/">extensions</a> provides a structured, yet powerful set of tools for producing <a href="http://www.r-graph-gallery.com/portfolio/ggplot2-package/">high-quality publication-ready graphs and figures</a>. Moreover, ggplot2 is part of the <a href="https://www.tidyverse.org/">tidyverse</a> and works well with <a href="https://cran.r-project.org/web/packages/broom/vignettes/broom.html">broom</a>. This has made data wrangling and analysis much more convenient and structured and structured for me.</p></li>
<li><p>The suite of tools around <a href="https://www.rstudio.com/">R Studio</a> make it perfect for documenting data analysis workflows and for teaching. You can provide easy instructions for installation and <a href="http://rmarkdown.rstudio.com/">R Markdown</a> files for your students to follow along. Everybody is going to use the same system. In Python, you are always dealing with questions like version 2 vs version 3, Spyder vs Jupyter Notebook, pip vs conda, etc. <a href="https://www.rstudio.com/products/rpackages/">Everything around R Studio</a> is very well maintained and comes with extensive documentation and detailed tutorials. You find add-ins for <a href="https://support.rstudio.com/hc/en-us/articles/200532077-Version-Control-with-Git-and-SVN">version control</a>, <a href="http://shiny.rstudio.com/">Shiny</a> apps, writing books or other documents (<a href="https://bookdown.org/yihui/bookdown/">bookdown</a>) and you can write presentations directly in R Markdown, including code + output and everything as <a href="http://rmarkdown.rstudio.com/beamer_presentation_format.html">LaTeX beamer presentations</a>, <a href="http://rmarkdown.rstudio.com/ioslides_presentation_format.html">ioslides</a> or <a href="http://rmarkdown.rstudio.com/revealjs_presentation_format.html">reveal.js</a>. You can also create <a href="http://rmarkdown.rstudio.com/flexdashboard/">Dashboards</a>, include interactive <a href="http://rmarkdown.rstudio.com/developer_html_widgets.html">HTML widgets</a> and you can even build your blog (as this one is) with <a href="https://bookdown.org/yihui/blogdown/">blogdown</a> conveniently from within RStudio!</p></li>
<li><p>If you are looking for advanced functionality, it is very likely that somebody has already written a package for it. There are packages that allow you to access <a href="https://spark.rstudio.com/">Spark</a>, <a href="https://cran.r-project.org/web/packages/h2o/index.html">H2O</a>, <a href="https://ropensci.org/tutorials/elastic_tutorial.html">elasticsearch</a>, <a href="https://tensorflow.rstudio.com/">TensorFlow</a>, <a href="https://tensorflow.rstudio.com/keras/">Keras</a>, <a href="https://ropensci.org/blog/blog/2016/11/16/tesseract">tesseract</a>, and so many more with no hassle at all. And you can even run <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/system2.html">bash</a>, <a href="https://github.com/rstudio/reticulate">Python</a> from within R!</p></li>
<li><p>There is a big - and very active - community! This is one of the things I most enjoy about working with R. You can find many high-quality <a href="https://cran.r-project.org/manuals.html">manuals</a>, <a href="https://cran.r-project.org/other-docs.html">resources</a> and tutorials for all kinds of topics. Most of them provided free of charge by people who often dedicate their spare time to help others. The same goes for asking questions on <a href="https://stackoverflow.com/questions/tagged/r">Stack Overflow</a>, putting up issues on <a href="https://github.com/">Github</a> or <a href="https://groups.google.com/forum/#!forum/r-help-archive">Google groups</a>: usually you will get several answers within a short period of time (from my experience minutes to hours). What other community is so supportive and so helpful!? But for most things, you wouldn’t even need to ask for help because many of the packages come with absolutely amazing vignettes, that describe the functions and workflows in a detailed, yet easy to understand way. If that’s not enough, you will very likely find additional tutorials on <a href="https://www.r-bloggers.com/">R-bloggers</a>, a site maintained by Tal Galili that aggregates hundreds of R-blogs. There are several <a href="https://www.r-project.org/conferences.html">R Conferences</a>, like the <a href="https://user2018.r-project.org/">useR</a>, <a href="https://ropensci.org/community/events.html">rOpenSci Unconference</a> and many <a href="https://jumpingrivers.github.io/meetingsR/r-user-groups.html">R-user groups</a> all around the globe.</p></li>
</ol>
<p>I can’t stress enough how much I appreciate all the people who are involved in the R-community; who write packages, tutorials, blogs, who share information, provide support and who think about how to make data analysis easy, more convenient and - dare I say - fun!</p>
<div class="figure">
<img src="https://shiring.github.io/netlify_images/circle-159252_1280_mfs0ku.png" alt="Community is everything!" />
<p class="caption">Community is everything!</p>
</div>
<p>The main drawbacks I experience with R are that scripts tends to be harder to deploy than Python (<a href="https://www.microsoft.com/en-us/cloud-platform/r-server">R-server</a> might be a solution, but I don’t know enough about it to really judge). Dealing with memory, space and security issues is often difficult in R. But there has already been a vast improvement over the last months/years, so I’m sure we will see development there in the future…</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Welcome to my page!]]></title>
    <link href="/page/about/"/>
    <id>/page/about/</id>
    <published>2017-09-12T16:06:06+02:00</published>
    <updated>2017-09-12T16:06:06+02:00</updated>
    <content type="html"><![CDATA[

<p>I&rsquo;m Shirin, a biologist turned bioinformatician turned data scientist. <img src="/img/Bewerbungsfoto_klein.jpg" alt="" /></p>

<p>I&rsquo;m especially interested in machine learning and data visualization. While I am using R most every day at work, I wanted to have an incentive to regularly explore other types of analyses and other types of data that I don&rsquo;t normally work with. I have also very often benefited from other people&rsquo;s published code in that it gave me ideas for my own work; and I hope that sharing my own analyses will inspire others as much as I often am by what can be be done with data.  It&rsquo;s amazing to me what can be learned from analyzing and visualizing data!</p>

<p>My tool of choice for data analysis so far has been R. I also organize the <a href="https://shiring.github.io/r_users_group/2017/05/20/muenster_r_user_group">MünsteR R-users group on meetup.com</a>.</p>

<p><img src="https://shiring.github.io/netlify_images/my_story_wml3zm.png" alt="My journey to Data Science" /></p>

<p>I love dancing and used to do competitive ballroom and latin dancing. Even though I don&rsquo;t have time for that anymore, I still enjoy teaching &ldquo;social dances&rdquo; once a week with the Hochschulsport (university sports courses).</p>

<p>I created the R package <a href="https://github.com/ShirinG/exprAnalysis">exprAnalysis</a>, designed to streamline my RNA-seq data analysis pipeline. It is available via Github. Instructions for installation and usage can be found <a href="https://shiring.github.io/rna-seq/microarray/2016/09/28/exprAnalysis">here</a>.</p>

<p>This blog will showcase some of the analyses I have been doing with different data sets (all freely available). I will also host teaching materials for students to access in conjunction with R courses I am giving.</p>

<hr />

<h2 id="contact-me">Contact me:</h2>

<ul>
<li><a href="https://www.codecentric.de/team/shirin-glander/">Codecentric AG</a></li>
<li><a href="mailto:shirin.glander@gmail.com">Email</a></li>
<li><a href="http://www.xing.com/profile/Shirin_Glander">Xing</a></li>
<li><a href="http://de.linkedin.com/in/shirin-glander-01120881">Linkedin</a></li>
<li><a href="http://twitter.com/ShirinGlander">Twitter</a></li>
</ul>

<hr />

<p>Also check out <a href="http://www.R-bloggers.com">R-bloggers</a> for lots of cool R stuff!</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Moving my blog to blogdown]]></title>
    <link href="/2017/09/moving-my-blog-to-blogdown/"/>
    <id>/2017/09/moving-my-blog-to-blogdown/</id>
    <published>2017-09-12T00:00:00+00:00</published>
    <updated>2017-09-12T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>It’s been a long time coming but I finally moved my blog from Jekyll/Bootstrap on Github pages to blogdown, Hugo and <a href="https://www.netlify.com/">Netlify</a>! Moreover, I also now have my own domain name <a href="https://www.shirin-glander.de">www.shirin-glander.de</a>. :-)</p>
<p>I followed the <a href="https://bookdown.org/yihui/blogdown/">blogdown ebook</a> to set up my blog. I chose Thibaud Leprêtre’s <a href="https://themes.gohugo.io/hugo-tranquilpeak-theme/">tranquilpeak theme</a>. It looks much more polished than my old blog.</p>
<p>My old blog will remain where it is, so that all the links that are out there will still work (and I don’t have to go through the hassle of migrating all my posts to my new site). You find a link to my old site in the sidebar.</p>
<p><br></p>
<hr />
<p>Just to test that everything works, I run the example code:</p>
<div id="r-markdown" class="section level1">
<h1>R Markdown</h1>
<p>This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <a href="http://rmarkdown.rstudio.com" class="uri">http://rmarkdown.rstudio.com</a>.</p>
<p>You can embed an R code chunk like this:</p>
<pre class="r"><code>summary(cars)
##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00
fit &lt;- lm(dist ~ speed, data = cars)
fit
## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Coefficients:
## (Intercept)        speed  
##     -17.579        3.932</code></pre>
</div>
<div id="including-plots" class="section level1">
<h1>Including Plots</h1>
<p>You can also embed plots. See Figure <a href="#fig:pie">1</a> for example:</p>
<pre class="r"><code>par(mar = c(0, 1, 0, 1))
pie(
  c(280, 60, 20),
  c(&#39;Sky&#39;, &#39;Sunny side of pyramid&#39;, &#39;Shady side of pyramid&#39;),
  col = c(&#39;#0292D8&#39;, &#39;#F7EA39&#39;, &#39;#C4B632&#39;),
  init.angle = -50, border = NA
)</code></pre>
<div class="figure"><span id="fig:pie"></span>
<img src="/post/2017-09-12-moving-my-blog-to-blogdown_files/figure-html/pie-1.png" alt="A fancy pie chart." width="672" />
<p class="caption">
Figure 1: A fancy pie chart.
</p>
</div>
</div>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Data Science for Fraud Detection]]></title>
    <link href="/2017/09/data-science-fraud-detection/"/>
    <id>/2017/09/data-science-fraud-detection/</id>
    <published>2017-09-06T00:00:00+00:00</published>
    <updated>2017-09-06T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I have written the following post about <a href="https://blog.codecentric.de/en/2017/09/data-science-fraud-detection/">Data Science for Fraud Detection</a> at my company <a href="https://blog.codecentric.de/en/">codecentric</a>&rsquo;s blog:</p>

<blockquote>
<p>Fraud can be defined as “the crime of getting money by deceiving people” (Cambridge Dictionary); it is as old as humanity: whenever two parties exchange goods or conduct business there is the potential for one party scamming the other. With an ever increasing use of the internet for shopping, banking, filing insurance claims, etc. these businesses have become targets of fraud in a whole new dimension. Fraud has become a major problem in e-commerce and a lot of resources are being invested to recognize and prevent it.</p>

<p>Traditional approaches to identifying fraud have been rule-based. This means that hard and fast rules for flagging a transaction as fraudulent have to be established manually and in advance. But this system isn’t flexible and inevitably results in an arms-race between the seller’s fraud detection system and criminals finding ways to circumnavigate these rules. The modern alternative is to leverage the vast amounts of Big Data that can be collected from online transactions and model it in a way that allows us to flag or predict fraud in future transactions. For this, Data Science and Machine Learning techniques, like Deep Neural Networks (DNNs), are the obvious solution!</p>

<p>Here, I am going to show an example of how Data Science techniques can be used to identify fraud in financial transactions. I will offer some insights into the inner workings of fraud analysis, aimed at non-experts to understand.</p>
</blockquote>

<p>Continue reading at <a href="https://blog.codecentric.de/en/2017/09/data-science-fraud-detection/">https://blog.codecentric.de/en/2017/09/data-science-fraud-detection/</a>&hellip;</p>

<p>The blog post is <a href="https://blog.codecentric.de/2017/09/fraud-analyse-mit-data-science-techniken/">also available in German</a>.</p>

<p><img src="https://shiring.github.io/netlify_images/r_mse_gklfsi.png" alt="" /></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Migrating from GitHub to GitLab with RStudio (Tutorial)]]></title>
    <link href="/2017/09/migrating-github-gitlab/"/>
    <id>/2017/09/migrating-github-gitlab/</id>
    <published>2017-09-04T00:00:00+00:00</published>
    <updated>2017-09-04T00:00:00+00:00</updated>
    <content type="html"><![CDATA[

<h2 id="github-vs-gitlab">GitHub vs. GitLab</h2>

<p>Git is a distributed implementation of version control. Many people have written very eloquently about why it is a good idea to use version control, not only if you collaborate in a team but also if you work on your own; one example is <a href="https://support.rstudio.com/hc/en-us/articles/200532077?version=1.0.153&amp;mode=desktop">this article from RStudio&rsquo;s Support pages</a>.</p>

<p>In short, its main feature is that version control allows you to keep track of the changes you make to your code. It will also keep a history of all the changes you have made in the past and allows you to go back to specific versions if you made a major mistake. And Git makes collaborating on the same code very easy.</p>

<p>Most R packages are also hosted on <a href="https://github.com/">GitHub</a>. You can check out their R code in the repositories if you want to get a deeper understanding of the functions, you can install the latest development versions of packages or install packages that are not on CRAN. The issue tracker function of GitHub also makes it easy to report and respond to issues/problems with your code.</p>

<h3 id="why-would-you-want-to-leave-github">Why would you want to leave GitHub?</h3>

<p>Public repositories are free on GitHub but you need to pay for private repos (if you are a student or work in academia, you <a href="https://education.github.com/discount_requests/new">get private repos for free</a>). Since I switched from academia to industry lately and no longer fulfil these criteria, all my private repos would have to be switched to public in the future. Here, GitLab is a great alternative!</p>

<p><a href="https://gitlab.com/">GitLab</a> offers very similar functionalities as GitHub. There are <a href="https://www.slant.co/versus/532/4860/~github_vs_gitlab">many pros and cons for using GitHub versus GitLab</a> but for me, the selling point was that GitLab offers unlimited private projects and collaborators in its free plan.</p>

<p><br></p>

<h1 id="tutorial">Tutorial</h1>

<p>Migrating from GitHub to <a href="https://gitlab.com/">GitLab</a> with RStudio is very easy! Here, I will show how I migrated my GitHub repositories of R projects, that I work with from within RStudio, to GitLab.</p>

<p><img src="https://shiring.github.io/netlify_images/GitLab_logo_yej6ht.png" alt="" /></p>

<p>Beware, that ALL code snippets below show Terminal code (they are NOT from the R console)!</p>

<p><br></p>

<h2 id="migrating-existing-repositories">Migrating existing repositories</h2>

<p>You first need to set up your GitLab account (you can login with your GitHub account) and connect your old GitHub account. Under <a href="https://gitlab.com/profile/account">Settings &amp;Account</a>, you will find &ldquo;Social sign-in&rdquo;; here click on &ldquo;Connect&rdquo; next to the GitHub symbol (if you signed in with your GitHub account, it will already be connected).</p>

<p>Once you have done this, you can import all your GitHub repositories to GitLab. To do this, you first need to create a new project. Click on the drop-down arrow next to the plus sign in the top-right corner and select &ldquo;New project&rdquo;. This will open the following window:</p>

<p><img src="https://shiring.github.io/netlify_images/Bildschirmfoto1_yuc7gb.png" alt="" /></p>

<p>Here, choose &ldquo;Import project from GitHub&rdquo; and choose the repositories you want to import.</p>

<p>If you go into one of your repositories, GitLab will show you a message at the top of the site that tells you that you need to add an SSH key. The SSH key is used for secure communication between the GitLab server and your computer when you want to share information, like push/pull commits.</p>

<p><img src="https://shiring.github.io/netlify_images/Bildschirmfoto2_diwetw.png" alt="" /></p>

<p>If you already work with GitHub on your computer, you will have an SSH key set up and you can <a href="https://gitlab.com/profile/keys">copy your public SSH key to GitLab</a>. Follow the instructions <a href="https://gitlab.com/help/ssh/README">here</a>.</p>

<p>Here is how you do it on a Mac:</p>

<ol>
<li>Look for your public key and copy it to the clipboard</li>
</ol>

<!-- -->

<pre><code>cat ~/.ssh/id_rsa.pub
pbcopy &lt; ~/.ssh/id_rsa.pub
</code></pre>

<p>Then paste it into the respective field <a href="https://gitlab.com/profile/keys">here</a>.</p>

<p>The next step is to change the remote URL for pushing/pulling your project from RStudio. In your Git window (tab next to &ldquo;Environment&rdquo; and &ldquo;History&rdquo; for me), click on Settings and &ldquo;Shell&rdquo;.</p>

<p><img src="https://shiring.github.io/netlify_images/Bildschirmfoto3_ydklnw.png" alt="" /></p>

<p>Then write in the shell window that opened:</p>

<pre><code>git remote set-url origin git@&lt;GITLABHOST&gt;:&lt;ORGNAME&gt;/&lt;REPO&gt;.git
</code></pre>

<p>You can copy the link in the navigation bar of your repo on GitLab.</p>

<p><img src="https://shiring.github.io/netlify_images/Bildschirmfoto4_dheikm.png" alt="" /></p>

<p>Check that you now have the correct new gitlab path by going to &ldquo;Tools&rdquo;, &ldquo;Project Options&rdquo; and &ldquo;Git/SVN&rdquo;.</p>

<p>Also check your SSH key configuration with:</p>

<pre><code>ssh -T git@&lt;GITLABHOST&gt;
</code></pre>

<p>If you get the following message</p>

<pre><code>The authenticity of host 'gitlab.com (52.167.219.168)' can't be established.
ECDSA key fingerprint is ...
Are you sure you want to continue connecting (yes/no)?
</code></pre>

<p>type &ldquo;yes&rdquo; (and enter passphrase if prompted).</p>

<p>If everything is okay, you now get a message saying <code>Welcome to GitLab!</code></p>

<p>Now, you can commit, push and pull from within RStudio just as you have done before!</p>

<p><br></p>

<h2 id="in-case-of-problems-with-pushing-pulling">In case of problems with pushing/pulling</h2>

<p>In my case, I migrated both, my private as well as my company&rsquo;s GitHub repos to GitLab. While my private repos could be migrated without a hitch, migrating my company&rsquo;s repos was a bit more tricky (because they had additional security settings, I assume).</p>

<p>Here is how I solved this problem with my company&rsquo;s repos:</p>

<p>I have protected my SSH key with a passphrase. When pushing or pulling commits via the shell with <code>git pull</code> and <code>git push origin master</code>, I am prompted to enter my passphrase and everything works fine. Pushing/pulling from within RStudio, however, threw an error:</p>

<pre><code>ssh_askpass: exec(/usr/X11R6/bin/ssh-askpass): No such file or directory
Permission denied (publickey).
fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.
</code></pre>

<p>I am using a MacBook Pro with MacOS Sierra version 10.12.6, so this might not be an issue with another operating system.</p>

<p>The following solution worked for me:</p>

<ol>
<li>Add your SSH key</li>
</ol>

<!-- -->

<pre><code>ssh-add ~/.ssh/id_rsa
</code></pre>

<ol>
<li>And reinstall <a href="https://vscode-eastus.azurewebsites.net/docs/setup/mac">VS Code</a></li>
</ol>

<p>Now I could commit, push and pull from within RStudio just as before!</p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Social Network Analysis and Topic Modeling of codecentric’s Twitter friends and followers]]></title>
    <link href="/2017/07/twitter-analysis-codecentric/"/>
    <id>/2017/07/twitter-analysis-codecentric/</id>
    <published>2017-07-28T00:00:00+00:00</published>
    <updated>2017-07-28T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>I have written the following post about <a href="https://blog.codecentric.de/en/2017/07/combining-social-network-analysis-topic-modeling-characterize-codecentrics-twitter-friends-followers/">Social Network Analysis and Topic Modeling of codecentric&rsquo;s Twitter friends and followers</a> for <a href="https://blog.codecentric.de/en/">codecentric</a>&rsquo;s blog:</p>

<blockquote>
<p>Recently, Matthias Radtke has written a very nice blog post on Topic Modeling of the codecentric Blog Articles, where he is giving a comprehensive introduction to Topic Modeling. In this article I am showing a real-world example of how we can use Data Science to gain insights from text data and social network analysis.</p>

<p>I am using publicly available Twitter data to characterize codecentric&rsquo;s friends and followers for identifying the most &ldquo;influential&rdquo; followers and using text analysis tools like sentiment analysis to characterize their interests from their user descriptions, performing Social Network Analysis on friends, followers and a subset of second degree connections to identify key players who will be able to pass on information to a wide reach of other users and combing this network analysis with topic modeling to identify meta-groups with similar interests.</p>

<p>Knowing the interests and social network positions of our followers allows us to identify key users who are likely to retweet posts that fall within their range of interests and who will reach a wide audience.</p>

<p>&hellip;</p>
</blockquote>

<p>Continue reading at <a href="https://blog.codecentric.de/en/2017/07/combining-social-network-analysis-topic-modeling-characterize-codecentrics-twitter-friends-followers/">https://blog.codecentric.de/en/2017/07/combining-social-network-analysis-topic-modeling-characterize-codecentrics-twitter-friends-followers/</a>&hellip;</p>

<p>The entire analysis has been done in R 3.4.0 and you can find my code on <a href="https://github.com/ShirinG/blog_posts_prep/blob/master/twitter/twitter_codecentric.Rmd">Github</a>.</p>

<p><img src="https://shiring.github.io/netlify_images/twitter_net_topics_lnu3j9.png" alt="" /></p>
]]></content>
  </entry>
  <entry>
    <title type="html"><![CDATA[Find all my other posts on my old website!]]></title>
    <link href="/2017/07/find-all-my-other-posts-on-my-old-website/"/>
    <id>/2017/07/find-all-my-other-posts-on-my-old-website/</id>
    <published>2017-07-01T00:00:00+00:00</published>
    <updated>2017-07-01T00:00:00+00:00</updated>
    <content type="html"><![CDATA[<p>For all my other posts, see my old website:
<a href="https://shiring.github.io">shiring.github.io</a></p>
]]></content>
  </entry>
</feed>