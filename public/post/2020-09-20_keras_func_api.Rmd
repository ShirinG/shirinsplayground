---
title: "Using the Keras Functional API"
draft: true
author: Dr. Shirin Elsinghorst
date: '2020-09-20'
categories: ["R", "keras"]
tags: ["R", "keras", "image classification", "tensorflow"]
thumbnailImagePosition: left
thumbnailImage: 
metaAlignment: center
coverMeta: out
slug: keras_funct_api
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE,  echo = TRUE, message = FALSE, warning = FALSE)
```

I have been working with Keras for a while now, and I've also been writing quite a few [blogposts about it](https://shirinsplayground.netlify.app//tags/keras/); the most recent one being [an update to image classification using TF 2.0](https://shirinsplayground.netlify.app/2020/09/keras_fruits_update/).

However, in my blogposts I have always been using **Keras sequential models** and never shown how to use the [**functional API**](https://keras.rstudio.com/articles/functional_api.html). The reason is that the functional API is usually applied when building more complex models, like multi-input or multi-output models. And in my blogposts I usually show simple examples that don't take too much time and computing power to run, so that everybody can follow along.

But the [**functional API**](https://keras.rstudio.com/articles/functional_api.html) can also be used to build simple models (in fact, every model built with the sequential API can also be built with the functional API but *not vice versa*). Let me show you how it works! :-)

---

**Workshop announcement:** Because this year's UseR 2020 in Munich couldn't happen as an in-person event, I will be giving my [workshop on Deep Learning with Keras and TensorFlow](https://user2020.r-project.org/program/tutorials/) as an online event. I'll be announcing the date once it's been finalized but if you are interested in participating: we are looking at the first week of October (13:00 UTC).

---

If you have questions or would like to talk about this article (or something else data-related), you can now [book 15-minute timeslots](https://shirinsplayground.netlify.app/page/bookme/) with me (it's free - one slot available per weekday): 

<img src="https://www.appointletcdn.com/loader/buttons/F62459.png" data-appointlet-organization="shirin-elsinghorst" data-appointlet-service="357892"><script src="https://www.appointletcdn.com/loader/loader.min.js" async="" defer=""></script>

*If you have been enjoying my content and would like to help me be able to create more, please consider sending me a donation at <button>[paypal.me](https://paypal.me/ShirinGlander)</button>. Thank you!* :-)

---

**Note**:

I have been having an [issue](https://github.com/rstudio/blogdown/issues/473) with `blogdown`, where the code below runs perfectly fine in my R console from within the RMarkdown file but whenever I include any function from the `keras` package, `blogdown::serve_site()` throws an error: `Error in render_page(f) : Failed to render 'content/post/keras.Rmd'`.

Since I have no idea what's going on there (`blogdown::serve_site()` builds my site without trouble as long as I don't include `keras` functions) and haven't gotten a reply to the [issue](https://github.com/rstudio/blogdown/issues/473) I posted to Github, I had to write this blogpost without actually running the code while building the site.

I worked around that problem by not evaluating most of the code below and instead ran the code locally on my computer. Objects and images, I then saved as *.RData* or *.png* files and manually put them back into the document. By just reading this blogpost as it rendered on my site, you won't notice much of a difference (except where I copy/pasted the output as comments below the code) but in order to be fully transparent, you can see on [Github](https://github.com/ShirinG/shirinsplayground/blob/master/content/post/2020-09-13_keras_fruits_update.Rmd) what I did with hidden code chunks to create this post.

---

## Loading libraries

```{r eval=TRUE}
# load libraries
library(tidyverse)
library(tensorflow)
library(keras)
```

```{r}
tf$random$set_seed(42)

# check TF version
tf_version()
#[1] ‘2.2’

# check if keras is available
is_keras_available()
#[1] TRUE
```

## Sequential models

For many models, `keras_model_sequential` is sufficient! Whenever your model has one input (i.e. one set of images, one matrix, one set of texts, etc.), one output layer and a linear order of layers in between, you can use the `keras_model_sequential()` function. For example, it can be used to build simple [MLPs](https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_mlp.R), [CNNs](https://github.com/rstudio/keras/blob/master/vignettes/examples/cifar10_cnn.R) and [(Bidirectional) LSTMs](https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_bidirectional_lstm.R).

You can find a full working example [here](https://shirinsplayground.netlify.app/2020/09/keras_fruits_update/).

## Complex models

So, when would you use the **functional API**? You can't use the `keras_model_sequential` when you want to build a more complex model, e.g. in case you have multiple (types of) inputs (like images and text) or multiple outputs, or even complex connections between layers that can't be described in a linear fashion. Examples include an [Auxiliary Classifier Generative Adversarial Network (ACGAN)](https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_acgan.R) and [neural style transfer](https://github.com/rstudio/keras/blob/master/vignettes/examples/eager_styletransfer.R).


Some tasks, for instance, require multimodal inputs: they merge data coming from different input sources, processing each type of data using different kinds of neural layers. Imagine a deep-learning model trying to predict the most likely market price of a second-hand piece of clothing, using the following inputs: user-provided metadata (such as the item’s brand, age, and so on), a user-provided text description, and a picture of the item. If you had only the metadata available, you could one-hot encode it and use a densely connected network to predict the price. If you had only the text description available, you could use an RNN or a 1D convnet. If you had only the picture, you could use a 2D convnet. But how can you use all three at the same time? A naive approach would be to train three separate models and then do a weighted average of their predictions. But this may be suboptimal, because the information extracted by the models may be redundant. A better way is to jointly learn a more accurate model of the data by using a model that can see all available input modalities simultaneously: a model with three input branches (see figure 7.2).

Similarly, some tasks need to predict multiple target attributes of input data. Given the text of a novel or short story, you might want to automatically classify it by genre (such as romance or thriller) but also predict the approximate date it was written. Of course, you could train two separate models: one for the genre and one for the date. But because these attributes aren’t statistically independent, you could build a better model by learning to jointly predict both genre and date at the same time. Such a joint model would then have two outputs, or heads (see figure 7.3). Due to correlations between genre and date, knowing the date of a novel would help the model learn rich, accurate representations of the space of novel genres, and vice versa.

Additionally, many recently developed neural architectures require nonlinear network topology: networks structured as directed acyclic graphs. The Inception family of networks (developed by Szegedy et al. at Google),[1] for instance, relies on Inception modules, where the input is processed by several parallel convolutional branches whose outputs are then merged back into a single tensor (see figure 7.4). There’s also the recent trend of adding residual connections to a model, which started with the ResNet family of networks (developed by He et al. at Microsoft).[2] A residual connection consists of reinjecting previous representations into the downstream flow of data by adding a past output tensor to a later output tensor (see figure 7.5), which helps prevent information loss along the data-processing flow. There are many other examples of such graph-like networks.

In the functional API, you build your input and output layers and then pass them to the keras_model function. This model can be trained just like Keras sequential models.

The only part that may seem a bit magical at this point is passing only an input tensor and an output tensor to the keras_model function. Behind the scenes, Keras retrieves every layer involved in going from input_tensor to output_tensor, bringing them together into a graph-like data structure—a model. Of course, the reason it works is that output_tensor was obtained by repeatedly transforming input_tensor. If you tried to build a model from inputs and outputs that weren’t related, you’d get an error:

This error tells you, in essence, that Keras couldn’t reach input_1 from the provided output tensor.

When it comes to compiling, training, or evaluating a model built this way, the API is the same as that of sequential models:

The functional API can be used to build models that have multiple inputs. Typically, such models at some point merge their different input branches using a layer that can combine several tensors: by adding them, concatenating them, and so on. This is usually done via a Keras merge operation such as layer_add, layer_concatenate, and so on. Let’s look at a very simple example of a multi-input model: a question-answering model.

A typical question-answering model has two inputs: a natural-language question and a text snippet (such as a news article) providing information to be used for answering the question. The model must then produce an answer: in the simplest possible setup, this is a one-word answer obtained via a softmax over some predefined vocabulary (see figure 7.6).

In the same way, you can use the functional API to build models with multiple outputs (or multiple heads). A simple example is a network that attempts to simultaneously predict different properties of the data, such as a network that takes as input a series of social media posts from a single anonymous person and tries to predict attributes of that person, such as age, gender, and income level (see figure 7.7).

With the functional API, not only can you build models with multiple inputs and multiple outputs, but you can also implement networks with a complex internal topology. Neural networks in Keras are allowed to be arbitrary directed acyclic graphs of layers. The qualifier acyclic is important: these graphs can’t have cycles. It’s impossible for a tensor x to become the input of one of the layers that generated x. The only processing loops that are allowed (that is, recurrent connections) are those internal to recurrent layers.

Several common neural-network components are implemented as graphs. Two notable ones are Inception modules and residual connections. To better understand how the functional API can be used to build graphs of layers, let’s take a look at how you can implement both of them in Keras.

Inception modules
Inception[3] is a popular type of network architecture for convolutional neural networks; it was developed by Christian Szegedy and his colleagues at Google in 2013–2014, inspired by the earlier network-in-network architecture.[4] It consists of a stack of modules that themselves look like small independent networks, split into several parallel branches. The most basic form of an Inception module has three to four branches starting with a 1 × 1 convolution, followed by a 3 × 3 convolution, and ending with the concatenation of the resulting features. This setup helps the network separately learn spatial features and channel-wise features, which is more efficient than learning them jointly. More-complex versions of an Inception module are also possible, typically involving pooling operations, different spatial convolution sizes (for example, 5 × 5 instead of 3 × 3 on some branches), and branches without a spatial convolution (only a 1 × 1 convolution). An example of such a module, taken from Inception V3, is shown in figure 7.8.




---

If you have questions or would like to talk about this article (or something else data-related), you can now [book 15-minute timeslots](http://127.0.0.1:4321/page/bookme/) with me (it's free - one slot available per weekday): 

<img src="https://www.appointletcdn.com/loader/buttons/F62459.png" data-appointlet-organization="shirin-elsinghorst" data-appointlet-service="357892"><script src="https://www.appointletcdn.com/loader/loader.min.js" async="" defer=""></script>

*If you have been enjoying my content and would like to help me be able to create more, please consider sending me a donation at <button>[paypal.me](https://paypal.me/ShirinGlander)</button>. Thank you!* :-)

---

```{r eval=TRUE}
# load libraries
library(tidyverse)
library(keras)
```

```{r}
# check if keras is available
is_keras_available()
#[1] TRUE
```

## Loading images (data)

```{r echo=TRUE, eval=FALSE}
# path to image folders
train_image_files_path <- "/fruits/Training/"
```

```{r echo=FALSE}
# path to image folders
train_image_files_path <- "/Users/shiringlander/Documents/Github/Data/fruits-360/Training/"
```

```{r}
# list of fruits to modle
fruit_list <- c("Kiwi", "Banana", "Apricot", "Avocado", "Cocos", "Clementine", "Mandarine", "Orange",
                "Limes", "Lemon", "Peach", "Plum", "Raspberry", "Strawberry", "Pineapple", "Pomegranate")

# number of output classes (i.e. fruits)
output_n <- length(fruit_list)

# image size to scale down to (original images are 100 x 100 px)
img_width <- 20
img_height <- 20
target_size <- c(img_width, img_height)

# RGB = 3 channels
channels <- 3

# define batch size
batch_size <- 32
```

```{r}
train_data_gen <- image_data_generator(
  rescale = 1/255,
  validation_split = 0.3)
```

Now we load the images into memory and resize them. 

```{r}
# training images
train_image_array_gen <- flow_images_from_directory(train_image_files_path, 
                                          train_data_gen,
                                          subset = 'training',
                                          target_size = target_size,
                                          class_mode = "categorical",
                                          classes = fruit_list,
                                          batch_size = batch_size,
                                          seed = 42)
#Found 5401 images belonging to 16 classes.

# validation images
valid_image_array_gen <- flow_images_from_directory(train_image_files_path, 
                                          train_data_gen,
                                          subset = 'validation',
                                          target_size = target_size,
                                          class_mode = "categorical",
                                          classes = fruit_list,
                                          batch_size = batch_size,
                                          seed = 42)
#Found 2308 images belonging to 16 classes.
```

## Training the model

Now, I define and train the model just as before:

```{r}
# number of training samples
train_samples <- train_image_array_gen$n
# number of validation samples
valid_samples <- valid_image_array_gen$n

# define number of epochs
epochs <- 10
```

```{r}
# input layer
inputs <- layer_input(shape = c(img_width, img_height, channels))

# outputs compose input + dense layers
predictions <- inputs %>%
  layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = "same") %>%
  layer_activation("relu") %>%
  
  # Second hidden layer
  layer_conv_2d(filter = 16, kernel_size = c(3,3), padding = "same") %>%
  layer_activation_leaky_relu(0.5) %>%
  layer_batch_normalization() %>%

  # Use max pooling
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_dropout(0.25) %>%
  
  # Flatten max filtered output into feature vector 
  # and feed into dense layer
  layer_flatten() %>%
  layer_dense(100) %>%
  layer_activation("relu") %>%
  layer_dropout(0.5) %>%
  
  # Outputs from dense layer are projected onto output layer
  layer_dense(output_n) %>% 
  layer_activation("softmax")

# create and compile model
model <- keras_model(inputs = inputs, outputs = predictions)

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(lr = 0.0001, decay = 1e-6),
  metrics = "accuracy"
)
```

```{r}
# fit
hist <- model %>% fit_generator(
  # training data
  train_image_array_gen,
  
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = epochs, 
  
  # validation data
  validation_data = valid_image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size)
)
```

```{r }
plot(hist)
```

![](/img/hist.png)
---

```{r echo=FALSE}
xfun::session_info('blogdown')
```

```{r eval=TRUE}
devtools::session_info()
```


