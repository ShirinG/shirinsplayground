<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine learning on Shirin&#39;s playgRound</title>
    <link>https://shirinsplayground.netlify.app/tags/machine-learning/</link>
    <description>Recent content in machine learning on Shirin&#39;s playgRound</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Mar 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://shirinsplayground.netlify.app/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Update to Code for case study - Customer Churn with Keras/TensorFlow and H2O</title>
      <link>https://shirinsplayground.netlify.app/2021/03/update_customer_churn/</link>
      <pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2021/03/update_customer_churn/</guid>
      <description>This is an UPDATE to this old post with updated links &amp;amp; descriptions
This is code that accompanies a book chapter on customer churn that I have written for the German dpunkt Verlag. The book is in German, however.
The code you find below can be used to recreate all figures and analyses from this book chapter. Because the content is exclusively for the book, my descriptions around the code had to be minimal.</description>
    </item>
    
    <item>
      <title>How to prepare data for NLP (text classification) with Keras and TensorFlow</title>
      <link>https://shirinsplayground.netlify.app/2019/01/text_classification_keras_data_prep/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2019/01/text_classification_keras_data_prep/</guid>
      <description>In the past, I have written and taught quite a bit about image classification with Keras (e.g. here). Text classification isn’t too different in terms of using the Keras principles to train a sequential or function model. You can even use Convolutional Neural Nets (CNNs) for text classification.
What is very different, however, is how to prepare raw text data for modeling. When you look at the IMDB example from the Deep Learning with R Book, you get a great explanation of how to train the model.</description>
    </item>
    
    <item>
      <title>My course on Hyperparameter Tuning in R is now on Data Camp!</title>
      <link>https://shirinsplayground.netlify.app/2019/01/data_camp_hyperparameter_tuning/</link>
      <pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2019/01/data_camp_hyperparameter_tuning/</guid>
      <description>UPDATE: Before you take this course please consider the info about the sexual harrassment scandal surrounding DataCamp!

I am very happy to announce that (after many months) my interactive course on Hyperparameter Tuning in R has now been officially launched on Data Camp!

 Course Description  For many machine learning problems, simply running a model out-of-the-box and getting a prediction is not enough; you want the best model with the most accurate prediction.</description>
    </item>
    
    <item>
      <title>Lecture slides: Real-World Data Science (Fraud Detection, Customer Churn &amp; Predictive Maintenance)</title>
      <link>https://shirinsplayground.netlify.app/2019/01/lecture_slides_real_world_ds/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2019/01/lecture_slides_real_world_ds/</guid>
      <description>These are slides from a lecture I gave at the School of Applied Sciences in Münster. In this lecture, I talked about Real-World Data Science and showed examples on Fraud Detection, Customer Churn &amp;amp; Predictive Maintenance.

  Real-World Data Science (Fraud Detection, Customer Churn &amp;amp; Predictive Maintenance)  von Shirin Glander  The slides were created with xaringan.</description>
    </item>
    
    <item>
      <title>How do Convolutional Neural Nets (CNNs) learn? &#43; Keras example</title>
      <link>https://shirinsplayground.netlify.app/2019/01/how_cnns_learn/</link>
      <pubDate>Wed, 09 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2019/01/how_cnns_learn/</guid>
      <description>As with the other videos from our codecentric.ai Bootcamp (Random Forests, Neural Nets &amp;amp; Gradient Boosting), I am again sharing an English version of the script (plus R code) for this most recent addition on How Convolutional Neural Nets work.
In this lesson, I am going to explain how computers learn to see; meaning, how do they learn to recognize images or object on images? One of the most commonly used approaches to teach computers “vision” are Convolutional Neural Nets.</description>
    </item>
    
    <item>
      <title>Code for case study - Customer Churn with Keras/TensorFlow and H2O</title>
      <link>https://shirinsplayground.netlify.app/2018/12/customer_churn_code/</link>
      <pubDate>Wed, 12 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/12/customer_churn_code/</guid>
      <description>This is code that accompanies a book chapter on customer churn that I have written for the German dpunkt Verlag. The book is in German and will probably appear in February: https://www.dpunkt.de/buecher/13208/9783864906107-data-science.html.
The code you find below can be used to recreate all figures and analyses from this book chapter. Because the content is exclusively for the book, my descriptions around the code had to be minimal. But I’m sure, you can get the gist, even without the book.</description>
    </item>
    
    <item>
      <title>Trust in ML models. Slides from TWiML &amp; AI EMEA Meetup &#43; iX Articles</title>
      <link>https://shirinsplayground.netlify.app/2018/12/trust_in_ml_slides_ix/</link>
      <pubDate>Thu, 06 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/12/trust_in_ml_slides_ix/</guid>
      <description>Update: There is now a recording of the meetup up on YouTube.
 Here you find my slides the TWiML &amp;amp; AI EMEA Meetup about Trust in ML models, where I presented the Anchors paper by Carlos Guestrin et al..
  I have also just written two articles for the German IT magazin iX about the same topic of Explaining Black-Box Machine Learning Models:
 A short article in the iX 12/2018</description>
    </item>
    
    <item>
      <title>Machine Learning Basics - Gradient Boosting &amp; XGBoost</title>
      <link>https://shirinsplayground.netlify.app/2018/11/ml_basics_gbm/</link>
      <pubDate>Thu, 29 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/11/ml_basics_gbm/</guid>
      <description>In a recent video, I covered Random Forests and Neural Nets as part of the codecentric.ai Bootcamp.
In the most recent video, I covered Gradient Boosting and XGBoost.
You can find the video on YouTube and the slides on slides.com. Both are again in German with code examples in Python.
But below, you find the English version of the content, plus code examples in R for caret, xgboost and h2o.</description>
    </item>
    
    <item>
      <title>&#39;How do neural nets learn?&#39; A step by step explanation using the H2O Deep Learning algorithm.</title>
      <link>https://shirinsplayground.netlify.app/2018/11/neural_nets_explained/</link>
      <pubDate>Tue, 06 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/11/neural_nets_explained/</guid>
      <description>In my last blogpost about Random Forests I introduced the codecentric.ai Bootcamp. The next part I published was about Neural Networks and Deep Learning. Every video of our bootcamp will have example code and tasks to promote hands-on learning. While the practical parts of the bootcamp will be using Python, below you will find the English R version of this Neural Nets Practical Example, where I explain how neural nets learn and how the concepts and techniques translate to training neural nets in R with the H2O Deep Learning function.</description>
    </item>
    
    <item>
      <title>Machine Learning Basics - Random Forest</title>
      <link>https://shirinsplayground.netlify.app/2018/10/ml_basics_rf/</link>
      <pubDate>Tue, 30 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/10/ml_basics_rf/</guid>
      <description>A few colleagues of mine and I from codecentric.ai are currently working on developing a free online course about machine learning and deep learning. As part of this course, I am developing a series of videos about machine learning basics - the first video in this series was about Random Forests.
You can find the video on YouTube but as of now, it is only available in German. Same goes for the slides, which are also currently German only.</description>
    </item>
    
    <item>
      <title>Slides from my talk at the R-Ladies Meetup about Interpretable Deep Learning with R, Keras and LIME</title>
      <link>https://shirinsplayground.netlify.app/2018/10/rladieslondon_slides/</link>
      <pubDate>Wed, 17 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/10/rladieslondon_slides/</guid>
      <description>During my stay in London for the m3 conference, I also gave a talk at the R-Ladies London Meetup on Tuesday, October 16th, about one of my favorite topics: Interpretable Deep Learning with R, Keras and LIME.
 Keras is a high-level open-source deep learning framework that by default works on top of TensorFlow. Keras is minimalistic, efficient and highly flexible because it works with a modular layer system to define, compile and fit neural networks.</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI: Evaluating Model Explainability Methods with Sara Hooker</title>
      <link>https://shirinsplayground.netlify.app/2018/10/twimlai189/</link>
      <pubDate>Fri, 12 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/10/twimlai189/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Evaluating Model Explainability Methods with Sara Hooker:
Sketchnotes from TWiMLAI talk: Evaluating Model Explainability Methods with Sara Hooker
 You can listen to the podcast here.
  In this, the first episode of the Deep Learning Indaba series, we’re joined by Sara Hooker, AI Resident at Google Brain. I had the pleasure of speaking with Sara in the run-up to the Indaba about her work on interpretability in deep neural networks.</description>
    </item>
    
    <item>
      <title>Slides from talk: &#39;Decoding The Black Box&#39; at the Frankfurt Data Science Meetup</title>
      <link>https://shirinsplayground.netlify.app/2018/09/ffm_datascience_meetup_slides/</link>
      <pubDate>Thu, 27 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/09/ffm_datascience_meetup_slides/</guid>
      <description>On Wednesday, September 26th, I gave a talk about ‘Decoding The Black Box’ at the Frankfurt Data Science Meetup.
My slides were created with beautiful.ai and can be found here.
   DECODING THE BLACK BOX
  And finally we will have with us Dr.Shirin Glander, whom we were inviting for a long time back. Shirin lives in Münster and works as a Data Scientist at codecentric, she has lots of practical experience.</description>
    </item>
    
    <item>
      <title>I&#39;ll be talking about &#39;Decoding The Black Box&#39; at the Frankfurt Data Science Meetup</title>
      <link>https://shirinsplayground.netlify.app/2018/09/ffm_datascience_meetup/</link>
      <pubDate>Wed, 19 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/09/ffm_datascience_meetup/</guid>
      <description>I have yet another Meetup talk to announce:
On Wednesday, September 26th, I’ll be talking about ‘Decoding The Black Box’ at the Frankfurt Data Science Meetup.
Particularly cool with this meetup is that they will livestream the event at www.youtube.com/c/FrankfurtDataScience!
 TALK#2: DECODING THE BLACK BOX
  And finally we will have with us Dr.Shirin Glander, whom we were inviting for a long time back. Shirin lives in Münster and works as a Data Scientist at codecentric, she has lots of practical experience.</description>
    </item>
    
    <item>
      <title>I&#39;ll be talking at the R-Ladies Meetup about Interpretable Deep Learning with R, Keras and LIME</title>
      <link>https://shirinsplayground.netlify.app/2018/09/rladieslondontalk/</link>
      <pubDate>Mon, 17 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/09/rladieslondontalk/</guid>
      <description>Today I am very happy to announce that during my stay in London for the m3 conference, I’ll also be giving a talk at the R-Ladies London Meetup on Tuesday, October 16th, about one of my favorite topics: Interpretable Deep Learning with R, Keras and LIME.
You can register via Eventbrite: https://www.eventbrite.co.uk/e/interpretable-deep-learning-with-r-lime-and-keras-tickets-50118369392
 ABOUT THE TALK
  Keras is a high-level open-source deep learning framework that by default works on top of TensorFlow.</description>
    </item>
    
    <item>
      <title>W-JAX 2018 talk: Deep Learning - a Primer</title>
      <link>https://shirinsplayground.netlify.app/2018/08/wjax2018/</link>
      <pubDate>Wed, 29 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/08/wjax2018/</guid>
      <description>On November 7th, I’ll be in Munich for the W-JAX conference where I’ll be giving the talk that my colleague Uwe Friedrichsen and I gave at the JAX conference this April again: Deep Learning - a Primer.
Let me know if any of you here are going to be there and would like to meet up!

Slides from the original talk can be found here: https://www.slideshare.net/ShirinGlander/deep-learning-a-primer-95197733
 Deep Learning is one of the “hot” topics in the AI area – a lot of hype, a lot of inflated expectation, but also quite some impressive success stories.</description>
    </item>
    
    <item>
      <title>Slides from my SAP webinar: Explaining Keras Image Classification Models with LIME</title>
      <link>https://shirinsplayground.netlify.app/2018/08/sap_webinar_slides/</link>
      <pubDate>Tue, 21 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/08/sap_webinar_slides/</guid>
      <description>Here I am sharing the slides for a webinar I gave for SAP about Explaining Keras Image Classification Models with LIME.
Slides can be found here: https://www.slideshare.net/ShirinGlander/sap-webinar-explaining-keras-image-classification-models-with-lime
 Keras is a high-level open-source deep learning framework that by default works on top of TensorFlow. Keras is minimalistic, efficient and highly flexible because it works with a modular layer system to define, compile and fit neural networks. It has been written in Python but can also be used from within R.</description>
    </item>
    
    <item>
      <title>Explaining Black-Box Machine Learning Models - Code Part 2: Text classification with LIME</title>
      <link>https://shirinsplayground.netlify.app/2018/07/explaining_ml_models_code_text_lime/</link>
      <pubDate>Thu, 26 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/07/explaining_ml_models_code_text_lime/</guid>
      <description>This is code that will encompany an article that will appear in a special edition of a German IT magazine. The article is about explaining black-box machine learning models. In that article I’m showcasing three practical examples:

Explaining supervised classification models built on tabular data using caret and the iml package Explaining image classification models with keras and lime Explaining text classification models with xgboost and lime</description>
    </item>
    
    <item>
      <title>Explaining Black-Box Machine Learning Models - Code Part 1: tabular data &#43; caret &#43; iml</title>
      <link>https://shirinsplayground.netlify.app/2018/07/explaining_ml_models_code_caret_iml/</link>
      <pubDate>Fri, 20 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/07/explaining_ml_models_code_caret_iml/</guid>
      <description>This is code that will encompany an article that will appear in a special edition of a German IT magazine. The article is about explaining black-box machine learning models. In that article I’m showcasing three practical examples:

Explaining supervised classification models built on tabular data using caret and the iml package Explaining image classification models with keras and lime Explaining text classification models with xgboost and lime</description>
    </item>
    
    <item>
      <title>Code for Workshop: Introduction to Machine Learning with R</title>
      <link>https://shirinsplayground.netlify.app/2018/06/intro_to_ml_workshop_heidelberg/</link>
      <pubDate>Fri, 29 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/06/intro_to_ml_workshop_heidelberg/</guid>
      <description>These are the slides from my workshop: Introduction to Machine Learning with R which I gave at the University of Heidelberg, Germany on June 28th 2018. The entire code accompanying the workshop can be found below the video.
The workshop covered the basics of machine learning. With an example dataset I went through a standard machine learning workflow in R with the packages caret and h2o:
 reading in data exploratory data analysis missingness feature engineering training and test split model training with Random Forests, Gradient Boosting, Neural Nets, etc.</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI: Practical Deep Learning with Rachel Thomas</title>
      <link>https://shirinsplayground.netlify.app/2018/06/twimlai138/</link>
      <pubDate>Mon, 18 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/06/twimlai138/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Practical Deep Learning with Rachel Thomas:
Sketchnotes from TWiMLAI talk: Practical Deep Learning with Rachel Thomas
 You can listen to the podcast here.
 In this episode, i’m joined by Rachel Thomas, founder and researcher at Fast AI. If you’re not familiar with Fast AI, the company offers a series of courses including Practical Deep Learning for Coders, Cutting Edge Deep Learning for Coders and Rachel’s Computational Linear Algebra course.</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI: Adversarial Attacks Against Reinforcement Learning Agents with Ian Goodfellow &amp; Sandy Huang</title>
      <link>https://shirinsplayground.netlify.app/2018/05/twimlai_adversarial_attacks/</link>
      <pubDate>Mon, 14 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/05/twimlai_adversarial_attacks/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Adversarial Attacks Against Reinforcement Learning Agents with Ian Goodfellow &amp;amp; Sandy Huang:
Sketchnotes from TWiMLAI talk: Adversarial Attacks Against Reinforcement Learning Agents with Ian Goodfellow &amp;amp; Sandy Huang
 You can listen to the podcast here.
 In this episode, I’m joined by Ian Goodfellow, Staff Research Scientist at Google Brain and Sandy Huang, Phd Student in the EECS department at UC Berkeley, to discuss their work on the paper Adversarial Attacks on Neural Network Policies.</description>
    </item>
    
    <item>
      <title>Comparing dependencies of popular machine learning packages with `pkgnet`</title>
      <link>https://shirinsplayground.netlify.app/2018/04/pkgnet/</link>
      <pubDate>Mon, 30 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/04/pkgnet/</guid>
      <description>When looking through the CRAN list of packages, I stumbled upon this little gem:
 pkgnet is an R library designed for the analysis of R libraries! The goal of the package is to build a graph representation of a package and its dependencies.
 And I thought it would be fun to play around with it. The little analysis I ended up doing was to compare dependencies of popular machine learning packages.</description>
    </item>
    
    <item>
      <title>Slides from my JAX 2018 talk: Deep Learning - a Primer</title>
      <link>https://shirinsplayground.netlify.app/2018/04/jax2018_slides/</link>
      <pubDate>Fri, 27 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/04/jax2018_slides/</guid>
      <description>Here I am sharing the slides for a talk that my colleague Uwe Friedrichsen and I gave about Deep Learning - a Primer at the JAX conference on Tuesday, April 24th 2018 in Mainz, Germany.
Slides can be found here: https://www.slideshare.net/ShirinGlander/deep-learning-a-primer-95197733
 Deep Learning is one of the “hot” topics in the AI area – a lot of hype, a lot of inflated expectation, but also quite some impressive success stories.</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI #121: Reproducibility and the Philosophy of Data with Clare Gollnick</title>
      <link>https://shirinsplayground.netlify.app/2018/04/twimlai121/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/04/twimlai121/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Reproducibility and the Philosophy of Data with Clare Gollnick:
Sketchnotes from TWiMLAI talk #121: Reproducibility and the Philosophy of Data with Clare Gollnick
 You can listen to the podcast here.
 In this episode, i’m joined by Clare Gollnick, CTO of Terbium Labs, to discuss her thoughts on the “reproducibility crisis” currently haunting the scientific landscape.</description>
    </item>
    
    <item>
      <title>Update: Can we predict flu outcome with Machine Learning in R?</title>
      <link>https://shirinsplayground.netlify.app/2018/04/flu_prediction/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/04/flu_prediction/</guid>
      <description>Since I migrated my blog from Github Pages to blogdown and Netlify, I wanted to start migrating (most of) my old posts too - and use that opportunity to update them and make sure the code still works.
Here I am updating my very first machine learning post from 27 Nov 2016: Can we predict flu deaths with Machine Learning and R?. Changes are marked as bold comments.
The main changes I made are:</description>
    </item>
    
    <item>
      <title>HH Data Science Meetup slides: Explaining complex machine learning models with LIME</title>
      <link>https://shirinsplayground.netlify.app/2018/04/hh_datascience_meetup_2018_slides/</link>
      <pubDate>Wed, 18 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/04/hh_datascience_meetup_2018_slides/</guid>
      <description>On April 12th, 2018 I gave a talk about Explaining complex machine learning models with LIME at the Hamburg Data Science Meetup - so if you’re intersted: the slides can be found here: https://www.slideshare.net/ShirinGlander/hh-data-science-meetup-explaining-complex-machine-learning-models-with-lime-94218890
 Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria.</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI #124: Systems and Software for Machine Learning at Scale with Jeff Dean</title>
      <link>https://shirinsplayground.netlify.app/2018/04/twimlai124/</link>
      <pubDate>Wed, 18 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/04/twimlai124/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Systems and Software for Machine Learning at Scale with Jeff Dean:
Sketchnotes from TWiMLAI talk #124: Systems and Software for Machine Learning at Scale with Jeff Dean
 You can listen to the podcast here.
 In this episode I’m joined by Jeff Dean, Google Senior Fellow and head of the company’s deep learning research team Google Brain, who I had a chance to sit down with last week at the Googleplex in Mountain View.</description>
    </item>
    
    <item>
      <title>Meetup slides: Introducing Deep Learning with Keras</title>
      <link>https://shirinsplayground.netlify.app/2018/04/ruhrpy_meetup_2018_slides/</link>
      <pubDate>Wed, 11 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/04/ruhrpy_meetup_2018_slides/</guid>
      <description>On April 4th, 2018 I gave a talk about Deep Learning with Keras at the Ruhr.Py Meetup in Essen, Germany. The talk was not specific to Python, though - so if you’re intersted: the slides can be found here: https://www.slideshare.net/ShirinGlander/ruhrpy-introducing-deep-learning-with-keras-and-python
  Ruhr.PY - Introducing Deep Learning with Keras and Python  von Shirin Glander  There is also a video recording of my talk, which you can see here: https://youtu.</description>
    </item>
    
    <item>
      <title>My upcoming meetup talks about Deep Learning with Keras and explaining complex Machine Learning Models with LIME</title>
      <link>https://shirinsplayground.netlify.app/2018/03/meetup_talk_ruhrpy_april_18/</link>
      <pubDate>Wed, 28 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/03/meetup_talk_ruhrpy_april_18/</guid>
      <description>I’ll be talking about Deep Learning with Keras in R and Python at the following upcoming meetup:
 Ruhr.Py 2018 on Wednesday, April 4th   Introducing Deep Learning with Keras and Python Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. In this talk we build, train and visualize a Model using Python and Keras - all interactive with Jupyter Notebooks!</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI #115: Scaling Machine Learning at Uber with Mike Del Balso</title>
      <link>https://shirinsplayground.netlify.app/2018/03/twimlai115/</link>
      <pubDate>Wed, 07 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/03/twimlai115/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Scaling Machine Learning at Uber with Mike Del Balso:
Sketchnotes from TWiMLAI talk #115: Scaling Machine Learning at Uber with Mike Del Balso
 You can listen to the podcast here.
 In this episode, I speak with Mike Del Balso, Product Manager for Machine Learning Platforms at Uber. Mike and I sat down last fall at the Georgian Partners Portfolio conference to discuss his presentation “Finding success with machine learning in your company.</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI #111: Learning “Common Sense” and Physical Concepts with Roland Memisevic</title>
      <link>https://shirinsplayground.netlify.app/2018/02/twimlai111/</link>
      <pubDate>Mon, 19 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/02/twimlai111/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Learning “Common Sense” and Physical Concepts with Roland Memisevic:
Sketchnotes from TWiMLAI talk #111: Learning “Common Sense” and Physical Concepts with Roland Memisevic
 You can listen to the podcast here.
 In today’s episode, I’m joined by Roland Memisevic, co-founder, CEO, and chief scientist at Twenty Billion Neurons. Roland joined me at the RE•WORK Deep Learning Summit in Montreal to discuss the work his company is doing to train deep neural networks to understand physical actions.</description>
    </item>
    
    <item>
      <title>Announcing my talk about explainability of machine learning models at Minds Mastering Machines conference</title>
      <link>https://shirinsplayground.netlify.app/2018/02/m3_2018/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/02/m3_2018/</guid>
      <description>On Wednesday, April 25th 2018 I am going to talk about explainability of machine learning models at the Minds Mastering Machines conference in Cologne. The conference will be in German, though.
  ERKLÄRBARKEIT VON MACHINE LEARNING: WIE KÖNNEN WIR VERTRAUEN IN KOMPLEXE MODELLE SCHAFFEN?
  Mit Machine-Learning getroffene Entscheidungen sind inhärent schwierig – wenn nicht gar unmöglich – nachzuvollziehen. Die Komplexität einiger der besten Modelle, wie Neuronale Netzwerke, ist genau das, was sie so erfolgreich macht.</description>
    </item>
    
    <item>
      <title>I talk about machine learning with Daniel Mies (Podcast in German, though)</title>
      <link>https://shirinsplayground.netlify.app/2018/02/herr_mies_wills_wissen/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/02/herr_mies_wills_wissen/</guid>
      <description>For those of you out there who speak German:
I was interviewed for a tech podcast where I talked about machine learning, neural nets, why I love R and Rstudio and how I became a Data Scientist.
You can download and listen to the podcast here: https://mies.me/2018/01/31/hmww17-machine-learning-mit-dr-shirin-glander/
  In der aktuellen Episode gibt Dr. Shirin Glander (Twitter, Homepage) uns ein paar Einblicke in das Thema Machine Learning. Wir klären zunächst, was Machine Learning ist und welche Möglichkeiten es bietet bevor wir etwas mehr in die Tiefe gehen.</description>
    </item>
    
    <item>
      <title>JAX 2018 talk announcement: Deep Learning - a Primer</title>
      <link>https://shirinsplayground.netlify.app/2018/01/jax2018/</link>
      <pubDate>Tue, 30 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/01/jax2018/</guid>
      <description>I am happy to announce that on Tuesday, April 24th 2018 Uwe Friedrichsen and I will give a talk about Deep Learning - a Primer at the JAX conference in Mainz, Germany.
 Deep Learning is one of the “hot” topics in the AI area – a lot of hype, a lot of inflated expectation, but also quite some impressive success stories. As some AI experts already predict that Deep Learning will become “Software 2.</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI #94: Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley</title>
      <link>https://shirinsplayground.netlify.app/2018/01/twimlai94/</link>
      <pubDate>Sun, 28 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/01/twimlai94/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley:
Sketchnotes from TWiMLAI talk #94: Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley
 You can listen to the podcast here.
 Kenneth studied under TWiML Talk #47 guest Risto Miikkulainen at UT Austin, and joined Uber AI Labs after Geometric Intelligence , the company he co-founded with Gary Marcus and others, was acquired in late 2016.</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI #92: Learning State Representations with Yael Niv</title>
      <link>https://shirinsplayground.netlify.app/2018/01/twimlai92/</link>
      <pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/01/twimlai92/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Learning State Representations with Yael Niv: https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/
Sketchnotes from TWiMLAI talk #92: Learning State Representations with Yael Niv
 You can listen to the podcast here.
 In this interview Yael and I explore the relationship between neuroscience and machine learning. In particular, we discusses the importance of state representations in human learning, some of her experimental results in this area, and how a better understanding of representation learning can lead to insights into machine learning problems such as reinforcement and transfer learning.</description>
    </item>
    
    <item>
      <title>How to make your machine learning model available as an API with the plumber package</title>
      <link>https://shirinsplayground.netlify.app/2018/01/plumber/</link>
      <pubDate>Tue, 16 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/01/plumber/</guid>
      <description>The plumber package for R makes it easy to expose existing R code as a webservice via an API (https://www.rplumber.io/, Trestle Technology, LLC 2017).
You take an existing R script and make it accessible with plumber by simply adding a few lines of comments. If you have worked with Roxygen before, e.g. when building a package, you will already be familiar with the core concepts. If not, here are the most important things to know:</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI #91: Philosophy of Intelligence with Matthew Crosby</title>
      <link>https://shirinsplayground.netlify.app/2018/01/twimlai91/</link>
      <pubDate>Sun, 14 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/01/twimlai91/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Philosophy of Intelligence with Matthew Crosby: https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/
Sketchnotes from TWiMLAI talk #92: Philosophy of Intelligence with Matthew Crosby
 You can listen to the podcast here.
 This week on the podcast we’re featuring a series of conversations from the NIPs conference in Long Beach, California. I attended a bunch of talks and learned a ton, organized an impromptu roundtable on Building AI Products, and met a bunch of great people, including some former TWiML Talk guests.</description>
    </item>
    
    <item>
      <title>Looking beyond accuracy to improve trust in machine learning</title>
      <link>https://shirinsplayground.netlify.app/2018/01/looking_beyond_accuracy_to_improve_trust_in_ml/</link>
      <pubDate>Wed, 10 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/01/looking_beyond_accuracy_to_improve_trust_in_ml/</guid>
      <description>I have written another blogpost about Looking beyond accuracy to improve trust in machine learning at my company codecentric&amp;rsquo;s blog:
 Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected.</description>
    </item>
    
    <item>
      <title>TWiMLAI talk 88 sketchnotes: Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru</title>
      <link>https://shirinsplayground.netlify.app/2018/01/twimlai88_sketchnotes/</link>
      <pubDate>Wed, 10 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2018/01/twimlai88_sketchnotes/</guid>
      <description> These are my sketchnotes taken from the “This week in Machine Learning &amp;amp; AI” podcast number 88 about Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru:
Sketchnotes from TWiMLAI talk #88: Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru
 </description>
    </item>
    
    <item>
      <title>Explaining Predictions of Machine Learning Models with LIME - Münster Data Science Meetup</title>
      <link>https://shirinsplayground.netlify.app/2017/12/lime_sketchnotes/</link>
      <pubDate>Tue, 12 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2017/12/lime_sketchnotes/</guid>
      <description>Slides from Münster Data Science Meetup These are my slides from the Münster Data Science Meetup on December 12th, 2017.
knitr::include_url(&amp;quot;https://shiring.github.io/netlify_images/lime_meetup_slides_wvsh6s.pdf&amp;quot;)  
My sketchnotes were collected from these two podcasts:
 https://twimlai.com/twiml-talk-7-carlos-guestrin-explaining-predictions-machine-learning-models/ https://dataskeptic.com/blog/episodes/2016/trusting-machine-learning-models-with-lime  Sketchnotes: TWiML Talk #7 with Carlos Guestrin – Explaining the Predictions of Machine Learning Models &amp;amp; Data Skeptic Podcast - Trusting Machine Learning Models with Lime
  Example Code  the following libraries were loaded:  library(tidyverse) # for tidy data analysis library(farff) # for reading arff file library(missForest) # for imputing missing values library(dummies) # for creating dummy variables library(caret) # for modeling library(lime) # for explaining predictions Data The Chronic Kidney Disease dataset was downloaded from UC Irvine’s Machine Learning repository: http://archive.</description>
    </item>
    
    <item>
      <title>Blockchain &amp; distributed ML - my report from the data2day conference</title>
      <link>https://shirinsplayground.netlify.app/2017/09/data2day/</link>
      <pubDate>Thu, 28 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2017/09/data2day/</guid>
      <description>Yesterday and today I attended the data2day, a conference about Big Data, Machine Learning and Data Science in Heidelberg, Germany. Topics and workshops covered a range of topics surrounding (big) data analysis and Machine Learning, like Deep Learning, Reinforcement Learning, TensorFlow applications, etc. Distributed systems and scalability were a major part of a lot of the talks as well, reflecting the growing desire to build bigger and more complex models that can’t (or would take too long to) run on a single computer.</description>
    </item>
    
    <item>
      <title>Data Science for Fraud Detection</title>
      <link>https://shirinsplayground.netlify.app/2017/09/data-science-fraud-detection/</link>
      <pubDate>Wed, 06 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://shirinsplayground.netlify.app/2017/09/data-science-fraud-detection/</guid>
      <description>I have written the following post about Data Science for Fraud Detection at my company codecentric&amp;rsquo;s blog:
 Fraud can be defined as “the crime of getting money by deceiving people” (Cambridge Dictionary); it is as old as humanity: whenever two parties exchange goods or conduct business there is the potential for one party scamming the other. With an ever increasing use of the internet for shopping, banking, filing insurance claims, etc.</description>
    </item>
    
  </channel>
</rss>
